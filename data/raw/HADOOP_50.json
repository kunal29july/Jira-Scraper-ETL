[
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13524803",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13524803",
    "key": "HADOOP-18632",
    "fields": {
      "parent": {
        "id": "13569990",
        "key": "HADOOP-19092",
        "self": "https://issues.apache.org/jira/rest/api/2/issue/13569990",
        "fields": {
          "summary": "ABFS phase 4: post Hadoop 3.4.0 features",
          "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "name": "Open",
            "id": "1",
            "statusCategory": {
              "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
              "id": 2,
              "key": "new",
              "colorName": "blue-gray",
              "name": "To Do"
            }
          },
          "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
          },
          "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
          }
        }
      },
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=sreeb",
        "name": "sreeb",
        "key": "JIRAUSER294537",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "Sree Bhattacharyya",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
        "description": "The issue is open and ready for the assignee to start work on it.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
        "name": "Open",
        "id": "1",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
          "id": 2,
          "key": "new",
          "colorName": "blue-gray",
          "name": "To Do"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12328416",
          "id": "12328416",
          "name": "fs/azure",
          "description": "Azure WASB filesystem client"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=sreeb",
        "name": "sreeb",
        "key": "JIRAUSER294537",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "Sree Bhattacharyya",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=sreeb",
        "name": "sreeb",
        "key": "JIRAUSER294537",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "Sree Bhattacharyya",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18632/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
        "id": "7",
        "description": "The sub-task of the issue",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
        "name": "Sub-task",
        "subtask": true,
        "avatarId": 21146
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@37c5535b[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1626f0fe[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@24e59a26[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@5653f44[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7f3f5d6f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@7cd5f213[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@74e97305[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@585aedc3[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5d8b0782[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@e9d334a[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@315308c7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@3e68cfdb[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Sun Oct 26 00:24:25 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18632/watchers",
        "watchCount": 4,
        "isWatching": false
      },
      "created": "2023-02-15T09:21:31.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-26T00:24:25.000+0000",
      "timeoriginalestimate": null,
      "description": "In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker.\r\n\r\nFor this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success.",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12354106",
          "id": "12354106",
          "description": "Hadoop 3.5.0",
          "name": "3.5.0",
          "archived": false,
          "released": false
        }
      ],
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "ABFS: Customize and optimize timeouts made based on each separate request",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17688965",
            "id": "17688965",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft opened a new pull request, #5399:\nURL: https://github.com/apache/hadoop/pull/5399\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker.\r\n   \r\n   For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-15T09:23:22.003+0000",
            "updated": "2023-02-15T09:23:22.003+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17688968",
            "id": "17688968",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431009246\n\n   ---",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-15T09:28:42.094+0000",
            "updated": "2023-02-15T09:28:42.094+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17688973",
            "id": "17688973",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431015489\n\n   ------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   HNS-OAuth\r\n   ========================\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\r\n   [INFO] \r\n   [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Errors: \r\n   [ERROR]   ITestAzureBlobFileSystemLease.testAcquireRetry:329 \u00bb TestTimedOut test timed o...\r\n   [INFO] \r\n   [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected:<1> but was:<0>\r\n   [INFO] \r\n   [ERROR] Tests run: 336, Failures: 1, Errors: 0, Skipped: 41\r\n   \r\n   HNS-SharedKey\r\n   ========================\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\r\n   [INFO] \r\n   [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Errors: \r\n   [ERROR]   ITestAzureBlobFileSystemLease.testAcquireRetry:334 \u00bb TestTimedOut test timed o...\r\n   [INFO] \r\n   [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54\r\n   [INFO] Results:\r\n   [INFO] \r\n   [WARNING] Tests run: 336, Failures: 0, Errors: 0, Skipped: 41\r\n   \r\n   NonHNS-SharedKey\r\n   ========================\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\r\n   [INFO] \r\n   [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   ITestAzureBlobFileSystemRandomRead.testValidateSeekBounds:269->Assert.assertTrue:42->Assert.fail:89 There should not be any network I/O (elapsedTimeMs=526).\r\n   [ERROR] Errors: \r\n   [ERROR]   ITestAzureBlobFileSystemLease.testAcquireRetry:336 \u00bb TestTimedOut test timed o...\r\n   [INFO] \r\n   [ERROR] Tests run: 569, Failures: 1, Errors: 1, Skipped: 278\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   ITestAbfsTerasort.test_110_teragen:244->executeStage:211->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 teragen(1000, abfs://testcontainer@sreebcitestnonhns.dfs.core.windows.net/ITestAbfsTerasort/sortin) failed expected:<0> but was:<1>\r\n   [ERROR]   ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected:<1> but was:<0>\r\n   [ERROR] Errors: \r\n   [ERROR]   ITestAbfsJobThroughManifestCommitter.test_0420_validateJob \u00bb OutputValidation ...\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testCommitLifecycle \u00bb OutputValidation `abfs:/...\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testCommitterWithDuplicatedCommit \u00bb OutputValidation\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testConcurrentCommitTaskWithSubDir \u00bb OutputValidation\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testMapFileOutputCommitter \u00bb OutputValidation ...\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testOutputFormatIntegration \u00bb OutputValidation\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testParallelJobsToAdjacentPaths \u00bb OutputValidation\r\n   [ERROR]   ITestAbfsManifestCommitProtocol.testTwoTaskAttemptsCommit \u00bb OutputValidation `...\r\n   [INFO] \r\n   [ERROR] Tests run: 336, Failures: 2, Errors: 8, Skipped: 46\r\n   \r\n   AppendBlob-HNS-OAuth\r\n   ========================\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\r\n   [INFO] \r\n   [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Errors: \r\n   [ERROR]   ITestAzureBlobFileSystemLease.testAcquireRetry:344->lambda$testAcquireRetry$6:345 \u00bb TestTimedOut\r\n   [INFO] \r\n   [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54\r\n   [INFO] Results:\r\n   [INFO] \r\n   [ERROR] Failures: \r\n   [ERROR]   ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected:<1> but was:<0>\r\n   [INFO] \r\n   [ERROR] Tests run: 336, Failures: 1, Errors: 0, Skipped: 41\r\n   \r\n   Time taken: 39 mins 34 secs.\r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-15T09:33:39.918+0000",
            "updated": "2023-02-15T09:33:39.918+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689043",
            "id": "17689043",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431176861\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  43m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | -1 :x: |  javac  |   0m 34s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 14 new + 55 unchanged - 0 fixed = 69 total (was 55)  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 2 unchanged - 0 fixed = 12 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 37s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-asflicense.txt) |  The patch generated 3 ASF License warnings.  |\r\n   |  |   | 105m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux 379ced44d885 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c9e53a79ec634a3f9a496425cafe215625784690 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/testReport/ |\r\n   | Max. process+thread count | 563 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-15T11:10:07.693+0000",
            "updated": "2023-02-15T11:10:07.693+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689510",
            "id": "17689510",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1107015958\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n\nReview Comment:\n   Lets add it inside else block above. Reason being, if block is always having this key false.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n+        }\n+    }\n+\n+    private void updateUrl() {\n+        // updates URL with existing request timeout value\n+        URL updatedUrl = null;\n+        try {\n+            URIBuilder uriBuilder = new URIBuilder(url.toURI());\n+            uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout));\n+            updatedUrl = uriBuilder.build().toURL();\n+        } catch (URISyntaxException e) {\n+\n+        } catch (MalformedURLException e) {\n+\n+        }\n\nReview Comment:\n   should RuntimeException be thrown here?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java:\n##########\n@@ -0,0 +1,155 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+\n+public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest {\n+\n+    private boolean optimizeTimeout;\n+    private int maxRequestTimeout;\n+    private int requestTimeoutIncRate;\n+    private HashMap<AbfsRestOperationType, Integer> opMap = new HashMap<AbfsRestOperationType, Integer>();\n+\n+    public ITestAbfsCustomTimeout() throws Exception {\n+        super();\n+        initOpTypeRequestTimeout();\n+    }\n+\n+    @Test\n+    public void testOptimizer() throws IOException, IllegalAccessException {\n+\n+        AbfsConfiguration abfsConfig = getConfiguration();\n+        abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"true\");\n+        abfsConfig.set(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT, \"90\");\n+        abfsConfig.set(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE, \"2\");\n+        optimizeTimeout =true;\n+        maxRequestTimeout = 90;\n+        requestTimeoutIncRate = 2;\n+        AbfsConfiguration newConfig = new AbfsConfiguration(abfsConfig.getRawConfiguration(), getAccountName());\n+\n+        for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) {\n+            AbfsRestOperationType opType = it.getKey();\n+            int timeout = it.getValue();\n+            String config = \"\";\n+            if (opType == AbfsRestOperationType.CreateFileSystem) {\n+                config = ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+                config = ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+                config = ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+                config = ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.ListPaths) {\n+                config = ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.CreatePath) {\n+                config = ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.RenamePath) {\n+                config = ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.GetAcl) {\n+                config = ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.GetPathProperties) {\n+                config = ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.SetPathProperties) {\n+                config = ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.SetAcl) {\n+                config = ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.SetOwner) {\n+                config = ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.SetPermissions) {\n+                config = ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.CheckAccess) {\n+                config = ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT;\n+            }\n+            else if (opType == AbfsRestOperationType.GetPathStatus) {\n+                config = ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT;\n+            }\n+            abfsConfig.set(config, Integer.toString(timeout));\n+            testInitTimeoutOptimizer(opType, 3, timeout, newConfig);\n+            abfsConfig.unset(config);\n+        }\n+\n+        abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\");\n+\n+    }\n+\n+    public void testInitTimeoutOptimizer(AbfsRestOperationType opType, int maxRetryCount, int expectedReqTimeout, AbfsConfiguration abfsConfig) throws IOException {\n+\n+        AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration());\n+        AbfsClient client = fs.getAbfsStore().getClient();\n+        String query = client.createDefaultUriQueryBuilder().toString();\n+        URL url = client.createRequestUrl(\"/testPath\", query);\n+        TimeoutOptimizer opt = new TimeoutOptimizer(url, opType, client.getRetryPolicy(), getConfiguration());\n+        int retryCount = 0;\n+        while (retryCount <= maxRetryCount) {\n\nReview Comment:\n   Great test. This would be a unit-test for the functionality of TimeoutOptimizer.\r\n   Can we add a test where-in, processResponse() of abfsHttpOperation fails, and we assert how abfsRestOperation and TimeoutOptimizer is working.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n\nReview Comment:\n   lets use switch case.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n\nReview Comment:\n   Let have it as Integer and not primitive int.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n\nReview Comment:\n   readTimeout and connTimeout are in ms, requestTimeout is in second.\r\n   \r\n   so if requestTimeout is 90\r\n   it will set readTimeout as 90 which JDK understands 90 MILLISECONDS and NOT seconds.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n\nReview Comment:\n   requestTimeout == -1? What would be the implications.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n\nReview Comment:\n   Lets have a NPE check. Agree that abfsclient adds the param. But if in future, some other class wants to use this class.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n\nReview Comment:\n   timeout == null || timeout.isEmpty()\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n\nReview Comment:\n   Also, what if we have a field in AbfsRestOperationType enum containing the config name?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T04:48:24.334+0000",
            "updated": "2023-02-16T04:48:24.334+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689513",
            "id": "17689513",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1432508287\n\n   Please add the test class in https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/pom.xml#L601-L608 and https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/pom.xml#L644-L652, else it will break the runTest script runs.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T04:51:32.504+0000",
            "updated": "2023-02-16T04:51:32.504+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689516",
            "id": "17689516",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108011727\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java:\n##########\n@@ -276,14 +280,15 @@ public AbfsHttpOperation(final URL url, final String method, final List<AbfsHttp\n       }\n     }\n \n-    this.connection.setConnectTimeout(CONNECT_TIMEOUT);\n-    this.connection.setReadTimeout(READ_TIMEOUT);\n+    this.connection.setConnectTimeout(timeoutOptimizer.getConnTimeout(CONNECT_TIMEOUT));\n+    this.connection.setReadTimeout(timeoutOptimizer.getReadTimeout(READ_TIMEOUT));\n \n     this.connection.setRequestMethod(method);\n \n     for (AbfsHttpHeader header : requestHeaders) {\n       this.connection.setRequestProperty(header.getName(), header.getValue());\n     }\n+\n\nReview Comment:\n   Can we remove the extra lines, makes it difficult to backport \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:03:50.913+0000",
            "updated": "2023-02-16T05:03:50.913+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689517",
            "id": "17689517",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108012054\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java:\n##########\n@@ -555,6 +560,7 @@ public static class AbfsHttpOperationWithFixedResult extends AbfsHttpOperation {\n     public AbfsHttpOperationWithFixedResult(final URL url,\n         final String method,\n         final int httpStatus) {\n+\n\nReview Comment:\n   Remove extra line.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:04:05.939+0000",
            "updated": "2023-02-16T05:04:05.939+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689528",
            "id": "17689528",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108030121\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -117,9 +125,10 @@ String getSasToken() {\n   AbfsRestOperation(final AbfsRestOperationType operationType,\n                     final AbfsClient client,\n                     final String method,\n-                    final URL url,\n+                    URL url,\n\nReview Comment:\n   URL can be made to final in timeoutoptimizer also\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:27:38.220+0000",
            "updated": "2023-02-16T05:27:38.220+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689531",
            "id": "17689531",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108031419\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n\nReview Comment:\n   Add javadoc for the class and comments.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:30:14.173+0000",
            "updated": "2023-02-16T05:30:14.173+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689534",
            "id": "17689534",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108034619\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n\nReview Comment:\n   should we add a null check here as well ?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:37:03.885+0000",
            "updated": "2023-02-16T05:37:03.885+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689535",
            "id": "17689535",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108030121\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -117,9 +125,10 @@ String getSasToken() {\n   AbfsRestOperation(final AbfsRestOperationType operationType,\n                     final AbfsClient client,\n                     final String method,\n-                    final URL url,\n+                    URL url,\n\nReview Comment:\n   URL can be made to final in timeoutoptimizer also\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:38:13.830+0000",
            "updated": "2023-02-16T05:38:13.830+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689536",
            "id": "17689536",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108034619\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n\nReview Comment:\n   should we add a null check here as well or should we have default values for this as we are taking dependency on some config ?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n\nReview Comment:\n   Same as above.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:38:38.869+0000",
            "updated": "2023-02-16T05:38:38.869+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689537",
            "id": "17689537",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108035699\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n\nReview Comment:\n   Line break.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:39:18.916+0000",
            "updated": "2023-02-16T05:39:18.916+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689540",
            "id": "17689540",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108037262\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n\nReview Comment:\n   Since we are taking dependency on configs everywhere we should add a NP check or add default value for each config.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:42:29.083+0000",
            "updated": "2023-02-16T05:42:29.083+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689541",
            "id": "17689541",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108037646\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n+        }\n+    }\n+\n+    private void updateUrl() {\n+        // updates URL with existing request timeout value\n+        URL updatedUrl = null;\n+        try {\n+            URIBuilder uriBuilder = new URIBuilder(url.toURI());\n+            uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout));\n+            updatedUrl = uriBuilder.build().toURL();\n+        } catch (URISyntaxException e) {\n+\n+        } catch (MalformedURLException e) {\n+\n+        }\n\nReview Comment:\n   should we throw back the exception in catch block ?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T05:43:19.112+0000",
            "updated": "2023-02-16T05:43:19.112+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689547",
            "id": "17689547",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108048351\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n\nReview Comment:\n   Are you suggesting moving just the if block code to the above else block? Or including the if check and the following code in the block together in the above else block? \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T06:04:10.337+0000",
            "updated": "2023-02-16T06:04:10.337+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689548",
            "id": "17689548",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108048702\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n\nReview Comment:\n   Can try to have an enum with the AbfsRestOperationType and corresponding ConfigurationKey\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T06:05:00.360+0000",
            "updated": "2023-02-16T06:05:00.360+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689550",
            "id": "17689550",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108049244\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n+        }\n+    }\n+\n+    private void updateUrl() {\n+        // updates URL with existing request timeout value\n+        URL updatedUrl = null;\n+        try {\n+            URIBuilder uriBuilder = new URIBuilder(url.toURI());\n+            uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout));\n+            updatedUrl = uriBuilder.build().toURL();\n+        } catch (URISyntaxException e) {\n+\n+        } catch (MalformedURLException e) {\n+\n+        }\n\nReview Comment:\n   Had kept this empty because an already formed URL will be used (just modifying one query parameter), but can throw RuntimeException over here.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T06:06:00.441+0000",
            "updated": "2023-02-16T06:06:00.441+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689563",
            "id": "17689563",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108070379\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n\nReview Comment:\n   Thanks for pointing this out. Have made the necessary changes.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T06:43:12.275+0000",
            "updated": "2023-02-16T06:43:12.275+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689567",
            "id": "17689567",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108079007\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n\nReview Comment:\n   Setting request timeout (and all other timeouts) to -1 can be thought of as a flag value that is being used. Although the value for request timeout does not get checked, the other timeout values get checked (getReadTimeout and getConnTimeout calls). So to keep with the other timeouts initializations this is also set to -1. Would you suggest changing this in any way? \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T06:56:58.045+0000",
            "updated": "2023-02-16T06:56:58.045+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689573",
            "id": "17689573",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108081097\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n\nReview Comment:\n   Added the change.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T07:00:13.258+0000",
            "updated": "2023-02-16T07:00:13.258+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17689576",
            "id": "17689576",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1108083553\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n\nReview Comment:\n   Are you referring to a check on the url.getQuery() or the timeout parameter itself? \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T07:03:59.124+0000",
            "updated": "2023-02-16T07:03:59.124+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690126",
            "id": "17690126",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109277491\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n+        }\n+    }\n+\n+    private void updateUrl() {\n+        // updates URL with existing request timeout value\n+        URL updatedUrl = null;\n+        try {\n+            URIBuilder uriBuilder = new URIBuilder(url.toURI());\n+            uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout));\n+            updatedUrl = uriBuilder.build().toURL();\n+        } catch (URISyntaxException e) {\n+\n+        } catch (MalformedURLException e) {\n+\n+        }\n\nReview Comment:\n   i understand that url will always be correct on which we would do manipulation, but still better to throw RuntimeException. maybe in future, some other class wants to use this timeoutOptimizer, it can use it. This class should be agnostic to whatever flowing in.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T03:56:24.210+0000",
            "updated": "2023-02-17T03:56:24.210+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690128",
            "id": "17690128",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109278671\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n\nReview Comment:\n   what does server understands from requestTimeout == -1?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T03:58:14.352+0000",
            "updated": "2023-02-17T03:58:14.352+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690129",
            "id": "17690129",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109279454\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n\nReview Comment:\n   ```\r\n   if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\r\n                   this.shouldOptimizeTimeout = false;\r\n               }\r\n               else {\r\n                   this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\r\n                 if (this.shouldOptimizeTimeout) {\r\n                   this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(\r\n                       ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\r\n                   this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(\r\n                       ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\r\n                   initTimeouts();\r\n                   updateUrl();\r\n                 }\r\n               }\r\n   ```\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T03:59:29.401+0000",
            "updated": "2023-02-17T03:59:29.401+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690130",
            "id": "17690130",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109283269\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n\nReview Comment:\n   We understand that the request can take maximum of (connTimeout + requestTimeout) and JDK will not throw any issue? Feel that connTimeout can be kept more aggressive.\r\n   \r\n   Why is readTimeout, connTimeout a function of requestTimeout? Feel that diff types of exception should change diff parameters.\r\n   \r\n   CC: @snvijaya @anmolanmol1234 \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T04:05:49.769+0000",
            "updated": "2023-02-17T04:05:49.769+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690132",
            "id": "17690132",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109284406\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n\nReview Comment:\n   Suggestion: lets have timeoutPos as Integer and not primitive. Then:\r\n   `if(timeoutPos != null && timeoutPos < 0)`\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T04:07:49.877+0000",
            "updated": "2023-02-17T04:07:49.877+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690149",
            "id": "17690149",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109316607\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n\nReview Comment:\n   ReadTimeout can be a function of requestTimeout, but connectionTimeout is totally independent to requestTimeout. Until connection is not established, there is no significance of requestTimeout as server would not have started processing.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T05:25:10.491+0000",
            "updated": "2023-02-17T05:25:10.491+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690150",
            "id": "17690150",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109318466\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n\nReview Comment:\n   This I understand is per-call level. Should we use the feedback loop across the lifetime of abfsClient.\r\n   \r\n   ex: 10th Read call should use the intellegence it achieved in the first Read call.\r\n   \r\n   Also, since we say that we will start from aggressive value, the retries would be same across all API calls in the lifetime of AbfsClient object.\r\n   \r\n   CC: @snvijaya @anmolanmol1234 \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T05:28:46.068+0000",
            "updated": "2023-02-17T05:28:46.068+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17690154",
            "id": "17690154",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1109318466\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n\nReview Comment:\n   This I understand is per-call level. Should we use the feedback loop across the lifetime of abfsClient.\r\n   \r\n   ex: 10th Read call should use the intellegence it achieved in the first Read call.\r\n   \r\n   Also, since we say that we will start from aggressive value, the retries would be same across all API calls in the lifetime of AbfsClient object. \r\n   ex: lets take out connTimeout is aggresive say 100 ms. And network is not able to connect in that time. And through our heuristic lets say on 10th retry, connTimeout become 1 sec which is good with network.\u00a0Now all API call would have to fail nearly 10 times to work on that network\r\n   \r\n   CC: @snvijaya @anmolanmol1234 \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-17T05:38:41.869+0000",
            "updated": "2023-02-17T05:38:41.869+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17691025",
            "id": "17691025",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1111514936\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n+        }\n+    }\n+\n+    private void updateUrl() {\n+        // updates URL with existing request timeout value\n+        URL updatedUrl = null;\n+        try {\n+            URIBuilder uriBuilder = new URIBuilder(url.toURI());\n+            uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout));\n+            updatedUrl = uriBuilder.build().toURL();\n+        } catch (URISyntaxException e) {\n+\n+        } catch (MalformedURLException e) {\n+\n+        }\n\nReview Comment:\n   Have added RuntimeException for both the catch blocks in TimeoutOptimizer class.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-20T06:42:36.296+0000",
            "updated": "2023-02-20T06:42:36.296+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17691039",
            "id": "17691039",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1111563391\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n\nReview Comment:\n   Null Pointer Check is added at the end, in case any of the configs are not set. At present in case any of the configs are not set, manually setting this timeout value to the default of 90. Setting a default value for the config can also be considered.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-20T07:22:18.320+0000",
            "updated": "2023-02-20T07:22:18.320+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17691134",
            "id": "17691134",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1436762577\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  43m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 42s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | -1 :x: |  javac  |   0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55)  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15)  |\r\n   | -1 :x: |  spotbugs  |   1m  8s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-asflicense.txt) |  The patch generated 3 ASF License warnings.  |\r\n   |  |   | 105m  0s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead  At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead  At TimeoutOptimizer.java:[line 137] |\r\n   |  |  Switch statement found in org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() where default case is missing  At TimeoutOptimizer.java:where default case is missing  At TimeoutOptimizer.java:[lines 149-205] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux 5f0464b0b1b3 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 10fd6c2ac7054d7e306286dc18e1824e26c3da47 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/testReport/ |\r\n   | Max. process+thread count | 711 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-20T11:07:38.872+0000",
            "updated": "2023-02-20T11:07:38.872+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17691470",
            "id": "17691470",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1112771893\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n\nReview Comment:\n   Understood, thanks! \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-21T09:11:48.678+0000",
            "updated": "2023-02-21T09:11:48.678+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17691981",
            "id": "17691981",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1439558099\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 29s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javac  |   0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  compile  |   0m 31s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | -1 :x: |  javac  |   0m 31s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2)  |\r\n   | -1 :x: |  mvnsite  |   0m 30s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15)  |\r\n   | -1 :x: |  spotbugs  |   0m 29s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 35s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/results-asflicense.txt) |  The patch generated 3 ASF License warnings.  |\r\n   |  |   | 105m 45s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux e74be7b85008 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0761741b5e6964ee63964d308bdd596c69eecf52 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/testReport/ |\r\n   | Max. process+thread count | 662 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-22T07:38:21.931+0000",
            "updated": "2023-02-22T07:38:21.931+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17692536",
            "id": "17692536",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1115313395\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n\nReview Comment:\n   This value of the request timeout would never reach the server. This is just used as a flag value. In the case where request timeout / read timeout stays set as -1, the optimizer can be considered to not be doing any work (ie, when either the config for whether optimization should happen or not is set to false, or when the request in consideration does not have a timeout value in its query parameters, like create Filesystem, or when any of the configs are incorrectly set).\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-23T07:28:28.239+0000",
            "updated": "2023-02-23T07:28:28.239+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17692542",
            "id": "17692542",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1115320710\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetFileSystemProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.DeleteFileSystem) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ListPaths) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CreatePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.RenamePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPathProperties) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetAcl) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetOwner) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.SetPermissions) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Append) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.CheckAccess) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.GetPathStatus) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.Flush) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.ReadFile) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+        }\n+        else if (opType == AbfsRestOperationType.LeasePath) {\n+            timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+        }\n+        if (timeout == null) {\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n+        readTimeout = requestTimeout;\n+        connTimeout = requestTimeout - 1;\n+        updateUrl();\n+    }\n+\n+    private void updateTimeouts(int retryCount) {\n+        if (retryCount == 0) {\n+            return;\n+        }\n+        int maxRetryCount = retryPolicy.getRetryCount();\n+        if (retryCount <= maxRetryCount && timeoutIncRate > 0) {\n+            // retry count is still valid\n+            // timeout increment rate is a valid value\n+            if ((requestTimeout * timeoutIncRate) > maxReqTimeout) {\n+                requestTimeout = maxReqTimeout;\n+            } else {\n+                requestTimeout *= timeoutIncRate;\n+            }\n+            readTimeout = requestTimeout;\n+            connTimeout = requestTimeout - 1;\n\nReview Comment:\n   Made the necessary changes.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-23T07:39:34.356+0000",
            "updated": "2023-02-23T07:39:34.356+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17692609",
            "id": "17692609",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "saxenapranav commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1115418467\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java:\n##########\n@@ -167,6 +168,10 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n+  public TimeoutOptimizer getTimeoutOptimizer() {\n\nReview Comment:\n   seems unused. Lets remove it. In case needed in test, you may have package-protected access and have @visibleForTesting annotation.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -94,6 +95,8 @@ public URL getUrl() {\n     return url;\n   }\n \n+  public TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; }\n\nReview Comment:\n   lets have package-protected access and @visibleForTesting annotation.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n\nReview Comment:\n   initTimeouts can happen only if shouldOptimizeTimeout is true. Lets remove this if-block?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        Integer timeoutPos = new Integer(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n\nReview Comment:\n   lets say we did requestTimeout = -1 and returned, the constructor then calls updateUrl(), it will set -1.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        Integer timeoutPos = new Integer(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        switch(opType) {\n+            case CreateFileSystem:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+                break;\n+            case GetFileSystemProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+                break;\n+            case SetFileSystemProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+                break;\n+            case DeleteFileSystem:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+                break;\n+            case ListPaths:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+                break;\n+            case CreatePath:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+                break;\n+            case RenamePath:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+                break;\n+            case GetAcl:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+                break;\n+            case GetPathProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+                break;\n+            case SetPathProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+                break;\n+            case SetAcl:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+                break;\n+            case SetOwner:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+                break;\n+            case SetPermissions:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+                break;\n+            case Append:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+                break;\n+            case CheckAccess:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+                break;\n+            case GetPathStatus:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+                break;\n+            case Flush:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+                break;\n+            case ReadFile:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+                break;\n+            case LeasePath:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+                break;\n+        }\n+        if (timeout == null || timeout.isEmpty()) {\n+            // if any of the timeout values are not set\n+            // despite optimize config set to true\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n\nReview Comment:\n   can we set max value for requestTimeout. Reason being, what if config-setter, thought its in ms and give something like 10000 thinking it 10s but is actually 10000sec,\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n\nReview Comment:\n   its future works. lets resolve it.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java:\n##########\n@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static java.net.HttpURLConnection.HTTP_OK;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FILESYSTEM;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_HEAD;\n+import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RESOURCE;\n+import static org.mockito.ArgumentMatchers.nullable;\n+\n+\n+public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest {\n+    private int maxRequestTimeout;\n+    private int requestTimeoutIncRate;\n+    private HashMap<AbfsRestOperationType, Integer> opMap = new HashMap<AbfsRestOperationType, Integer>();\n+    private HashMap<AbfsRestOperationType, String> opTimeoutConfigMap = new HashMap<AbfsRestOperationType, String>();\n+\n+    public ITestAbfsCustomTimeout() throws Exception {\n+        super();\n+        initOpTypeConfigs();\n+    }\n+\n+    @Test\n+    public void testOptimizer() throws IOException, IllegalAccessException {\n+\n+        AbfsConfiguration abfsConfig = getModifiedTestConfig();\n+\n+        for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) {\n+            AbfsRestOperationType opType = it.getKey();\n+            int timeout = it.getValue();\n+            String config = opTimeoutConfigMap.get(opType);\n+            abfsConfig.set(config, Integer.toString(timeout));\n+            testInitTimeoutOptimizer(opType, 3, timeout, abfsConfig);\n+            abfsConfig.unset(config);\n+        }\n+\n+        abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\");\n+\n+    }\n+\n+    /**\n+     * Test to verify working of timeout optimization with AbfsRestOperation execute calls\n+     * Currently tests only for a single API\n+     * @throws IOException\n+     * @throws IllegalAccessException\n+     */\n+    @Test\n+    public void testOptimizationInRestCall() throws IOException, IllegalAccessException {\n+        AbfsConfiguration abfsConfig = getModifiedTestConfig();\n+        AzureBlobFileSystem newFs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration());\n+        for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) {\n+            AbfsRestOperationType opType = it.getKey();\n+            int timeout = it.getValue();\n+            String config = opTimeoutConfigMap.get(opType);\n+            abfsConfig.set(config, Integer.toString(timeout));\n+            AbfsRestOperation op = getMockAbfsRestOp(opType, newFs);\n+            final int[] finalTimeout = {timeout};\n+            Mockito.doAnswer(new Answer() {\n+                int requestCount = 4;\n\nReview Comment:\n   Lets keep it outside Mockito.doAnswer.\r\n   something like:\r\n   ```\r\n   int[] request = new int[1];\r\n   request[0]=4;\r\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        Integer timeoutPos = new Integer(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n\nReview Comment:\n   why? i feel timeout=90 is added everywhere.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java:\n##########\n@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static java.net.HttpURLConnection.HTTP_OK;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FILESYSTEM;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_HEAD;\n+import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RESOURCE;\n+import static org.mockito.ArgumentMatchers.nullable;\n+\n+\n+public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest {\n+    private int maxRequestTimeout;\n+    private int requestTimeoutIncRate;\n+    private HashMap<AbfsRestOperationType, Integer> opMap = new HashMap<AbfsRestOperationType, Integer>();\n+    private HashMap<AbfsRestOperationType, String> opTimeoutConfigMap = new HashMap<AbfsRestOperationType, String>();\n+\n+    public ITestAbfsCustomTimeout() throws Exception {\n+        super();\n+        initOpTypeConfigs();\n+    }\n+\n+    @Test\n+    public void testOptimizer() throws IOException, IllegalAccessException {\n+\n+        AbfsConfiguration abfsConfig = getModifiedTestConfig();\n+\n+        for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) {\n+            AbfsRestOperationType opType = it.getKey();\n+            int timeout = it.getValue();\n+            String config = opTimeoutConfigMap.get(opType);\n+            abfsConfig.set(config, Integer.toString(timeout));\n+            testInitTimeoutOptimizer(opType, 3, timeout, abfsConfig);\n+            abfsConfig.unset(config);\n+        }\n+\n+        abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\");\n+\n+    }\n+\n+    /**\n+     * Test to verify working of timeout optimization with AbfsRestOperation execute calls\n+     * Currently tests only for a single API\n+     * @throws IOException\n+     * @throws IllegalAccessException\n+     */\n+    @Test\n+    public void testOptimizationInRestCall() throws IOException, IllegalAccessException {\n+        AbfsConfiguration abfsConfig = getModifiedTestConfig();\n+        AzureBlobFileSystem newFs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration());\n+        for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) {\n+            AbfsRestOperationType opType = it.getKey();\n+            int timeout = it.getValue();\n+            String config = opTimeoutConfigMap.get(opType);\n+            abfsConfig.set(config, Integer.toString(timeout));\n+            AbfsRestOperation op = getMockAbfsRestOp(opType, newFs);\n+            final int[] finalTimeout = {timeout};\n+            Mockito.doAnswer(new Answer() {\n+                int requestCount = 4;\n+\n+                public Object answer(InvocationOnMock invocation) {\n+                    if (requestCount > 0) {\n+                        requestCount--;\n+                        assertEquals(finalTimeout[0], op.getTimeoutOptimizer().getRequestTimeout());\n+                        if (finalTimeout[0] * requestTimeoutIncRate > maxRequestTimeout) {\n+                            finalTimeout[0] = maxRequestTimeout;\n+                        } else {\n+                            finalTimeout[0] *= requestTimeoutIncRate;\n+                        }\n+                    }\n+                    return op.getResult();\n+                }\n+            }).when(op).createHttpOperationInstance();\n+            op.execute(getTestTracingContext(newFs, true));\n+            abfsConfig.unset(config);\n+        }\n+        abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\");\n+    }\n+\n+    private AbfsRestOperation getMockAbfsRestOp(AbfsRestOperationType opType, AzureBlobFileSystem fs) throws IOException {\n+\n+        AbfsClient spyClient = Mockito.spy(getAbfsClient(fs.getAbfsStore()));\n+\n+        // creating the parameters (Url and request headers) to initialize AbfsRestOperation\n+        AbfsUriQueryBuilder queryBuilder = spyClient.createDefaultUriQueryBuilder();\n+        URL url = spyClient.createRequestUrl(\"/\", queryBuilder.toString());\n+\n+        AbfsRestOperation spyRestOp = Mockito.spy(new AbfsRestOperation(opType, spyClient, HTTP_METHOD_HEAD, url, new ArrayList<>()));\n+\n+        AbfsHttpOperation mockHttpOp = Mockito.spy(spyRestOp.createHttpOperationInstance());\n+        Mockito.doAnswer(new Answer() {\n+            private int count = 0;\n\nReview Comment:\n   let keep count outside mockito.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-23T09:55:44.561+0000",
            "updated": "2023-02-23T09:55:44.561+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17692620",
            "id": "17692620",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1441540148\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  43m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 48s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | -1 :x: |  javac  |   0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55)  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 2 unchanged - 0 fixed = 12 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15)  |\r\n   | -1 :x: |  spotbugs  |   1m  6s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-asflicense.txt) |  The patch generated 4 ASF License warnings.  |\r\n   |  |   | 105m  5s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead  At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead  At TimeoutOptimizer.java:[line 132] |\r\n   |  |  Switch statement found in org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() where default case is missing  At TimeoutOptimizer.java:where default case is missing  At TimeoutOptimizer.java:[lines 143-199] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux 8525889b434d 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 75c6d196ae6cdac9d45872cabe034648bee7342d |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/testReport/ |\r\n   | Max. process+thread count | 566 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-23T10:43:57.974+0000",
            "updated": "2023-02-23T10:43:57.974+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693142",
            "id": "17693142",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1116842887\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        Integer timeoutPos = new Integer(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n\nReview Comment:\n   Not added in cases like createFilesystem\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T11:12:32.372+0000",
            "updated": "2023-02-24T11:12:32.372+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693150",
            "id": "17693150",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1116849374\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        Integer timeoutPos = new Integer(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n\nReview Comment:\n   updated this accordingly, removing any init to -1.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T11:18:37.995+0000",
            "updated": "2023-02-24T11:18:37.995+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693170",
            "id": "17693170",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1116879406\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java:\n##########\n@@ -167,6 +168,10 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n+  public TimeoutOptimizer getTimeoutOptimizer() {\n\nReview Comment:\n   Keeping it for any test cases that might be added in the future. Making package protected. \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T11:47:40.979+0000",
            "updated": "2023-02-24T11:47:40.979+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693174",
            "id": "17693174",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1116885652\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n\nReview Comment:\n   Updated!\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T11:54:21.742+0000",
            "updated": "2023-02-24T11:54:21.742+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693199",
            "id": "17693199",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1443690838\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 42s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javac  |   0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  compile  |   0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | -1 :x: |  javac  |   0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 12 new + 2 unchanged - 0 fixed = 14 total (was 2)  |\r\n   | -1 :x: |  mvnsite  |   0m 18s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 21s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/results-asflicense.txt) |  The patch generated 4 ASF License warnings.  |\r\n   |  |   |  89m 58s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux 56921c473dfe 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a00f099b26b7592853c26deabbfb237994c10766 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/testReport/ |\r\n   | Max. process+thread count | 597 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T13:35:31.720+0000",
            "updated": "2023-02-24T13:35:31.720+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693783",
            "id": "17693783",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "saxenapranav commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1118282237\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java:\n##########\n@@ -167,6 +169,11 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n+  @VisibleForTesting\n+  protected TimeoutOptimizer getTimeoutOptimizer() {\n\nReview Comment:\n   We have it package-protected:\r\n   ```\r\n   TimeoutOptimizer getTimeoutOptimizer()\r\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -94,6 +96,9 @@ public URL getUrl() {\n     return url;\n   }\n \n+  @VisibleForTesting\n+  protected TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; }\n\nReview Comment:\n   we have it package-protected: \r\n   ```\r\n   TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; }\r\n   ```\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-27T04:49:58.351+0000",
            "updated": "2023-02-27T04:49:58.351+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693793",
            "id": "17693793",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1118295742\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,227 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+public class TimeoutOptimizer {\n+    AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int connTimeout = -1;\n+    private int maxReqTimeout;\n+    private int timeoutIncRate;\n+    private boolean shouldOptimizeTimeout;\n+\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) {\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS));\n+            }\n+            if (this.shouldOptimizeTimeout) {\n+                this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                initTimeouts();\n+                updateUrl();\n+            }\n+\n+        } else {\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; }\n+\n+    public int getRequestTimeout() { return requestTimeout; }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    public int getConnTimeout() {\n+        return connTimeout;\n+    }\n+\n+    public int getConnTimeout(final int defaultTimeout) {\n+        if (connTimeout == -1) {\n+            return defaultTimeout;\n+        }\n+        return connTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        int timeoutPos = query.indexOf(\"timeout\");\n+        if (timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            connTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        if (opType == AbfsRestOperationType.CreateFileSystem) {\n\nReview Comment:\n   Resolving for future addition. \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-27T05:22:51.364+0000",
            "updated": "2023-02-27T05:22:51.364+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693796",
            "id": "17693796",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "sreeb-msft commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1118303110\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        if (!shouldOptimizeTimeout) {\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            return;\n+        }\n+\n+        String query = url.getQuery();\n+        Integer timeoutPos = new Integer(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            requestTimeout = -1;\n+            readTimeout = -1;\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        switch(opType) {\n+            case CreateFileSystem:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT);\n+                break;\n+            case GetFileSystemProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT);\n+                break;\n+            case SetFileSystemProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT);\n+                break;\n+            case DeleteFileSystem:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT);\n+                break;\n+            case ListPaths:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT);\n+                break;\n+            case CreatePath:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT);\n+                break;\n+            case RenamePath:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT);\n+                break;\n+            case GetAcl:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT);\n+                break;\n+            case GetPathProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+                break;\n+            case SetPathProperties:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT);\n+                break;\n+            case SetAcl:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT);\n+                break;\n+            case SetOwner:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT);\n+                break;\n+            case SetPermissions:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT);\n+                break;\n+            case Append:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT);\n+                break;\n+            case CheckAccess:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT);\n+                break;\n+            case GetPathStatus:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT);\n+                break;\n+            case Flush:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT);\n+                break;\n+            case ReadFile:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT);\n+                break;\n+            case LeasePath:\n+                timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT);\n+                break;\n+        }\n+        if (timeout == null || timeout.isEmpty()) {\n+            // if any of the timeout values are not set\n+            // despite optimize config set to true\n+            timeout = DEFAULT_TIMEOUT;\n+        }\n+        requestTimeout = Integer.parseInt(timeout);\n\nReview Comment:\n   Added a check for this.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-27T05:39:18.924+0000",
            "updated": "2023-02-27T05:39:18.924+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693819",
            "id": "17693819",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "saxenapranav commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1118348558\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -134,7 +140,7 @@ String getSasToken() {\n   AbfsRestOperation(final AbfsRestOperationType operationType,\n                     final AbfsClient client,\n                     final String method,\n-                    final URL url,\n+                    URL url,\n\nReview Comment:\n   same comment for final.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java:\n##########\n@@ -117,9 +122,10 @@ String getSasToken() {\n   AbfsRestOperation(final AbfsRestOperationType operationType,\n                     final AbfsClient client,\n                     final String method,\n-                    final URL url,\n+                    URL url,\n\nReview Comment:\n   it can be left final. Any reason to remove it?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-27T07:02:06.384+0000",
            "updated": "2023-02-27T07:02:06.384+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693820",
            "id": "17693820",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1445810167\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 39s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 43s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 18s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javac  |   0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 12 new + 2 unchanged - 0 fixed = 14 total (was 2)  |\r\n   | -1 :x: |  mvnsite  |   0m 17s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 15s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08.  |\r\n   | -1 :x: |  spotbugs  |   0m 17s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 30s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 19s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 35s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/results-asflicense.txt) |  The patch generated 4 ASF License warnings.  |\r\n   |  |   |  92m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux d974df9dd342 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6ded5e25c369a53787cbdf2950a5d751d0921842 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/testReport/ |\r\n   | Max. process+thread count | 761 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-27T07:06:12.102+0000",
            "updated": "2023-02-27T07:06:12.102+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17693971",
            "id": "17693971",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1446163972\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | -1 :x: |  javac  |   0m 34s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55)  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15)  |\r\n   | -1 :x: |  spotbugs  |   1m  9s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 11s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 36s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-asflicense.txt) |  The patch generated 4 ASF License warnings.  |\r\n   |  |   |  94m  7s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead  At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead  At TimeoutOptimizer.java:[line 126] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux c6eee2203c75 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9390100e60530e7c19139900d27b9d22d0d65b66 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/testReport/ |\r\n   | Max. process+thread count | 664 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-27T11:27:03.930+0000",
            "updated": "2023-02-27T11:27:03.930+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17695589",
            "id": "17695589",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1451568775\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  12m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  38m  4s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 39s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/results-asflicense.txt) |  The patch generated 4 ASF License warnings.  |\r\n   |  |   | 105m 26s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5399 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle |\r\n   | uname | Linux 76f35de8c855 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 08000d1a7f2db671247d677ebed9be4f0f364c07 |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-02T09:34:36.591+0000",
            "updated": "2023-03-02T09:34:36.591+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17699713",
            "id": "17699713",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#discussion_r1134213829\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java:\n##########\n@@ -0,0 +1,243 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.util.Preconditions;\n+import org.apache.http.client.utils.URIBuilder;\n+\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT;\n+\n+/**\n+ * Class handling whether timeout values should be optimized.\n+ * Timeout values optimized per request level,\n+ * based on configs in the settings.\n+ */\n+public class TimeoutOptimizer {\n+    private AbfsConfiguration abfsConfiguration;\n+    private URL url;\n+    private AbfsRestOperationType opType;\n+    private ExponentialRetryPolicy retryPolicy;\n+    private int requestTimeout;\n+    private int readTimeout = -1;\n+    private int maxReqTimeout = -1;\n+    private int timeoutIncRate = -1;\n+    private boolean shouldOptimizeTimeout;\n+\n+    /**\n+     * Constructor to initialize the parameters in class,\n+     * depending upon what is configured in the settings.\n+     * @param url request URL\n+     * @param opType operation type\n+     * @param retryPolicy retry policy set for this instance of AbfsClient\n+     * @param abfsConfiguration current configuration\n+     */\n+    public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) {\n+        this.url = url;\n+        this.opType = opType;\n+        if (opType != null) {\n+            this.retryPolicy = retryPolicy;\n+            this.abfsConfiguration = abfsConfiguration;\n+            String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS);\n+            if (shouldOptimize == null || shouldOptimize.isEmpty()) {\n+                // config is not set\n+                this.shouldOptimizeTimeout = false;\n+            }\n+            else {\n+                this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize);\n+                if (this.shouldOptimizeTimeout) {\n+                    // config is set to true\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) {\n+                        this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT));\n+                    }\n+                    if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) {\n+                        this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE));\n+                    }\n+                    if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) {\n+                        this.shouldOptimizeTimeout = false;\n+                    } else {\n+                        initTimeouts();\n+                        updateUrl();\n+                    }\n+                }\n+            }\n+        } else {\n+            // optimization not required for opType == null\n+            this.shouldOptimizeTimeout = false;\n+        }\n+    }\n+\n+    public void updateRetryTimeout(int retryCount) {\n+        if (!this.shouldOptimizeTimeout) {\n+            return;\n+        }\n+\n+        // update all timeout values\n+        updateTimeouts(retryCount);\n+        updateUrl();\n+    }\n+\n+    public URL getUrl() {\n+        return url;\n+    }\n+    public boolean getShouldOptimizeTimeout() {\n+        return this.shouldOptimizeTimeout;\n+    }\n+\n+    public int getRequestTimeout() {\n+        return requestTimeout;\n+    }\n+\n+    public int getReadTimeout() {\n+        return readTimeout;\n+    }\n+\n+    public int getReadTimeout(final int defaultTimeout) {\n+        if (readTimeout != -1 && shouldOptimizeTimeout) {\n+            return readTimeout;\n+        }\n+        return defaultTimeout;\n+    }\n+\n+    private void initTimeouts() {\n+        String query = url.getQuery();\n+        Integer timeoutPos = Integer.valueOf(query.indexOf(\"timeout\"));\n+        if (timeoutPos != null && timeoutPos < 0) {\n+            // no value of timeout exists in the URL\n+            // no optimization is needed for this particular request as well\n+            shouldOptimizeTimeout = false;\n+            return;\n+        }\n+\n+        String timeout = \"\";\n+        switch(opType) {\n+            case CreateFileSystem:\n\nReview Comment:\n   why not add a field to AbfsRestOperationType giving the string prefix for all parameters, e.g. \"createfilesystem\" which is then mapped to fs.azure.request.createfilesystem.timeout  and would allow for any new per-request options to be added.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -46,6 +46,32 @@ public final class ConfigurationKeys {\n   public static final String AZURE_BACKOFF_INTERVAL = \"fs.azure.io.retry.backoff.interval\";\n   public static final String AZURE_MAX_IO_RETRIES = \"fs.azure.io.retry.max.retries\";\n   public static final String AZURE_CUSTOM_TOKEN_FETCH_RETRY_COUNT = \"fs.azure.custom.token.fetch.retry.count\";\n+  public static final String AZURE_REQUEST_TIMEOUT_INCREASE_RATE = \"fs.azure.timeout.increase.rate\";\n+  public static final String AZURE_MAX_REQUEST_TIMEOUT = \"fs.azure.max.request.timeout\";\n+\n+  // API-specific request timeout configurations\n+  public static final String AZURE_CREATE_FS_REQUEST_TIMEOUT = \"fs.azure.createfs.request.timeout\";\n\nReview Comment:\n   prefer fs.azure.request.createfs.timeout\r\n   \r\n   why? isolates all requests under the \"fs.azure.request\" prefix and avoids mixing them with any other config option\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-13T15:58:16.829+0000",
            "updated": "2023-03-13T15:58:16.829+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17700610",
            "id": "17700610",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-1469788058\n\n   currently the max timeout is 90s, right?\r\n   is there any way to extend this for those operations we know may be extra slow (directory delete...)\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-15T10:56:33.494+0000",
            "updated": "2023-03-15T10:56:33.494+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/17802843",
            "id": "17802843",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=slfan1989",
              "name": "slfan1989",
              "key": "slfan1989",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=slfan1989&avatarId=40935",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=slfan1989&avatarId=40935",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=slfan1989&avatarId=40935",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=slfan1989&avatarId=40935"
              },
              "displayName": "Shilun Fan",
              "active": true,
              "timeZone": "America/Vancouver"
            },
            "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=slfan1989",
              "name": "slfan1989",
              "key": "slfan1989",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=slfan1989&avatarId=40935",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=slfan1989&avatarId=40935",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=slfan1989&avatarId=40935",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=slfan1989&avatarId=40935"
              },
              "displayName": "Shilun Fan",
              "active": true,
              "timeZone": "America/Vancouver"
            },
            "created": "2024-01-04T09:48:55.532+0000",
            "updated": "2024-01-04T09:48:55.532+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524803/comment/18032977",
            "id": "18032977",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] commented on PR #5399:\nURL: https://github.com/apache/hadoop/pull/5399#issuecomment-3447891002\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-26T00:24:25.097+0000",
            "updated": "2025-10-26T00:24:25.097+0000"
          }
        ],
        "maxResults": 56,
        "total": 56,
        "startAt": 0
      },
      "customfield_12311820": "0|z1fy5k:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13524576",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13524576",
    "key": "HADOOP-18629",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12351867",
          "id": "12351867",
          "description": "Incremental update of 3.3.3",
          "name": "3.3.4",
          "archived": false,
          "released": true,
          "releaseDate": "2022-08-08"
        }
      ],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": null,
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
        "description": "The issue is open and ready for the assignee to start work on it.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
        "name": "Open",
        "id": "1",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
          "id": 2,
          "key": "new",
          "colorName": "blue-gray",
          "name": "To Do"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12319644",
          "id": "12319644",
          "name": "tools/distcp",
          "description": "Distcp"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=zhuyaogai",
        "name": "zhuyaogai",
        "key": "JIRAUSER293523",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "zhuyaogai",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=zhuyaogai",
        "name": "zhuyaogai",
        "key": "JIRAUSER293523",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "zhuyaogai",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18629/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
        "id": "2",
        "description": "A new feature of the product, which has yet to be developed.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
        "name": "New Feature",
        "subtask": false,
        "avatarId": 21141
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@60a00cda[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5b0a9382[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@512e5873[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@5eb74757[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@55195ed9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@26401ada[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@58e33a1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@61e741b8[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7e1724cb[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@58a76719[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2cc05497[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@77f12a36[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Sun Oct 26 00:24:27 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18629/watchers",
        "watchCount": 2,
        "isWatching": false
      },
      "created": "2023-02-14T03:12:19.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-26T00:24:27.000+0000",
      "timeoriginalestimate": null,
      "description": "When importing large scale data to HBase, we always generate the hfiles with other Hadoop cluster, use the Distcp tool to copy the data to the HBase cluster, and bulkload data to HBase table. However, the data locality is rather low which may result in high query latency. After taking a compaction it will recover. Therefore, we can increase the data locality by specifying the favoredNodes in Distcp.\r\n\r\nCould I submit a pull request to optimize it?",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": null,
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "Hadoop DistCp supports specifying favoredNodes for data copying",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17688351",
            "id": "17688351",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai opened a new pull request, #5391:\nURL: https://github.com/apache/hadoop/pull/5391\n\n   ### Description of PR\r\n   Hadoop DistCp supports specifying favoredNodes for data copying.\r\n   \r\n   ### How was this patch tested?\r\n   Add new UT.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-14T08:00:44.590+0000",
            "updated": "2023-02-14T08:00:44.590+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17688460",
            "id": "17688460",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429555546\n\n   -1 to anything exposing internal hdfs implementation methods. Sorry\r\n   \r\n    People start using them and expect them to be stable and maintained. There is also the little detail that in cloud deployments do not always have hdfs jars on the class path; this PR would break those deployments.\r\n   \r\n   What would make sense would be to use createFile() and for hdfs to add a .opt() option for those favoured nodes, createFile() is the public api, .opt() options can be ignorred by other filesystems, *or reimplemented*. There is a lot more in terms of design and wiring up but the benefit is that portability and maintainability.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-14T11:16:12.460+0000",
            "updated": "2023-02-14T11:16:12.460+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17688483",
            "id": "17688483",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429650998\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  17m 32s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  23m 39s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |  20m 40s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 49s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 28s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  22m 41s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |  22m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  20m 43s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |  20m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   3m 37s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 9 new + 54 unchanged - 1 fixed = 63 total (was 55)  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | -1 :x: |  unit  |  27m 55s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  1s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestDistCpSync |\r\n   |   | hadoop.tools.TestDistCpWithRawXAttrs |\r\n   |   | hadoop.tools.TestDistCpSystem |\r\n   |   | hadoop.tools.TestDistCpWithAcls |\r\n   |   | hadoop.tools.TestDistCpWithXAttrs |\r\n   |   | hadoop.tools.contract.TestLocalContractDistCp |\r\n   |   | hadoop.tools.contract.TestHDFSContractDistCp |\r\n   |   | hadoop.tools.TestDistCpSyncReverseFromTarget |\r\n   |   | hadoop.tools.TestDistCpSyncReverseFromSource |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5391 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 060e891702b0 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / be67a5984771d510d7cfd96439215bccc3d5baca |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/testReport/ |\r\n   | Max. process+thread count | 566 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-14T12:13:35.375+0000",
            "updated": "2023-02-14T12:13:35.375+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17688488",
            "id": "17688488",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429680256\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 42s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  37m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  23m 10s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |  20m 29s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 47s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  22m 26s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |  22m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  20m 28s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |  20m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   3m 37s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 8 new + 55 unchanged - 1 fixed = 63 total (was 56)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   4m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | -1 :x: |  unit  |  29m 24s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 247m 24s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestDistCpSync |\r\n   |   | hadoop.tools.TestDistCpWithRawXAttrs |\r\n   |   | hadoop.tools.TestDistCpSystem |\r\n   |   | hadoop.tools.TestDistCpSyncReverseFromTarget |\r\n   |   | hadoop.tools.TestDistCpWithAcls |\r\n   |   | hadoop.tools.contract.TestHDFSContractDistCp |\r\n   |   | hadoop.tools.TestDistCpSyncReverseFromSource |\r\n   |   | hadoop.tools.contract.TestLocalContractDistCp |\r\n   |   | hadoop.tools.TestDistCpWithXAttrs |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5391 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a25803cbcfde 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 08c0cafef93ba76b6b69613bdfcc2542467ca5d3 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/testReport/ |\r\n   | Max. process+thread count | 735 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-14T12:35:44.529+0000",
            "updated": "2023-02-14T12:35:44.529+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17688631",
            "id": "17688631",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1430118718\n\n   > \r\n   @steveloughran hi, I have fixed some problems, could you please review it again? and please correct me if I'm wrong. Thank you :)\r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-14T17:29:59.661+0000",
            "updated": "2023-02-14T17:29:59.661+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17688711",
            "id": "17688711",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1430405499\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  17m 17s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  24m 53s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |  22m 10s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 49s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  22m 25s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |  22m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  20m 25s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |  20m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   4m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | -1 :x: |  unit  |  14m 46s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 59s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 240m 40s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.contract.TestLocalContractDistCp |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5391 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d770bec9be9c 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1182895b78d4d92012d09f1fa11d88abd50ef6b7 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/testReport/ |\r\n   | Max. process+thread count | 667 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-14T21:31:40.621+0000",
            "updated": "2023-02-14T21:31:40.621+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17689452",
            "id": "17689452",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1432392281\n\n   @steveloughran hi, thanks for your suggestion:) I know what you mean, but I find that it also uses  the hdfs public/stable API in source code. \r\n   https://github.com/apache/hadoop/blob/723535b788070f6b103be3bae621fefe3b753081/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java#L230\r\n   I just refer to its practice in the if branch, and if you think my code changes affect too much, can I just change the else branch code and add favoredNodes option in it? Please correct me if I'm wrong. Thank you :)\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-16T02:33:57.344+0000",
            "updated": "2023-02-16T02:33:57.344+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17693035",
            "id": "17693035",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1442901558\n\n   @steveloughran @toddlipcon @phunt \r\n   hi, could you give me some advice? Thank you!\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T07:12:07.805+0000",
            "updated": "2023-02-24T07:12:07.805+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17693036",
            "id": "17693036",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1442901997\n\n   @steveloughran @toddlipcon @phunt \r\n   hi, could you give me some advice? Thank you!\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T07:12:38.654+0000",
            "updated": "2023-02-24T07:12:38.654+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17693210",
            "id": "17693210",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1443728823\n\n   This is interesting. I know there are deployments without hdfs around (e.g. azure clusters), but do see that the import is there for snapshot updates (hdfs only) with explicit imports of SnapshotDiffReport. tracing it back, if you use \"-diff\" on the command line then hdfs *must* be on the classpath.\r\n   \r\n   your link flags up that it is already in copymapper; looking at that I don't see it in branch-3.3; it came in with HADOOP-14254. \r\n   \r\n   Given it is in there on a codepath hit with the option to copy erasure policy (and skipped if not), then again, provided the change goes in such that it is optional, your patch isn't going to force in a new run-time dependency, is it?\r\n   \r\n   let me look at the new patch some more without worrying about that detail...the builder API is public. \r\n   \r\n   Be aware that we are always very nervous about touching distcp because it is fairly old and brittle code that is used incredibly broadly -not just on the command line but actually at the Java API from applications like hive. I think this is fairly low risk but will highlight the JIRA on the HDFS mailing list so they can review it to.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T14:05:31.577+0000",
            "updated": "2023-02-24T14:05:31.577+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17693219",
            "id": "17693219",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#discussion_r1117059919\n\n\n##########\nhadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java:\n##########\n@@ -594,4 +595,49 @@ public void testUpdateRoot() throws Exception {\n     assertEquals(srcStatus.getModificationTime(),\n         destStatus2.getModificationTime());\n   }\n+\n+  @Test\n+  public void testFavoredNodes() throws Exception {\n+    final String testRoot = \"/testdir\";\n+    final String testSrc = testRoot + \"/\" + SRCDAT;\n+    final String testDst = testRoot + \"/\" + DSTDAT;\n+\n+    String nnUri = FileSystem.getDefaultUri(conf).toString();\n+    String rootStr = nnUri + testSrc;\n+    String tgtStr = nnUri + testDst;\n+\n+    FileEntry[] srcFiles = {\n+        new FileEntry(SRCDAT, true),\n+        new FileEntry(SRCDAT + \"/file10\", false)\n+    };\n+\n+    DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(URI.create(nnUri), conf);\n+    createFiles(fs, testRoot, srcFiles, -1);\n+\n+    // sad path\n+    assertNotEquals(ToolRunner.run(conf, new DistCp(),\n\nReview Comment:\n   the args are the wrong way round fro these assertions.\r\n   \r\n   switch to AssertJ assertions and add good .descriptions, so if a jenkins runs fails we know what went wrong, rather than just what line. Right\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/site/markdown/DistCp.md.vm:\n##########\n@@ -364,6 +364,7 @@ Command Line Options\n | `-direct` | Write directly to destination paths | Useful for avoiding potentially very expensive temporary file rename operations when the destination is an object store |\n | `-useiterator` | Uses single threaded listStatusIterator to build listing | Useful for saving memory at the client side. Using this option will ignore the numListstatusThreads option |\n | `-updateRoot` | Update root directory attributes (eg permissions, ownership ...) | Useful if you need to enforce root directory attributes update when using distcp |\n+| `-favoredNodes` | Specify favored nodes (Desired option input format: host1:port1,host2:port2,...) | Useful if you need to specify favored nodes when using distcp |\n\nReview Comment:\n   +. Requires the destination to be an hdfs filesystem\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java:\n##########\n@@ -32,6 +32,7 @@\n import java.util.List;\n import java.util.Random;\n \n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n\nReview Comment:\n   nit: must go in the right place in the org.apache.hadoop imports.\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n         context);\n   }\n \n+  private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException {\n+    List<InetSocketAddress> result = new ArrayList<>();\n+    for (String hostAndPort : favoredNodesStr.split(\",\")) {\n+      String[] split = hostAndPort.split(\":\");\n\nReview Comment:\n   log at debug, or maybe even at info.\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java:\n##########\n@@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args)\n       }\n     }\n \n+    if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) {\n+      String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim());\n+      if (StringUtils.isEmpty(favoredNodesStr)) {\n\nReview Comment:\n   if you pull this out to a @VisibleForTesting package scoped method then unit tests could to try to break it through invalid args, e.g\r\n   * trailing ,\r\n   * empty string\r\n    invalid node/valid node bad port.\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n         context);\n   }\n \n+  private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException {\n+    List<InetSocketAddress> result = new ArrayList<>();\n+    for (String hostAndPort : favoredNodesStr.split(\",\")) {\n+      String[] split = hostAndPort.split(\":\");\n+      if (split.length != 2) {\n+        throw new IllegalArgumentException(\"Illegal favoredNodes parameter: \" + hostAndPort);\n\nReview Comment:\n   prefer `org.apache.hadoop.util.Preconditions` here, e.g \r\n   ```\r\n   checkArgument(split.length == 2, \"\"Illegal favoredNodes parameter: %s\", hostAndPort)\r\n   ```\r\n    \r\n   \n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java:\n##########\n@@ -164,6 +164,8 @@ public final class DistCpOptions {\n \n   private final boolean updateRoot;\n \n+  private final String favoredNodes;\n+\n\nReview Comment:\n   nit: add javadocs.\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n         context);\n   }\n \n+  private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException {\n\nReview Comment:\n   javadocs to explain what happens, when exceptions are raised.\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n         context);\n   }\n \n+  private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException {\n+    List<InetSocketAddress> result = new ArrayList<>();\n+    for (String hostAndPort : favoredNodesStr.split(\",\")) {\n\nReview Comment:\n   what happens if an empty string is passed in? it should be an error, right? so add a test\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpOptions.java:\n##########\n@@ -574,4 +574,15 @@ public void testUpdateRoot() {\n         .build();\n     Assert.assertTrue(options.shouldUpdateRoot());\n   }\n+\n+  @Test\n+  public void testFavoredNodes() {\n+    final DistCpOptions options = new DistCpOptions.Builder(\n+        Collections.singletonList(\n+            new Path(\"hdfs://localhost:8020/source\")),\n+        new Path(\"hdfs://localhost:8020/target/\"))\n+        .withFavoredNodes(\"localhost:50010\")\n+        .build();\n+    Assert.assertNotNull(options.getFavoredNodes());\n\nReview Comment:\n   prefer assertJ for new tests, with a message, and ideally verification the node value went all the way through\n\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n       FSDataOutputStream out;\n       ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum);\n       if (!preserveEC || ecPolicy == null) {\n-        out = targetFS.create(targetPath, permission,\n-            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n-            repl, blockSize, context, checksumOpt);\n+        if (targetFS instanceof DistributedFileSystem\n\nReview Comment:\n   There should be one path if preserving ec or favoredNodes is set, so switch to hdfsbuilder, leaving the other path as create()\r\n   \r\n   \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-02-24T14:32:55.075+0000",
            "updated": "2023-02-24T14:32:55.075+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696596",
            "id": "17696596",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1455156173\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  15m 28s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  25m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  23m 12s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |  20m 37s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 50s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 29s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  22m 29s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javac  |  22m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  20m 27s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  javac  |  20m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  14m  7s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  3s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 222m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5391 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e50831db0918 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 662045ad8a6c5d2c5dff9907a0ac7e6842844153 |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/4/testReport/ |\r\n   | Max. process+thread count | 686 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-05T17:45:32.092+0000",
            "updated": "2023-03-05T17:45:32.092+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696659",
            "id": "17696659",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on code in PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#discussion_r1125831678\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n         context);\n   }\n \n+  private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException {\n+    List<InetSocketAddress> result = new ArrayList<>();\n+    for (String hostAndPort : favoredNodesStr.split(\",\")) {\n\nReview Comment:\n   I have checked if `favoredNodesStr ` is an empty string above, and will skip `favoredNodesStr` if empty.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T03:01:01.395+0000",
            "updated": "2023-03-06T03:01:01.395+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696661",
            "id": "17696661",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on code in PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#discussion_r1125837868\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java:\n##########\n@@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args)\n       }\n     }\n \n+    if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) {\n+      String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim());\n+      if (StringUtils.isEmpty(favoredNodesStr)) {\n\nReview Comment:\n   Add new UT in `TestOptionsParser`, and I think it's ok that YARN cluster can resolve favored nodes. BTW, it seems that DN does not care about `port` but `hostname`?\r\n   \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T03:11:42.135+0000",
            "updated": "2023-03-06T03:11:42.135+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696662",
            "id": "17696662",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on code in PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#discussion_r1125837868\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java:\n##########\n@@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args)\n       }\n     }\n \n+    if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) {\n+      String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim());\n+      if (StringUtils.isEmpty(favoredNodesStr)) {\n\nReview Comment:\n   Add new UT in `TestOptionsParser`, and I think it's ok that YARN cluster can resolve favored nodes. BTW, it seems that DN does not care about `port` but `hostname`?\r\n   https://github.com/apache/hadoop/blob/2a0dc2ab2f5fb46dc540ed440d6c8b2896dd195b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java#L703\r\n   \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T03:13:27.540+0000",
            "updated": "2023-03-06T03:13:27.540+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696669",
            "id": "17696669",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on code in PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#discussion_r1125843372\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n       FSDataOutputStream out;\n       ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum);\n       if (!preserveEC || ecPolicy == null) {\n-        out = targetFS.create(targetPath, permission,\n-            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n-            repl, blockSize, context, checksumOpt);\n+        if (targetFS instanceof DistributedFileSystem\n\nReview Comment:\n   If I only change else branch, some UT will be fail when I have set `favoredNodes`, because in my UT it just goes into the if branch (ec settings if not set). Is there a better solution?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T03:20:02.565+0000",
            "updated": "2023-03-06T03:20:02.565+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696670",
            "id": "17696670",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "zhuyaogai commented on code in PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#discussion_r1125847429\n\n\n##########\nhadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java:\n##########\n@@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS,\n       FSDataOutputStream out;\n       ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum);\n       if (!preserveEC || ecPolicy == null) {\n-        out = targetFS.create(targetPath, permission,\n-            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize,\n-            repl, blockSize, context, checksumOpt);\n+        if (targetFS instanceof DistributedFileSystem\n\nReview Comment:\n   Thank you for your code review and so many valuable comments\uff01\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T03:25:12.628+0000",
            "updated": "2023-03-06T03:25:12.628+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696707",
            "id": "17696707",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1455526533\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  17m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  15m 36s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  25m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  23m  0s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |  20m 26s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 49s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 28s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 15s | [/patch-mvninstall-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-mvninstall-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch failed.  |\r\n   | -1 :x: |  compile  |  12m 30s | [/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) |  root in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.  |\r\n   | -1 :x: |  javac  |  12m 30s | [/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) |  root in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.  |\r\n   | -1 :x: |  compile  |  11m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.  |\r\n   | -1 :x: |  javac  |  11m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   3m 18s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 63 unchanged - 0 fixed = 64 total (was 63)  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-mvnsite-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 21s | [/patch-javadoc-hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) |  hadoop-distcp in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.  |\r\n   | -1 :x: |  javadoc  |   0m 21s | [/patch-javadoc-hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) |  hadoop-distcp in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.  |\r\n   | -1 :x: |  spotbugs  |   0m 23s | [/patch-spotbugs-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-spotbugs-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 33s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 21s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 198m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5391 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6b1c4e22c346 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b20d228c6939cded9fb2ca92ef667feb84b5397d |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/testReport/ |\r\n   | Max. process+thread count | 648 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T06:23:12.917+0000",
            "updated": "2023-03-06T06:23:12.917+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/17696924",
            "id": "17696924",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-1456058542\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  15m 13s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  25m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  22m 59s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |  20m 26s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 48s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 28s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  22m 24s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javac  |  22m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  20m 33s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  javac  |  20m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   3m 38s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 62 unchanged - 0 fixed = 63 total (was 62)  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   1m 38s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-hdfs-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  13m 56s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 221m 26s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5391 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux fecad139efb7 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c906c78e3e7aecd12a818fcb76d281f713387323 |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/testReport/ |\r\n   | Max. process+thread count | 731 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-06T12:37:26.267+0000",
            "updated": "2023-03-06T12:37:26.267+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13524576/comment/18032979",
            "id": "18032979",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] commented on PR #5391:\nURL: https://github.com/apache/hadoop/pull/5391#issuecomment-3447891027\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-26T00:24:27.476+0000",
            "updated": "2025-10-26T00:24:27.476+0000"
          }
        ],
        "maxResults": 20,
        "total": 20,
        "startAt": 0
      },
      "customfield_12311820": "0|z1fwrc:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13521209",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13521209",
    "key": "HADOOP-18603",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
        "name": "BilwaST",
        "key": "bilwast",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Bilwa S T",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
        "description": "The issue is open and ready for the assignee to start work on it.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
        "name": "Open",
        "id": "1",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
          "id": 2,
          "key": "new",
          "colorName": "blue-gray",
          "name": "To Do"
        }
      },
      "components": [],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
        "name": "BilwaST",
        "key": "bilwast",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Bilwa S T",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
        "name": "BilwaST",
        "key": "bilwast",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Bilwa S T",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18603/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
        "id": "1",
        "description": "A problem which impairs or prevents the functions of the product.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
        "name": "Bug",
        "subtask": false,
        "avatarId": 21133
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@66a0944a[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3874df02[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6dc3e790[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@7351b9e5[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@62f73083[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@74456857[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@648fe28[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@78a8e195[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2380a52b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@7cd866dc[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3da07ed1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@2bb94b9[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Thu Oct 23 00:22:35 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18603/watchers",
        "watchCount": 2,
        "isWatching": false
      },
      "created": "2023-01-25T06:35:20.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-23T00:22:35.000+0000",
      "timeoriginalestimate": null,
      "description": "Steps to reproduce this issue:\r\n\r\nEnable ldap auth with tls by configuring these in core-site.xml\r\n1. hadoop.http.authentication.multi-scheme-auth-handler.schemes = basic\r\n2. hadoop.http.authentication.multi-scheme-auth-handler.schemes.basic.handler = ldap\r\n3. hadoop.http.authentication.ldap.enablestarttls = true\r\n\r\n\u00a0\r\n\r\nTrace:\r\n{noformat}\r\njava.lang.NullPointerException\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateWithTlsExtension(LdapAuthenticationHandler.java:261)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateUser(LdapAuthenticationHandler.java:238)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticate(LdapAuthenticationHandler.java:202)\r\n{noformat}",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": null,
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "NPE in LdapAuthenticationHandler as disableHostNameVerification is never initialized",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17680466",
            "id": "17680466",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
              "name": "BilwaST",
              "key": "bilwast",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Bilwa S T",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "cc [~hgadre]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
              "name": "BilwaST",
              "key": "bilwast",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Bilwa S T",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-25T06:38:29.976+0000",
            "updated": "2023-01-25T06:38:29.976+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17715271",
            "id": "17715271",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "BilwaST opened a new pull request, #5581:\nURL: https://github.com/apache/hadoop/pull/5581\n\n   \u2026ication is never initialized.\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-04-22T11:04:35.480+0000",
            "updated": "2023-04-22T11:04:35.480+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17715293",
            "id": "17715293",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#issuecomment-1518671506\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  25m 14s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |  21m 43s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  24m 37s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javac  |  24m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  21m 45s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  javac  |  21m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 22s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 197m 58s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5581/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5581 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 76cd0460e684 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bb7c1ea3c6dbcb1b48102a87b22fd3fd8d59692a |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5581/1/testReport/ |\r\n   | Max. process+thread count | 532 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-auth U: hadoop-common-project/hadoop-auth |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5581/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-04-22T14:23:53.610+0000",
            "updated": "2023-04-22T14:23:53.610+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17715586",
            "id": "17715586",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
              "name": "BilwaST",
              "key": "bilwast",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Bilwa S T",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "cc [~brahmareddy]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
              "name": "BilwaST",
              "key": "bilwast",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Bilwa S T",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-04-24T05:47:29.902+0000",
            "updated": "2023-04-24T05:47:29.902+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17716935",
            "id": "17716935",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "simbadzina commented on PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#issuecomment-1524300983\n\n   Could you please add a unit test that fails without your change and passes with the change.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-04-27T00:37:03.148+0000",
            "updated": "2023-04-27T00:37:03.148+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17797285",
            "id": "17797285",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "BilwaST commented on PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#issuecomment-1858380654\n\n   Thank you for the review @ayushtkn @simbadzina. I have added testcase for this.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-12-15T19:24:35.793+0000",
            "updated": "2023-12-15T19:24:35.793+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17797328",
            "id": "17797328",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#issuecomment-1858569516\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  43m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 26s |  |  trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m 55s |  |  trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 27s |  |  the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 50s |  |  the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |  14m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 24s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 59s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 189m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5581/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5581 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6dda81a4f9b7 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 15a4a26fdd8b65bfe7c783f9f756f852ea05e4f9 |\r\n   | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5581/2/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-auth U: hadoop-common-project/hadoop-auth |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5581/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-12-15T22:33:41.837+0000",
            "updated": "2023-12-15T22:33:41.837+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17850611",
            "id": "17850611",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
              "name": "BilwaST",
              "key": "bilwast",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Bilwa S T",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "can we merge this [~ayushsaxena] [~simbadzina] ?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=BilwaST",
              "name": "BilwaST",
              "key": "bilwast",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Bilwa S T",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2024-05-30T07:16:45.010+0000",
            "updated": "2024-05-30T07:16:45.010+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17851133",
            "id": "17851133",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "simbadzina commented on code in PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#discussion_r1622586601\n\n\n##########\nhadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/LdapAuthenticationHandler.java:\n##########\n@@ -100,6 +100,12 @@ public class LdapAuthenticationHandler implements AuthenticationHandler {\n    */\n   public static final String ENABLE_START_TLS = TYPE + \".enablestarttls\";\n \n+  /**\n+   * Constant for disabling the host name verification for this handler.\n+   */\n+  private static final String DISABLE_HOSTNAME_VERIFICATION = TYPE +\n+          \".hostname.verification.disable\";\n\nReview Comment:\n   Could you please also add this new config in core-site default.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2024-05-31T15:23:46.030+0000",
            "updated": "2024-05-31T15:23:46.030+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/17851161",
            "id": "17851161",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "BilwaST commented on PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#issuecomment-2142633137\n\n   @simbadzina None of the configs added in this file are added to core-default.xml. I'm thinking we can plan to add it as part of new jira. Would it be ok? Thanks\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2024-05-31T16:46:47.628+0000",
            "updated": "2024-05-31T16:46:47.628+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/18031942",
            "id": "18031942",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] commented on PR #5581:\nURL: https://github.com/apache/hadoop/pull/5581#issuecomment-3430009446\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-22T00:23:33.405+0000",
            "updated": "2025-10-22T00:23:33.405+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13521209/comment/18032304",
            "id": "18032304",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] closed pull request #5581: HADOOP-18603 NPE in LdapAuthenticationHandler as disableHostNameVerif\u2026\nURL: https://github.com/apache/hadoop/pull/5581\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T00:22:35.642+0000",
            "updated": "2025-10-23T00:22:35.642+0000"
          }
        ],
        "maxResults": 12,
        "total": 12,
        "startAt": 0
      },
      "customfield_12311820": "0|z1fc3c:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13481037",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13481037",
    "key": "HADOOP-18450",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
        "name": "Critical",
        "id": "2"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12346894",
          "id": "12346894",
          "description": "Hadoop 3.4.0",
          "name": "3.4.0",
          "archived": false,
          "released": true,
          "releaseDate": "2024-03-17"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12350632",
          "id": "12350632",
          "description": "Hadoop 3.3.5 off branch-3.3",
          "name": "3.3.5",
          "archived": false,
          "released": true,
          "releaseDate": "2023-03-22"
        }
      ],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [
        {
          "id": "12661016",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12661016",
          "type": {
            "id": "12310000",
            "name": "Duplicate",
            "inward": "is duplicated by",
            "outward": "duplicates",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
          },
          "outwardIssue": {
            "id": "13326665",
            "key": "HADOOP-17255",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13326665",
            "fields": {
              "summary": "JavaKeyStoreProvider fails to create a new key if the keystore is HDFS",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/4",
                "description": "This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/reopened.png",
                "name": "Reopened",
                "id": "4",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                  "id": 2,
                  "key": "new",
                  "colorName": "blue-gray",
                  "name": "To Do"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                "id": "1",
                "description": "A problem which impairs or prevents the functions of the product.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                "name": "Bug",
                "subtask": false,
                "avatarId": 21133
              }
            }
          }
        }
      ],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=svaughan",
        "name": "svaughan",
        "key": "svaughan",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
        },
        "displayName": "Steve Vaughan",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
        "description": "The issue is open and ready for the assignee to start work on it.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
        "name": "Open",
        "id": "1",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
          "id": 2,
          "key": "new",
          "colorName": "blue-gray",
          "name": "To Do"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12330961",
          "id": "12330961",
          "name": "common"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=svaughan",
        "name": "svaughan",
        "key": "svaughan",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
        },
        "displayName": "Steve Vaughan",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=svaughan",
        "name": "svaughan",
        "key": "svaughan",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
        },
        "displayName": "Steve Vaughan",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18450/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
        "id": "2",
        "description": "A new feature of the product, which has yet to be developed.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
        "name": "New Feature",
        "subtask": false,
        "avatarId": 21141
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2880f127[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@64f046c1[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2eb75182[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@4897ccdb[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2f6a4cf9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@22fcb33b[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@36d3dda7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@40d42eb2[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3d24afd5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@7a06b621[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@52107223[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@3350776d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Sat Oct 25 00:22:18 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18450/watchers",
        "watchCount": 3,
        "isWatching": false
      },
      "created": "2022-09-12T01:43:46.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-25T00:22:18.000+0000",
      "timeoriginalestimate": null,
      "description": "Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store.\u00a0 The calls to:\r\n{noformat}\r\nrenameOrFail(Path src, Path dest) throws IOException\u00a0{noformat}\r\n... fails with an IOException when it attempts to rename a file.\u00a0 The calling code catches FileNotFoundException since the src file may not exist.\r\n\r\n\u00a0\r\n\r\nExample:\r\n{noformat}\r\n$ hadoop key create sample\r\njava.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691'{noformat}\r\n\u00a0\r\n\r\nUpdate the implementation to check for the file, throwing a FileNotFoundException.",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12352336",
          "id": "12352336",
          "description": "Ongoing branch-3.3 work",
          "name": "3.3.9",
          "archived": false,
          "released": false
        }
      ],
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": "UBI 8 running Java 11",
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13481037/comment/17696163",
            "id": "17696163",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "snmvaughan opened a new pull request, #5452:\nURL: https://github.com/apache/hadoop/pull/5452\n\n   \u2026 in renameOrFail\r\n   \r\n   ### Description of PR\r\n   \r\n   Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store.  The calls to:\r\n   ```\r\n   renameOrFail(Path src, Path dest) throws IOException \r\n   ```\r\n   \r\n   ... fails with an IOException when it attempts to rename a file.  The calling code catches FileNotFoundException since the src file may not exist.\r\n   \r\n   Example:\r\n   ```\r\n   $ hadoop key create sample\r\n   java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691'\r\n    ```\r\n   \r\n   Update the implementation to check for the file, throwing a FileNotFoundException.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Running in an Hadoop development environment docker image.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-03T14:07:08.270+0000",
            "updated": "2023-03-03T14:07:08.270+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13481037/comment/17696239",
            "id": "17696239",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5452:\nURL: https://github.com/apache/hadoop/pull/5452#issuecomment-1453873533\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  23m 17s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |  20m 33s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  22m 23s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javac  |  22m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  20m 36s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  javac  |  20m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  8s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  18m 24s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 209m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5452/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5452 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 523383cac356 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9027fa0f097af13bcee9c54033e400d6994059a5 |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5452/1/testReport/ |\r\n   | Max. process+thread count | 1418 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5452/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-03T17:38:23.167+0000",
            "updated": "2023-03-03T17:38:23.167+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13481037/comment/17699118",
            "id": "17699118",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "virajjasani commented on code in PR #5452:\nURL: https://github.com/apache/hadoop/pull/5452#discussion_r1132896769\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java:\n##########\n@@ -640,6 +640,10 @@ private void revertFromOld(Path oldPath, boolean fileExisted)\n   private void renameOrFail(Path src, Path dest)\n       throws IOException {\n     if (!fs.rename(src, dest)) {\n+      if (!fs.exists(src)) {\n+        throw new FileNotFoundException(src.toUri().toString());\n+      }\n\nReview Comment:\n   How about keeping this check just before `fs#rename`? Could save efforts of doing rename in the first place?\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-03-10T21:18:25.893+0000",
            "updated": "2023-03-10T21:18:25.893+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13481037/comment/18032641",
            "id": "18032641",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] commented on PR #5452:\nURL: https://github.com/apache/hadoop/pull/5452#issuecomment-3440030224\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-24T00:20:34.241+0000",
            "updated": "2025-10-24T00:20:34.241+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13481037/comment/18032888",
            "id": "18032888",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] closed pull request #5452: HADOOP-18450. JavaKeyStoreProvider should throw FileNotFoundException\u2026\nURL: https://github.com/apache/hadoop/pull/5452\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-25T00:22:18.476+0000",
            "updated": "2025-10-25T00:22:18.476+0000"
          }
        ],
        "maxResults": 5,
        "total": 5,
        "startAt": 0
      },
      "customfield_12311820": "0|z18h3k:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13430359",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13430359",
    "key": "HADOOP-18142",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
        "name": "Minor",
        "id": "4"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": 0,
      "customfield_12312330": null,
      "versions": [],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": null,
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/10002",
        "description": "A patch for this issue has been uploaded to JIRA by a contributor.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/document.png",
        "name": "Patch Available",
        "id": "10002",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/4",
          "id": 4,
          "key": "indeterminate",
          "colorName": "yellow",
          "name": "In Progress"
        }
      },
      "components": [],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": 0,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
        "name": "vjasani",
        "key": "vjasani",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Viraj Jasani",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
        "name": "vjasani",
        "key": "vjasani",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Viraj Jasani",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "aggregateprogress": {
        "progress": 1200,
        "total": 1200,
        "percent": 100
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 1200,
        "total": 1200,
        "percent": 100
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18142/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 2,
        "worklogs": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/worklog/738570",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani opened a new pull request #4056:\nURL: https://github.com/apache/hadoop/pull/4056\n\n\n   ### Description of PR\r\n   As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here](https://github.com/apache/hadoop/pull/4000) where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2022-03-09T07:02:50.158+0000",
            "updated": "2022-03-09T07:02:50.158+0000",
            "started": "2022-03-09T07:02:50.158+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "738570",
            "issueId": "13430359"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/worklog/738596",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #4056:\nURL: https://github.com/apache/hadoop/pull/4056#issuecomment-1062649276\n\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  12m 35s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 28s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  18m 56s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  55m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4056/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/4056 |\r\n   | Optional Tests | dupname asflicense codespell shellcheck shelldocs |\r\n   | uname | Linux e6da429b81fc 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b017a660e554395f8e719dc11953372f0ecf9201 |\r\n   | Max. process+thread count | 548 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4056/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2022-03-09T08:00:23.343+0000",
            "updated": "2022-03-09T08:00:23.343+0000",
            "started": "2022-03-09T08:00:23.343+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "738596",
            "issueId": "13430359"
          }
        ]
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
        "id": "3",
        "description": "A task that needs to be done.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
        "name": "Task",
        "subtask": false,
        "avatarId": 21148
      },
      "timespent": 1200,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@890127[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@552e5a07[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3cf06fab[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@1ba9f450[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3934ef6a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@b7f4936[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@344df2c5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@60929b68[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7ec367af[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@f04a7b8[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5404b747[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@20b92f74[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": 1200,
      "customfield_12312520": null,
      "customfield_12312521": "Sat Oct 25 00:26:24 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18142/watchers",
        "watchCount": 7,
        "isWatching": false
      },
      "created": "2022-02-24T07:14:20.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-25T00:26:24.000+0000",
      "timeoriginalestimate": null,
      "description": "As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here|https://github.com/apache/hadoop/pull/4000] where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.",
      "customfield_10010": null,
      "timetracking": {
        "remainingEstimate": "0h",
        "timeSpent": "20m",
        "remainingEstimateSeconds": 0,
        "timeSpentSeconds": 1200
      },
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12354106",
          "id": "12354106",
          "description": "Hadoop 3.5.0",
          "name": "3.5.0",
          "archived": false,
          "released": false
        }
      ],
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "Increase precommit job timeout from 24 hr to 30 hr",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17497431",
            "id": "17497431",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk",
              "name": "kgyrtkirk",
              "key": "kgyrtkirk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"
              },
              "displayName": "Zoltan Haindrich",
              "active": true,
              "timeZone": "Europe/Budapest"
            },
            "body": "[~vjasani] I've noticed this ticket by chance and would like to mention a few things I've done in Hive to avoid kinda like the same issues:\r\n\r\n* [use rateLimit|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L21] to avoid building the same PR multiple time a day ; this naturally adds a 6 hour wait before the next would start\r\n* use a global lock to [limit the number|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L150] of concurrently running\r\n* [disable concurrent builds|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L23] as there is no point running the tests for someone who pushed new changes while it was still running => the contributor most likely will push more commits anyway which could launch even more builds...not starting a new build means it could pick up multiple trigger events while the one executing is still running\r\n* auto-kill the build in case the PR was updated while it waiting/running ; by calling [this method|https://github.com/apache/hive/blob/master/Jenkinsfile#L30-L45] at a few key points in the build\r\n\r\n\r\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk",
              "name": "kgyrtkirk",
              "key": "kgyrtkirk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"
              },
              "displayName": "Zoltan Haindrich",
              "active": true,
              "timeZone": "Europe/Budapest"
            },
            "created": "2022-02-24T14:30:20.326+0000",
            "updated": "2022-02-24T14:30:20.326+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17497613",
            "id": "17497613",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Thanks [~kgyrtkirk] for the nice suggestions! If you would like to take up this work, please feel free to go ahead with creating PR.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-02-24T18:18:25.899+0000",
            "updated": "2022-02-24T18:18:25.899+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17503046",
            "id": "17503046",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~kgyrtkirk] I went through the changes you have mentioned above, nice work indeed! Based on my recent observation with [PR#4000|https://github.com/apache/hadoop/pull/4000], Hadoop full build definitely exceeds current timeout of 24 hr (regardless of whether we rate limit i.e. run only 1 build or run multiple builds concurrently) hence increasing timeout to 30 hr is a certain requirement for the entire hadoop build to be finished.\r\n\r\nFor the improvements that you have mentioned above (specifically disabling concurrent builds and auto-kill for updated PR), will create a new follow-up Jira.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-03-08T16:20:33.536+0000",
            "updated": "2022-03-24T13:30:33.619+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17514409",
            "id": "17514409",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=tasanuma",
              "name": "tasanuma",
              "key": "tasanuma0829",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=tasanuma0829&avatarId=40046",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tasanuma0829&avatarId=40046",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tasanuma0829&avatarId=40046",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tasanuma0829&avatarId=40046"
              },
              "displayName": "Takanobu Asanuma",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "It seems the recent QBTs finished within 20 hours. Maybe we should wait and see for a while.\r\nhttps://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=tasanuma",
              "name": "tasanuma",
              "key": "tasanuma0829",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=tasanuma0829&avatarId=40046",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tasanuma0829&avatarId=40046",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tasanuma0829&avatarId=40046",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tasanuma0829&avatarId=40046"
              },
              "displayName": "Takanobu Asanuma",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2022-03-30T02:49:58.286+0000",
            "updated": "2022-03-30T02:49:58.286+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17533516",
            "id": "17533516",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~tasanuma] It seems QBTs run only single JDK version builds, whereas PR builds run full QA with both Java 8 and 11 (for trunk PRs), hence 24 hr is not sufficient for many runs.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-05-08T18:49:35.469+0000",
            "updated": "2022-05-08T18:49:35.469+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17539134",
            "id": "17539134",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "One recent example of build timeout after 24 hr https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4267/17/",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-05-18T23:40:35.189+0000",
            "updated": "2022-05-18T23:40:35.189+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/17802851",
            "id": "17802851",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=slfan1989",
              "name": "slfan1989",
              "key": "slfan1989",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=slfan1989&avatarId=40935",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=slfan1989&avatarId=40935",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=slfan1989&avatarId=40935",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=slfan1989&avatarId=40935"
              },
              "displayName": "Shilun Fan",
              "active": true,
              "timeZone": "America/Vancouver"
            },
            "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=slfan1989",
              "name": "slfan1989",
              "key": "slfan1989",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=slfan1989&avatarId=40935",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=slfan1989&avatarId=40935",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=slfan1989&avatarId=40935",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=slfan1989&avatarId=40935"
              },
              "displayName": "Shilun Fan",
              "active": true,
              "timeZone": "America/Vancouver"
            },
            "created": "2024-01-04T09:51:01.404+0000",
            "updated": "2024-01-04T09:51:01.404+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13430359/comment/18032903",
            "id": "18032903",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "virajjasani closed pull request #4056: HADOOP-18142. Increase precommit job timeout from 24 hr to 30 hr\nURL: https://github.com/apache/hadoop/pull/4056\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-25T00:26:24.741+0000",
            "updated": "2025-10-25T00:26:24.741+0000"
          }
        ],
        "maxResults": 8,
        "total": 8,
        "startAt": 0
      },
      "customfield_12311820": "0|z0zwao:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13415301",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13415301",
    "key": "HADOOP-18033",
    "fields": {
      "fixVersions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12350181",
          "id": "12350181",
          "description": "Hadoop 3.3.2",
          "name": "3.3.2",
          "archived": false,
          "released": true,
          "releaseDate": "2022-03-02"
        }
      ],
      "resolution": {
        "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
        "id": "1",
        "description": "A fix for this issue is checked into the tree and tested.",
        "name": "Fixed"
      },
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": 0,
      "customfield_12312330": null,
      "versions": [],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [
        {
          "id": "12642155",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12642155",
          "type": {
            "id": "10032",
            "name": "Blocker",
            "inward": "is blocked by",
            "outward": "blocks",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"
          },
          "inwardIssue": {
            "id": "13202706",
            "key": "HADOOP-15984",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13202706",
            "fields": {
              "summary": "Update jersey from 1.19 to 2.x",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                "id": "4",
                "description": "An improvement or enhancement to an existing feature or task.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                "name": "Improvement",
                "subtask": false,
                "avatarId": 21140
              }
            }
          }
        },
        {
          "id": "12644716",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12644716",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13473049",
            "key": "BIGTOP-3760",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13473049",
            "fields": {
              "summary": "Bump Hadoop to 3.3.4",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                "id": "7",
                "description": "The sub-task of the issue",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                "name": "Sub-task",
                "subtask": true,
                "avatarId": 21146
              }
            }
          }
        },
        {
          "id": "12642098",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12642098",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13450848",
            "key": "HADOOP-18303",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13450848",
            "fields": {
              "summary": "Remove shading exclusion of javax.ws.rs-api from hadoop-client-runtime",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
                "name": "Critical",
                "id": "2"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                "id": "1",
                "description": "A problem which impairs or prevents the functions of the product.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                "name": "Bug",
                "subtask": false,
                "avatarId": 21133
              }
            }
          }
        },
        {
          "id": "12627837",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12627837",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13359270",
            "key": "HADOOP-17534",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13359270",
            "fields": {
              "summary": "Upgrade Jackson databind to 2.10.5.1",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                "id": "1",
                "description": "A problem which impairs or prevents the functions of the product.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                "name": "Bug",
                "subtask": false,
                "avatarId": 21133
              }
            }
          }
        },
        {
          "id": "12644336",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12644336",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13473053",
            "key": "HADOOP-18356",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13473053",
            "fields": {
              "summary": "Update jackson from 2.12.7 to 2.13.3",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                "name": "Minor",
                "id": "4"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                "id": "4",
                "description": "An improvement or enhancement to an existing feature or task.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                "name": "Improvement",
                "subtask": false,
                "avatarId": 21140
              }
            }
          }
        },
        {
          "id": "12628045",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12628045",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "inwardIssue": {
            "id": "13289681",
            "key": "HADOOP-16908",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13289681",
            "fields": {
              "summary": "Prune Jackson 1 from the codebase and restrict it's usage for future",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                "id": "7",
                "description": "The sub-task of the issue",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                "name": "Sub-task",
                "subtask": true,
                "avatarId": 21146
              }
            }
          }
        },
        {
          "id": "12637343",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12637343",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "inwardIssue": {
            "id": "13435952",
            "key": "HADOOP-18178",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13435952",
            "fields": {
              "summary": "Upgrade jackson to 2.13.2 and jackson-databind to 2.13.2.2. CVE-2020-36518",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                "id": "1",
                "description": "A problem which impairs or prevents the functions of the product.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                "name": "Bug",
                "subtask": false,
                "avatarId": 21133
              }
            }
          }
        },
        {
          "id": "12643694",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12643694",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "inwardIssue": {
            "id": "13470996",
            "key": "HADOOP-18332",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13470996",
            "fields": {
              "summary": "Remove rs-api dependency by downgrading jackson to 2.12.7",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                "id": "4",
                "description": "An improvement or enhancement to an existing feature or task.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                "name": "Improvement",
                "subtask": false,
                "avatarId": 21140
              }
            }
          }
        },
        {
          "id": "12642009",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12642009",
          "type": {
            "id": "12310050",
            "name": "Regression",
            "inward": "is broken by",
            "outward": "breaks",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"
          },
          "outwardIssue": {
            "id": "13447661",
            "key": "TEZ-4420",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13447661",
            "fields": {
              "summary": "Upgrade to Hadoop 3.3.4",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                "id": "4",
                "description": "An improvement or enhancement to an existing feature or task.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                "name": "Improvement",
                "subtask": false,
                "avatarId": 21140
              }
            }
          }
        }
      ],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
        "name": "vjasani",
        "key": "vjasani",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Viraj Jasani",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
        "name": "Resolved",
        "id": "5",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
          "id": 3,
          "key": "done",
          "colorName": "green",
          "name": "Done"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12311543",
          "id": "12311543",
          "name": "build",
          "description": "Build scripts"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": 0,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
        "name": "aajisaka",
        "key": "ajisakaa",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
        },
        "displayName": "Akira Ajisaka",
        "active": true,
        "timeZone": "Asia/Tokyo"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
        "name": "aajisaka",
        "key": "ajisakaa",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
        },
        "displayName": "Akira Ajisaka",
        "active": true,
        "timeZone": "Asia/Tokyo"
      },
      "aggregateprogress": {
        "progress": 23400,
        "total": 23400,
        "percent": 100
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 23400,
        "total": 23400,
        "percent": 100
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18033/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 39,
        "worklogs": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/690538",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani opened a new pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749\n\n\n   ### Description of PR\r\n   Upgrade fasterxml Jackson to 2.13.0.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-04T15:40:03.660+0000",
            "updated": "2021-12-04T15:40:03.660+0000",
            "started": "2021-12-04T15:40:03.659+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "690538",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/690632",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749#issuecomment-986210486\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  13m  4s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  25m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  28m 53s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  compile  |  24m 20s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  mvnsite  |  31m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 33s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 45s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  28m  6s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | -1 :x: |  javac  |  28m  6s | [/results-compile-javac-root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/3/artifact/out/results-compile-javac-root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt) |  root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 generated 5 new + 1923 unchanged - 0 fixed = 1928 total (was 1923)  |\r\n   | +1 :green_heart: |  compile  |  24m 14s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | -1 :x: |  javac  |  24m 14s | [/results-compile-javac-root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/3/artifact/out/results-compile-javac-root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.txt) |  root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 generated 5 new + 1799 unchanged - 0 fixed = 1804 total (was 1799)  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  26m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  xml  |   0m  4s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 55s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 32s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 848m  1s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/3/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1170m 59s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.rbfbalance.TestRouterDistCpProcedure |\r\n   |   | hadoop.fs.contract.router.web.TestRouterWebHDFSContractCreate |\r\n   |   | hadoop.mapred.TestMRTimelineEventHandling |\r\n   |   | hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMHATimelineCollectors |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMRestart |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisherForV2 |\r\n   |   | hadoop.yarn.server.resourcemanager.TestResourceTrackerService |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices |\r\n   |   | hadoop.yarn.server.timeline.security.TestTimelineAuthenticationFilterForV1 |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL |\r\n   |   | hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices |\r\n   |   | hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp |\r\n   |   | hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer |\r\n   |   | hadoop.yarn.server.TestMiniYarnCluster |\r\n   |   | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 |\r\n   |   | hadoop.yarn.server.timelineservice.TestTimelineServiceClientIntegration |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageEntities |\r\n   |   | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRunCompaction |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageDomain |\r\n   |   | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowActivity |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageApps |\r\n   |   | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRun |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestTimelineReaderHBaseDown |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.collection.TestDocumentOperations |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.TestDocumentStoreTimelineReaderImpl |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.TestDocumentStoreTimelineWriterImpl |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.TestCosmosDBDocumentStoreWriter |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.TestCosmosDBDocumentStoreReader |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery |\r\n   |   | hadoop.yarn.server.nodemanager.webapp.TestNMWebServices |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineUIDConverter |\r\n   |   | hadoop.yarn.server.timelineservice.collector.TestPerNodeTimelineCollectorsAuxService |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderServer |\r\n   |   | hadoop.yarn.server.timelineservice.collector.TestTimelineCollector |\r\n   |   | hadoop.yarn.server.timelineservice.collector.TestNMTimelineCollectorManager |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestFileSystemTimelineReaderImpl |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServices |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesACL |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestFileSystemTimelineWriterImpl |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSTimelineV15 |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSWithMultipleNodeManager |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSTimelineV10 |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSTimelineV20 |\r\n   |   | hadoop.yarn.api.records.timelineservice.TestTimelineServiceRecords |\r\n   |   | hadoop.yarn.api.records.timeline.TestTimelineRecords |\r\n   |   | hadoop.yarn.util.timeline.TestShortenedFlowName |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClientImpl |\r\n   |   | hadoop.yarn.client.TestApplicationMasterServiceProtocolForTimelineV2 |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.tools.fedbalance.TestDistCpProcedure |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3749 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml shellcheck shelldocs |\r\n   | uname | Linux dde5646358b5 4.15.0-156-generic #163-Ubuntu SMP Thu Aug 19 23:31:58 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1f3d3771b2843246a0f00ff6c1110b3f5777d01e |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/3/testReport/ |\r\n   | Max. process+thread count | 3764 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-05T11:14:07.350+0000",
            "updated": "2021-12-05T11:14:07.350+0000",
            "started": "2021-12-05T11:14:07.349+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "690632",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/690633",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749#issuecomment-986210562\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  12m 55s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  25m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  28m 57s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  compile  |  23m 52s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  mvnsite  |  30m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 40s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 17s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 44s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  33m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  28m 14s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | -1 :x: |  javac  |  28m 14s | [/results-compile-javac-root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/4/artifact/out/results-compile-javac-root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt) |  root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 generated 5 new + 1923 unchanged - 0 fixed = 1928 total (was 1923)  |\r\n   | +1 :green_heart: |  compile  |  24m  0s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | -1 :x: |  javac  |  24m  0s | [/results-compile-javac-root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/4/artifact/out/results-compile-javac-root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.txt) |  root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 generated 5 new + 1800 unchanged - 0 fixed = 1805 total (was 1800)  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  25m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  xml  |   0m  5s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 22s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 46s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 851m 12s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/4/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1171m 32s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.rbfbalance.TestRouterDistCpProcedure |\r\n   |   | hadoop.mapred.TestMRTimelineEventHandling |\r\n   |   | hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMHATimelineCollectors |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMRestart |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisherForV2 |\r\n   |   | hadoop.yarn.server.resourcemanager.TestResourceTrackerService |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices |\r\n   |   | hadoop.yarn.server.timeline.security.TestTimelineAuthenticationFilterForV1 |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL |\r\n   |   | hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices |\r\n   |   | hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp |\r\n   |   | hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer |\r\n   |   | hadoop.yarn.server.TestMiniYarnCluster |\r\n   |   | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 |\r\n   |   | hadoop.yarn.server.timelineservice.TestTimelineServiceClientIntegration |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageEntities |\r\n   |   | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRunCompaction |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageDomain |\r\n   |   | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowActivity |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageApps |\r\n   |   | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRun |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestTimelineReaderHBaseDown |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.collection.TestDocumentOperations |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.TestDocumentStoreTimelineReaderImpl |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.TestDocumentStoreTimelineWriterImpl |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.TestCosmosDBDocumentStoreWriter |\r\n   |   | hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.TestCosmosDBDocumentStoreReader |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery |\r\n   |   | hadoop.yarn.server.nodemanager.webapp.TestNMWebServices |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineUIDConverter |\r\n   |   | hadoop.yarn.server.timelineservice.collector.TestPerNodeTimelineCollectorsAuxService |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderServer |\r\n   |   | hadoop.yarn.server.timelineservice.collector.TestTimelineCollector |\r\n   |   | hadoop.yarn.server.timelineservice.collector.TestNMTimelineCollectorManager |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestFileSystemTimelineReaderImpl |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServices |\r\n   |   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesACL |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestFileSystemTimelineWriterImpl |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSTimelineV15 |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSWithMultipleNodeManager |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSTimelineV10 |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDSTimelineV20 |\r\n   |   | hadoop.yarn.api.records.timelineservice.TestTimelineServiceRecords |\r\n   |   | hadoop.yarn.api.records.timeline.TestTimelineRecords |\r\n   |   | hadoop.yarn.util.timeline.TestShortenedFlowName |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClientImpl |\r\n   |   | hadoop.yarn.client.TestApplicationMasterServiceProtocolForTimelineV2 |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.tools.fedbalance.TestDistCpProcedure |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3749 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml shellcheck shelldocs |\r\n   | uname | Linux 69347a92c8fe 4.15.0-156-generic #163-Ubuntu SMP Thu Aug 19 23:31:58 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1f3d3771b2843246a0f00ff6c1110b3f5777d01e |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/4/testReport/ |\r\n   | Max. process+thread count | 3784 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-05T11:14:30.209+0000",
            "updated": "2021-12-05T11:14:30.209+0000",
            "started": "2021-12-05T11:14:30.208+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "690633",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/690937",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749#issuecomment-986713916\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  12m 46s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  21m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  22m  1s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  compile  |  19m 10s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  25m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   7m 54s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m  7s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 49s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  21m  9s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  21m 34s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javac  |  21m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  19m 13s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  javac  |  19m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  xml  |   0m 22s |  |  The patch has no ill-formed XML file.  |\r\n   | -1 :x: |  javadoc  |   7m 44s | [/patch-javadoc-root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.  |\r\n   | -1 :x: |  javadoc  |   7m 52s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.txt) |  root in the patch failed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10.  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | -1 :x: |  shadedclient  |  52m  5s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 781m 11s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1174m 56s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationInterceptorRESTRetry |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3749 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle shellcheck shelldocs |\r\n   | uname | Linux 0e98eea1449e 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2a1f8dc2c924882556a8f5b36cc7f697f10a80dd |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/testReport/ |\r\n   | Max. process+thread count | 2968 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/10/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-06T12:06:42.742+0000",
            "updated": "2021-12-06T12:06:42.742+0000",
            "started": "2021-12-06T12:06:42.742+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "690937",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/690989",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749#issuecomment-986765667\n\n\n   While the full QA is going on, could you please take a look @aajisaka @jojochuang @tasanuma ?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-06T13:15:57.820+0000",
            "updated": "2021-12-06T13:15:57.820+0000",
            "started": "2021-12-06T13:15:57.820+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "690989",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/691563",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749#issuecomment-987696563\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  12m 45s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  21m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  23m 34s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  compile  |  20m  2s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  26m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m  1s |  |  trunk passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 10s |  |  trunk passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  54m 26s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 46s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  21m 26s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javac  |  21m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  19m 13s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +1 :green_heart: |  javac  |  19m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   3m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  1s |  |  No new issues.  |\r\n   | +1 :green_heart: |  xml  |   0m 23s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   7m 52s |  |  the patch passed with JDK Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m  3s |  |  the patch passed with JDK Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 772m  9s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/17/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1175m 31s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/17/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3749 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle shellcheck shelldocs |\r\n   | uname | Linux b469fe66f022 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 33f572a2e17a22b29388f217a51339b86898a2be |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_292-8u292-b10-0ubuntu1~20.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/17/testReport/ |\r\n   | Max. process+thread count | 3158 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3749/17/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-07T08:42:41.981+0000",
            "updated": "2021-12-07T08:42:41.981+0000",
            "started": "2021-12-07T08:42:41.980+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "691563",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/691565",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749#issuecomment-987698685\n\n\n   Out of all the modules, the only two tests failed in the latest result are known failures and not related to this Jackson upgrade.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-07T08:45:45.315+0000",
            "updated": "2021-12-07T08:45:45.315+0000",
            "started": "2021-12-07T08:45:45.314+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "691565",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692284",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "aajisaka merged pull request #3749:\nURL: https://github.com/apache/hadoop/pull/3749\n\n\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-08T07:52:54.120+0000",
            "updated": "2021-12-08T07:52:54.120+0000",
            "started": "2021-12-08T07:52:54.120+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692284",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692408",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani opened a new pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764\n\n\n   ### Description of PR\r\n   branch-3.3 backport PR of #3749 \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-08T10:16:43.627+0000",
            "updated": "2021-12-08T10:16:43.627+0000",
            "started": "2021-12-08T10:16:43.627+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692408",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692962",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989478787\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   5m 16s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 48s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  29m 10s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  19m 23s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 42s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  25m 38s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m  2s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 57s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  18m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 26s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   8m 10s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 30s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  57m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 682m 47s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/5/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1037m 11s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   |   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux 7512b5c92476 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / db997a2ec66c450da2dacc191be3a1540e875e42 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/5/testReport/ |\r\n   | Max. process+thread count | 3100 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/5/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T03:37:10.463+0000",
            "updated": "2021-12-09T03:37:10.463+0000",
            "started": "2021-12-09T03:37:10.463+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692962",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692965",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989481219\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   5m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  29m 30s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  19m  9s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 39s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  25m 34s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m  2s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 57s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  18m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 25s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   8m  7s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 29s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  57m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 689m 10s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/4/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1043m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux e020662c5503 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / db997a2ec66c450da2dacc191be3a1540e875e42 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/4/testReport/ |\r\n   | Max. process+thread count | 2814 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/4/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T03:43:18.406+0000",
            "updated": "2021-12-09T03:43:18.406+0000",
            "started": "2021-12-09T03:43:18.406+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692965",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692968",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989486251\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   6m 45s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 43s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 59s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  17m 29s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 44s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  24m 53s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m  1s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 30s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 58s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  42m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  19m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  19m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 26s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   8m 29s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 19s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  59m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 697m 12s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/3/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1056m  1s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   |   | hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits |\r\n   |   | hadoop.hdfs.server.datanode.TestBPOfferService |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   |   | hadoop.hdfs.server.datanode.TestDirectoryScanner |\r\n   |   | hadoop.security.token.delegation.TestZKDelegationTokenSecretManager |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux 1d155fb8b121 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / db997a2ec66c450da2dacc191be3a1540e875e42 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/3/testReport/ |\r\n   | Max. process+thread count | 3753 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/3/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T03:55:45.825+0000",
            "updated": "2021-12-09T03:55:45.825+0000",
            "started": "2021-12-09T03:55:45.825+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692968",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692972",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989489762\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  10m 18s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 44s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m  9s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  17m 26s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 38s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 34s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   7m 33s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 29s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 59s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  37m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 27s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 19s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  54m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 724m 45s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 30s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1065m 30s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.namenode.TestNameNodeMXBean |\r\n   |   | hadoop.hdfs.TestDecommissionWithStriped |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   |   | hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux ebd7a008c194 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / db997a2ec66c450da2dacc191be3a1540e875e42 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/2/testReport/ |\r\n   | Max. process+thread count | 3303 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/2/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T04:04:11.965+0000",
            "updated": "2021-12-09T04:04:11.965+0000",
            "started": "2021-12-09T04:04:11.964+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692972",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692973",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989494204\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  10m  5s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 42s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 21s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  17m 30s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 39s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 57s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   8m 39s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 31s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 58s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  37m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  17m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 25s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   7m 41s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  54m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 732m 43s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1076m 13s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.sls.TestSLSRunner |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   |   | hadoop.hdfs.server.datanode.TestDirectoryScanner |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux 3cf5db0ea2c2 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / db997a2ec66c450da2dacc191be3a1540e875e42 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/1/testReport/ |\r\n   | Max. process+thread count | 3158 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/1/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T04:13:57.849+0000",
            "updated": "2021-12-09T04:13:57.849+0000",
            "started": "2021-12-09T04:13:57.849+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692973",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/692987",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989527535\n\n\n   Reverted the last commit. Thanks @aajisaka \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T05:30:21.733+0000",
            "updated": "2021-12-09T05:30:21.733+0000",
            "started": "2021-12-09T05:30:21.732+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "692987",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/693042",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989621041\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 45s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |   9m 52s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-mvninstall-root.txt) |  root in branch-3.3 failed.  |\r\n   | -1 :x: |  compile  |   3m  4s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-compile-root.txt) |  root in branch-3.3 failed.  |\r\n   | -0 :warning: |  checkstyle  |   1m 11s | [/buildtool-branch-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/buildtool-branch-checkstyle-root.txt) |  The patch fails to run checkstyle in root  |\r\n   | -1 :x: |  mvnsite  |   2m  6s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-mvnsite-root.txt) |  root in branch-3.3 failed.  |\r\n   | -1 :x: |  javadoc  |   3m 16s | [/branch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-javadoc-root.txt) |  root in branch-3.3 failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  spotbugs  |   6m 26s | [/branch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-root.txt) |  root in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/branch-spotbugs-hadoop-client-modules_hadoop-client-minicluster.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-client-modules_hadoop-client-minicluster.txt) |  hadoop-client-minicluster in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 20s | [/branch-spotbugs-hadoop-client-modules_hadoop-client-runtime.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-client-modules_hadoop-client-runtime.txt) |  hadoop-client-runtime in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/branch-spotbugs-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   2m 10s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 24s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt) |  hadoop-yarn-applications-distributedshell in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 21s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt) |  hadoop-yarn-services-core in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt) |  hadoop-yarn-server-router in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 20s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt) |  hadoop-yarn-server-tests in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timeline-pluginstorage.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timeline-pluginstorage.txt) |  hadoop-yarn-server-timeline-pluginstorage in branch-3.3 failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-hbase-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-hbase-tests.txt) |  hadoop-yarn-server-timelineservice-hbase-tests in branch-3.3 failed.  |\r\n   | -1 :x: |  shadedclient  |  10m 49s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 49s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   5m 16s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-client-modules_hadoop-client-minicluster.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-client-modules_hadoop-client-minicluster.txt) |  hadoop-client-minicluster in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 16s | [/patch-mvninstall-hadoop-client-modules_hadoop-client-runtime.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-client-modules_hadoop-client-runtime.txt) |  hadoop-client-runtime in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 19s | [/patch-mvninstall-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 18s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt) |  hadoop-yarn-applications-distributedshell in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt) |  hadoop-yarn-services-core in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 18s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt) |  hadoop-yarn-server-router in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 18s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt) |  hadoop-yarn-server-tests in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timeline-pluginstorage.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timeline-pluginstorage.txt) |  hadoop-yarn-server-timeline-pluginstorage in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 17s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-hbase-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-hbase-tests.txt) |  hadoop-yarn-server-timelineservice-hbase-tests in the patch failed.  |\r\n   | -1 :x: |  compile  |   3m  5s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |   3m  5s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m  9s | [/buildtool-patch-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/buildtool-patch-checkstyle-root.txt) |  The patch fails to run checkstyle in root  |\r\n   | -1 :x: |  mvnsite  |   2m  6s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  xml  |   0m 25s |  |  The patch has no ill-formed XML file.  |\r\n   | -1 :x: |  javadoc  |   3m 10s | [/patch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-javadoc-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 18s |  |  hadoop-project has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   6m 25s | [/patch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/patch-spotbugs-hadoop-client-modules_hadoop-client-minicluster.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-client-modules_hadoop-client-minicluster.txt) |  hadoop-client-minicluster in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/patch-spotbugs-hadoop-client-modules_hadoop-client-runtime.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-client-modules_hadoop-client-runtime.txt) |  hadoop-client-runtime in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt) |  hadoop-yarn-applications-distributedshell in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt) |  hadoop-yarn-services-core in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt) |  hadoop-yarn-server-router in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt) |  hadoop-yarn-server-tests in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timeline-pluginstorage.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timeline-pluginstorage.txt) |  hadoop-yarn-server-timeline-pluginstorage in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 18s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-hbase-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-hbase-tests.txt) |  hadoop-yarn-server-timelineservice-hbase-tests in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |  11m  0s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  66m 41s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 45s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 176m  7s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.csi.client.TestCsiClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux 6dc2b290939d 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / 27eb7ddcdac1c333dd6851252744f5d04963a860 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/testReport/ |\r\n   | Max. process+thread count | 2158 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common . hadoop-client-modules/hadoop-client-minicluster hadoop-client-modules/hadoop-client-runtime hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/6/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T08:26:06.093+0000",
            "updated": "2021-12-09T08:26:06.093+0000",
            "started": "2021-12-09T08:26:06.092+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "693042",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/693173",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-989775789\n\n\n   Recent failure is not relevant, local compilation is not failing. I am bit surprised by this build result.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-09T11:42:35.953+0000",
            "updated": "2021-12-09T11:42:35.953+0000",
            "started": "2021-12-09T11:42:35.953+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "693173",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/693723",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-990630077\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 44s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m  5s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  17m 22s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 40s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 29s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   7m 40s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 29s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 32s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 55s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  17m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 25s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   7m 36s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 21s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  53m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 732m 11s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/7/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1064m 38s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   |   | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   |   | hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown |\r\n   |   | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux 17cbbbb6c201 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / e282589776fdfd69eb0870c65fae085c547c8bf8 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/7/testReport/ |\r\n   | Max. process+thread count | 3136 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/7/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-10T05:28:38.335+0000",
            "updated": "2021-12-10T05:28:38.335+0000",
            "started": "2021-12-10T05:28:38.334+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "693723",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/693743",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-990667261\n\n\n   Looks like we are good now. Build#6 seems to have had some intermittent issue as the compilation failure was not relevant.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-10T07:00:28.862+0000",
            "updated": "2021-12-10T07:00:28.862+0000",
            "started": "2021-12-10T07:00:28.862+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "693743",
            "issueId": "13415301"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/worklog/693814",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3764:\nURL: https://github.com/apache/hadoop/pull/3764#issuecomment-990788402\n\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   3m 44s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 23s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  17m 28s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 40s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 48s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   7m 43s |  |  branch-3.3 passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 29s |  |  branch/hadoop-client-modules/hadoop-client-minicluster no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-client-modules/hadoop-client-runtime no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 32s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 59s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  xml  |   0m 25s |  |  The patch has no ill-formed XML file.  |\r\n   | +1 :green_heart: |  javadoc  |   7m 34s |  |  the patch passed  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-runtime has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  hadoop-client-modules/hadoop-client-minicluster has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  53m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 672m 30s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/10/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1001m  3s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.csi.client.TestCsiClient |\r\n   |   | hadoop.yarn.applications.distributedshell.TestDistributedShell |\r\n   |   | hadoop.tools.dynamometer.TestDynamometerInfra |\r\n   |   | hadoop.hdfs.TestBlockStoragePolicy |\r\n   |   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n   |   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |\r\n   |   | hadoop.hdfs.TestReconstructStripedFile |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3764 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell xml spotbugs checkstyle |\r\n   | uname | Linux 351bd5317043 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / e282589776fdfd69eb0870c65fae085c547c8bf8 |\r\n   | Default Java | Private Build-1.8.0_292-8u292-b10-0ubuntu1~18.04-b10 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/10/testReport/ |\r\n   | Max. process+thread count | 3149 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-minicluster . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3764/10/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: common-issues-unsubscribe@hadoop.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-12-10T09:47:45.105+0000",
            "updated": "2021-12-10T09:47:45.105+0000",
            "started": "2021-12-10T09:47:45.105+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "693814",
            "issueId": "13415301"
          }
        ]
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
        "id": "4",
        "description": "An improvement or enhancement to an existing feature or task.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
        "name": "Improvement",
        "subtask": false,
        "avatarId": 21140
      },
      "timespent": 23400,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@10027785[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7a388b5e[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7ae91971[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@44350f09[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2124d2c7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@7f12c19d[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4d026d9f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@21237e71[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@26963278[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@4fe1111e[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1151f3e8[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@2b3bd745[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": 23400,
      "customfield_12312520": null,
      "customfield_12312521": "Sat Oct 25 00:26:09 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": "2021-12-13T04:53:32.000+0000",
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-18033/watchers",
        "watchCount": 12,
        "isWatching": false
      },
      "created": "2021-12-03T14:54:23.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-25T00:26:09.000+0000",
      "timeoriginalestimate": null,
      "description": "Spark 3.2.0 depends on Jackson 2.12.3. Let's upgrade to 2.12.5 (2.12.x latest as of now) or upper.\r\n\r\nh2. this has been reverted.\r\n\r\nwe had to revert this as it broke tez.",
      "customfield_10010": null,
      "timetracking": {
        "remainingEstimate": "0h",
        "timeSpent": "6.5h",
        "remainingEstimateSeconds": 0,
        "timeSpentSeconds": 23400
      },
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12346894",
          "id": "12346894",
          "description": "Hadoop 3.4.0",
          "name": "3.4.0",
          "archived": false,
          "released": true,
          "releaseDate": "2024-03-17"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12350632",
          "id": "12350632",
          "description": "Hadoop 3.3.5 off branch-3.3",
          "name": "3.3.5",
          "archived": false,
          "released": true,
          "releaseDate": "2023-03-22"
        }
      ],
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "Upgrade fasterxml Jackson to 2.13.0",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17453182",
            "id": "17453182",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Let's upgrade to 2.13.0? It already seems to have good number of usages [here|https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core/2.13.0/usages].",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-03T19:37:39.521+0000",
            "updated": "2021-12-03T19:38:18.343+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17453187",
            "id": "17453187",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Build fails with 2.13.0\r\n{code:java}\r\n[WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:\r\nDuplicate classes found:\r\n\r\n\r\n\u00a0 Found in:\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 Duplicate classes:\r\n\u00a0 \u00a0 META-INF/versions/11/module-info.class\r\n\r\n\r\n\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]\r\n[ERROR]  {code}\r\nWill get back to this after some time.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-03T20:05:15.745+0000",
            "updated": "2021-12-03T20:05:15.745+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17454049",
            "id": "17454049",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "With this upgrade, we will also need to explicitly add new dependency {_}javax.ws.rs:javax.ws.rs-api{_}.\r\n\r\nWe also need to exclude it from shading, else we will get multiple duplicate class clash with existing javax.ws.rs dependencies.\r\n\r\nFor the record, let me provide duplicate class details:\r\n{code:java}\r\nDuplicate classes found:\u00a0 Found in:\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 Duplicate classes:\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/POST.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbLink.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NotFoundException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/PreMatching.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/FeatureContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/HttpHeaders.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/PATCH.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent$Builder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/GenericType.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/SseBroadcaster.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$2.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/StreamingOutput.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/GenericEntity.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/PathSegment.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/BadRequestException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ExceptionMapper.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/ClientBuilder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/Priorities.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/HeaderParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Context.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ResourceContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ConstrainedTo.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/Encoded.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/AbstractMultivaluedMap.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/Entity.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/SyncInvoker.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NameBinding.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/Invocation$Builder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/MessageBodyReader.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/ResponseProcessingException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/FactoryFinder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/FactoryFinder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestFilter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/RuntimeDelegate$HeaderDelegate.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Response$Status$Family.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ReaderInterceptor.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ContainerResponseContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ApplicationPath.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/WriterInterceptorContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/PUT.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ResourceInfo.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Response$ResponseBuilder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/MessageBodyWriter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSource.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/FormParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/PathParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Application.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Link$Builder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NotAcceptableException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NotAllowedException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/InterceptorContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ConnectionCallback.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/TimeoutHandler.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Request.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/WebApplicationException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/WriterInterceptor.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/RedirectionException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/RuntimeDelegate.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/CookieParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/CompletionCallback.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/Path.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/Invocation.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/EntityTag.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/UriBuilder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSource$Builder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/DefaultValue.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/Client.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/FactoryFinder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NotSupportedException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/HEAD.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Link.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverter$Lazy.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/QueryParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Response$StatusType.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/ClientResponseFilter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/RxInvoker.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/MultivaluedHashMap.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/UriBuilderException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/ClientRequestFilter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/RxInvokerProvider.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/SseEvent.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/DELETE.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/Produces.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/MediaType.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/NoContentException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/OPTIONS.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/Provider.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/BeanParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/ClientRequestContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Feature.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Form.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/Consumes.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ClientErrorException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/CompletionStageRxInvoker.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/MultivaluedMap.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ReaderInterceptorContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/InboundSseEvent.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/NewCookie.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Variant$VariantListBuilder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/WebTarget.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Configuration.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ForbiddenException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/RuntimeType.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$1.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/MatrixParam.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/InvocationCallback.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/Suspended.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/Providers.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/InternalServerErrorException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/DynamicFeature.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ContextResolver.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Cookie.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/HttpMethod.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ServiceUnavailableException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/GET.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSink.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/Sse.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/AsyncResponse.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ContainerResponseFilter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Configurable.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Response.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ServerErrorException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Form$1.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ProcessingException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/ClientResponseContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Response$Status.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NotAuthorizedException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Variant.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/CacheControl.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/UriInfo.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/client/AsyncInvoker.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbAdapter.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverterProvider.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/SecurityContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent.class {code}",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-06T14:36:01.098+0000",
            "updated": "2021-12-06T14:36:01.098+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17454675",
            "id": "17454675",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "With a quick search, I realized we already have Jira HADOOP-16908 to remove all org.codehaus.jackson usages from the codebase. Let me link it with this Jira. I can take up that work once this is resolved.\r\n\r\nThanks",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-07T14:24:45.794+0000",
            "updated": "2021-12-07T14:24:45.794+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17455021",
            "id": "17455021",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "Merged the PR into trunk. Hi [~vjasani], would you create a PR for branch-3.3?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2021-12-08T07:53:28.333+0000",
            "updated": "2021-12-08T07:53:28.333+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17455047",
            "id": "17455047",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Sure [~aajisaka], I am on it.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-08T08:43:03.853+0000",
            "updated": "2021-12-08T08:43:03.853+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17458150",
            "id": "17458150",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "Committed to branch-3.3. Thank you [~vjasani]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2021-12-13T04:53:26.629+0000",
            "updated": "2021-12-13T04:53:26.629+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555470",
            "id": "17555470",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "guess, this broke the way for tez to upgrade post 3.3.1 and there after for hive as well. Have been discussing the upgrade stuff for Tez internally and I think this is the one, any way we have decided to settle for at 3.3.1. and that works....\r\n\r\n->\r\n{quote}With this upgrade, we will also need to explicitly add new dependency javax.ws.rs:javax.ws.rs-api.\r\n\r\nWe also need to exclude it from shading, else we will get multiple duplicate class clash with existing javax.ws.rs dependencies.\r\n{quote}\r\nMost probably this is the reason, I will create a Jira and try to see what can be done or if something in trunk already sorted this, I see a couple of Jiras linked to this ticket",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-17T08:38:20.037+0000",
            "updated": "2022-06-17T08:38:20.037+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555741",
            "id": "17555741",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "what is the problem here?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2022-06-17T19:00:09.261+0000",
            "updated": "2022-06-17T19:00:09.261+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555743",
            "id": "17555743",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "[~stevel@apache.org] It added javax.ws.rs:javax.ws.rs-api and that isn't shaded also and is conflicting with jsr311-api in tez.\r\n\r\nCan see the error here as well:\r\n\r\n[https://ci-hadoop.apache.org/job/tez-multibranch/job/PR-213/4/testReport/org.apache.tez.dag.history.ats.acls/TestATSHistoryWithACLs/testDagLoggingDisabled/]\r\n\r\n\u00a0\r\n\r\nand someone quoted some problem here as well(I didn't check what is that though):\r\n\r\nhttps://github.com/apache/hadoop/pull/3764#issuecomment-1158641569",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-17T19:04:13.634+0000",
            "updated": "2022-06-17T19:04:13.634+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555804",
            "id": "17555804",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~ayushtkn] Thanks for posting your findings, just had a high level glance at the above failure stacktrace in Tez.\r\n\r\n\u00a0\r\n{quote}at org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager.createTimelineDomain(ATSHistoryACLPolicyManager.java:127)\r\n{quote}\r\nFor this one, is it be convenient to include javax.ws.rs-api in [https://github.com/apache/tez/blob/master/tez-plugins/tez-yarn-timeline-history-with-acls/pom.xml] ? Or does that also conflict with jsr311-api?\r\n\r\nI understand the pain with minor release upgrade when it has to deal with such issues. FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line, given that 3.3 is the latest release line. Let me also check if something can be done in the meantime. (As you already noticed, the problem here is that with shading, we get multiple duplicate class clashes for javax.ws.rs dependencies and hence we have no choice but to remove it from shading)",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T01:38:39.491+0000",
            "updated": "2022-06-18T01:38:39.491+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555805",
            "id": "17555805",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Ah I see, HADOOP-18178 has also bumped Jackson to 2.13.2 in light of fixing CVE-2020-36518, and it has made it's way to 3.3.2 release but I guess the pain related to javax.ws.rs-api remains the same.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T01:45:24.952+0000",
            "updated": "2022-06-18T01:45:24.952+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555848",
            "id": "17555848",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "{quote}For this one, is it be convenient to include javax.ws.rs-api in [https://github.com/apache/tez/blob/master/tez-plugins/tez-yarn-timeline-history-with-acls/pom.xml] ? Or does that also conflict with jsr311-api?\r\n{quote}\r\n\u00a0\r\nThe actual error is :\r\njava.lang.AbstractMethodError: javax.ws.rs.core.UriBuilder.uri(Ljava/lang/String;)Ljavax/ws/rs/core/UriBuilder;\r\n\r\nand this is due to conflict with jsr311-api, \u00a0javax.ws.rs-api already got included as transitive dependency, if I exclude javax.ws.rs-api in Tez it can make the test pass, but we don't want to play with exclusions as we aren't sure what runtime issues it can create",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-18T06:39:49.349+0000",
            "updated": "2022-06-18T06:39:49.349+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555916",
            "id": "17555916",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "Spent some time checking if we have any quick solution or not and see how things are:\r\nBoth {{jsr311-api}} and {{javax.ws.rs-api}} have couple of similar classes and different implementations, That is why this duplicate classes issue started surfacing, I guess Jackson 2 requires implementation classes from {{javax.ws.rs-api}} at runtime or so. In ideal situation we should either have {{javax.ws.rs-api}} or {{js311-api}} in our code, when adding {{javax.ws.rs-api}} if we could have got rid of {{js311-api}} then everything would have been sorted for the shading part. But I guess we have some dependencies on {{{}js311-api{}}}, and it is coming from some other thirdparty libs as well, so may be we have to explore and upgrade them to a version, where they ditch {{js311-api}} for {{{}javax.ws.rs-api{}}}. Then our shading jar should get sorted. How tough is that we don't know, a normal exclude of {{js311-api}} as a transitive dependency isn't a solution because {{javax.ws.rs-api}} has different implementation of methods.\r\n\r\nThe duplicate class exception that we saw here was actually an alarm here that these two dependencies can't stay in peace together, but we got away with that by an exclude...\r\n\r\nNow coming for Tez, Tez still has {{js311-api}} as a dependency, if we some how ditch that and move to {{javax.ws.rs-api}} in hadoop, I am not very sure if Tez too have to adapt to our Jackson version and do the same to get things working..\r\n\r\n\u00a0\r\n{quote}FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line\r\n{quote}\r\nRevert isn't an option now, HADOOP-18178 got its way clear only because of this, else it would have been facing this same issue and would have crashed. Now we have a CVE fixed in 3.3.2 & 3.3.3, we can't get it back in 3.3.4, We won't fix a thirdparty CVE we could have said, but after fixing and claiming we have fixed one, we can't get it back AFAIK, this issue only somehow we have to fix.\r\n\r\n\u00a0\r\n\r\nBTW. I am not sure what Spark and Kyubi issues are exactly, that also seems class conflicts may be.. [~pan3793] can you share some more information about that here",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-18T14:20:15.713+0000",
            "updated": "2022-06-18T14:20:15.713+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555953",
            "id": "17555953",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=chengpan",
              "name": "chengpan",
              "key": "chengpan",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=chengpan&avatarId=52228",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=chengpan&avatarId=52228",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=chengpan&avatarId=52228",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=chengpan&avatarId=52228"
              },
              "displayName": "Cheng Pan",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "> BTW. I am not sure what Spark and Kyuubi issues are exactly, that also seems class conflicts may be\r\n\r\nYes, I think it's because Jackson requires some classes which only exist in javax.ws.rs-api, which are not bundled into the shaded client.\r\n\r\nHave a brief look, js311-api is only required by jersey 1.x? If yes, I think upgrading the jersey to 2.x which depends on javax.ws.rs-api and dropping js311-api may be the right direction. And I also see that Hadoop 3.3.2 mixed use jersey 1.x and 2.x in module hadoop-yarn-applications-catalog-webapp, not sure if it's a good practice.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cml",
              "name": "cml",
              "key": "cml",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Chris Lambertus",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T18:45:20.205+0000",
            "updated": "2022-11-24T00:31:27.910+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555973",
            "id": "17555973",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "{quote}The duplicate class exception that we saw here was actually an alarm here that these two dependencies can't stay in peace together, but we got away with that by an exclude...\r\n{quote}\r\nI don't think it was as simple as \"completely removing jsr311-api from Hadoop\" would allow us to exclude shading javax.ws.rs-api from both hadoop-client-minicluster and hadoop-client-runtime. I have already tried this before, it doesn't work AFAIK. At least, one of them would have to keep the exclusion on.\r\n\r\n\u00a0\r\n\r\nHADOOP-15983 has upgraded all com.sun.jersey dependencies (jersey-core, jersey-servlet etc) to the latest version in 1.x line and the latest version of jersey-core pulls-in jsr311-api with it:\r\n\r\n\u00a0\r\n{code:java}\r\n[INFO] |  +- com.sun.jersey:jersey-core:jar:1.19.4:compile\r\n[INFO] |  |  \\- javax.ws.rs:jsr311-api:jar:1.1.1:compile {code}\r\nI don't think without exclusion (and maybe some additional code change, if JAX-RS 1.x and 2.x incompatibilities are in use), we might be able to get rid of jsr311-api. I would expect the same for Tez and other dependencies as well. Tez and other dependencies also can explore the similar path of excluding jsr311-api completely and only rely on JAX-RS 2.x based javax.ws.rs-api (specifically if already using jersey 2.x release versions).\r\n\r\n\u00a0\r\n\r\njsr311-api is the official spec jar for the JAX-RS 1.x line and the latest central release available is from Nov, 2009 (too old) [https://mvnrepository.com/artifact/javax.ws.rs/jsr311-api]\r\n\r\nwhereas javax.ws.rs-api is jar for JAX-RS 2.x line [https://mvnrepository.com/artifact/javax.ws.rs/javax.ws.rs-api] (latest version from 2018), hence we can expect more upgraded thirdparty libraries (just like Jackson2) having dependency on javax.ws.rs-api and less on jsr311-api. So all downstreamers (Tez, Hadoop, Spark) should try to get rid of jsr311-api anyways, totally agree here. One dependency doesn't necessarily have to wait for another to remove it, for instance, Tez can go ahead with exclusion of jsr311-api even before upgrading to Hadoop 3.3.4 because if not Hadoop, some of it's other dependencies (like jersey-core latest version, as mentioned above) would likely anyways pull it in transitively.\r\n\r\n\u00a0\r\n\r\nOn the other hand, let Hadoop also get rid of jsr311-api. But I am pretty sure, removing it won't solve shading issue completely. Will come up with patch because I do recall I have already tried this as part of this Jira only. We can also run full build QA (all modules) and I can manually verify HDFS, MapReduce and ATSv2 working on pseudo-distributed mode.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T20:10:38.066+0000",
            "updated": "2022-06-18T20:19:25.740+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555977",
            "id": "17555977",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=chengpan",
              "name": "chengpan",
              "key": "chengpan",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=chengpan&avatarId=52228",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=chengpan&avatarId=52228",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=chengpan&avatarId=52228",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=chengpan&avatarId=52228"
              },
              "displayName": "Cheng Pan",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "The mvnrepository[1] tips that com.sun.jersey:jersey-core was moved to [org.glassfish.jersey.core|https://mvnrepository.com/artifact/org.glassfish.jersey.core]\r\n\r\nUpgrading(or migrating) to glassfish jersey 2.x should help.\r\n\r\n[1] [https://mvnrepository.com/artifact/com.sun.jersey/jersey-core]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cml",
              "name": "cml",
              "key": "cml",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Chris Lambertus",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T20:21:28.491+0000",
            "updated": "2022-11-24T00:31:27.812+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555979",
            "id": "17555979",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~pan3793] Thanks for your comments.\r\n{quote}And I also see that Hadoop 3.3.2 mixed use jersey 1.x and 2.x in module hadoop-yarn-applications-catalog-webapp, not sure if it's a good practice.\r\n{quote}\r\nDo you mean jersey-json or\u00a0jersey-media-json-jackson? HADOOP-15983 has been a recent work and jersey-media-json-jackson is a test dependency.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T20:25:53.187+0000",
            "updated": "2022-06-18T20:25:53.187+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555980",
            "id": "17555980",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~pan3793] HADOOP-15984 has migration to Jersey 2.x related efforts going on.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T20:28:37.981+0000",
            "updated": "2022-06-18T20:28:37.981+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555982",
            "id": "17555982",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Remove jsr311-api dependency: [https://github.com/apache/hadoop/pull/4460] (to see how QA results go for now)",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T20:35:18.008+0000",
            "updated": "2022-06-18T20:35:18.008+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555993",
            "id": "17555993",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "Removing jsr311-api from hadoop, will not cause duplicate file exception in shading, because when I tried it didn't...\r\n\r\nBut whether we can do that? because jsr311-api\u00a0 & javax.ws.rs-api aren't compatible with each other. That is one thing I am sure, because that only caused Tez to give that AbstractMethod Error...\r\n\r\nThe build will pass I think, If test fails it is good, at least we will come to know what is broken and what needs to be fixed and we can some how figure out how.\u00a0 It might come green as well, because couple of tests which I tried were passing.(Running whole Hadoop test suite isn't easy to run locally)\r\n\r\nElse if jersey is using jsr311-api, that can create runtime issues, earlier we had jsr311-api in our client jar, now we won't be having that, what impact to downstream projects, that will stay a mystery...\r\n\r\nAtleast we have to try we don't need have any transitive dependency of jsr311-api\r\n\r\nI think downstream projects have to get rid of jsr311-api and upgrade Jackson to adapt to this even if we sort this\r\n\r\nFWIW. Tez is already going ahead with 3.3.1 for the current release:\r\n\r\n[https://lists.apache.org/thread/7sw84rcc729fgw31g0w9h9y9r61tok9d]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-18T21:25:31.326+0000",
            "updated": "2022-06-18T21:25:31.326+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17555999",
            "id": "17555999",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Thanks [~ayushtkn], yes we should hopefully get the full build QA results in ~24 hrs.\u00a0\r\n\r\nOn the shading side, I meant:\r\n{quote}I don't think it was as simple as \"completely removing jsr311-api from Hadoop\" would allow us to exclude shading javax.ws.rs-api from both hadoop-client-minicluster and hadoop-client-runtime\r\n{quote}\r\nI tried this again and the build fails with the same error that I faced earlier:\r\n{code:java}\r\n[INFO] -------< org.apache.hadoop:hadoop-client-check-test-invariants >--------\r\n[INFO] Building Apache Hadoop Client Packaging Invariants for Test 3.4.0-SNAPSHOT [105/112]\r\n[INFO] --------------------------------[ pom ]---------------------------------\r\n[INFO]\u00a0\r\n[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-client-check-test-invariants ---\r\n[INFO] Deleting /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants/target\r\n[INFO] Deleting /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants (includes = [dependency-reduced-pom.xml], excludes = [])\r\n[INFO]\u00a0\r\n[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-client-check-test-invariants ---\r\n[INFO] Executing tasks\r\n\r\n\r\nmain:\r\n\u00a0 \u00a0 [mkdir] Created dir: /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants/target/test-dir\r\n[INFO] Executed tasks\r\n[INFO]\u00a0\r\n[INFO] --- maven-enforcer-plugin:3.0.0:enforce (enforce-banned-dependencies) @ hadoop-client-check-test-invariants ---\r\n[INFO] Adding ignore: module-info\r\n[INFO] Adding ignore: META-INF/versions/*/module-info\r\n[INFO] Adding ignorable dependency: org.apache.hadoop:hadoop-annotations:null\r\n[INFO] \u00a0 Adding ignore: *\r\n[WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:\r\nDuplicate classes found:\r\n\r\n\r\n\u00a0 Found in:\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 Duplicate classes:\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/POST.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbLink.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/NotFoundException.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/PreMatching.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/FeatureContext.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/HttpHeaders.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/PATCH.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent$Builder.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/GenericType.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/sse/SseBroadcaster.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$2.class\r\n\u00a0 \u00a0 org/apache/hadoop/shaded/javax/ws/rs/core/StreamingOutput.class\r\n...\r\n...\r\n... {code}\r\nHence, with the above PR, I have removed exclusion only from hadoop-client-runtime shade. Now we can confirm that these classes are present in hadoop-client-runtime but not on hadoop-client-minicluster jar:\r\n{code:java}\r\n$ jar tf hadoop-client-modules/hadoop-client-runtime/target/hadoop-client-runtime-3.4.0-SNAPSHOT.jar | grep \"AbstractMultivaluedMap\"\r\norg/apache/hadoop/shaded/javax/ws/rs/core/AbstractMultivaluedMap.class\r\n\r\n$ jar tf hadoop-client-modules/hadoop-client-minicluster/target/hadoop-client-minicluster-3.4.0-SNAPSHOT.jar | grep \"AbstractMultivaluedMap\" \r\n(no-output){code}\r\n\u00a0\r\n\r\nOne question for you: how do we determine which client module to shade the new dependency in? Is it always hadoop-client-runtime (for downstreamers)?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T21:58:44.216+0000",
            "updated": "2022-06-18T22:02:24.634+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556000",
            "id": "17556000",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Actually, javax.ws.rs-api is not even clashing with jsr311-api (weird, didn't expect this).\r\n\r\nI just applied this patch, and the build is successful:\r\n{code:java}\r\ndiff --git a/hadoop-client-modules/hadoop-client-runtime/pom.xml b/hadoop-client-modules/hadoop-client-runtime/pom.xml\r\nindex 35fbd7665fb..0879ce1e3bc 100644\r\n--- a/hadoop-client-modules/hadoop-client-runtime/pom.xml\r\n+++ b/hadoop-client-modules/hadoop-client-runtime/pom.xml\r\n@@ -163,7 +163,6 @@\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>org.bouncycastle:*</exclude>\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <!-- Leave snappy that includes native methods which cannot be relocated. -->\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>org.xerial.snappy:*</exclude>\r\n-\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>javax.ws.rs:javax.ws.rs-api</exclude>\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </excludes>\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </artifactSet>\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <filters> {code}\r\nCreated PR [https://github.com/apache/hadoop/pull/4461] for full build results. We can compare QA results for both PRs tomorrow.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T22:20:47.665+0000",
            "updated": "2022-06-18T22:20:47.665+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556001",
            "id": "17556001",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "{quote}Actually, javax.ws.rs-api is not even clashing with jsr311-api (weird, didn't expect this).\r\n{quote}\r\nThe duplicate error that you were quoting was conflict in between hadoop-client-runtime vs hadoop-minicluster :(\r\n\r\nLooks like yes:\r\n{noformat}\r\nDuplicate classes found:\u00a0 Found in:\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile\r\n\u00a0 \u00a0 org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile{noformat}\r\nAFAIK Whatever gets shaded in hadoop-client-runtime we anyway have to exclude from hadoop-minicluster, we can't shade it in both,\u00a0 there are bunch of lines like. : {{exclude everything that comes in via the shaded runtime and api}}\r\n\r\n{{exclude things that came in via transitive in shaded runtime and api}}\r\n\r\nthe problem that [~pan3793] quoted for spark & kyubi should get sorted atleast by this",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-18T22:32:37.450+0000",
            "updated": "2022-06-18T22:32:37.450+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556002",
            "id": "17556002",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Yeah that problem should be at least resolved by #4461 even if #4460 turns out to be complicated. If #4460 is good, maybe we can pursue some testing in ATSv2 and Yarn to verify the basic functionalities with REST APIs.\r\n{quote}The duplicate error that you were quoting was conflict in between hadoop-client-runtime vs hadoop-minicluster\r\n{quote}\r\nUnfortunately, yes i also realized this now :(",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T22:54:22.292+0000",
            "updated": "2022-06-18T22:54:22.292+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556003",
            "id": "17556003",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Once QA results are available, will create new Jira to link both PRs #4460 and #4461.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T22:57:30.002+0000",
            "updated": "2022-06-18T22:57:30.002+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556004",
            "id": "17556004",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=chengpan",
              "name": "chengpan",
              "key": "chengpan",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=chengpan&avatarId=52228",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=chengpan&avatarId=52228",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=chengpan&avatarId=52228",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=chengpan&avatarId=52228"
              },
              "displayName": "Cheng Pan",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "Looks reasonable, thanks [~ayushtkn] and [~vjasani] for investigating this issue. cc [~csun]\u00a0",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cml",
              "name": "cml",
              "key": "cml",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Chris Lambertus",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-18T23:02:15.143+0000",
            "updated": "2022-11-24T00:31:27.841+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556066",
            "id": "17556066",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "Shading issue we might get rid of, because the issue wasn't in the same jar, so that isn't a problem & that might solve problem for some. Tez doesn't pull in hadoop-client dependency AFAIK,\r\n\r\nNow after this it is pulling in both javax.ws.rs-api and jsr311-api which is creating runtime issues like URIBuilder Class is there in both the packages and so.\r\nExcluding/Removing javax.ws.rs-api will make Jackson cry. I tried this as well a lot of test fails.\r\nExcluding/Removing jsr311-api makes Jersey cry. Like TestHttpServer#testJersey fails if we exclude it. I tried this only but some more can fail as well.\r\nThis gives the same error what Tez is getting:\r\n{noformat}\r\n2022-06-19 17:53:24,623 WARN  server.HttpChannel (HttpChannel.java:handleException(689)) - /jersey/foo\r\njava.lang.AbstractMethodError: javax.ws.rs.core.UriBuilder.uri(Ljava/lang/String;)Ljavax/ws/rs/core/UriBuilder;\r\n{noformat}\r\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-06-19T12:26:29.714+0000",
            "updated": "2022-06-19T12:26:29.714+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17556118",
            "id": "17556118",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "{quote}Excluding/Removing javax.ws.rs-api will make Jackson cry.\r\n{quote}\r\n\u00a0\r\n{quote}Excluding/Removing jsr311-api makes Jersey cry.\r\n{quote}\r\nThat is so true.\r\n\r\nUnfortunately we have not received a single QA result on PR#4460 so far. Although Jenkins is getting shut down it seems, at least we have bunch of test results available on PR#4461, hence perhaps some test failures are making builds difficult on 4460. Anyways, we might have to spend another day to see the full results. But we are sure we will have test failures (likely more than expected, as Ayush already mentioned about TestHttpServer#testJersey).\r\n\r\n\u00a0\r\n\r\nOverall, it seems we are back to HADOOP-15984 (upgrading Jersey to 2.x) because so long as we have \"com.sun.jersey:jersey-core\", things will stay complicated.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-06-19T19:55:19.072+0000",
            "updated": "2022-06-19T19:55:19.072+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564664",
            "id": "17564664",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "Thank you [~ayushtkn] [~pan3793] [~vjasani] for your discussion. I'm really sorry for not caching up with.\r\n\r\nIn my past experience, Jersey 2.x upgrade takes a lot of time and I think it will cause some incompatible changes. Therefore I think we should revert the patch and update to 2.12.x latest to avoid the above issue for (at least) Hadoop 3.3.x. What do you think?\r\n\r\nJackson 2.12.x would work because I think the change in  https://github.com/FasterXML/jackson-jaxrs-providers/issues/134 caused the issue and it is only in Jackson 2.13.0 and upper.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2022-07-10T10:25:47.769+0000",
            "updated": "2022-07-10T10:29:45.908+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564665",
            "id": "17564665",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "Thanx [~aajisaka] for checking, I am +1 on revert\r\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-07-10T10:41:34.068+0000",
            "updated": "2022-07-10T10:41:34.068+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564702",
            "id": "17564702",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=chengpan",
              "name": "chengpan",
              "key": "chengpan",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=chengpan&avatarId=52228",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=chengpan&avatarId=52228",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=chengpan&avatarId=52228",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=chengpan&avatarId=52228"
              },
              "displayName": "Cheng Pan",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "Reverting Jackson in the 3.3 branch looks reasonable to me, since Kyuubi and Spark use Hadoop shaded client, downgrading Jackson from 2.13 to 2.12 should not cause another dependency issue.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cml",
              "name": "cml",
              "key": "cml",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Chris Lambertus",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-07-10T16:34:26.971+0000",
            "updated": "2022-11-24T00:31:27.856+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564712",
            "id": "17564712",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "{quote}In my past experience, Jersey 2.x upgrade takes a lot of time and I think it will cause some incompatible changes.\r\n{quote}\r\nI agree that 3.3 subsequent releases should not wait for Jersey 2 because of the sheer volume of changes and incompatibility with Jersey 1.\r\n\r\n\u00a0\r\n\r\nFrom my previous comment:\r\n{quote}FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line, given that 3.3 is the latest release line.\r\n{quote}\r\nwe might have to call out on the Jackson CVE that we claimed to have fixed with 3.3.2 and 3.3.3 and now 3.3.4 would get it exposed with the revert.\r\n\r\n\u00a0\r\n\r\nIIRC, Jersey 1.19 is not flagged by security for active CVEs but Jackson versions <= 2.12 are?\r\n\r\nBut I can understand that since it is breaking downstreamers, it might be worth reverting this and HADOOP-18178 at the expense of known CVE exposure.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-07-10T18:31:30.033+0000",
            "updated": "2022-07-10T19:08:04.103+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564761",
            "id": "17564761",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "bq. we might have to call out on the Jackson CVE\r\n\r\nThe CVE is fixed in 2.12.6.1 or upper (https://github.com/FasterXML/jackson-databind/issues/2816), therefore we should change the version to 2.12.7 (the latest 2.12.x as of now). That way the vulnerability will be still fixed.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2022-07-11T02:27:50.100+0000",
            "updated": "2022-07-11T02:28:39.724+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564780",
            "id": "17564780",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "Note: When reverting this issue, I recommend to use a separate JIRA because the change was released in 3.3.2 and 3.3.3. That way we can easily track what change is in the specific release.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2022-07-11T03:50:45.910+0000",
            "updated": "2022-07-11T03:50:45.910+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564787",
            "id": "17564787",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~aajisaka] do you recommend downgrading to 2.12.7 only for branch-3.3 or for trunk as well? Can trunk continue using Jackson 2.13.2 because Jersey upgrade work is in progress? For 3.4.0 release, HADOOP-15984 can be treated as blocker that way?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-07-11T04:25:42.991+0000",
            "updated": "2022-07-11T04:27:04.533+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564792",
            "id": "17564792",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "[~vjasani] Currently I recommend downgrading to 2.12.7 in both trunk and branch-3.3. That way we don't need to treat HADOOP-15984 as a blocker for 3.4.0.\r\n\r\n(maybe HADOOP-15984 is still a blocker for 3.4.0 regardless of this issue as it blocks compile Hadoop with Java 11)",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2022-07-11T04:35:42.779+0000",
            "updated": "2022-07-11T04:36:39.434+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564839",
            "id": "17564839",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "Well, Jersey upgrade is good to have, for Java-11, kind of new feature support. If we have it in a good and safe manor, But yes I too believe it will have a bunch of incompatible changes, need to see, how to handle that...\r\n\r\nReverting for now in both trunk & branch-3.3, makes sense, it will allow us time and won't be blocking any of the release lines.\r\n\r\nI reverted locally these two commits and pushed a PR. Hopefully  it shouldn't break any tests.\r\nhttps://github.com/apache/hadoop/pull/4544",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-07-11T08:09:41.391+0000",
            "updated": "2022-07-11T08:09:41.391+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564957",
            "id": "17564957",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "ok. someone do the 3.3.x revert and i will get it into the 3.3.4 release i will kick off once it is in",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2022-07-11T11:36:38.122+0000",
            "updated": "2022-07-11T11:36:38.122+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564966",
            "id": "17564966",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "[~ayushtkn] would https://issues.apache.org/jira/browse/HADOOP-18332 be worth trying first before looking to undo jackson/rs-api changes? I've been doing a build locally and so far, at least, things look ok (that jsr311-api dependency can be removed).",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-07-11T11:58:02.960+0000",
            "updated": "2022-07-11T11:58:02.960+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17564983",
            "id": "17564983",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "[~pj.fanning] I think that was tried:\r\nhttps://github.com/apache/hadoop/pull/4460\r\n\r\nTry TestHttpServer#testJersey if it passes with your code change\r\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-07-11T12:44:13.569+0000",
            "updated": "2022-07-11T12:44:13.569+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17565050",
            "id": "17565050",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "So the Tez issue seems (possibly) to be caused by https://github.com/FasterXML/jackson-jaxrs-providers/issues/134 - is it ok to downgrade jackson to 2.12.7? - has latest CVE fixes but not this change",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-07-11T15:06:39.347+0000",
            "updated": "2022-07-11T15:06:39.347+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17565079",
            "id": "17565079",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "{quote}is it ok to downgrade jackson to 2.12.7? - has latest CVE fixes but not this change\r\n{quote}\r\nSounds good to me , if we get rid of javax.ws.rs-api dependency without compromising on the CVE, I think there isn't anything better which we can think of.\r\n[~aajisaka] too pointed that we can explore moving to 2.12.7. Initially this Jira too was raised to move Jackson to 2.12.x latest. I think if the build doesn't complain post removing javax.ws.rs-api and moving to 2.12.7, then we are sorted",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-07-11T16:13:10.022+0000",
            "updated": "2022-07-11T16:13:10.022+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17565145",
            "id": "17565145",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "{quote}Currently I recommend downgrading to 2.12.7 in both trunk and branch-3.3. That way we don't need to treat\u00a0HADOOP-15984\u00a0as a blocker for 3.4.0.\r\n{quote}\r\nI understand that if we are doing the revert with a new Jira, the new Jira should ideally land on trunk before making it's way to active release branches, but Jackson downgrade to 2.12.7 and removal of javax.ws.rs-api would also likely need to be reverted as part of HADOOP-15984, so for HADOOP-15984 it will be too much work staying upto date with trunk (it's already struggling to do so btw with whatever progress is made), and now it will have to reintroduce javax.ws.rs-api and remove jsr311-api. So far I have jsr311-api removed from the current local patch, but if trunk removes javax.ws.rs-api as part of revert of HADOOP-18033 on trunk, there will be rework (basically, revert of revert of HADOOP-18033 for HADOOP-15984 to make progress) that would make the overall progress for HADOOP-15984 even more complicated.\r\n\r\nHence, I am requesting if we could only restrict the revert of HADOOP-18033 for branch-3.3 to unblock 3.3.4 release. IIUC, we are anyways not ready for 3.4.0 release anytime soon?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-07-11T18:46:38.793+0000",
            "updated": "2022-07-11T19:11:11.296+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17565967",
            "id": "17565967",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "i want to kick off a 3.3 3+ cve-only release this week, with the real \"branch-3.3\" coming later.\r\n\r\nwhat do we do here? as the longer we think about this the more PRs to update even more jars will surface",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2022-07-12T18:46:48.261+0000",
            "updated": "2022-07-12T18:46:48.261+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17565978",
            "id": "17565978",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "[~stevel@apache.org] I've had to make a change to https://github.com/apache/hadoop/pull/4547 - there is also https://github.com/apache/hadoop/pull/4544 (which builds ok).\r\n\r\nThe difference is that 4544 uses an older version of Jackson - but both PRs involve downgrading Jackson.\r\n\r\nIs there a branch you would prefer us to target for your 3.3.3+ cve-only release?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-07-12T19:26:13.648+0000",
            "updated": "2022-07-12T19:26:13.648+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17565986",
            "id": "17565986",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "[~stevel@apache.org] I have a PR which reverts the two commits here:\r\n\r\n[https://github.com/apache/hadoop/pull/4544]\r\n\r\nSo, initial thought was to revert those commits and unblock the releases.\u00a0\r\n\r\nThen\u00a0HADOOP-18332 came up with revert 2 + move to Jackson 2.12.7, so we don't expose the CVE as well and remove the new jar which is creating problems. (Let me know if need separate commits, like 2 different revert commits & one upgrade, will do some CLI stuff with HADOOP-18332)\r\n\r\nBoth revert PR & the new PR have green builds, unfortunately I have a review comment on the new one but that is no big stuff and to me that is the final solution, unless other people come and block us. The plan was to try the Tez stuff as well with that change & ask the other folks who flagged Spark issues to try that as well, but considering the timelines, lets not spend too much time there...\r\n\r\n{*}So, in best case should unblock the release by day after{*}, considering the build will take some 24 hours, if updated tomorrow.\r\n\r\nRegarding trunk vs only branch-3.3, in favour of keeping all the branches in sync for now, otherwise if some change comes in trunk which uses this new jar, then we would be doing this revert exercise again and with new set of problems. Moreover no point in keeping the trunk also in broken state.\r\n\r\n[~vjasani] regarding the effort due to this revert activity and so. The best offer I have is \"I can help or worst get some help\", may be with some rebase effort, so this revert activity doesn't become an overhead for you.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-07-12T19:37:06.269+0000",
            "updated": "2022-07-12T19:37:06.269+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17566573",
            "id": "17566573",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "I have approved the PR at HADOOP-18332, Tried the two Tez tests which failed with 3.3.3. They pass locally with those changes. Haven't run all the tests though...\r\n\r\nIt is kind of revert of these changes, So, once folks involved here are convinced with the new changes. It can be merged. Nothing blocking from my side now.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2022-07-14T01:51:54.664+0000",
            "updated": "2022-07-14T01:51:54.664+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17566593",
            "id": "17566593",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Thanks [~ayushtkn], HADOOP-18332 PRs (trunk and 3.3) seem good enough to unblock 3.3.4.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-07-14T02:46:21.918+0000",
            "updated": "2022-07-14T02:46:21.918+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17731624",
            "id": "17731624",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "transient CVE issues (snakeyaml) are generating motivation for upgrading hadoop jackson. \r\n\r\nanyone got an idea about how to do this in a way which could work downstream?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2023-06-12T13:45:50.161+0000",
            "updated": "2023-06-12T13:45:50.161+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17731654",
            "id": "17731654",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "We're stuck on Jackson 2.12 because of jersey v1. Jackson 2.13 has a change that drops support for jersey v1.\r\n\r\nOptions include:\r\n * forking the jackson module for jaxrs to undo the change that drops jersey v1 support\r\n * or removing the dependence on that jackson module by doing https://issues.apache.org/jira/browse/HADOOP-18619\r\n * or completing the move to jersey 2 (https://issues.apache.org/jira/browse/HADOOP-15984)",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=pj.fanning",
              "name": "pj.fanning",
              "key": "pj.fanning",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "PJ Fanning",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-06-12T15:09:45.404+0000",
            "updated": "2023-06-12T15:14:37.004+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17731687",
            "id": "17731687",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "I have almost lost track of this and honestly didn't return back once Tez-Hive upgrade got sorted. :(\u00a0\r\n\r\nJersey upgrade is the best thing to do, but that is stuck, we need that anyway for JDK-11 compile time support as well.\r\n\r\nbut if thats not working, HADOOP-18619 could be a way out, forking would be a trouble during next upgrades and all.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=ayushtkn",
              "name": "ayushtkn",
              "key": "ayushtkn",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Ayush Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2023-06-12T16:39:00.971+0000",
            "updated": "2023-06-12T16:39:00.971+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/17735643",
            "id": "17735643",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "java11, please please please",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2023-06-21T09:59:11.773+0000",
            "updated": "2023-06-21T09:59:11.773+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13415301/comment/18032902",
            "id": "18032902",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "virajjasani closed pull request #4460: HADOOP-18033. [WIP] Remove jsr311-api dependency\nURL: https://github.com/apache/hadoop/pull/4460\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-25T00:26:09.530+0000",
            "updated": "2025-10-25T00:26:09.530+0000"
          }
        ],
        "maxResults": 54,
        "total": 54,
        "startAt": 0
      },
      "customfield_12311820": "0|z0xc7c:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13379741",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13379741",
    "key": "HADOOP-17725",
    "fields": {
      "fixVersions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12350181",
          "id": "12350181",
          "description": "Hadoop 3.3.2",
          "name": "3.3.2",
          "archived": false,
          "released": true,
          "releaseDate": "2022-03-02"
        }
      ],
      "resolution": {
        "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
        "id": "1",
        "description": "A fix for this issue is checked into the tree and tested.",
        "name": "Fixed"
      },
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": 0,
      "customfield_12312330": null,
      "versions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12343764",
          "id": "12343764",
          "description": "3.3.0 release",
          "name": "3.3.0",
          "archived": false,
          "released": true,
          "releaseDate": "2020-07-14"
        }
      ],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [
        {
          "id": "12652503",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12652503",
          "type": {
            "id": "12310560",
            "name": "Problem/Incident",
            "inward": "is caused by",
            "outward": "causes",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310560"
          },
          "outwardIssue": {
            "id": "13506258",
            "key": "HADOOP-18542",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13506258",
            "fields": {
              "summary": "Azure Token provider requires tenant and client IDs despite being optional",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                "id": "1",
                "description": "A problem which impairs or prevents the functions of the product.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                "name": "Bug",
                "subtask": false,
                "avatarId": 21133
              }
            }
          }
        }
      ],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
        "name": "vjasani",
        "key": "vjasani",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Viraj Jasani",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
        "name": "Resolved",
        "id": "5",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
          "id": 3,
          "key": "done",
          "colorName": "green",
          "name": "Done"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12328416",
          "id": "12328416",
          "name": "fs/azure",
          "description": "Azure WASB filesystem client"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12335903",
          "id": "12335903",
          "name": "hadoop-thirdparty"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": 0,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=ivan.sadikov",
        "name": "ivan.sadikov",
        "key": "ivan.sadikov",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "Ivan Sadikov",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=ivan.sadikov",
        "name": "ivan.sadikov",
        "key": "ivan.sadikov",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
        },
        "displayName": "Ivan Sadikov",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "aggregateprogress": {
        "progress": 28800,
        "total": 28800,
        "percent": 100
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 28800,
        "total": 28800,
        "percent": 100
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-17725/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 48,
        "worklogs": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600409",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani opened a new pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041\n\n\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T15:06:01.076+0000",
            "updated": "2021-05-21T15:06:01.076+0000",
            "started": "2021-05-21T15:06:01.076+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600409",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600434",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "sadikovi commented on a change in pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#discussion_r637006602\n\n\n\n##########\nFile path: hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java\n##########\n@@ -361,6 +366,56 @@ public void testAccessTokenProviderPrecedence()\n     testGlobalAndAccountOAuthPrecedence(abfsConf, null, AuthType.OAuth);\n   }\n \n+  @Test\n+  public void testConfigPropNotFound() throws Exception {\n+    final String accountName = \"account\";\n+\n+    final Configuration conf = new Configuration();\n+    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\n+\n+    setAuthConfig(abfsConf, true, AuthType.OAuth);\n+    abfsConf.unset(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT + \".\" + accountName);\n\nReview comment:\n       Hmm... Can you create some kind of for loop to iterate over the keys, otherwise, the code is duplicated quite a bit. I would also suggest creating separate tests for each.\n\n##########\nFile path: hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/ClientCredsTokenProvider.java\n##########\n@@ -38,19 +42,27 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(AccessTokenProvider.class);\n \n-\n   public ClientCredsTokenProvider(final String authEndpoint,\n-                                  final String clientId, final String clientSecret) {\n-\n-    Preconditions.checkNotNull(authEndpoint, \"authEndpoint\");\n-    Preconditions.checkNotNull(clientId, \"clientId\");\n-    Preconditions.checkNotNull(clientSecret, \"clientSecret\");\n+      final String clientId, final String clientSecret)\n+      throws AzureBlobFileSystemException {\n+    validateClientCredsTokenProvider(\n\nReview comment:\n       That is not going to work. You probably want to keep non-null check here, since this class does not depend on Hadoop configuration.\n\n##########\nFile path: hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/ClientCredsTokenProvider.java\n##########\n@@ -38,19 +42,27 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(AccessTokenProvider.class);\n \n-\n   public ClientCredsTokenProvider(final String authEndpoint,\n-                                  final String clientId, final String clientSecret) {\n-\n-    Preconditions.checkNotNull(authEndpoint, \"authEndpoint\");\n-    Preconditions.checkNotNull(clientId, \"clientId\");\n-    Preconditions.checkNotNull(clientSecret, \"clientSecret\");\n+      final String clientId, final String clientSecret)\n+      throws AzureBlobFileSystemException {\n+    validateClientCredsTokenProvider(\n+        ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT, authEndpoint);\n+    validateClientCredsTokenProvider(\n+        ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID, clientId);\n+    validateClientCredsTokenProvider(\n+        ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET, clientSecret);\n \n     this.authEndpoint = authEndpoint;\n     this.clientId = clientId;\n     this.clientSecret = clientSecret;\n   }\n \n+  private void validateClientCredsTokenProvider(final String configName,\n\nReview comment:\n       IMHO, it is better to move it to AbfsConfiguration.java. It will allow you to cover other token providers there as well.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T15:26:39.085+0000",
            "updated": "2021-05-21T15:26:39.085+0000",
            "started": "2021-05-21T15:26:39.085+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600434",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600440",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "sadikovi commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-846035235\n\n\n   I would appreciate it if you could address the comments, thanks.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T15:29:40.171+0000",
            "updated": "2021-05-21T15:29:40.171+0000",
            "started": "2021-05-21T15:29:40.171+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600440",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600443",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "vinaysbadami commented on a change in pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#discussion_r637013026\n\n\n\n##########\nFile path: hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/ClientCredsTokenProvider.java\n##########\n@@ -38,19 +42,27 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(AccessTokenProvider.class);\n \n-\n   public ClientCredsTokenProvider(final String authEndpoint,\n-                                  final String clientId, final String clientSecret) {\n-\n-    Preconditions.checkNotNull(authEndpoint, \"authEndpoint\");\n-    Preconditions.checkNotNull(clientId, \"clientId\");\n-    Preconditions.checkNotNull(clientSecret, \"clientSecret\");\n+      final String clientId, final String clientSecret)\n+      throws AzureBlobFileSystemException {\n+    validateClientCredsTokenProvider(\n+        ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT, authEndpoint);\n+    validateClientCredsTokenProvider(\n+        ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID, clientId);\n+    validateClientCredsTokenProvider(\n+        ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET, clientSecret);\n \n     this.authEndpoint = authEndpoint;\n     this.clientId = clientId;\n     this.clientSecret = clientSecret;\n   }\n \n+  private void validateClientCredsTokenProvider(final String configName,\n\nReview comment:\n       move this to the code which gets the config and instantiates the tokenprovider\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T15:33:29.999+0000",
            "updated": "2021-05-21T15:33:29.999+0000",
            "started": "2021-05-21T15:33:29.999+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600443",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600485",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-846087977\n\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 40s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 29s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  17m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  85m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3041 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell |\r\n   | uname | Linux 7a4eee430293 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8c078c1b40802f3b3bd888ea403c74ccb662ff7b |\r\n   | Default Java | Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/1/testReport/ |\r\n   | Max. process+thread count | 542 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T16:33:04.971+0000",
            "updated": "2021-05-21T16:33:04.971+0000",
            "started": "2021-05-21T16:33:04.971+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600485",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600519",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-846118546\n\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  74m 52s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3041 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell |\r\n   | uname | Linux e960ce344c8d 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 19fe670637850f993228fc352846a89788222cf4 |\r\n   | Default Java | Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/2/testReport/ |\r\n   | Max. process+thread count | 675 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T17:24:24.351+0000",
            "updated": "2021-05-21T17:24:24.351+0000",
            "started": "2021-05-21T17:24:24.351+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600519",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600602",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-846190598\n\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  83m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3041 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell |\r\n   | uname | Linux ab52e391e0eb 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a67a1b20e529973b2776c00e16df75d87bc47eb6 |\r\n   | Default Java | Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/3/testReport/ |\r\n   | Max. process+thread count | 652 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-21T19:18:41.826+0000",
            "updated": "2021-05-21T19:18:41.826+0000",
            "started": "2021-05-21T19:18:41.826+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600602",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/600927",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-846559147\n\n\n   > I would appreciate it if you could address the comments, thanks.\r\n   \r\n   Thanks for the review @sadikovi, I missed out of on other token providers initially. Please take a look.\r\n   Thanks\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-23T12:56:30.651+0000",
            "updated": "2021-05-23T12:56:30.651+0000",
            "started": "2021-05-23T12:56:30.651+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "600927",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/601289",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-847197229\n\n\n   @aajisaka would you like to take a look?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-24T17:04:13.368+0000",
            "updated": "2021-05-24T17:04:13.368+0000",
            "started": "2021-05-24T17:04:13.368+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "601289",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602249",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-848670302\n\n\n   @surendralilhore @goiri @steveloughran Could you please review this PR?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-26T10:52:56.119+0000",
            "updated": "2021-05-26T10:52:56.119+0000",
            "started": "2021-05-26T10:52:56.119+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602249",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602904",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "steveloughran commented on a change in pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#discussion_r640540792\n\n\n\n##########\nFile path: hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java\n##########\n@@ -361,6 +375,37 @@ public void testAccessTokenProviderPrecedence()\n     testGlobalAndAccountOAuthPrecedence(abfsConf, null, AuthType.OAuth);\n   }\n \n+  @Test\n+  public void testConfigPropNotFound() throws Exception {\n+    final String accountName = \"account\";\n+\n+    final Configuration conf = new Configuration();\n+    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\n+\n+    for (String key : CONFIG_KEYS) {\n+      setAuthConfig(abfsConf, true, AuthType.OAuth);\n+      abfsConf.unset(key + \".\" + accountName);\n+      testMissingConfigKey(abfsConf, key);\n+    }\n+\n+    unsetAuthConfig(abfsConf, false);\n+    unsetAuthConfig(abfsConf, true);\n+  }\n+\n+  private void testMissingConfigKey(final AbfsConfiguration abfsConf,\n+      final String confKey) {\n+    try {\n+      abfsConf.getTokenProvider().getClass().getTypeName();\n\nReview comment:\n       Use LambdaTestUtils.intercept. \n\n##########\nFile path: hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\n##########\n@@ -402,6 +402,24 @@ public String getPasswordString(String key) throws IOException {\n     return null;\n   }\n \n+  /**\n+   * Returns a value for the key if the value exists and is not null.\n+   * Otherwise, throws {@link ConfigurationPropertyNotFoundException} with\n+   * key name.\n+   *\n+   * @param key Account-agnostic configuration key\n+   * @return value if exists\n+   * @throws IOException if error in fetching password or if value does not\n\nReview comment:\n       +mention ConfigurationPropertyNotFoundException for missing key\n\n##########\nFile path: hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java\n##########\n@@ -19,8 +19,16 @@\n package org.apache.hadoop.fs.azurebfs;\n \n import java.io.IOException;\n-\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions\n\nReview comment:\n       usual losing battle on import isolation, split from non org.apache stuffl\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-27T11:37:46.328+0000",
            "updated": "2021-05-27T11:37:46.328+0000",
            "started": "2021-05-27T11:37:46.328+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602904",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602905",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus removed a comment on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-846087977\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-27T11:38:40.686+0000",
            "updated": "2021-05-27T11:38:40.686+0000",
            "started": "2021-05-27T11:38:40.686+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602905",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602906",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "steveloughran commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-849561183\n\n\n   > @steveloughran Could you please review this PR?\r\n   \r\n   which endpoint did you run against? Remember: \"no tests: no review\"\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-27T11:39:13.868+0000",
            "updated": "2021-05-27T11:39:13.868+0000",
            "started": "2021-05-27T11:39:13.867+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602906",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602944",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-849599287\n\n\n   > > @steveloughran Could you please review this PR?\r\n   > \r\n   > which endpoint did you run against? Remember: \"no tests: no review\"\r\n   \r\n   Unfortunately, I don't have access to test env as of today (non-tech reasons), sorry about that. However, the reason I offered help for this Jira is because of well written unit tests that I was exploring some time back. I was able to reproduce the exact issue reported by @sadikovi using unit test and then realized that source changes are not complex enough to test on real time test env. Please let me know if this works.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-27T12:39:18.203+0000",
            "updated": "2021-05-27T12:39:18.203+0000",
            "started": "2021-05-27T12:39:18.203+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602944",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602945",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "virajjasani commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-849599813\n\n\n   Addressed comments from recent review cycle.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-27T12:40:03.661+0000",
            "updated": "2021-05-27T12:40:03.661+0000",
            "started": "2021-05-27T12:40:03.660+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602945",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/602998",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "hadoop-yetus commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-849648181\n\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  the patch passed with JDK Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  the patch passed with JDK Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  76m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/3041 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell |\r\n   | uname | Linux bd9a9f098e43 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c8ed0cc25bd094240b6274df447bdc33aa93546c |\r\n   | Default Java | Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.10+9-Ubuntu-0ubuntu1.20.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_282-8u282-b08-0ubuntu1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/4/testReport/ |\r\n   | Max. process+thread count | 687 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3041/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0-SNAPSHOT https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-27T13:46:06.460+0000",
            "updated": "2021-05-27T13:46:06.460+0000",
            "started": "2021-05-27T13:46:06.460+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "602998",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/603482",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "sadikovi commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-850349549\n\n\n   I concur, the issue can be easily reproduced in unit tests and the fix is fairly straightforward. Let me run a manual test with this commit in my dev environment.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-28T11:23:42.088+0000",
            "updated": "2021-05-28T11:23:42.088+0000",
            "started": "2021-05-28T11:23:42.087+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "603482",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/603486",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "sadikovi commented on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-850372026\n\n\n   Tested commit c8ed0cc25bd094240b6274df447bdc33aa93546c. Ran the following code \r\n   \r\n   ```scala\r\n   import org.apache.hadoop.fs._\r\n   \r\n   val conf = spark.sessionState.newHadoopConf\r\n   \r\n   conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\r\n   conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\r\n   conf.set(\"fs.azure.account.oauth2.client.id\", \"<client-id>\")\r\n   // conf.set(\"fs.azure.account.oauth2.client.secret.<account>.dfs.core.windows.net\", \"<client-secret>\")\r\n   // conf.set(\"fs.azure.account.oauth2.client.secret\", \"<client-secret>\")\r\n   conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"https://login.microsoftonline.com/<endpoint>\")\r\n   \r\n   val path = new Path(\"abfss://<container>@<account>.dfs.core.windows.net/\")\r\n   val fs = path.getFileSystem(conf)\r\n   fs.getFileStatus(path)\r\n   ``` \r\n   \r\n   with the storage account in West US, everything seems to be work correctly. If I comment out one or more configs, e.g. client-secret or client-id, the error message is as follows:\r\n   ```\r\n   TokenAccessProviderException: Unable to load OAuth token provider class.\r\n   ...\r\n   Caused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found.\r\n   ...\r\n   ```\r\n   \r\n   The code works when commenting out `fs.azure.account.oauth2.client.secret` or `fs.azure.account.oauth2.client.secret.<account>.dfs.core.windows.net`, the same is applicable to other configs.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-28T12:07:29.539+0000",
            "updated": "2021-05-28T12:07:29.539+0000",
            "started": "2021-05-28T12:07:29.538+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "603486",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/603487",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "sadikovi edited a comment on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-850372026\n\n\n   Tested commit c8ed0cc25bd094240b6274df447bdc33aa93546c. Ran the following code \r\n   \r\n   ```scala\r\n   import org.apache.hadoop.fs._\r\n   \r\n   val conf = new Configuration()\r\n   \r\n   conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\r\n   conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\r\n   conf.set(\"fs.azure.account.oauth2.client.id\", \"<client-id>\")\r\n   // conf.set(\"fs.azure.account.oauth2.client.secret.<account>.dfs.core.windows.net\", \"<client-secret>\")\r\n   // conf.set(\"fs.azure.account.oauth2.client.secret\", \"<client-secret>\")\r\n   conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"https://login.microsoftonline.com/<endpoint>\")\r\n   \r\n   val path = new Path(\"abfss://<container>@<account>.dfs.core.windows.net/\")\r\n   val fs = path.getFileSystem(conf)\r\n   fs.getFileStatus(path)\r\n   ``` \r\n   \r\n   with the storage account in West US, everything seems to be work correctly. If I comment out one or more configs, e.g. client-secret or client-id, the error message is as follows:\r\n   ```\r\n   TokenAccessProviderException: Unable to load OAuth token provider class.\r\n   ...\r\n   Caused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found.\r\n   ...\r\n   ```\r\n   \r\n   The code works when commenting out `fs.azure.account.oauth2.client.secret` or `fs.azure.account.oauth2.client.secret.<account>.dfs.core.windows.net`, the same is applicable to other configs.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-28T12:07:53.836+0000",
            "updated": "2021-05-28T12:07:53.836+0000",
            "started": "2021-05-28T12:07:53.836+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "603487",
            "issueId": "13379741"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/worklog/603488",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "comment": "sadikovi edited a comment on pull request #3041:\nURL: https://github.com/apache/hadoop/pull/3041#issuecomment-850372026\n\n\n   Tested commit c8ed0cc25bd094240b6274df447bdc33aa93546c. Ran the following code \r\n   \r\n   ```scala\r\n   import org.apache.hadoop.conf._\r\n   import org.apache.hadoop.fs._\r\n   \r\n   val conf = new Configuration()\r\n   \r\n   conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\r\n   conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\r\n   conf.set(\"fs.azure.account.oauth2.client.id\", \"<client-id>\")\r\n   // conf.set(\"fs.azure.account.oauth2.client.secret.<account>.dfs.core.windows.net\", \"<client-secret>\")\r\n   // conf.set(\"fs.azure.account.oauth2.client.secret\", \"<client-secret>\")\r\n   conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"https://login.microsoftonline.com/<endpoint>\")\r\n   \r\n   val path = new Path(\"abfss://<container>@<account>.dfs.core.windows.net/\")\r\n   val fs = path.getFileSystem(conf)\r\n   fs.getFileStatus(path)\r\n   ``` \r\n   \r\n   with the storage account in West US, everything seems to be work correctly. If I comment out one or more configs, e.g. client-secret or client-id, the error message is as follows:\r\n   ```\r\n   TokenAccessProviderException: Unable to load OAuth token provider class.\r\n   ...\r\n   Caused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found.\r\n   ...\r\n   ```\r\n   \r\n   The code works when commenting out `fs.azure.account.oauth2.client.secret` or `fs.azure.account.oauth2.client.secret.<account>.dfs.core.windows.net`, the same is applicable to other configs.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
            "created": "2021-05-28T12:08:07.086+0000",
            "updated": "2021-05-28T12:08:07.086+0000",
            "started": "2021-05-28T12:08:07.086+0000",
            "timeSpent": "10m",
            "timeSpentSeconds": 600,
            "id": "603488",
            "issueId": "13379741"
          }
        ]
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
        "id": "4",
        "description": "An improvement or enhancement to an existing feature or task.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
        "name": "Improvement",
        "subtask": false,
        "avatarId": 21140
      },
      "timespent": 28800,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@4db6d004[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@10f3706[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4a171896[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@32d8a1d5[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1b23848f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@7cdc1fb2[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@efd6b17[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@79dca514[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7edc515c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@6261f0ba[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7a19a32[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@1bc97bd7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": 28800,
      "customfield_12312520": null,
      "customfield_12312521": "Sat Oct 25 00:25:35 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": "2022-05-04T10:41:10.000+0000",
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-17725/watchers",
        "watchCount": 7,
        "isWatching": false
      },
      "created": "2021-05-21T11:07:38.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-25T00:25:36.000+0000",
      "timeoriginalestimate": null,
      "description": "It would be good to improve error messages for token providers in ABFS. Currently, when a configuration key is not found or mistyped, the error is not very clear on what went wrong. It would be good to indicate that the key was required but not found in Hadoop configuration when creating a token provider.\r\n\r\nFor example, when running the following code:\r\n{code:java}\r\nimport org.apache.hadoop.conf._\r\nimport org.apache.hadoop.fs._\r\n\r\nval conf = new Configuration()\r\nconf.set(\"fs.azure.account.auth.type\", \"OAuth\")\r\nconf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\r\nconf.set(\"fs.azure.account.oauth2.client.id\", \"my-client-id\")\r\n// conf.set(\"fs.azure.account.oauth2.client.secret.my-account.dfs.core.windows.net\", \"my-secret\")\r\nconf.set(\"fs.azure.account.oauth2.client.endpoint\", \"my-endpoint\")\r\n\r\nval path = new Path(\"abfss://container@my-account.dfs.core.windows.net/\")\r\nval fs = path.getFileSystem(conf)\r\nfs.getFileStatus(path){code}\r\nThe following exception is thrown:\r\n{code:java}\r\nTokenAccessProviderException: Unable to load OAuth token provider class.\r\n...\r\nCaused by: UncheckedExecutionException: java.lang.NullPointerException: clientSecret\r\n...\r\nCaused by: NullPointerException: clientSecret {code}\r\nwhich does not tell what configuration key was not loaded.\r\n\r\n\u00a0\r\n\r\nIMHO, it would be good if the exception was something like this:\r\n{code:java}\r\nTokenAccessProviderException: Unable to load OAuth token provider class.\r\n...\r\nCaused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found. {code}",
      "customfield_10010": null,
      "timetracking": {
        "remainingEstimate": "0h",
        "timeSpent": "8h",
        "remainingEstimateSeconds": 0,
        "timeSpentSeconds": 28800
      },
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12346894",
          "id": "12346894",
          "description": "Hadoop 3.4.0",
          "name": "3.4.0",
          "archived": false,
          "released": true,
          "releaseDate": "2024-03-17"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12350632",
          "id": "12350632",
          "description": "Hadoop 3.3.5 off branch-3.3",
          "name": "3.3.5",
          "archived": false,
          "released": true,
          "releaseDate": "2023-03-22"
        }
      ],
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "Improve error message for token providers in ABFS",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17350021",
            "id": "17350021",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~stevel@apache.org]\u00a0Would you like to take a look at https://github.com/apache/hadoop/pull/3041?\r\n\r\nThanks",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-05-23T12:13:48.802+0000",
            "updated": "2021-05-23T12:55:05.928+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17453941",
            "id": "17453941",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "\r\nThis seems to be triggering some needless failures. We should look hard and see if it is asking for properties which do not always need to be set",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2021-12-06T10:58:36.526+0000",
            "updated": "2021-12-06T10:58:36.526+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17453960",
            "id": "17453960",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "The only parameters that were not covered by Preconditions.checkNotNull are clientId and tenantGuid used by MsiTokenProvider:\r\n{code:java}\r\n} else if (tokenProviderClass == MsiTokenProvider.class) {\r\n  String authEndpoint = getTrimmedPasswordString(\r\n      FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT,\r\n      AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\r\n  String tenantGuid =\r\n      getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT);\r\n  String clientId =\r\n      getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n  String authority = getTrimmedPasswordString(\r\n      FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY,\r\n      AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\r\n  authority = appendSlashIfNeeded(authority);\r\n  tokenProvider = new MsiTokenProvider(authEndpoint, tenantGuid,\r\n      clientId, authority);\r\n  LOG.trace(\"MsiTokenProvider initialized\");\r\n} {code}\r\nIf we replace them back to getPasswordString() and not encounter for precise error message for missing configs, perhaps the failures you are now seeing would be gone. But for any other params that are covered by getMandatoryPasswordString(), they are all also covered by Preconditions.checkNotNull meaning, they would fail with NPE anyways.\r\n\r\n\u00a0\r\n\r\nHence, are the needless failures relevant to MsiTokenProvider? If yes, this patch would revert the behaviour for them:\r\n{code:java}\r\ndiff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\r\nindex 9719da7dc16..d43f1d99a77 100644\r\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\r\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\r\n@@ -812,9 +812,9 @@ public AccessTokenProvider getTokenProvider() throws TokenAccessProviderExceptio\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT,\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String tenantGuid =\r\n-\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT);\r\n+\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT);\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String clientId =\r\n-\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n+\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String authority = getTrimmedPasswordString(\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY,\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); {code}",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-06T11:34:09.560+0000",
            "updated": "2021-12-06T11:34:09.560+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17454733",
            "id": "17454733",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~ivan.sadikov] Would you like to verify above info? If you still have your account setup, could you also create PR and run tests against the accessible zone?\r\n{quote}We should look hard and see if it is asking for properties which do not always need to be set\r\n{quote}\r\nI have confirmed that the above mentioned props (fs.azure.account.oauth2.msi.tenant and\u00a0fs.azure.account.oauth2.client.id) are the ones that are not always required to be set.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-07T16:13:57.761+0000",
            "updated": "2021-12-07T16:13:57.761+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17457541",
            "id": "17457541",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=mehakmeetSingh",
              "name": "mehakmeetSingh",
              "key": "mehakmeetsingh",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Mehakmeet Singh",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "Re-opening as it caused some unexpected failures in our environment. Since, MSI tenant ID and client ID are optional, as stated in:  https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html#OAuth_2.0_Client_Credentials and also discussed above by Viraj and steve, I think we should not mandate them to be set.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=mehakmeetSingh",
              "name": "mehakmeetSingh",
              "key": "mehakmeetsingh",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Mehakmeet Singh",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2021-12-11T05:28:46.581+0000",
            "updated": "2021-12-11T05:29:38.601+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17457542",
            "id": "17457542",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~mehakmeetSingh] I can create addendum PR if you would like but I don't have setup to run against an endpoint as of now. Would you like to run tests against any endpoint you have access to?\r\n\r\nEdit: Managed to get an endpoint to run tests against, but OAuth is not yet setup.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-11T05:45:33.535+0000",
            "updated": "2021-12-11T20:14:46.894+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17457545",
            "id": "17457545",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=mehakmeetSingh",
              "name": "mehakmeetSingh",
              "key": "mehakmeetsingh",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Mehakmeet Singh",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "I can run the tests against an endpoint(on sharedKey auth setup), but I think for this to verify we need OAuth creds, which unfortunately I don't have set up as of now. If [~ivan.sadikov]already have that setup, that's good. We did have some problems setting OAuth tests on our local machines the last time we tried. You can open up a PR in the meantime, and we can start reviewing it.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=mehakmeetSingh",
              "name": "mehakmeetSingh",
              "key": "mehakmeetsingh",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Mehakmeet Singh",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2021-12-11T05:59:52.299+0000",
            "updated": "2021-12-11T05:59:52.299+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17457552",
            "id": "17457552",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Opened the addendum [PR|https://github.com/apache/hadoop/pull/3788].\r\n\r\nUnfortunately, without this addendum, 3.3.2 RC0 might need to sink. However, let [~stevel@apache.org] comment for the final decision.\r\n\r\n\u00a0\r\n\r\nFYI [~csun] [~mehakmeetSingh] [~ivan.sadikov]\u00a0",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-11T07:33:23.830+0000",
            "updated": "2021-12-12T08:11:11.652+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17457969",
            "id": "17457969",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "{quote}We did have some problems setting OAuth tests on our local machines the last time we tried.\r\n{quote}\r\nI am also facing issues with tests using OAuth providers as mentioned on the PR: [https://github.com/apache/hadoop/pull/3788]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-12-12T14:27:23.832+0000",
            "updated": "2021-12-12T14:27:23.832+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17499249",
            "id": "17499249",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=sworisbreathing",
              "name": "sworisbreathing",
              "key": "sworisbreathing",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Steven Swor",
              "active": true,
              "timeZone": "Australia/Sydney"
            },
            "body": "[~vjasani] [~stevel@apache.org] Can this be bumped to a later release so that 3.3.2 can go out? This seems to be the only remaining open issue marked for that release.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=sworisbreathing",
              "name": "sworisbreathing",
              "key": "sworisbreathing",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Steven Swor",
              "active": true,
              "timeZone": "Australia/Sydney"
            },
            "created": "2022-03-01T00:14:52.030+0000",
            "updated": "2022-03-01T00:14:52.030+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17499588",
            "id": "17499588",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~stevel@apache.org] Since the first commit already landed on 3.3.2, I believe we can't remove 3.3.2 from fixVersion. Could you please confirm?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vjasani",
              "name": "vjasani",
              "key": "vjasani",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Viraj Jasani",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2022-03-01T14:45:50.277+0000",
            "updated": "2022-03-01T14:45:50.277+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17531650",
            "id": "17531650",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "3.3.2 has this patch in, so closing as fixed in that version.\r\n\r\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2022-05-04T10:40:57.792+0000",
            "updated": "2022-05-04T10:40:57.792+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17558102",
            "id": "17558102",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=clevasseur",
              "name": "clevasseur",
              "key": "JIRAUSER291518",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Carl",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "The changes related to this ticket force to set optional fields and prevent from relying on Azure's metadata service to set them automatically.\r\n\r\nBoth [~vjasani] 's PR ([https://github.com/apache/hadoop/pull/3788)] and mine fix it ([https://github.com/apache/hadoop/pull/4262|https://github.com/apache/hadoop/pull/4262)]).\r\n\r\nCan we get one of these merged please ?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=clevasseur",
              "name": "clevasseur",
              "key": "JIRAUSER291518",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Carl",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-06-23T13:56:01.194+0000",
            "updated": "2022-06-23T13:56:01.194+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17638658",
            "id": "17638658",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "CLevasseur commented on PR #4262:\nURL: https://github.com/apache/hadoop/pull/4262#issuecomment-1327466165\n\n   Hi @pranavsaxena-microsoft, I've tried to follow[ this section ](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/site/markdown/testing_azure.md#generating-test-run-configurations-and-test-triggers-over-various-config-combinations)of the documentation that you mentioned, but it's outdated:\r\n   \r\n   - The folder `./src/test/resources/accountSettings/` doesn't exist, nor the template file that I should use to create my account settings file\r\n   - `dev-support/testrun-scripts/runtests.sh` should prompt a menu, but in my case it runs `AppendBlob-HNS-OAuth`  and gives me no choice\r\n   \r\n   I followed the rest of the documentation by copying  `./src/test/resources/azure-auth-keys.xml.template` to `./src/test/resources/azure-auth-keys.xml` and replacing those variables by the right values in the xml file:\r\n   - `{ABFS_ACCOUNT_NAME}`\r\n   - `{ACCOUNT_ACCESS_KEY}`\r\n   - `{TENANTID}`\r\n   - `{WASB_ACCOUNT_NAME}`\r\n   - `{WASB_FILESYSTEM}`\r\n   - `{CONTAINER_NAME}`\r\n   - `{ACCOUNT_NAME}`\r\n   \r\n   Then I ran\r\n   ```\r\n   dev-support/testrun-scripts/runtests.sh -c \"NonHNS-SharedKey\"\r\n   [...]\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] BUILD SUCCESS\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] Total time:  11.030 s\r\n   [INFO] Finished at: 2022-11-25T13:08:18Z\r\n   [INFO] ------------------------------------------------------------------------\r\n   \r\n   Running the combination: NonHNS-SharedKey...\r\n   # Then it terminates without running anything\r\n   ```\r\n   \r\n   So it doesn't run the tests, same for `HNS-OAuth` and `HNS-SharedKey`. Do you know what I am missing for those tests to run ?\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-11-25T13:14:31.964+0000",
            "updated": "2022-11-25T13:14:31.964+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17639758",
            "id": "17639758",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on PR #4262:\nURL: https://github.com/apache/hadoop/pull/4262#issuecomment-1328485053\n\n   Hi @CLevasseur , the testing section requires you to have latest trunk commits into your branch (as the testing section has been updated for the trunk). Hence requesting you to kindly back-merge apache:trunk into your branch. \r\n   \r\n   Thanks.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-11-28T03:22:37.022+0000",
            "updated": "2022-11-28T03:22:37.022+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/17639946",
            "id": "17639946",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "CLevasseur commented on PR #4262:\nURL: https://github.com/apache/hadoop/pull/4262#issuecomment-1328860096\n\n   I have pulled the latest changes. It now fails when running the tests. Note that it also fails when I run the tests from the trunk branch of the `apache/hadoop` repository.\r\n   \r\n   I haven't setup OAuth2, is that alright if we just run those tests using the Shared Key ?\r\n   \r\n   Also, it looks like those tests create a lot of containers in the storage account, is there an easy way to clean those ?\r\n   \r\n   **Tests Output (Both apache/hadoop:trunk and CLevasseur/hadoop:trunk give the same errors)**\r\n   \r\n   **NonHNS-SharedKey**:\r\n   \r\n   ```\r\n   Choose action:\r\n   [Note - SET_ACTIVE_TEST_CONFIG will help activate the config for IDE/single test class runs]\r\n   1) SET_ACTIVE_TEST_CONFIG\r\n   2) RUN_TEST\r\n   3) CLEAN_UP_OLD_TEST_CONTAINERS\r\n   4) SET_OR_CHANGE_TEST_ACCOUNT\r\n   5) PRINT_LOG4J_LOG_PATHS_FROM_LAST_RUN\r\n   #? 2\r\n   Enter parallel test run process count [default - 8]:\r\n   \r\n   Set the active test combination to run the action:\r\n   1) HNS-OAuth\r\n   2) HNS-SharedKey\r\n   3) nonHNS-SharedKey\r\n   4) AppendBlob-HNS-OAuth\r\n   5) AllCombinationsTestRun\r\n   6) Quit\r\n   #? 3\r\n   \r\n   Combination specific property setting: [ key=fs.azure.account.auth.type , value=SharedKey ]\r\n   \r\n   Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: dataenginfraus3prod for combination NonHNS-SharedKey\r\n   Running test for combination NonHNS-SharedKey on account dataenginfraus3prod [ProcessCount=8]\r\n    Test run report can be seen in dev-support/testlogs/2022-11-28_10-12-22/Test-Logs-NonHNS-SharedKey.txt\r\n   pcregrep: pcre_exec() gave error -27 while matching text that starts:\r\n   \r\n   [ERROR]   ITestAzureBlobFileSystemMainOperation>FSMainOperationsBaseTest.testGlobStatusFilterWithEmptyPathResults:492 ? AbfsRestOperation\r\n   [ERROR]   ITestAzureBlobF\r\n   \r\n   pcregrep: Error -8, -21 or -27 means that a resource limit was exceeded.\r\n   pcregrep: Check your regex for nested unlimited loops.\r\n   \r\n   ---",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2022-11-28T10:36:34.101+0000",
            "updated": "2022-11-28T10:36:34.101+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13379741/comment/18032899",
            "id": "18032899",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "virajjasani closed pull request #3788: HADOOP-17725. Keep MSI tenant ID and client ID optional (ADDENDUM)\nURL: https://github.com/apache/hadoop/pull/3788\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-25T00:25:35.921+0000",
            "updated": "2025-10-25T00:25:35.921+0000"
          }
        ],
        "maxResults": 17,
        "total": 17,
        "startAt": 0
      },
      "customfield_12311820": "0|z0r95s:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13340414",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13340414",
    "key": "HADOOP-17377",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "0.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12344337",
          "id": "12344337",
          "name": "3.2.1",
          "archived": false,
          "released": true,
          "releaseDate": "2019-09-22"
        }
      ],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [
        {
          "id": "12668306",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12668306",
          "type": {
            "id": "10032",
            "name": "Blocker",
            "inward": "is blocked by",
            "outward": "blocks",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"
          },
          "inwardIssue": {
            "id": "13548316",
            "key": "HADOOP-18860",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13548316",
            "fields": {
              "summary": "Upgrade mockito to 4.11.0",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                "id": "4",
                "description": "An improvement or enhancement to an existing feature or task.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                "name": "Improvement",
                "subtask": false,
                "avatarId": 21140
              }
            }
          }
        },
        {
          "id": "12608827",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12608827",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13284729",
            "key": "HADOOP-16857",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13284729",
            "fields": {
              "summary": "ABFS: Optimize HttpRequest retry triggers",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                "id": "7",
                "description": "The sub-task of the issue",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                "name": "Sub-task",
                "subtask": true,
                "avatarId": 21146
              }
            }
          }
        },
        {
          "id": "12608828",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12608828",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13313370",
            "key": "HADOOP-17092",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13313370",
            "fields": {
              "summary": "ABFS: Long waits and unintended retries when multiple threads try to fetch token using ClientCreds",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                "id": "7",
                "description": "The sub-task of the issue",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                "name": "Sub-task",
                "subtask": true,
                "avatarId": 21146
              }
            }
          }
        },
        {
          "id": "12673017",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12673017",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "outwardIssue": {
            "id": "13558460",
            "key": "HIVE-27884",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13558460",
            "fields": {
              "summary": "LLAP: Reuse FileSystem objects from cache across different tasks in the same LLAP daemon",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                "name": "Closed",
                "id": "6",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                "id": "4",
                "description": "An improvement or enhancement to an existing feature or task.",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                "name": "Improvement",
                "subtask": false,
                "avatarId": 21140
              }
            }
          }
        }
      ],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": null,
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
        "description": "The issue is open and ready for the assignee to start work on it.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
        "name": "Open",
        "id": "1",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
          "id": 2,
          "key": "new",
          "colorName": "blue-gray",
          "name": "To Do"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12328416",
          "id": "12328416",
          "name": "fs/azure",
          "description": "Azure WASB filesystem client"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=brandonvin",
        "name": "brandonvin",
        "key": "brandonvin",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Brandon",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=brandonvin",
        "name": "brandonvin",
        "key": "brandonvin",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Brandon",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-17377/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
        "id": "1",
        "description": "A problem which impairs or prevents the functions of the product.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
        "name": "Bug",
        "subtask": false,
        "avatarId": 21133
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@1602c314[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5cee04ff[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4367ad02[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6f82806d[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@80f1387[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@25bf9300[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5b4e2508[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@203457e4[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5057eb4e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@9a945d7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2365478f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@216caf70[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Fri Oct 24 08:10:35 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-17377/watchers",
        "watchCount": 4,
        "isWatching": false
      },
      "created": "2020-11-13T03:16:31.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-24T08:10:35.000+0000",
      "timeoriginalestimate": null,
      "description": "*Summary*\r\n The instance metadata service has its own guidance for error handling and retry which are different from the Blob store. [https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#error-handling]\r\n\r\nIn particular, it responds with HTTP 429 if request rate is too high. Whereas Blob store will respond with HTTP 503. The retry policy used only accounts for the latter as it will retry any status >=500. This can result in job instability when running multiple processes on the same host.\r\n\r\n*Environment*\r\n * Spark talking to an ABFS store\r\n\r\n * Hadoop 3.2.1\r\n\r\n * Running on an Azure VM with user-assigned identity, ABFS configured to use MsiTokenProvider\r\n\r\n * 6 executor processes on each VM\r\n\r\n*Example*\r\n Here's an example error message and stack trace. It's always the same stack trace. This appears in logs a few hundred to low thousands of times a day. It's luckily skating by since the download operation is wrapped in 3 retries.\r\n{noformat}\r\nAADToken: HTTP connection failed for getting token from AzureAD. Http response: 429 null\r\nContent-Type: application/json; charset=utf-8 Content-Length: 90 Request ID:  Proxies: none\r\nFirst 1K of Body: {\"error\":\"invalid_request\",\"error_description\":\"Temporarily throttled, too many requests\"}\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:190)\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125)\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:506)\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:489)\r\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:208)\r\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:473)\r\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:437)\r\n\tat org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1717)\r\n\tat org.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:747)\r\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:724)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:496)\r\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$7(Executor.scala:812)\r\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$7$adapted(Executor.scala:803)\r\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)\r\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\r\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\r\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\r\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)\r\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748){noformat}\r\n\u00a0CC [~mackrorysd], [~stevel@apache.org]",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": null,
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17232676",
            "id": "17232676",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "[~snvijaya]",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2020-11-16T10:42:04.413+0000",
            "updated": "2020-11-16T10:42:04.413+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17283947",
            "id": "17283947",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=brandonvin",
              "name": "brandonvin",
              "key": "brandonvin",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Brandon",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Another note. Very rarely, I've also seen HTTP 410 errors from the Instance Metadata Service. ABFS currently doesn't retry those. Azure documentation suggests 410 and 500 response codes should be retried: [https://docs.microsoft.com/en-in/azure/virtual-machines/linux/instance-metadata-service?tabs=windows#errors-and-debugging]\r\n\r\nHere's the full error message and stack trace for reference:\r\n{noformat}\r\nAADToken: HTTP connection failed for getting token from AzureAD. Http response: 410 Gone\r\nContent-Type: text/html Content-Length: 35 Request ID:  Proxies: none\r\nFirst 1K of Body: The page you requested was removed.\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:190)\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125)\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:506)\r\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:489)\r\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:208)\r\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:473)\r\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:437)\r\n\tat org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1717){noformat}",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=brandonvin",
              "name": "brandonvin",
              "key": "brandonvin",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Brandon",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2021-02-12T19:54:36.420+0000",
            "updated": "2021-02-12T19:54:36.420+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17654473",
            "id": "17654473",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370941112\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 14s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 39s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 36s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6b243f21df19 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 56c6f7de6ead9500d2c449ba768d2301fc7e4d85 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/testReport/ |\r\n   | Max. process+thread count | 627 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-04T13:39:16.845+0000",
            "updated": "2023-01-04T13:39:16.845+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17654474",
            "id": "17654474",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370942634\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  2s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 103m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ab9041391762 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 56c6f7de6ead9500d2c449ba768d2301fc7e4d85 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/testReport/ |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-04T13:40:25.598+0000",
            "updated": "2023-01-04T13:40:25.598+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17654475",
            "id": "17654475",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370943150\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 40s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  1s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  94m 31s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6f4158513412 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2c25b39b727a259ca0feb754705adbcfe5e80331 |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/testReport/ |\r\n   | Max. process+thread count | 694 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-04T13:40:49.808+0000",
            "updated": "2023-01-04T13:40:49.808+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17654820",
            "id": "17654820",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1371868477\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 40s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 29s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  93m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 7099b9bc66b4 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3be0b00a59ca2e0286bdd44dbb20e3d0f76b8d4d |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/testReport/ |\r\n   | Max. process+thread count | 560 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-05T07:26:14.127+0000",
            "updated": "2023-01-05T07:26:14.127+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17654831",
            "id": "17654831",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1371873722\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  39m 22s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 38s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 16s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  2s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  99m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 8916fc660cd7 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3be0b00a59ca2e0286bdd44dbb20e3d0f76b8d4d |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/testReport/ |\r\n   | Max. process+thread count | 586 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-05T07:34:38.124+0000",
            "updated": "2023-01-05T07:34:38.124+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17654899",
            "id": "17654899",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1372011908\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 38s |  |  trunk passed  |\r\n   | -1 :x: |  javadoc  |   0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  2s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 102m 31s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 7d84852c9f33 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e04011ca7ca13cb0ab3146ee54e5c9bbf7aefddb |\r\n   | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/testReport/ |\r\n   | Max. process+thread count | 534 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-05T10:03:27.516+0000",
            "updated": "2023-01-05T10:03:27.516+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17655292",
            "id": "17655292",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1063226881\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java:\n##########\n@@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) {\n     return retryCount < this.retryCount\n         && (statusCode == -1\n         || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT\n+        || statusCode == HttpURLConnection.HTTP_GONE\n+        || statusCode == HTTP_TOO_MANY_REQUESTS\n\nReview Comment:\n   Should we keep this in AzureADAuthentication condition.\r\n   \r\n   Reason being, now if any API in AbfsHttpOperation get 429 or 410, it will be retried 30 times.\r\n   Right now what would happen in 429 / 410:\r\n   `executeHttpOperation` would give true and in completeExecute after very first call:\r\n   ```\r\n    if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\r\n         throw new AbfsRestOperationException(result.getStatusCode(), result.getStorageErrorCode(),\r\n             result.getStorageErrorMessage(), null, result);\r\n       }\r\n   \r\n   ```\r\n   would throw exception.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-06T08:19:46.987+0000",
            "updated": "2023-01-06T08:19:46.987+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17656022",
            "id": "17656022",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "pranavsaxena-microsoft commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1064481413\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java:\n##########\n@@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) {\n     return retryCount < this.retryCount\n         && (statusCode == -1\n         || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT\n+        || statusCode == HttpURLConnection.HTTP_GONE\n+        || statusCode == HTTP_TOO_MANY_REQUESTS\n\nReview Comment:\n   Removing from ExponentialRetryPolicy, we can have https://github.com/apache/hadoop/pull/5273/files#diff-dff9c93d1668203c206aa1c092aef9d2921dc6e20af8888d06fae34778991531R320-R321 as\r\n   \r\n   ```\r\n   !succeeded && isRecoverableFailure\r\n   \r\n   && (tokenFetchRetryPolicy.shouldRetry(retryCount, httperror)||httpError==429 ||httpError==410);\r\n   ```\r\n   \r\n   reason being, this check is only required in ADAuthenticator.\r\n   \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-09T10:25:43.178+0000",
            "updated": "2023-01-09T10:25:43.178+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17657093",
            "id": "17657093",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1066593607\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java:\n##########\n@@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) {\n     return retryCount < this.retryCount\n         && (statusCode == -1\n         || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT\n+        || statusCode == HttpURLConnection.HTTP_GONE\n+        || statusCode == HTTP_TOO_MANY_REQUESTS\n\nReview Comment:\n   We plan to retry for these status codes, if they come as a response from backend as well. Hence adding these into a centralized exponential retry policy class.\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-01-11T05:47:35.642+0000",
            "updated": "2023-01-11T05:47:35.642+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17707914",
            "id": "17707914",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1494098831\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  20m 56s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  9s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  94m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | JIRA Issue | HADOOP-17377 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 78e6cbb88986 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 62834104a7e75dfb14c6774e9ef4dfc6e803e10e |\r\n   | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/7/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-04-03T10:51:21.565+0000",
            "updated": "2023-04-03T10:51:21.565+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17711386",
            "id": "17711386",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1164119014\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -321,8 +321,23 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>4.11.0</version>\n\nReview Comment:\n   sorry, hadoop-project defines the version, and through properties. revert this\n\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -321,8 +321,23 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>4.11.0</version>\n       <scope>test</scope>\n     </dependency>\n+\n+    <dependency>\n+      <groupId>org.mockito</groupId>\n+      <artifactId>mockito-inline</artifactId>\n+      <version>4.11.0</version>\n\nReview Comment:\n   if this is new to hadoop, declare it in hadoop-project/pom.xml, with versions and exclusions, then declare here without those\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -40,13 +47,16 @@\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY;\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT;\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT;\n+import static org.mockito.Mockito.times;\n \n /**\n  * Test MsiTokenProvider.\n  */\n public final class ITestAbfsMsiTokenProvider\n     extends AbstractAbfsIntegrationTest {\n \n+  private static final int HTTP_TOO_MANY_REQUESTS = 429;\n\nReview Comment:\n   refer to the value in the src/main code\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java:\n##########\n@@ -58,6 +58,13 @@ public class ExponentialRetryPolicy {\n    */\n   private static final double MAX_RANDOM_RATIO = 1.2;\n \n+  /**\n+   * Qualifies for retry based on\n\nReview Comment:\n   needs a . in it, maybe split into\r\n   \"qualifies for retry.\"\r\n   and \"see...\"\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testRetryForThrottling() throws Exception {\n+    AbfsConfiguration conf = getConfiguration();\n+\n+    // Exception to be thrown with throttling error code 429.\n+    AzureADAuthenticator.HttpException httpException\n+        = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS,\n+        \"abc\", \"abc\", \"abc\", \"abc\", \"abc\");\n+\n+    String tenantGuid = \"abcd\";\n+    String clientId = \"abcd\";\n+    String authEndpoint = getTrimmedPasswordString(conf,\n\nReview Comment:\n   what if these are undefined? skip the test?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   * @throws Exception\n\nReview Comment:\n   cut this @throws \n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java:\n##########\n@@ -341,7 +341,7 @@ private static boolean isRecoverableFailure(IOException e) {\n         || e instanceof FileNotFoundException);\n   }\n \n-  private static AzureADToken getTokenSingleCall(String authEndpoint,\n+  public static AzureADToken getTokenSingleCall(String authEndpoint,\n\nReview Comment:\n   now it is public, add a javadoc\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java:\n##########\n@@ -58,6 +58,13 @@ public class ExponentialRetryPolicy {\n    */\n   private static final double MAX_RANDOM_RATIO = 1.2;\n \n+  /**\n+   * Qualifies for retry based on\n+   * https://learn.microsoft.com/en-us/azure/active-directory/\n+   * managed-identities-azure-resources/how-to-use-vm-token#error-handling\n+   */\n+  private static final int HTTP_TOO_MANY_REQUESTS = 429;\n\nReview Comment:\n   make public and refer from tests, maybe put in a different file for this\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-04-12T13:18:42.131+0000",
            "updated": "2023-04-12T13:18:42.131+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757269",
            "id": "17757269",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1687757407\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 54s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 29s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  95m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d2957048b6f1 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cf73a70fc72b27a28f18058b8399784e3d7e891e |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/8/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-22T08:51:16.197+0000",
            "updated": "2023-08-22T08:51:16.197+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757331",
            "id": "17757331",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1687925288\n\n   @anmolanmol1234 still need those (minor) changes -otherwise it is ready to merge\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-22T10:29:25.793+0000",
            "updated": "2023-08-22T10:29:25.793+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757548",
            "id": "17757548",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1301867025\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testRetryForThrottling() throws Exception {\n+    AbfsConfiguration conf = getConfiguration();\n+\n+    // Exception to be thrown with throttling error code 429.\n+    AzureADAuthenticator.HttpException httpException\n+        = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS,\n+        \"abc\", \"abc\", \"abc\", \"abc\", \"abc\");\n+\n+    String tenantGuid = \"abcd\";\n+    String clientId = \"abcd\";\n+    String authEndpoint = getTrimmedPasswordString(conf,\n\nReview Comment:\n   these are hardcoded here, no need for skipping\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-22T16:01:07.074+0000",
            "updated": "2023-08-22T16:01:07.074+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757601",
            "id": "17757601",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1302010099\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsHandlerImpl.java:\n##########\n@@ -192,7 +192,7 @@ public void testMountController() throws IOException {\n     assertTrue(\"cgroup dir should be cerated\", cgroup.mkdirs());\n     //Since we enabled (deferred) cgroup controller mounting, no interactions\n     //should have occurred, with this mock\n-    verifyZeroInteractions(privilegedOperationExecutorMock);\n+    Mockito.verifyNoInteractions(privilegedOperationExecutorMock);\n\nReview Comment:\n   use static import for consistency with the others.\n\n\n\n##########\nhadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java:\n##########\n@@ -24,7 +24,6 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n-import static org.mockito.Matchers.any;\n\nReview Comment:\n   reinstate so this file doesn't change\n\n\n\n##########\nhadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java:\n##########\n@@ -423,7 +423,7 @@ public void testSubclusterDown() throws Exception {\n     FSNamesystem ns0 = nn0.getNamesystem();\n     HAContext nn0haCtx = (HAContext)getInternalState(ns0, \"haContext\");\n     HAContext mockCtx = mock(HAContext.class);\n-    doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(any());\n+    doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(Mockito.any());\n\nReview Comment:\n   return to the existing any() static import\n\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -1308,7 +1308,20 @@\n       <dependency>\n         <groupId>org.mockito</groupId>\n         <artifactId>mockito-core</artifactId>\n-        <version>2.28.2</version>\n+        <version>4.11.0</version>\n\nReview Comment:\n   add a new property mockito.version and reference in both places\n\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/gpu/TestGpuResourceAllocator.java:\n##########\n@@ -210,7 +210,7 @@ private void assertAllocatedGpus(int gpus, int deniedGpus,\n   private void assertNoAllocation(GpuAllocation allocation) {\n     assertEquals(1, allocation.getDeniedGPUs().size());\n     assertEquals(0, allocation.getAllowedGPUs().size());\n-    verifyZeroInteractions(nmStateStore);\n+    Mockito.verifyNoInteractions(nmStateStore);\n\nReview Comment:\n   use static import\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-22T18:09:28.358+0000",
            "updated": "2023-08-22T18:09:28.358+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757808",
            "id": "17757808",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689282173\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 16 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  14m 41s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  10m 41s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   9m 40s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   8m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m 53s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 22s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +0 :ok: |  spotbugs  |   0m 19s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 35s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   5m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  10m  1s |  |  the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  10m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m 39s |  |  the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  javac  |   9m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   2m 24s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 3 new + 86 unchanged - 0 fixed = 89 total (was 86)  |\r\n   | +1 :green_heart: |  mvnsite  |   8m 48s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +0 :ok: |  spotbugs  |   0m 20s |  |  hadoop-project has no data from spotbugs  |\r\n   | -1 :x: |  shadedclient  |  21m 14s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 22s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  16m  5s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 192m 15s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  86m  6s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | -1 :x: |  unit  |  22m 15s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt) |  hadoop-yarn-server-nodemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  19m 25s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  20m 26s |  |  hadoop-yarn-services-core in the patch passed.  |\r\n   | -1 :x: |  unit  | 208m 56s | [/patch-unit-hadoop-yarn-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-unit-hadoop-yarn-project.txt) |  hadoop-yarn-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 26s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  8s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 778m 13s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.linux.runtime.TestDockerContainerRuntime |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.linux.runtime.TestDockerContainerRuntime |\r\n   |   | hadoop.yarn.client.TestRMFailover |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 8b3e21c22fc4 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0688809c43766c8d001a39c972e263d102b2df82 |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/testReport/ |\r\n   | Max. process+thread count | 3325 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-23T04:57:59.506+0000",
            "updated": "2023-08-23T04:57:59.506+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757824",
            "id": "17757824",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689324456\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 18s |  |  https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/10/console |\r\n   | versions | git=2.17.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-23T05:55:57.854+0000",
            "updated": "2023-08-23T05:55:57.854+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757827",
            "id": "17757827",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689328770\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 18s |  |  https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/11/console |\r\n   | versions | git=2.17.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-23T06:01:23.818+0000",
            "updated": "2023-08-23T06:01:23.818+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757828",
            "id": "17757828",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689330087\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 17s |  |  https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/12/console |\r\n   | versions | git=2.17.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-23T06:02:58.232+0000",
            "updated": "2023-08-23T06:02:58.232+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757860",
            "id": "17757860",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689440680\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  32m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  25m 33s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 11s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m  9s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 11s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 10s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 11s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  spotbugs  |   0m 11s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |   2m 25s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 10s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 12s |  |  ASF License check generated no output?  |\r\n   |  |   |  68m  5s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 1b921f5b2a8a 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8b5e8839d108e3d02d0b1b0f872701bd79d8354c |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/testReport/ |\r\n   | Max. process+thread count | 625 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-23T07:35:21.784+0000",
            "updated": "2023-08-23T07:35:21.784+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17757861",
            "id": "17757861",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689442903\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  32m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  25m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  25m 26s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 10s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m  9s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 10s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 10s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 11s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  spotbugs  |   0m 11s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |   2m 27s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 11s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 12s |  |  ASF License check generated no output?  |\r\n   |  |   |  68m  8s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 5b0ba2fb9772 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8b5e8839d108e3d02d0b1b0f872701bd79d8354c |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/testReport/ |\r\n   | Max. process+thread count | 552 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-23T07:37:07.565+0000",
            "updated": "2023-08-23T07:37:07.565+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17759960",
            "id": "17759960",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1697434060\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  35m  9s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  21m 27s |  |  branch has errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 42s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 15s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 21s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 50s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  21m 21s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 29s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  90m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 04bde9dbd212 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7ba573ff39a6fa1ac0a78391d345bd2219fc9c97 |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/testReport/ |\r\n   | Max. process+thread count | 412 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-29T13:20:31.853+0000",
            "updated": "2023-08-29T13:20:31.853+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17760264",
            "id": "17760264",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1698648099\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  25m 29s |  |  branch has errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  25m 44s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 19s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  24m 24s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 47s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  96m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux e4005f0bafa0 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 462a3b6a0c7aadefe39960218092651bad10c979 |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/testReport/ |\r\n   | Max. process+thread count | 455 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-30T07:34:18.219+0000",
            "updated": "2023-08-30T07:34:18.219+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17760517",
            "id": "17760517",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1310529985\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   */\n+  @Test\n+  public void testRetryForThrottling() throws Exception {\n+    AbfsConfiguration conf = getConfiguration();\n+\n+    // Exception to be thrown with throttling error code 429.\n+    AzureADAuthenticator.HttpException httpException\n+        = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS,\n+        \"abc\", \"abc\", \"abc\", \"abc\", \"abc\");\n+\n+    String tenantGuid = \"abcd\";\n+    String clientId = \"abcd\";\n+    String authEndpoint = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\n+    String authority = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\n+\n+    // Mock the getTokenSingleCall to throw exception so the retry logic comes into place.\n+    try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic(\n+        AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) {\n+      adAuthenticator.when(\n+          () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(),\n+              Mockito.anyString(), Mockito.any(), Mockito.anyString(),\n+              Mockito.anyBoolean())).thenThrow(httpException);\n+\n+      // Mock the tokenFetchRetryPolicy to verify retries.\n+      ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy(\n+          conf.getOauthTokenFetchRetryPolicy());\n+      Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField(\n\nReview Comment:\n   this kind of stuff is trouble as it makes maintenance a nightmare; you can't see where the field is access, all you have is a mocking test failing.\r\n   \r\n   proposed: add a static setter to AzureADAuthenticator; mark as @VisibleForTesting. \n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-30T16:29:27.852+0000",
            "updated": "2023-08-30T16:29:27.852+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17760518",
            "id": "17760518",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1310534958\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   */\n+  @Test\n+  public void testRetryForThrottling() throws Exception {\n+    AbfsConfiguration conf = getConfiguration();\n+\n+    // Exception to be thrown with throttling error code 429.\n+    AzureADAuthenticator.HttpException httpException\n+        = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS,\n+        \"abc\", \"abc\", \"abc\", \"abc\", \"abc\");\n+\n+    String tenantGuid = \"abcd\";\n+    String clientId = \"abcd\";\n+    String authEndpoint = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\n+    String authority = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\n+\n+    // Mock the getTokenSingleCall to throw exception so the retry logic comes into place.\n+    try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic(\n+        AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) {\n+      adAuthenticator.when(\n+          () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(),\n+              Mockito.anyString(), Mockito.any(), Mockito.anyString(),\n+              Mockito.anyBoolean())).thenThrow(httpException);\n+\n+      // Mock the tokenFetchRetryPolicy to verify retries.\n+      ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy(\n+          conf.getOauthTokenFetchRetryPolicy());\n+      Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField(\n+          \"tokenFetchRetryPolicy\");\n+      tokenFetchRetryPolicy.setAccessible(true);\n+      tokenFetchRetryPolicy.set(ExponentialRetryPolicy.class,\n+          exponentialRetryPolicy);\n+\n+      AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint,\n+          tenantGuid, clientId, authority);\n+      AzureADToken token = null;\n+      intercept(AzureADAuthenticator.HttpException.class,\n+          tokenProvider::getToken);\n+\n+      // If the status code doesn't qualify for retry shouldRetry returns false and the loop ends.\n+      // It being called multiple times verifies that the retry was done for the throttling status code 429.\n+      Mockito.verify(exponentialRetryPolicy,\n\nReview Comment:\n   so ExponentialRetryPolicy.getRetryCount() is there to let you pass a non-mocked policy in and then assert on it. how about using that here? it probably needs making the accessors public, rather than package scoped, but that's all. The less use we make of mockito, the less things will break with every mockito upgrade\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-30T16:29:42.859+0000",
            "updated": "2023-08-30T16:29:42.859+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17760521",
            "id": "17760521",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1310523986\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -321,8 +321,23 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>4.11.0</version>\n\nReview Comment:\n   again, cut this now; the version in hadoop project is the one you now expect\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-30T16:32:58.056+0000",
            "updated": "2023-08-30T16:32:58.056+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17760975",
            "id": "17760975",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1701148507\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 50s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  27m 31s |  |  branch has errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  27m 51s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 17s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 21s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 50s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  26m  8s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 54s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 31s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 102m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 7a1812c7f588 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 78329dece95a9e2d53d4f523a95c3325b09a2d6d |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/testReport/ |\r\n   | Max. process+thread count | 455 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-08-31T14:21:16.718+0000",
            "updated": "2023-08-31T14:21:16.718+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17782689",
            "id": "17782689",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=agnes",
              "name": "agnes",
              "key": "agnes",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
              },
              "displayName": "Agnes Tevesz",
              "active": true,
              "timeZone": "America/Chicago"
            },
            "body": "[~stevel@apache.org] [~brandonvin] Can you help to move this change forward? Who should be the owner of this task? The ticket is not assigned to anyone and there is no activity on the change since end of August. This fix should land in hadoop. The pod identity in azure was deprecated: [https://github.com/Azure/aad-pod-identity] If we get the token directly from the instance metadata service we hit this HTTP 429 issue with tpcds tests very frequently: [https://azure.github.io/azure-workload-identity/docs/] The pod identity component most likely provided the retry logic before, but we cannot install depreciated components on an AKS cluster. Can this change get finished?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=agnes",
              "name": "agnes",
              "key": "agnes",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
              },
              "displayName": "Agnes Tevesz",
              "active": true,
              "timeZone": "America/Chicago"
            },
            "created": "2023-11-03T18:08:37.050+0000",
            "updated": "2023-11-03T18:08:37.050+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17785506",
            "id": "17785506",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "nandorKollar commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1808268461\n\n   I think this this PR is great, however there's still one related open problem: the default values (2) for `fs.azure.oauth.token.fetch.retry.delta.backoff` is incorrect. The value of 2 is consistent with MS recommendation (https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/how-to-use-vm-token#retry-guidance), but it is assumed in **seconds**, but as this is used in Thread.sleep [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L326), it will be measured in **milliseconds**. I think we should change the default to 2000. @steveloughran @anmolanmol1234 do you think we can implement this minimal change in this PR, or we should open a separate one?\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-11-13T14:28:06.614+0000",
            "updated": "2023-11-13T14:28:06.614+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17785663",
            "id": "17785663",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1809005084\n\n   I'll go with whatever @saxenapranav thinks here...we have seen this ourselves and need a fix.\r\n   \r\n   However, that PR to update mockito bounced, so either\r\n   1. another attempt is made to update mockito, including the shaded client\r\n   2. this PR can be done without updating mockito (easier)\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-11-13T20:20:23.334+0000",
            "updated": "2023-11-13T20:20:23.334+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17786173",
            "id": "17786173",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811834929\n\n   > I'll go with whatever @saxenapranav thinks here...we have seen this ourselves and need a fix.\r\n   > \r\n   > However, that PR to update mockito bounced, so either\r\n   > \r\n   > 1. another attempt is made to update mockito, including the shaded client\r\n   > 2. this PR can be done without updating mockito (easier)\r\n   \r\n   The mockito upgrade was needed as part of this PR to mock static methods. So would it be fine if we remove that test method or if not I will attempt to upgrade mockito, including the shaded client.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-11-15T05:31:10.213+0000",
            "updated": "2023-11-15T05:31:10.213+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17786175",
            "id": "17786175",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811839555\n\n   > I think this this PR is great, however there's still one related open problem: the default values (2) for `fs.azure.oauth.token.fetch.retry.delta.backoff` is incorrect. The value of 2 is consistent with MS recommendation (https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/how-to-use-vm-token#retry-guidance), but it is assumed in **seconds**, but as this is used in Thread.sleep [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L326), it will be measured in **milliseconds**. I think we should change the default to 2000. @steveloughran @anmolanmol1234 do you think we can implement this minimal change in this PR, or we should open a separate one?\r\n   \r\n   Will update this change as an iteration of this PR, but will some time for the mockito upgrade PR.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-11-15T05:37:11.864+0000",
            "updated": "2023-11-15T05:37:11.864+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17786189",
            "id": "17786189",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811896245\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 20s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 21s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  spotbugs  |   0m 21s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   2m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |   2m 42s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 20s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  javac  |   0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 20s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  spotbugs  |   0m 20s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 20s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 20s |  |  ASF License check generated no output?  |\r\n   |  |   |  12m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 58c5a67f711c 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b0563a19157c485d782faa96d8300020c408ea8e |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/testReport/ |\r\n   | Max. process+thread count | 24 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-11-15T06:50:10.145+0000",
            "updated": "2023-11-15T06:50:10.145+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17786191",
            "id": "17786191",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811906117\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 20s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 20s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 20s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   4m 44s |  |  trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   7m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |   7m 32s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 21s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  javac  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m  9s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m  9s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05.  |\r\n   | -1 :x: |  spotbugs  |   0m  9s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |   2m 52s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m  9s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 11s |  |  ASF License check generated no output?  |\r\n   |  |   |  15m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 5348f26896a5 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 225541ef7a765d856eb28966b7340744fe049654 |\r\n   | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/testReport/ |\r\n   | Max. process+thread count | 88 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-11-15T07:00:48.461+0000",
            "updated": "2023-11-15T07:00:48.461+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/17794370",
            "id": "17794370",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r1419336218\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -323,6 +323,13 @@\n       <artifactId>mockito-core</artifactId>\n       <scope>test</scope>\n     </dependency>\n+\n+    <dependency>\n\nReview Comment:\n   is this needed? because its not in the base project pom.\r\n   \r\n   I would rather this PR doesn't need that mockito upgrade as mockito upgrades are always a painful piece of work which never gets backported.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -48,7 +48,7 @@ public final class FileSystemConfigurations {\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS = 5;\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL = 0;\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS;\n-  public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2;\n+  public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2 * 1000;\n\nReview Comment:\n   use 2_000\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-12-07T17:33:07.968+0000",
            "updated": "2023-12-07T17:33:07.968+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032105",
            "id": "18032105",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3432466591\n\n   @anujmodi2021 can you revisit this so I can get it in?\r\n   \r\n   we do appear to have been using it internally since december 2023, so I'm happy it works.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-22T13:49:38.403+0000",
            "updated": "2025-10-22T13:49:38.403+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032374",
            "id": "18032374",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3435471034\n\n   @steveloughran will backport the PR for merge\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T07:10:25.605+0000",
            "updated": "2025-10-23T07:10:25.605+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032510",
            "id": "18032510",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437618030\n\n   fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`)\r\n   \r\n   ```\r\n   [ERROR]   ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1)                                                                                                                                                                                   \r\n   [ERROR]   ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit                                                                                                                                                      \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)                                                                                                                     \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                                                                  \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src  \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src      \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src            \r\n   [ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src          \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1)                                                                                                           \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                   \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                             \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                 \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                       \r\n   [ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                     \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1)                                                                                                                                                                                       \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                     \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src               \r\n   [ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src             \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1)                                                                                                                   \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                 \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src           \r\n   [ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n   [ERROR]   ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1)                                                                                        \r\n   [ERROR]   ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                             \r\n   [ERROR]   ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src     \r\n   [ERROR]   ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                   \r\n   [INFO] \r\n   [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13\r\n   [INFO] \r\n   ```\r\n   \r\n   This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h\r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T15:23:57.701+0000",
            "updated": "2025-10-23T15:23:57.701+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032513",
            "id": "18032513",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437685484\n\n   When these uploads fail we do leave incomplete uploads in progress:\r\n   ```\r\n   Listing uploads under path \"\"\r\n   job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98\r\n   job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I\r\n   job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\n   job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV\r\n   job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh\r\n   job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC\r\n   job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw--\r\n   job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq\r\n   test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA--\r\n   test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA--\r\n   Total 10 uploads found.\r\n   ```\r\n   \r\n   Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. \r\n   \r\n   The attempt to complete failed.\r\n   ```\r\n   [ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190 \u00bb AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1)          \r\n   ```\r\n   \r\n   Yet the uploads list afterwards finds it\r\n   ```\r\n   job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\n   ```\r\n   \r\n   I have to conclude that the list of pending uploads was briefly offline/inconsistent.\r\n   \r\n   This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute.\r\n   \r\n   Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\". \r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T15:34:46.844+0000",
            "updated": "2025-10-23T15:34:46.844+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032516",
            "id": "18032516",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "steveloughran commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437730498\n\n   sorry, commenting on wrong PR. will cut.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T15:43:21.320+0000",
            "updated": "2025-10-23T15:43:21.320+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032530",
            "id": "18032530",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438000237\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 48s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 35s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  30m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  30m 40s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 58s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 31s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 116m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 8088428d1bb5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c7d46718136c7e1585bcd3f1d82becc8446f0b50 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/testReport/ |\r\n   | Max. process+thread count | 611 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:36:18.506+0000",
            "updated": "2025-10-23T16:36:18.506+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032536",
            "id": "18032536",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r2456024943\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -321,8 +321,23 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>4.11.0</version>\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -321,8 +321,23 @@\n     <dependency>\n       <groupId>org.mockito</groupId>\n       <artifactId>mockito-core</artifactId>\n+      <version>4.11.0</version>\n       <scope>test</scope>\n     </dependency>\n+\n+    <dependency>\n+      <groupId>org.mockito</groupId>\n+      <artifactId>mockito-inline</artifactId>\n+      <version>4.11.0</version>\n\nReview Comment:\n   removed dependency\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:49:17.443+0000",
            "updated": "2025-10-23T16:49:17.443+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032538",
            "id": "18032538",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r2456032575\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java:\n##########\n@@ -58,6 +58,13 @@ public class ExponentialRetryPolicy {\n    */\n   private static final double MAX_RANDOM_RATIO = 1.2;\n \n+  /**\n+   * Qualifies for retry based on\n+   * https://learn.microsoft.com/en-us/azure/active-directory/\n+   * managed-identities-azure-resources/how-to-use-vm-token#error-handling\n+   */\n+  private static final int HTTP_TOO_MANY_REQUESTS = 429;\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -40,13 +47,16 @@\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY;\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT;\n import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT;\n+import static org.mockito.Mockito.times;\n \n /**\n  * Test MsiTokenProvider.\n  */\n public final class ITestAbfsMsiTokenProvider\n     extends AbstractAbfsIntegrationTest {\n \n+  private static final int HTTP_TOO_MANY_REQUESTS = 429;\n\nReview Comment:\n   taken\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:50:42.784+0000",
            "updated": "2025-10-23T16:50:42.784+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032541",
            "id": "18032541",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r2456039983\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   */\n+  @Test\n+  public void testRetryForThrottling() throws Exception {\n+    AbfsConfiguration conf = getConfiguration();\n+\n+    // Exception to be thrown with throttling error code 429.\n+    AzureADAuthenticator.HttpException httpException\n+        = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS,\n+        \"abc\", \"abc\", \"abc\", \"abc\", \"abc\");\n+\n+    String tenantGuid = \"abcd\";\n+    String clientId = \"abcd\";\n+    String authEndpoint = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\n+    String authority = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\n+\n+    // Mock the getTokenSingleCall to throw exception so the retry logic comes into place.\n+    try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic(\n+        AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) {\n+      adAuthenticator.when(\n+          () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(),\n+              Mockito.anyString(), Mockito.any(), Mockito.anyString(),\n+              Mockito.anyBoolean())).thenThrow(httpException);\n+\n+      // Mock the tokenFetchRetryPolicy to verify retries.\n+      ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy(\n+          conf.getOauthTokenFetchRetryPolicy());\n+      Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField(\n\nReview Comment:\n   taken\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:52:12.402+0000",
            "updated": "2025-10-23T16:52:12.402+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032542",
            "id": "18032542",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r2456041107\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java:\n##########\n@@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key,\n     return value.trim();\n   }\n \n+  /**\n+   * Test to verify that token fetch is retried for throttling errors (too many requests 429).\n+   */\n+  @Test\n+  public void testRetryForThrottling() throws Exception {\n+    AbfsConfiguration conf = getConfiguration();\n+\n+    // Exception to be thrown with throttling error code 429.\n+    AzureADAuthenticator.HttpException httpException\n+        = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS,\n+        \"abc\", \"abc\", \"abc\", \"abc\", \"abc\");\n+\n+    String tenantGuid = \"abcd\";\n+    String clientId = \"abcd\";\n+    String authEndpoint = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\n+    String authority = getTrimmedPasswordString(conf,\n+        FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY,\n+        DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\n+\n+    // Mock the getTokenSingleCall to throw exception so the retry logic comes into place.\n+    try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic(\n+        AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) {\n+      adAuthenticator.when(\n+          () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(),\n+              Mockito.anyString(), Mockito.any(), Mockito.anyString(),\n+              Mockito.anyBoolean())).thenThrow(httpException);\n+\n+      // Mock the tokenFetchRetryPolicy to verify retries.\n+      ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy(\n+          conf.getOauthTokenFetchRetryPolicy());\n+      Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField(\n+          \"tokenFetchRetryPolicy\");\n+      tokenFetchRetryPolicy.setAccessible(true);\n+      tokenFetchRetryPolicy.set(ExponentialRetryPolicy.class,\n+          exponentialRetryPolicy);\n+\n+      AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint,\n+          tenantGuid, clientId, authority);\n+      AzureADToken token = null;\n+      intercept(AzureADAuthenticator.HttpException.class,\n+          tokenProvider::getToken);\n+\n+      // If the status code doesn't qualify for retry shouldRetry returns false and the loop ends.\n+      // It being called multiple times verifies that the retry was done for the throttling status code 429.\n+      Mockito.verify(exponentialRetryPolicy,\n\nReview Comment:\n   taken\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:52:26.382+0000",
            "updated": "2025-10-23T16:52:26.382+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032543",
            "id": "18032543",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r2456045985\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -48,7 +48,7 @@ public final class FileSystemConfigurations {\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS = 5;\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL = 0;\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS;\n-  public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2;\n+  public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2 * 1000;\n\nReview Comment:\n   taken\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:53:23.884+0000",
            "updated": "2025-10-23T16:53:23.884+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032544",
            "id": "18032544",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on code in PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#discussion_r2456046384\n\n\n##########\nhadoop-tools/hadoop-azure/pom.xml:\n##########\n@@ -323,6 +323,13 @@\n       <artifactId>mockito-core</artifactId>\n       <scope>test</scope>\n     </dependency>\n+\n+    <dependency>\n\nReview Comment:\n   removed dependency\n\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T16:53:38.055+0000",
            "updated": "2025-10-23T16:53:38.055+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032561",
            "id": "18032561",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438376772\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  16m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  17m  2s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  16m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 12s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  69m 12s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux df85de01587f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fd260666ade501f736fe7af3476e96f6231f2f0b |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/testReport/ |\r\n   | Max. process+thread count | 613 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T17:55:35.975+0000",
            "updated": "2025-10-23T17:55:35.975+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032571",
            "id": "18032571",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "hadoop-yetus commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438560491\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 48s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 26s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  27m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  28m 14s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 55s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 110m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/5273 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c145c3f5b775 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fd260666ade501f736fe7af3476e96f6231f2f0b |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/testReport/ |\r\n   | Max. process+thread count | 629 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-23T18:38:58.039+0000",
            "updated": "2025-10-23T18:38:58.039+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032695",
            "id": "18032695",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anujmodi2021 commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3441679165\n\n   Thanks @anmolanmol1234 for refreshing up this.\r\n   @steveloughran this LGTM now and should be ready to merge.\r\n   \r\n   The spotbugs adnd javadoc warnings are due to https://issues.apache.org/jira/browse/HADOOP-19731\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-24T08:04:04.099+0000",
            "updated": "2025-10-24T08:04:04.099+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13340414/comment/18032698",
            "id": "18032698",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "anmolanmol1234 commented on PR #5273:\nURL: https://github.com/apache/hadoop/pull/5273#issuecomment-3441710243\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 873, Failures: 0, Errors: 0, Skipped: 217\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 169\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 715, Failures: 0, Errors: 0, Skipped: 282\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 873, Failures: 0, Errors: 0, Skipped: 228\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 722, Failures: 0, Errors: 0, Skipped: 140\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 712, Failures: 0, Errors: 0, Skipped: 284\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 719, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 198\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 747, Failures: 0, Errors: 0, Skipped: 226\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 712, Failures: 0, Errors: 0, Skipped: 281\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-24T08:10:35.531+0000",
            "updated": "2025-10-24T08:10:35.531+0000"
          }
        ],
        "maxResults": 53,
        "total": 53,
        "startAt": 0
      },
      "customfield_12311820": "0|z0kjyw:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13297259",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/13297259",
    "key": "HADOOP-16964",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
        "name": "Minor",
        "id": "4"
      },
      "labels": [],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "1.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12344337",
          "id": "12344337",
          "name": "3.2.1",
          "archived": false,
          "released": true,
          "releaseDate": "2019-09-22"
        }
      ],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": null,
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
        "description": "The issue is open and ready for the assignee to start work on it.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
        "name": "Open",
        "id": "1",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
          "id": 2,
          "key": "new",
          "colorName": "blue-gray",
          "name": "To Do"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12310689",
          "id": "12310689",
          "name": "fs",
          "description": "Generic FileSystem code"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
        "name": "bianqi",
        "key": "bianqi",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
        },
        "displayName": "bianqi",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
        "name": "bianqi",
        "key": "bianqi",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
        },
        "displayName": "bianqi",
        "active": true,
        "timeZone": "Etc/UTC"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16964/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
        "id": "4",
        "description": "An improvement or enhancement to an existing feature or task.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
        "name": "Improvement",
        "subtask": false,
        "avatarId": 21140
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@28010b4[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@211e4f0d[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6536bc4e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@3683f1c2[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@53785f93[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@3c0bacb5[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6c0fa3b6[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@45ab4214[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@543e19f5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@35b169c3[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@24b26ed0[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@3b9633b0[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Fri Apr 10 02:00:46 UTC 2020",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16964/watchers",
        "watchCount": 3,
        "isWatching": false
      },
      "created": "2020-04-09T04:17:47.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-20T16:40:10.000+0000",
      "timeoriginalestimate": null,
      "description": "Modify constant for AbstractFileSystem",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": null,
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12999708",
          "id": "12999708",
          "filename": "HADOOP-16964-001.patch",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
            "name": "bianqi",
            "key": "bianqi",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
            },
            "displayName": "bianqi",
            "active": true,
            "timeZone": "Etc/UTC"
          },
          "created": "2020-04-12T16:05:56.011+0000",
          "size": 756,
          "mimeType": "text/plain",
          "content": "https://issues.apache.org/jira/secure/attachment/12999708/HADOOP-16964-001.patch"
        }
      ],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "Modify constant for AbstractFileSystem",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13297259/comment/17078901",
            "id": "17078901",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
              "name": "bianqi",
              "key": "bianqi",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
              },
              "displayName": "bianqi",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "Change hard coding to constant\u00a0",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
              "name": "bianqi",
              "key": "bianqi",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
              },
              "displayName": "bianqi",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2020-04-09T04:19:33.790+0000",
            "updated": "2020-04-09T04:19:33.790+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13297259/comment/17079650",
            "id": "17079650",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "a good little cleanup.\r\n\r\nCould you submit it as a github PR against trunk? That will kick off the automated testing",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2020-04-09T18:27:18.952+0000",
            "updated": "2020-04-09T18:27:18.952+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13297259/comment/17080165",
            "id": "17080165",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
              "name": "bianqi",
              "key": "bianqi",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
              },
              "displayName": "bianqi",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "[~stevel@apache.org]\u00a0Thank review.\u00a0\u00a0I have submit it as a github PR against trunk.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bianqi",
              "name": "bianqi",
              "key": "bianqi",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bianqi&avatarId=39717",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bianqi&avatarId=39717",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bianqi&avatarId=39717",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bianqi&avatarId=39717"
              },
              "displayName": "bianqi",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2020-04-10T02:00:46.751+0000",
            "updated": "2020-04-10T02:00:46.751+0000"
          }
        ],
        "maxResults": 3,
        "total": 3,
        "startAt": 0
      },
      "customfield_12311820": "0|z0dfvk:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "12971246",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/12971246",
    "key": "HADOOP-13184",
    "fields": {
      "fixVersions": [],
      "resolution": {
        "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
        "id": "1",
        "description": "A fix for this issue is checked into the tree and tested.",
        "name": "Fixed"
      },
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "12.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [
        {
          "id": "12484050",
          "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12484050",
          "type": {
            "id": "10030",
            "name": "Reference",
            "inward": "is related to",
            "outward": "relates to",
            "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
          },
          "inwardIssue": {
            "id": "13014656",
            "key": "YARN-5772",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13014656",
            "fields": {
              "summary": "Replace old Hadoop logo with new one",
              "status": {
                "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                "name": "Resolved",
                "id": "5",
                "statusCategory": {
                  "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                  "id": 3,
                  "key": "done",
                  "colorName": "green",
                  "name": "Done"
                }
              },
              "priority": {
                "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                "name": "Major",
                "id": "3"
              },
              "issuetype": {
                "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                "id": "7",
                "description": "The sub-task of the issue",
                "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                "name": "Sub-task",
                "subtask": true,
                "avatarId": 21146
              }
            }
          }
        }
      ],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
        "name": "kspk",
        "key": "kspk",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Abhishek",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
        "name": "Resolved",
        "id": "5",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
          "id": 3,
          "key": "done",
          "colorName": "green",
          "name": "Done"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12324522",
          "id": "12324522",
          "name": "site"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
        "name": "cdouglas",
        "key": "chris.douglas",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Christopher Douglas",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
        "name": "cdouglas",
        "key": "chris.douglas",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
        },
        "displayName": "Christopher Douglas",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13184/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
        "id": "3",
        "description": "A task that needs to be done.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
        "name": "Task",
        "subtask": false,
        "avatarId": 21148
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2ffc5200[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7f1bf07c[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4ff8db39[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6fc05aaa[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@67dd47c9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@2a605ee7[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@23c87981[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@4e306258[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@43de9758[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@6be17ea2[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@b85062e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@46ea87a4[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Thu Oct 23 17:04:47 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": "2016-09-26T07:02:09.000+0000",
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13184/watchers",
        "watchCount": 26,
        "isWatching": false
      },
      "created": "2016-05-19T18:42:06.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/customFieldOption/10343",
          "value": "Reviewed",
          "id": "10343",
          "disabled": false
        }
      ],
      "customfield_12310230": null,
      "updated": "2025-10-23T17:04:47.000+0000",
      "timeoriginalestimate": null,
      "description": "Many ASF projects include \"Apache\" in their logo. We should add it to Hadoop.",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": null,
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828667",
          "id": "12828667",
          "filename": "hadoop-logo-new.pdf",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.442+0000",
          "size": 7583638,
          "mimeType": "application/pdf",
          "content": "https://issues.apache.org/jira/secure/attachment/12828667/hadoop-logo-new.pdf"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828668",
          "id": "12828668",
          "filename": "hadoop-logo-new.psd",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.466+0000",
          "size": 10747684,
          "mimeType": "image/vnd.adobe.photoshop",
          "content": "https://issues.apache.org/jira/secure/attachment/12828668/hadoop-logo-new.psd"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828669",
          "id": "12828669",
          "filename": "hadoop-logo-new.tif",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.496+0000",
          "size": 8089612,
          "mimeType": "image/tiff",
          "content": "https://issues.apache.org/jira/secure/attachment/12828669/hadoop-logo-new.tif"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828671",
          "id": "12828671",
          "filename": "hadoop-logo-no-back-1000.png",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.524+0000",
          "size": 67287,
          "mimeType": "image/png",
          "content": "https://issues.apache.org/jira/secure/attachment/12828671/hadoop-logo-no-back-1000.png"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828670",
          "id": "12828670",
          "filename": "hadoop-logo-no-back-500.png",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.520+0000",
          "size": 31716,
          "mimeType": "image/png",
          "content": "https://issues.apache.org/jira/secure/attachment/12828670/hadoop-logo-no-back-500.png"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828672",
          "id": "12828672",
          "filename": "hadoop-logo-no-back-5000.png",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.527+0000",
          "size": 413818,
          "mimeType": "image/png",
          "content": "https://issues.apache.org/jira/secure/attachment/12828672/hadoop-logo-no-back-5000.png"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828673",
          "id": "12828673",
          "filename": "hadoop-logo-no-back-8000.png",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.531+0000",
          "size": 1020753,
          "mimeType": "image/png",
          "content": "https://issues.apache.org/jira/secure/attachment/12828673/hadoop-logo-no-back-8000.png"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828675",
          "id": "12828675",
          "filename": "hadoop-logo-white-back-1000.jpg",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.540+0000",
          "size": 161093,
          "mimeType": "image/jpeg",
          "content": "https://issues.apache.org/jira/secure/attachment/12828675/hadoop-logo-white-back-1000.jpg"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828678",
          "id": "12828678",
          "filename": "hadoop-logo-white-back-10000.jpg",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.558+0000",
          "size": 3341529,
          "mimeType": "image/jpeg",
          "content": "https://issues.apache.org/jira/secure/attachment/12828678/hadoop-logo-white-back-10000.jpg"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828674",
          "id": "12828674",
          "filename": "hadoop-logo-white-back-500.jpg",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.537+0000",
          "size": 76183,
          "mimeType": "image/jpeg",
          "content": "https://issues.apache.org/jira/secure/attachment/12828674/hadoop-logo-white-back-500.jpg"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828676",
          "id": "12828676",
          "filename": "hadoop-logo-white-back-5000.jpg",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.544+0000",
          "size": 1200214,
          "mimeType": "image/jpeg",
          "content": "https://issues.apache.org/jira/secure/attachment/12828676/hadoop-logo-white-back-5000.jpg"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12828677",
          "id": "12828677",
          "filename": "hadoop-logo-white-back-8000.jpg",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
            "name": "kspk",
            "key": "kspk",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Abhishek",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2016-09-15T16:19:06.550+0000",
          "size": 2369367,
          "mimeType": "image/jpeg",
          "content": "https://issues.apache.org/jira/secure/attachment/12828677/hadoop-logo-white-back-8000.jpg"
        }
      ],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "Add \"Apache\" to Hadoop project logo",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15291924",
            "id": "15291924",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Our current set of logos are [here|http://svn.apache.org/repos/asf/hadoop/logos/].",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-05-19T18:48:05.005+0000",
            "updated": "2016-05-19T18:48:05.005+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15317736",
            "id": "15317736",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
              "name": "kspk",
              "key": "kspk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Abhishek",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Options for the new logo with APACHE included:\n\n*Option 1*\n\n!https://ske0hq-bn1305.files.1drv.com/y3mfuiXQm9OGG3-dQGy2hTzYQPu1Xdv0C7wAA2rAA0uoEHS08BQxlReQoL_sLPyy_1JNi04LFfrpEGzyhLUpNJXXfY8mkSDezepfzY2rfo_aw3sEYesjbb_DTdJP3xCmSZ4X66UrJM85HPIa4AoTxjl_Nbjzx4V6HQFVv64rmqPSYc?width=1000&height=267&cropmode=none!\n\n\\\\\n\\\\\n\n*Option 2*\n\n!https://skevhq-bn1305.files.1drv.com/y3m3jGYvgeu2uIHc5C98M3dGA5-pUo_ZgNDCANBWJYEQqZeYdyGFwV1UWOIFrpD56FnUNAJkUJOywicSIG_nBdG6v1RvI3BGGkEBlnLcbH5Kz7QoU5j7gI6vghNkDD3HSTSaNDK2PVMqivI005IRdrqTJfduaImaVy4ZyTn_CaJMNY?width=1000&height=291&cropmode=none!\n\n\\\\\n\\\\\n\n*Option 3*\n\n!https://skeuhq-bn1305.files.1drv.com/y3mwENxSi1zzJ6g0hX9wyZu-7wFj77cz6NWYKuhvFyn67Uo7boeqbqw4YPCP8DW05h8lQAEt4XDyC9c_yNspOkwuPnMqFeK_chXzjZBGVPAD7t1UP5iw7TtGmmMn70H1W7hjR-kByyJHvuA3Y4Gjbm6ZzQv8peMLxvggE6dUSVMIZc?width=1000&height=267&cropmode=none!\n\n\\\\\n\\\\\n\n*Option 4*\n\n!https://ske1hq-bn1305.files.1drv.com/y3msqsFXMBqxWYj8kk_-_ShZb1spGcfIzuYD5ShOT4oQB-EMVE2_18GrQPS8rc8K4Gh4Zo6dP76dGkSfvEj7vNoAOU3FAe3HNcpJxl5MZPA8hOx-1aLus_UtzE62bviKkmp9-NcHY_eVWf4EmKtJ5aMcy5jiDk7tBUXRl27YUSvy8Q?width=1000&height=298&cropmode=none!\n\n\\\\\n\\\\\n\n*Option 5*\n\n!https://r6eshq-bn1305.files.1drv.com/y3myRrrctLCNgHqbkjN85nDVzZUwKsGnN3mFMVlJf3uKnMFtMEzrkR4mI8A2bOZfiF3tPXrYkw5DOcYtfbnbwolbjwGusgc3kjovtmiCR8yYElqj6H3uLzeFSNxSgcAA0mAQLkGJOTH4fR89xCWGUSrRaw9vDToaWIGGaY662nE0MA?width=1000&height=291&cropmode=none!\n\n\\\\\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
              "name": "kspk",
              "key": "kspk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Abhishek",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-07T03:10:40.706+0000",
            "updated": "2016-06-07T03:14:15.610+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15318477",
            "id": "15318477",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=tgraves",
              "name": "tgraves",
              "key": "tgraves",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Thomas Graves",
              "active": true,
              "timeZone": "America/Chicago"
            },
            "body": "my vote would be option 4.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=tgraves",
              "name": "tgraves",
              "key": "tgraves",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Thomas Graves",
              "active": true,
              "timeZone": "America/Chicago"
            },
            "created": "2016-06-07T13:40:46.956+0000",
            "updated": "2016-06-07T13:40:46.956+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15318485",
            "id": "15318485",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=junping_du",
              "name": "junping_du",
              "key": "djp",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"
              },
              "displayName": "Junping Du",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "+1 on option 4 too.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=junping_du",
              "name": "junping_du",
              "key": "djp",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"
              },
              "displayName": "Junping Du",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "created": "2016-06-07T13:45:16.108+0000",
            "updated": "2016-06-07T13:45:16.108+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15318511",
            "id": "15318511",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "+1 on option 4.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2016-06-07T14:02:09.341+0000",
            "updated": "2016-06-07T14:02:09.341+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15318601",
            "id": "15318601",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kasha",
              "name": "kasha",
              "key": "kasha",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Karthik Kambatla",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "+1 on option 4. ",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kasha",
              "name": "kasha",
              "key": "kasha",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Karthik Kambatla",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-07T14:42:35.236+0000",
            "updated": "2016-06-07T14:42:35.236+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15318835",
            "id": "15318835",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=templedf",
              "name": "templedf",
              "key": "templedf",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=templedf&avatarId=24879",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=templedf&avatarId=24879",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=templedf&avatarId=24879",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=templedf&avatarId=24879"
              },
              "displayName": "Daniel Templeton",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "+1 on #4.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=templedf",
              "name": "templedf",
              "key": "templedf",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=templedf&avatarId=24879",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=templedf&avatarId=24879",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=templedf&avatarId=24879",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=templedf&avatarId=24879"
              },
              "displayName": "Daniel Templeton",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-07T16:40:46.762+0000",
            "updated": "2016-06-07T16:40:46.762+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15318946",
            "id": "15318946",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bikassaha",
              "name": "bikassaha",
              "key": "bikassaha",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"
              },
              "displayName": "Bikas Saha",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "+1 on 5",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bikassaha",
              "name": "bikassaha",
              "key": "bikassaha",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"
              },
              "displayName": "Bikas Saha",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-07T17:27:14.451+0000",
            "updated": "2016-06-07T17:27:14.451+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15319781",
            "id": "15319781",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "+1 on option 1",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-08T00:26:00.881+0000",
            "updated": "2016-06-08T00:26:00.881+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15319804",
            "id": "15319804",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=clr",
              "name": "clr",
              "key": "clr",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=clr&avatarId=38583",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=clr&avatarId=38583",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=clr&avatarId=38583",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=clr&avatarId=38583"
              },
              "displayName": "Craig L Russell",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Great idea to add Apache to the logo. While you're at it, how about adding (R) to Hadoop to show that it's a registered trademark?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=clr",
              "name": "clr",
              "key": "clr",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=clr&avatarId=38583",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=clr&avatarId=38583",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=clr&avatarId=38583",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=clr&avatarId=38583"
              },
              "displayName": "Craig L Russell",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-08T00:56:26.803+0000",
            "updated": "2016-06-08T00:56:26.803+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320007",
            "id": "15320007",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
              "name": "kspk",
              "key": "kspk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Abhishek",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "I'll add that in the final version. Thanks!",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
              "name": "kspk",
              "key": "kspk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Abhishek",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-08T05:04:34.266+0000",
            "updated": "2016-06-08T05:04:34.266+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320009",
            "id": "15320009",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=xiaochen",
              "name": "xiaochen",
              "key": "xiaochen",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=38038",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=38038",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=38038",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=38038"
              },
              "displayName": "Xiao Chen",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "+1 on option 1",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=xiaochen",
              "name": "xiaochen",
              "key": "xiaochen",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=38038",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=38038",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=38038",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=38038"
              },
              "displayName": "Xiao Chen",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-08T05:08:06.392+0000",
            "updated": "2016-06-08T05:08:06.392+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320013",
            "id": "15320013",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=hitliuyi",
              "name": "hitliuyi",
              "key": "hitliuyi",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Yi Liu",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "option 1 is more beautiful, +1.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=hitliuyi",
              "name": "hitliuyi",
              "key": "hitliuyi",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Yi Liu",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "created": "2016-06-08T05:13:49.912+0000",
            "updated": "2016-06-08T05:13:49.912+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320161",
            "id": "15320161",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=Naganarasimha",
              "name": "Naganarasimha",
              "key": "naganarasimha",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Naganarasimha G R",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "+1 on option 1",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=Naganarasimha",
              "name": "Naganarasimha",
              "key": "naganarasimha",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Naganarasimha G R",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2016-06-08T07:08:58.185+0000",
            "updated": "2016-06-08T07:08:58.185+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320163",
            "id": "15320163",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma",
              "name": "rohithsharma",
              "key": "rohithsharma",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Rohith Sharma K S",
              "active": true,
              "timeZone": "America/New_York"
            },
            "body": "+1 for option 1",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma",
              "name": "rohithsharma",
              "key": "rohithsharma",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Rohith Sharma K S",
              "active": true,
              "timeZone": "America/New_York"
            },
            "created": "2016-06-08T07:09:20.067+0000",
            "updated": "2016-06-08T07:09:20.067+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320173",
            "id": "15320173",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=varun_saxena",
              "name": "varun_saxena",
              "key": "varun_saxena",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Varun Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "+1 for option 1",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=varun_saxena",
              "name": "varun_saxena",
              "key": "varun_saxena",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Varun Saxena",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2016-06-08T07:22:47.036+0000",
            "updated": "2016-06-08T07:22:47.036+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320199",
            "id": "15320199",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt",
              "name": "bibinchundatt",
              "key": "bibinchundatt",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"
              },
              "displayName": "Bibin Chundatt",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "+1 for option 1",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt",
              "name": "bibinchundatt",
              "key": "bibinchundatt",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"
              },
              "displayName": "Bibin Chundatt",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2016-06-08T07:45:46.865+0000",
            "updated": "2016-06-08T07:45:46.865+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320214",
            "id": "15320214",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vinayakumarb",
              "name": "vinayakumarb",
              "key": "vinayrpet",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
              },
              "displayName": "Vinayakumar B",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "body": "+1 for option 1. Looks beautiful.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=vinayakumarb",
              "name": "vinayakumarb",
              "key": "vinayrpet",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
              },
              "displayName": "Vinayakumar B",
              "active": true,
              "timeZone": "Asia/Kolkata"
            },
            "created": "2016-06-08T07:57:36.048+0000",
            "updated": "2016-06-08T07:57:36.048+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15320224",
            "id": "15320224",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=drankye",
              "name": "drankye",
              "key": "drankye",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Kai Zheng",
              "active": true,
              "timeZone": "Asia/Chongqing"
            },
            "body": "+1 for option 1. Pretty cool.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=drankye",
              "name": "drankye",
              "key": "drankye",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Kai Zheng",
              "active": true,
              "timeZone": "Asia/Chongqing"
            },
            "created": "2016-06-08T08:08:39.959+0000",
            "updated": "2016-06-08T08:08:39.959+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15323208",
            "id": "15323208",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "Option 5",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2016-06-09T19:54:48.352+0000",
            "updated": "2016-06-09T19:54:48.352+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15323314",
            "id": "15323314",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "body": "Note that technically HADOOP as a name (i.e. without any reference to fonts, colors, or the like) is registered; the logo as a graphical element is not registered.  So legally the logo should have a \u2122, not an \u00ae.\n\nIf the PMC updates the logo, they could certainly request registering the logo (since this is such a major project), which would let us put the \u00ae there as well.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "created": "2016-06-09T20:49:30.399+0000",
            "updated": "2016-06-09T20:49:30.399+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15323327",
            "id": "15323327",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "body": "I love the idea of adding the \"APACHE\" to the logo; obviously decisions here are up to the PMC (so my opinion is non-binding).\n\nIt's a little odd with the feather, since it's a tilted & mirror image feather, but I understand that's the only way it works with your horizontal logo format.  But again; up to the PMC.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "created": "2016-06-09T20:52:57.410+0000",
            "updated": "2016-06-09T20:52:57.410+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15323425",
            "id": "15323425",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Good to know. The yellow elephant is registered, right? Do we need to update that graphic?\n\nWe should probably generate both \u2122 and \u00ae, while this is active. If/when it's registered, we can swap in the correct logo.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-09T21:42:07.488+0000",
            "updated": "2016-06-09T21:42:07.488+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15324370",
            "id": "15324370",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "body": "No, the only US registration the ASF holds is on HADOOP.  The PMC needs to request registration via tm-registrations@ for me to take action.\n\nWe don't typically register logos due to the additional cost and budget limitations.  However given Hadoop's reach I'd certainly process a registration for the elephant.\n\nNote that you register an *exact* graphical logo (modulo any TM/(R), which don't matter for the application).  So when you request registration, be sure to include the specific graphic file you want to register (i.e. either just the elephant, or the elephant & word).\n\nSeparately, does the PMC have records from the original logo author showing they're granting trademark rights to the ASF?  We can work on that on private@/tm@.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "created": "2016-06-10T12:31:26.467+0000",
            "updated": "2016-06-10T12:31:26.467+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15348561",
            "id": "15348561",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=sjlee0",
              "name": "sjlee0",
              "key": "sjlee0",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"
              },
              "displayName": "Sangjin Lee",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Belated +1 for option 4.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=sjlee0",
              "name": "sjlee0",
              "key": "sjlee0",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"
              },
              "displayName": "Sangjin Lee",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-24T16:46:46.434+0000",
            "updated": "2016-06-24T16:46:46.434+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15353951",
            "id": "15353951",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang",
              "name": "andrew.wang",
              "key": "andrew.wang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"
              },
              "displayName": "Andrew Wang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "I have the same feedback as Shane about the feather. +1 for option 4.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang",
              "name": "andrew.wang",
              "key": "andrew.wang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"
              },
              "displayName": "Andrew Wang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-06-29T00:02:33.888+0000",
            "updated": "2016-06-29T00:02:33.888+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15368578",
            "id": "15368578",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "body": "When the vote ends?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=aajisaka",
              "name": "aajisaka",
              "key": "ajisakaa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"
              },
              "displayName": "Akira Ajisaka",
              "active": true,
              "timeZone": "Asia/Tokyo"
            },
            "created": "2016-07-08T22:08:31.685+0000",
            "updated": "2016-07-08T22:08:31.685+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15383424",
            "id": "15383424",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "bq. When the vote ends?\nI'd hoped to avoid a vote on JIRA, but here we are.\n\nI count 9 votes for #1, 7 votes for #4, 2 for #5. [~kspk], could you create the final version of #1?\n\n[~curcuru], just to be clear, should the logo have a TM or an (R)?",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-07-19T01:08:15.867+0000",
            "updated": "2016-07-19T01:08:15.867+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15418823",
            "id": "15418823",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "body": "The logo should have a TM symbol; we do not currently hold a registration on the logo itself.\n\nThe mere use of the stylized HADOOP word within the logo does not mean the logo should be an (R), even though HADOOP as a word is registered in several countries.  Trademark law generally treats words separately from obviously stylized or graphical displays of words, so while the word (in any normal typeface) is registered, the logo depicting it is not.\n\nAlso, trademark law is often very specific about exact logos, or officially recognizing part of a logo vs. the entirety of the design.  Thus if the PMC wants to request registration of a new logo, we must provide the *exact* logo image we want to register.  While there are some trademark protections for similar designs, the strongest legal protection (and ability to use (R)) only extends to the same or virtually the same graphical image (although I'm pretty sure we could (R) a black & white version of the same image, if we registered the normal color version).",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=curcuru",
              "name": "curcuru",
              "key": "curcuru",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=curcuru&avatarId=29452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=curcuru&avatarId=29452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=curcuru&avatarId=29452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=curcuru&avatarId=29452"
              },
              "displayName": "Shane Curcuru",
              "active": true,
              "timeZone": "America/New_York"
            },
            "created": "2016-08-12T13:25:40.135+0000",
            "updated": "2016-08-12T13:25:40.135+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15493809",
            "id": "15493809",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
              "name": "kspk",
              "key": "kspk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Abhishek",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Uploading all files for the new logo. Various sizes and formats. \n\nList of files:\n* Original PSD\n* PDF generated with Adobe's Color profile. \n* TIFF version of original file\n* PNGs with white background and transparent background (multiple sizes).\n\nAll logos contain the (TM) symbol. I can generate more sizes and formats if needed. \n\nThanks,\nAbhishek",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=kspk",
              "name": "kspk",
              "key": "kspk",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Abhishek",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-09-15T16:19:06.570+0000",
            "updated": "2016-09-15T16:19:06.570+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/15522278",
            "id": "15522278",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "I committed this. Thanks Abhishek!",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=cdouglas",
              "name": "cdouglas",
              "key": "chris.douglas",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "Christopher Douglas",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2016-09-26T07:02:09.815+0000",
            "updated": "2016-09-26T07:02:09.815+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12971246/comment/18032548",
            "id": "18032548",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "body": "I want to roll back to the older version without the feather.\r\n\r\nDoes anyone have any ideas about how best to do this?\r\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org",
              "name": "stevel@apache.org",
              "key": "stevel@apache.org",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"
              },
              "displayName": "Steve Loughran",
              "active": true,
              "timeZone": "Europe/London"
            },
            "created": "2025-10-23T17:04:47.461+0000",
            "updated": "2025-10-23T17:04:47.461+0000"
          }
        ],
        "maxResults": 32,
        "total": 32,
        "startAt": 0
      },
      "customfield_12311820": "0|i2y893:",
      "customfield_12314139": null
    }
  },
  {
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "12924132",
    "self": "https://issues.apache.org/jira/rest/api/latest/issue/12924132",
    "key": "HADOOP-12677",
    "fields": {
      "fixVersions": [],
      "resolution": null,
      "customfield_12312322": null,
      "customfield_12312323": null,
      "customfield_12310420": "9223372036854775807",
      "customfield_12312320": null,
      "customfield_12312321": null,
      "customfield_12312328": null,
      "customfield_12312329": null,
      "customfield_12312326": null,
      "customfield_12310300": null,
      "customfield_12312327": null,
      "customfield_12312324": null,
      "customfield_12312720": null,
      "customfield_12312325": null,
      "lastViewed": null,
      "priority": {
        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
        "name": "Major",
        "id": "3"
      },
      "labels": [
        "pull-request-available"
      ],
      "customfield_12312333": null,
      "customfield_12312334": null,
      "customfield_12313422": "false",
      "customfield_12310310": "2.0",
      "customfield_12312331": null,
      "customfield_12312332": null,
      "aggregatetimeoriginalestimate": null,
      "timeestimate": null,
      "customfield_12312330": null,
      "versions": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12326144",
          "id": "12326144",
          "description": "2.4.0 release",
          "name": "2.4.0",
          "archived": false,
          "released": true,
          "releaseDate": "2014-04-07"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12327179",
          "id": "12327179",
          "description": "2.6.0 release",
          "name": "2.6.0",
          "archived": false,
          "released": true,
          "releaseDate": "2014-11-18"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/version/12335733",
          "id": "12335733",
          "description": "3.0.0-alpha1 release",
          "name": "3.0.0-alpha1",
          "archived": false,
          "released": true,
          "releaseDate": "2016-09-03"
        }
      ],
      "customfield_12311120": null,
      "customfield_12313826": null,
      "issuelinks": [],
      "customfield_12312339": null,
      "customfield_12313825": null,
      "assignee": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
        "name": "weichiu",
        "key": "jojochuang",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
        },
        "displayName": "Wei-Chiu Chuang",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "customfield_12312337": null,
      "customfield_12313823": null,
      "customfield_12312338": null,
      "customfield_12311920": null,
      "customfield_12313822": null,
      "customfield_12312335": null,
      "customfield_12313821": null,
      "customfield_12312336": null,
      "customfield_12313820": null,
      "status": {
        "self": "https://issues.apache.org/jira/rest/api/2/status/10002",
        "description": "A patch for this issue has been uploaded to JIRA by a contributor.",
        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/document.png",
        "name": "Patch Available",
        "id": "10002",
        "statusCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/4",
          "id": 4,
          "key": "indeterminate",
          "colorName": "yellow",
          "name": "In Progress"
        }
      },
      "components": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/component/12310687",
          "id": "12310687",
          "name": "io"
        }
      ],
      "archiveddate": null,
      "customfield_12312026": null,
      "customfield_12312023": null,
      "customfield_12312024": null,
      "aggregatetimeestimate": null,
      "customfield_12312022": null,
      "customfield_12310921": null,
      "customfield_12310920": "9223372036854775807",
      "customfield_12312823": null,
      "creator": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=laurent",
        "name": "laurent",
        "key": "laurentgo",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=laurentgo&avatarId=18552",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=laurentgo&avatarId=18552",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=laurentgo&avatarId=18552",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=laurentgo&avatarId=18552"
        },
        "displayName": "Laurent Goujon",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "subtasks": [],
      "reporter": {
        "self": "https://issues.apache.org/jira/rest/api/2/user?username=laurent",
        "name": "laurent",
        "key": "laurentgo",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=laurentgo&avatarId=18552",
          "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=laurentgo&avatarId=18552",
          "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=laurentgo&avatarId=18552",
          "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=laurentgo&avatarId=18552"
        },
        "displayName": "Laurent Goujon",
        "active": true,
        "timeZone": "America/Los_Angeles"
      },
      "aggregateprogress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313520": null,
      "customfield_12310250": null,
      "progress": {
        "progress": 0,
        "total": 0
      },
      "customfield_12313924": null,
      "votes": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12677/votes",
        "votes": 0,
        "hasVoted": false
      },
      "worklog": {
        "startAt": 0,
        "maxResults": 20,
        "total": 0,
        "worklogs": []
      },
      "archivedby": null,
      "customfield_12313920": null,
      "issuetype": {
        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
        "id": "1",
        "description": "A problem which impairs or prevents the functions of the product.",
        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
        "name": "Bug",
        "subtask": false,
        "avatarId": 21133
      },
      "timespent": null,
      "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@5984c9d[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@158d4e54[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@30d9db30[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@4fd9dc91[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@32b8ae0a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@5156108d[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2f2851cc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@2e942dec[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@592b1eca[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@3898bf1a[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@55d32ff4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@5aa783ef[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
      "customfield_12314141": null,
      "customfield_12314140": null,
      "project": {
        "self": "https://issues.apache.org/jira/rest/api/2/project/12310240",
        "id": "12310240",
        "key": "HADOOP",
        "name": "Hadoop Common",
        "projectTypeKey": "software",
        "avatarUrls": {
          "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095",
          "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
          "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
          "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"
        },
        "projectCategory": {
          "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292",
          "id": "10292",
          "description": "Scalable Distributed Computing",
          "name": "Hadoop"
        }
      },
      "aggregatetimespent": null,
      "customfield_12312520": null,
      "customfield_12312521": "Sun Oct 19 00:24:48 UTC 2025",
      "customfield_12314422": null,
      "customfield_12314421": null,
      "customfield_12314146": null,
      "customfield_12314420": null,
      "customfield_12314145": null,
      "customfield_12314144": null,
      "customfield_12314143": null,
      "resolutiondate": null,
      "workratio": -1,
      "customfield_12312923": null,
      "customfield_12312920": null,
      "customfield_12312921": null,
      "watches": {
        "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12677/watchers",
        "watchCount": 4,
        "isWatching": false
      },
      "created": "2015-12-24T02:38:03.000+0000",
      "customfield_12310192": null,
      "customfield_12310191": null,
      "customfield_12310230": null,
      "updated": "2025-10-19T00:24:48.000+0000",
      "timeoriginalestimate": null,
      "description": "DecompressorStream.skip(long) throws an IndexOutOfBoundException when using a long bigger than Integer.MAX_VALUE\n\nThis is because of this cast from long to int: https://github.com/apache/hadoop-common/blob/HADOOP-3628/src/core/org/apache/hadoop/io/compress/DecompressorStream.java#L125\n\nThe fix is probably to do the cast after applying Math.min: in that case, it should not be an issue since it should not be bigger than the buffer size (512)",
      "customfield_10010": null,
      "timetracking": {},
      "customfield_12314523": null,
      "customfield_12314127": null,
      "customfield_12314522": null,
      "customfield_12314126": null,
      "customfield_12314521": null,
      "customfield_12314125": null,
      "customfield_12310320": null,
      "customfield_12314520": null,
      "customfield_12314124": null,
      "customfield_12312340": null,
      "attachment": [
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12779391",
          "id": "12779391",
          "filename": "HADOOP-12677.001.patch",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
            "name": "weichiu",
            "key": "jojochuang",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
            },
            "displayName": "Wei-Chiu Chuang",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2015-12-24T06:26:25.984+0000",
          "size": 2695,
          "mimeType": "text/plain",
          "content": "https://issues.apache.org/jira/secure/attachment/12779391/HADOOP-12677.001.patch"
        },
        {
          "self": "https://issues.apache.org/jira/rest/api/2/attachment/12779660",
          "id": "12779660",
          "filename": "HADOOP-12677.002.patch",
          "author": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
            "name": "weichiu",
            "key": "jojochuang",
            "avatarUrls": {
              "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
              "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
              "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
              "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
            },
            "displayName": "Wei-Chiu Chuang",
            "active": true,
            "timeZone": "America/Los_Angeles"
          },
          "created": "2015-12-28T15:57:11.205+0000",
          "size": 2608,
          "mimeType": "text/plain",
          "content": "https://issues.apache.org/jira/secure/attachment/12779660/HADOOP-12677.002.patch"
        }
      ],
      "customfield_12314123": null,
      "customfield_12312341": null,
      "customfield_12312220": null,
      "customfield_12314122": null,
      "customfield_12314121": null,
      "customfield_12314120": null,
      "customfield_12314129": null,
      "customfield_12314524": null,
      "customfield_12314128": null,
      "summary": "DecompressorStream throws IndexOutOfBoundsException when calling skip(long)",
      "customfield_12314130": null,
      "customfield_12310291": null,
      "customfield_12310290": null,
      "customfield_12311024": null,
      "customfield_12314138": null,
      "customfield_12314137": null,
      "environment": null,
      "customfield_12314136": null,
      "customfield_12314135": null,
      "customfield_12311020": null,
      "customfield_12314134": null,
      "duedate": null,
      "customfield_12314132": null,
      "customfield_12314131": null,
      "comment": {
        "comments": [
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15070637",
            "id": "15070637",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
              "name": "weichiu",
              "key": "jojochuang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
              },
              "displayName": "Wei-Chiu Chuang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Thanks [~laurentgo] for reporting this issue!\nI was able to reproduce the exception and then created a patch and a regression test case.\n\nAttaching my rev01 patch:\n* In DecompressorStream.skip(), declare both skipped and len as long to avoid overflowing the number.\n* Create a test case in TestBlockDecompressorStream. BlockDecompressorStream inherits DecompressorStream, and DecompressorStream does not have its own test file.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
              "name": "weichiu",
              "key": "jojochuang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
              },
              "displayName": "Wei-Chiu Chuang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2015-12-24T06:26:25.989+0000",
            "updated": "2015-12-24T06:26:25.989+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15070706",
            "id": "15070706",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa",
              "name": "hadoopqa",
              "key": "hadoopqa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"
              },
              "displayName": "Hadoop QA",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 39s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 7m 57s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 37s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 3s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 46s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 50s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 1s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 7m 48s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 7m 48s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 34s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 8m 34s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 0s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 54s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 51s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 1s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 6m 12s {color} | {color:red} hadoop-common in the patch failed with JDK v1.8.0_66. {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 6m 25s {color} | {color:red} hadoop-common in the patch failed with JDK v1.7.0_91. {color} |\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 23s {color} | {color:red} Patch generated 1 ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 66m 49s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| JDK v1.8.0_66 Failed junit tests | hadoop.test.TestTimedOutTestsListener |\n| JDK v1.7.0_91 Failed junit tests | hadoop.security.TestShellBasedIdMapping |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:0ca8df7 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12779391/HADOOP-12677.001.patch |\n| JIRA Issue | HADOOP-12677 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 809b3b049d92 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / a308e86 |\n| Default Java | 1.7.0_91 |\n| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 |\n| findbugs | v3.0.0 |\n| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_66.txt |\n| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_91.txt |\n| unit test logs |  https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_91.txt |\n| JDK v1.7.0_91  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/testReport/ |\n| asflicense | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-asflicense-problems.txt |\n| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\n| Max memory used | 75MB |\n| Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org |\n| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/console |\n\n\nThis message was automatically generated.\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa",
              "name": "hadoopqa",
              "key": "hadoopqa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"
              },
              "displayName": "Hadoop QA",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2015-12-24T07:35:34.053+0000",
            "updated": "2015-12-24T07:35:34.053+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15070730",
            "id": "15070730",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
              "name": "weichiu",
              "key": "jojochuang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
              },
              "displayName": "Wei-Chiu Chuang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "Test failures are unrelated. ASF license warning is false positive.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
              "name": "weichiu",
              "key": "jojochuang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
              },
              "displayName": "Wei-Chiu Chuang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2015-12-24T07:57:50.349+0000",
            "updated": "2015-12-24T07:57:50.349+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15071090",
            "id": "15071090",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=laurent",
              "name": "laurent",
              "key": "laurentgo",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=laurentgo&avatarId=18552",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=laurentgo&avatarId=18552",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=laurentgo&avatarId=18552",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=laurentgo&avatarId=18552"
              },
              "displayName": "Laurent Goujon",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "The unit test is incorrect. According to http://docs.oracle.com/javase/7/docs/api/java/io/InputStream.html#skip%28long%29, it doesn't throw EOFException and should return how many bytes were skipped. So the catch clause should be removed, and instead the return value should be checked (it should be 4). It looks also that the following check \n{code:java} if (bufLen > 0){code}\nis not needed since you set bufLen to 4 just before.\n\nRegarding the fix, it looks correct. Style-wise, some parenthesis and casts are probably not needed, and I believe it could be on a single line:\n{code:java}\n// safe to cast to int as between between 0 and skipBytes.length\nint len = (int) Math.min(n - skipped, skipBytes.length);\n{code}",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=laurent",
              "name": "laurent",
              "key": "laurentgo",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=laurentgo&avatarId=18552",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=laurentgo&avatarId=18552",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=laurentgo&avatarId=18552",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=laurentgo&avatarId=18552"
              },
              "displayName": "Laurent Goujon",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2015-12-24T15:29:59.187+0000",
            "updated": "2015-12-24T15:29:59.187+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15072837",
            "id": "15072837",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
              "name": "weichiu",
              "key": "jojochuang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
              },
              "displayName": "Wei-Chiu Chuang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "[~laurentgo]\nThank you for the comments and reviews!\n\nI have posted patch #2 to address comment#2.\n\nRegarding comment#1, even though DecompressorStream inherits InputStream, its skip() implementation does not call InputStream.skip(). Instead, its internal implementation uses InputStream.read(), which returns -1 if end of stream. If InputStream.end() returns -1, an EOFException is thrown.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=weichiu",
              "name": "weichiu",
              "key": "jojochuang",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"
              },
              "displayName": "Wei-Chiu Chuang",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2015-12-28T15:57:11.213+0000",
            "updated": "2015-12-28T15:57:11.213+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15072867",
            "id": "15072867",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=laurent",
              "name": "laurent",
              "key": "laurentgo",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=laurentgo&avatarId=18552",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=laurentgo&avatarId=18552",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=laurentgo&avatarId=18552",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=laurentgo&avatarId=18552"
              },
              "displayName": "Laurent Goujon",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "body": "It's not about invoking InputStream.skip() but following the same behavior, as part of the API contract. Before your patch, DecompressorStream would also return the number of bytes skipped until EOF, changing it would mean introducing an incompatible change.",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=laurent",
              "name": "laurent",
              "key": "laurentgo",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=laurentgo&avatarId=18552",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=laurentgo&avatarId=18552",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=laurentgo&avatarId=18552",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=laurentgo&avatarId=18552"
              },
              "displayName": "Laurent Goujon",
              "active": true,
              "timeZone": "America/Los_Angeles"
            },
            "created": "2015-12-28T16:15:08.637+0000",
            "updated": "2015-12-28T16:15:08.637+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/15072928",
            "id": "15072928",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa",
              "name": "hadoopqa",
              "key": "hadoopqa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"
              },
              "displayName": "Hadoop QA",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "| (/) *{color:green}+1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 50s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 41s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 9m 20s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 5s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 54s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 57s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 7s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 38s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 37s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 8m 37s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 9m 18s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 9m 18s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 5s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 2s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 55s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 6s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 15s {color} | {color:green} hadoop-common in the patch passed with JDK v1.8.0_66. {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 14s {color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_91. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} Patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 74m 47s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:0ca8df7 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12779660/HADOOP-12677.002.patch |\n| JIRA Issue | HADOOP-12677 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 734aa7180e1a 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / a0249da |\n| Default Java | 1.7.0_91 |\n| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 |\n| findbugs | v3.0.0 |\n| JDK v1.7.0_91  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/8317/testReport/ |\n| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\n| Max memory used | 76MB |\n| Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org |\n| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/8317/console |\n\n\nThis message was automatically generated.\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa",
              "name": "hadoopqa",
              "key": "hadoopqa",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"
              },
              "displayName": "Hadoop QA",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2015-12-28T17:23:41.108+0000",
            "updated": "2015-12-28T17:23:41.108+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/17744264",
            "id": "17744264",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=xiangli",
              "name": "xiangli",
              "key": "water",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=water&avatarId=32871",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=water&avatarId=32871",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=water&avatarId=32871",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=water&avatarId=32871"
              },
              "displayName": "Xiang Li",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "body": "We exposed this problem when trying to read Spark executor log when it is greater than 2G (2^31 - 1, as Integer.MAX_VALUE) and would like to try the patch on our test environment. Thanks for the fix [~weichiu]!",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=xiangli",
              "name": "xiangli",
              "key": "water",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=water&avatarId=32871",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=water&avatarId=32871",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=water&avatarId=32871",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=water&avatarId=32871"
              },
              "displayName": "Xiang Li",
              "active": true,
              "timeZone": "Asia/Shanghai"
            },
            "created": "2023-07-18T15:00:57.882+0000",
            "updated": "2023-07-18T15:00:57.882+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/17746660",
            "id": "17746660",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "jojochuang opened a new pull request, #5886:\nURL: https://github.com/apache/hadoop/pull/5886\n\n   ### Description of PR\r\n   Cast DecompressorStream.skip() properly.\r\n   \r\n   ### How was this patch tested?\r\n   Unit test\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2023-07-24T22:10:04.844+0000",
            "updated": "2023-07-24T22:10:04.844+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/18030505",
            "id": "18030505",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] commented on PR #5886:\nURL: https://github.com/apache/hadoop/pull/5886#issuecomment-3413330729\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-17T00:22:37.872+0000",
            "updated": "2025-10-17T00:22:37.872+0000"
          },
          {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/12924132/comment/18030886",
            "id": "18030886",
            "author": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "body": "github-actions[bot] closed pull request #5886: HADOOP-12677. DecompressorStream throws IndexOutOfBoundsException when calling skip(long)\nURL: https://github.com/apache/hadoop/pull/5886\n\n\n",
            "updateAuthor": {
              "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
              "name": "githubbot",
              "key": "githubbot",
              "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
              },
              "displayName": "ASF GitHub Bot",
              "active": true,
              "timeZone": "Etc/UTC"
            },
            "created": "2025-10-19T00:24:48.661+0000",
            "updated": "2025-10-19T00:24:48.661+0000"
          }
        ],
        "maxResults": 11,
        "total": 11,
        "startAt": 0
      },
      "customfield_12311820": "0|i2qa0v:",
      "customfield_12314139": null
    }
  }
]