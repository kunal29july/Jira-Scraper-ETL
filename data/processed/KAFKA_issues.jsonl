{"id": "KAFKA-19836", "title": "Decouple ConsumerConfig and ShareConsumerConfig", "description": "ShareConsumerConfig and ConsumerConfig are inherent at the moment. The drawback is the config logic is mixed, for example: ShareAcknowledgementMode. We can decouple to prevent the logic is complicated in the future.", "status": "Open", "priority": "Major", "reporter": "TaiJuWu", "assignee": "TaiJuWu", "created": "2025-10-26T07:31:58.000+0000", "updated": "2025-10-26T07:45:28.000+0000", "labels": [], "components": [], "comments": [{"author": "TaiJuWu", "body": "Feel free to close if this is not necessary.", "created": "2025-10-26T07:45:28.413+0000"}], "derived_tasks": {"summary": "Decouple ConsumerConfig and ShareConsumerConfig - ShareConsumerConfig and ConsumerConfig are inherent at the moment", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19835", "title": "The Content-Security-Policy header must not be overridden", "description": "[https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13] The Content-Security-Policy header must not be overridden. There is now a standard way to add local exceptions to the CSP: [https://infra.apache.org/tools/csp.html] Please update the .htaccess file accordingly.", "status": "Open", "priority": "Major", "reporter": "Sebb", "assignee": "Kuan Po Tseng", "created": "2025-10-25T20:24:10.000+0000", "updated": "2025-10-26T09:15:54.000+0000", "labels": [], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "brandboat opened a new pull request, #733: URL: https://github.com/apache/kafka-site/pull/733 The Content-Security-Policy header must not be overridden. There is now a standard way to add local exceptions to the CSP: https://infra.apache.org/tools/csp.html", "created": "2025-10-26T03:37:32.928+0000"}, {"author": "ASF GitHub Bot", "body": "brandboat commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3447983723 quickstart local dev env screenshot: <img width=\"1725\" height=\"887\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f06abbe0-ec48-428f-b0e0-edf417a915ef\" />", "created": "2025-10-26T03:41:45.634+0000"}, {"author": "ASF GitHub Bot", "body": "sebbASF commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448072791 Note that the .htaccess file needs to document why the override is allowed.", "created": "2025-10-26T07:01:01.912+0000"}, {"author": "ASF GitHub Bot", "body": "brandboat commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448101426 @sebbASF, thanks for the comment! The reason we add youtube here is due to we use embedded youtube videos in QuickStart, KafkaStreams page, see https://kafka.apache.org/quickstart, https://kafka.apache.org/documentation/streams/ Without this, browsers will block these iframes and videos won't display.", "created": "2025-10-26T07:46:26.864+0000"}, {"author": "ASF GitHub Bot", "body": "sebbASF commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448151327 Your explanation covers why the override is needed. However it does not cover why the override is allowed. According to https://infra.apache.org/tools/csp.html \"Each additional host you add MUST have been pre-approved by VP Data Privacy ([privacy@apache.org](mailto:privacy@apache.org)), and SHOULD have an accompanying comment in the .htaccess file explaining why the CSP is changed and where permission was obtained.\"", "created": "2025-10-26T08:07:11.014+0000"}, {"author": "ASF GitHub Bot", "body": "brandboat commented on PR #733: URL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448248462 > Each additional host you add MUST have been pre-approved by VP Data Privacy ([privacy@apache.org](mailto:privacy@apache.org)) @sebbASF, thanks again for the comment! Could you please explain how this approval process is supposed to be done? I don\u2019t seem to have access to the email threads at `privacy@apache.org`, so I\u2019m wondering whether this step needs to be handled by an Apache member or a project lead. Sorry if this is a basic question \u2014 I\u2019m just a casual contributor and not very familiar with the internal process. c.c. @chia7712", "created": "2025-10-26T09:15:54.842+0000"}], "derived_tasks": {"summary": "The Content-Security-Policy header must not be overridden - [https://github", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19834", "title": "Cleanup suppressions.xml", "description": "Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.", "status": "Open", "priority": "Minor", "reporter": "majialong", "assignee": "majialong", "created": "2025-10-25T16:30:53.000+0000", "updated": "2025-10-25T16:30:53.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Cleanup suppressions.xml - Currently the rules in suppressions", "classifications": ["task"], "qa_pairs": []}}
{"id": "KAFKA-19833", "title": "Refactor Nullable Types to Use a Unified Pattern", "description": "see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676] Regarding the implementation of the nullable vs non-nullable types. We use 3 different approaches. # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES. # For array, we use one class ArraryOf, which takes a nullable param. # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA. We need\u00a0 to pick one approach to implement all nullable types in a consistent way.", "status": "Open", "priority": "Major", "reporter": "Lan Ding", "assignee": "Lan Ding", "created": "2025-10-25T03:21:54.000+0000", "updated": "2025-10-25T03:21:54.000+0000", "labels": [], "components": ["clients"], "comments": [], "derived_tasks": {"summary": "Refactor Nullable Types to Use a Unified Pattern - see [https://github", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19832", "title": "Move ClientOAuthIntegrationTest to clients-integration-tests", "description": "This is continue with the effort to move the clients test to {{clients-integration-tests}} a la KAFKA-19042.", "status": "Open", "priority": "Major", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-10-23T23:45:08.000+0000", "updated": "2025-10-23T23:45:21.000+0000", "labels": ["integration-test", "oauth2"], "components": ["clients", "consumer"], "comments": [], "derived_tasks": {"summary": "Move ClientOAuthIntegrationTest to clients-integration-tests - This is continue with the effort to move the clients test to {{clients-integration-t...", "classifications": ["test"], "qa_pairs": []}}
{"id": "KAFKA-19831", "title": "Failures in the StateUpdater thread may lead to inability to shut down a stream thread", "description": "If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown. See the attached test that reproduces the issue.", "status": "In Progress", "priority": "Major", "reporter": "Nikita Shupletsov", "assignee": "Nikita Shupletsov", "created": "2025-10-23T18:25:27.000+0000", "updated": "2025-10-24T23:16:05.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Arpit Goyal", "body": "Hi [~nikita-shupletsov]\u00a0 if you have not started ,can i pick this up ?", "created": "2025-10-24T02:29:01.071+0000"}, {"author": "Nikita Shupletsov", "body": "Hi [~goyarpit] I am already working on it. I will let you know if I need help. thanks!", "created": "2025-10-24T06:33:53.155+0000"}], "derived_tasks": {"summary": "Failures in the StateUpdater thread may lead to inability to shut down a stream thread - If during rebalance a failure occurs in the StateUpdater t...", "classifications": ["improvement", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19830", "title": "Refactor KafkaRaftClient to use event scheduler framework", "description": "", "status": "Open", "priority": "Major", "reporter": "Kevin Wu", "assignee": "Kevin Wu", "created": "2025-10-23T15:54:07.000+0000", "updated": "2025-10-23T15:54:07.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Refactor KafkaRaftClient to use event scheduler framework", "classifications": ["improvement", "task"], "qa_pairs": []}}
{"id": "KAFKA-19829", "title": "Implement group-level initial rebalance delay", "description": "During testing, an artifact of the new rebalance protocol showed up. In some cases, the first joining member gets all active tasks assigned, and is slow to revoke the tasks after more member has joined the group. This affects in particular cases where the first member is slow (possibly overloaded in the case of cloudlimits benchmarks) and there are a lot of tasks to be assigned. To help with this situation, we want to introduce a new group-specific configuration to delay the initial rebalance.", "status": "Patch Available", "priority": "Major", "reporter": "travis", "assignee": "travis", "created": "2025-10-23T03:58:54.000+0000", "updated": "2025-10-25T23:59:38.000+0000", "labels": [], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Implement group-level initial rebalance delay - During testing, an artifact of the new rebalance protocol showed up", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "KAFKA-19828", "title": "Intermittent test failures when using chained emit strategy on window close", "description": "Hi, I have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue The problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of !https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810 to make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline. Greg", "status": "Open", "priority": "Major", "reporter": "Greg F", "assignee": null, "created": "2025-10-23T00:51:44.000+0000", "updated": "2025-10-25T23:40:43.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Greg F", "body": "Most of the time I run the test, I see this: [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.880 s \u2013 in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3 [INFO] [INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] However, like I mentioned, every once in a while I see this: What that means, most of the time a record passes from input to output topic in the time expected, but every once in a while it does not.", "created": "2025-10-23T01:00:21.304+0000"}, {"author": "Greg F", "body": "the goal, of course, is to have unit test that is reliable, i.e. doesn't fail intermittently. I need to understand whether the issue is in my test code (attached) or elsewhere in kafka test framework.", "created": "2025-10-23T01:01:56.167+0000"}, {"author": "Greg F", "body": "I run the test in a loop in a shell script like this: #!/bin/bash set -e for i in \\{1..100}; do echo ======================================================== echo ======================== $i ============================ echo ======================================================== mvn surefire:test -Dtest=ChainedEmitStrategyTopologyTest3 done I am attaching output of one such run, where first 5 iterations succeeded, but the 6th one failed", "created": "2025-10-23T01:16:48.842+0000"}, {"author": "Greg F", "body": "@[~mjsax] ^^^^", "created": "2025-10-23T19:42:00.664+0000"}, {"author": "Matthias J. Sax", "body": "I am not sure \u2013 did spend some time but don't understand yet what's happening. It could be a bug in Kafka Streams. Needs more investigation.", "created": "2025-10-25T23:36:11.615+0000"}, {"author": "Greg F", "body": "Thanks. Good you were able to reproduce it. I've been banging my head at it for a week now. Keep me posted. Thanks", "created": "2025-10-25T23:40:43.435+0000"}], "derived_tasks": {"summary": "Intermittent test failures when using chained emit strategy on window close - Hi, I have a test case that contains a topology with 2 time windows a...", "classifications": ["bug"], "qa_pairs": [{"question": "org/jira/secure/viewavatar?", "answer": "Most of the time I run the test, I see this: [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.880 s \u2013 in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3 [INFO] [INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] However, like I mentioned, every once in a while I see this: What that means, most of the time a record passes from input to output topic in the time expected, but every once in a while it does not."}]}}
{"id": "KAFKA-19827", "title": "Call acknowledgement commit callback at end of waiting calls", "description": "The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-10-22T14:53:09.000+0000", "updated": "2025-10-24T07:57:46.000+0000", "labels": [], "components": ["clients"], "comments": [], "derived_tasks": {"summary": "Call acknowledgement commit callback at end of waiting calls - The acknowledgement commit callback in the share consumer gets called on the applica...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19826", "title": "KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator", "description": "Add a new allowed value for group.coordinator.append.linger.ms and share.coordinator.append.linger.ms of -1. When append.linger.ms is set to -1, use the flush strategy outlined in the KIP.", "status": "Open", "priority": "Minor", "reporter": "Sean Quah", "assignee": "Sean Quah", "created": "2025-10-22T13:48:59.000+0000", "updated": "2025-10-22T13:49:23.000+0000", "labels": [], "components": ["group-coordinator"], "comments": [], "derived_tasks": {"summary": "KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator - Add a new allowed value for group", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19825", "title": "KIP-1224: Add batch-linger-time and batch-flush-time metrics", "description": "", "status": "Open", "priority": "Minor", "reporter": "Sean Quah", "assignee": "Sean Quah", "created": "2025-10-22T13:47:41.000+0000", "updated": "2025-10-22T18:56:33.000+0000", "labels": [], "components": ["group-coordinator"], "comments": [], "derived_tasks": {"summary": "KIP-1224: Add batch-linger-time and batch-flush-time metrics", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19824", "title": "New ConnectorClientConfigOverridePolicy with allowlist of configurations", "description": "Jira for KIP-1188: https://cwiki.apache.org/confluence/x/2IkvFg", "status": "Open", "priority": "Major", "reporter": "Mickael Maison", "assignee": "Mickael Maison", "created": "2025-10-22T12:25:12.000+0000", "updated": "2025-10-24T09:51:27.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "New ConnectorClientConfigOverridePolicy with allowlist of configurations - Jira for KIP-1188: https://cwiki", "classifications": ["feature", "new feature"], "qa_pairs": []}}
{"id": "KAFKA-19823", "title": "PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions", "description": "There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}}\u00a0due to which we are setting\u00a0{{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than acquiredPartitionsSize", "status": "In Progress", "priority": "Major", "reporter": "Abhinav Dixit", "assignee": "Abhinav Dixit", "created": "2025-10-22T10:55:44.000+0000", "updated": "2025-10-24T15:25:07.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions - There is a bug in the broker logic of splitting bytes i...", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19822", "title": "Remove all static classes in Field except TaggedFieldsSection", "description": "All static classes in Field except TaggedFieldsSection are not really being used. We should remove them.", "status": "Open", "priority": "Major", "reporter": "Lan Ding", "assignee": "Lan Ding", "created": "2025-10-22T10:05:17.000+0000", "updated": "2025-10-22T10:05:17.000+0000", "labels": [], "components": ["clients"], "comments": [], "derived_tasks": {"summary": "Remove all static classes in Field except TaggedFieldsSection - All static classes in Field except TaggedFieldsSection are not really being used", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19821", "title": "Duplicated batches should be logged", "description": "When writing records with idempotent producer, the broker can help de-duplicate records. But when records being de-duplicated, there is no log in the broker side. This will confuse the producer because the records sent going to nowhere. Sometimes it's caused by the buggy producer that keeps using the wrong sequence number to cause the duplicated records. We should at least log the duplicated records info instead of pretending nothing happened.", "status": "Resolved", "priority": "Major", "reporter": "Luke Chen", "assignee": "Kuan Po Tseng", "created": "2025-10-22T07:43:55.000+0000", "updated": "2025-10-22T15:07:52.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Duplicated batches should be logged - When writing records with idempotent producer, the broker can help de-duplicate records", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19820", "title": "remove the unnecessary copy from AbstractFetch#fetchablePartitions", "description": "see https://github.com/apache/kafka/pull/14359#issuecomment-3372367774", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "TaiJuWu", "created": "2025-10-21T13:32:11.000+0000", "updated": "2025-10-23T03:06:20.000+0000", "labels": [], "components": ["clients", "consumer"], "comments": [], "derived_tasks": {"summary": "remove the unnecessary copy from AbstractFetch#fetchablePartitions - see https://github", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19819", "title": "Move BrokerMetadataPublisher to metadata module", "description": "", "status": "Open", "priority": "Major", "reporter": "Jimmy Wang", "assignee": "Jimmy Wang", "created": "2025-10-21T09:12:15.000+0000", "updated": "2025-10-21T09:12:15.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Move BrokerMetadataPublisher to metadata module", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19818", "title": "Move ClientQuotaMetadataManager to metadata module", "description": "", "status": "Open", "priority": "Major", "reporter": "Jimmy Wang", "assignee": "Jimmy Wang", "created": "2025-10-21T09:08:07.000+0000", "updated": "2025-10-21T09:08:07.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Move ClientQuotaMetadataManager to metadata module", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19817", "title": "Move DynamicTopicClusterQuotaPublisher to metadata module", "description": "", "status": "In Progress", "priority": "Major", "reporter": "Jimmy Wang", "assignee": "Jimmy Wang", "created": "2025-10-21T08:55:30.000+0000", "updated": "2025-10-21T09:05:12.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Move DynamicTopicClusterQuotaPublisher to metadata module", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19816", "title": "Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails", "description": "EosIntegrationTest.shouldNotViolateEosIfOneTaskFails is currently flaky for tests run with the \"streams\" group protocol.", "status": "Open", "priority": "Minor", "reporter": "Evan Zhou", "assignee": "Lucas Brutschy", "created": "2025-10-20T18:27:06.000+0000", "updated": "2025-10-22T17:06:49.000+0000", "labels": [], "components": ["streams", "unit tests"], "comments": [], "derived_tasks": {"summary": "Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails - EosIntegrationTest", "classifications": ["task", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19815", "title": "Implementation of ShareConsumer.acquisitionLockTimeoutMs() method", "description": "", "status": "Open", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-10-20T12:00:17.000+0000", "updated": "2025-10-20T12:00:17.000+0000", "labels": [], "components": ["clients"], "comments": [], "derived_tasks": {"summary": "Implementation of ShareConsumer.acquisitionLockTimeoutMs() method", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19814", "title": "Protocol schema and public API changes", "description": "This just adds the RPC schema and public API changes to let the implementation progress with multiple teams.", "status": "In Progress", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-10-20T11:58:18.000+0000", "updated": "2025-10-20T12:40:58.000+0000", "labels": [], "components": ["clients"], "comments": [], "derived_tasks": {"summary": "Protocol schema and public API changes - This just adds the RPC schema and public API changes to let the implementation progress with multiple teams", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19813", "title": "Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager", "description": "see https://github.com/apache/kafka/pull/14873#discussion_r2442794887, In below code snippets https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L116 https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L333 we use max.poll.interval.ms (default 300000) as the jitter value. Although RequestState.remainingBackoffMs guards with Math.max(0, \u2026), which won\u2019t make the value become negative, using a jitter that isn\u2019t in (0\u20131) is unexpected. In addition, we should validate that ExponentialBackoff receives a jitter strictly within (0, 1) to prevent this scenario in the future.", "status": "Open", "priority": "Major", "reporter": "Kuan Po Tseng", "assignee": "Kuan Po Tseng", "created": "2025-10-20T11:28:24.000+0000", "updated": "2025-10-22T15:23:45.000+0000", "labels": [], "components": ["clients", "consumer"], "comments": [], "derived_tasks": {"summary": "Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager - see https://github", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19812", "title": "Unbound Error Thrown if some variables are not set for SASL/SSL configuration", "description": "I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file \u2014 it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}. This issue is {*}just an enhancement to properly log the error details{*}.", "status": "Open", "priority": "Minor", "reporter": "Manas Poddar", "assignee": null, "created": "2025-10-19T14:13:48.000+0000", "updated": "2025-10-25T19:40:27.000+0000", "labels": [], "components": ["docker"], "comments": [{"author": "crw31", "body": "Hi [~scienmanas] , I\u2019d like to take this issue and work on improving the error handling for missing SASL/SSL environment variables in the Docker setup. Could you please assign this to me?", "created": "2025-10-23T17:57:30.310+0000"}, {"author": "Manas Poddar", "body": "Hi [~crw31] , I guess I don't have access to assign the issues.", "created": "2025-10-25T10:32:40.397+0000"}, {"author": "crw31", "body": "Hi [~scienmanas] , thanks for letting me know. Since I don\u2019t have assign permissions, I\u2019ll start working on this. If someone with commit rights wants to assign the ticket later, that would be great. Thanks!", "created": "2025-10-25T19:40:27.076+0000"}], "derived_tasks": {"summary": "Unbound Error Thrown if some variables are not set for SASL/SSL configuration - I was trying to set up a {*}Kafka container with SASL_SSL configura...", "classifications": ["improvement", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19811", "title": "Acked record on new topic not immediately visible to consumer", "description": "h2. Steps to reproduce * program uses a single broker (we see the issue with an in-JVM embedded kafka server) * create a new topic with 1 partition * produce a record with {{acks=all}} * await acknowledgement from the broker * start a consumer (configured to read from beginning of topic) * spuriously, _the consumer never sees the record_ The problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop. A delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible. We observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue. h2. Expected behavior A record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be immediately visible for all consumers.", "status": "Open", "priority": "Minor", "reporter": "Erik van Oosten", "assignee": null, "created": "2025-10-19T08:50:06.000+0000", "updated": "2025-10-23T13:05:53.000+0000", "labels": [], "components": [], "comments": [{"author": "Erik van Oosten", "body": "Addition: when the consumer misses the just produced record, it will also not see any record produced thereafter either (I didn't wait until a rebalance happened).", "created": "2025-10-19T15:39:42.424+0000"}, {"author": "Andrew Schofield", "body": "[~erikvanoosten] Can you provide the code for the consumer application? I am pretty confident that the problem as described in the issue would have been caught by automated tests. I wonder whether the consuming application design is making this occur.", "created": "2025-10-20T09:08:36.434+0000"}, {"author": "Erik van Oosten", "body": "Yes! The problem happens in the unit test of the open source zio-kafka library. Here are some pointer to tests that sometimes fail to consume any record (they time-out after 2 minutes): * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/SubscriptionsSpec.scala#L22-L54] * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/ConsumerSpec.scala#L170-L203] * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/ConsumerSpec.scala#L250-L289] (there are a few more) These test have in common that they follow the steps described in this issue.", "created": "2025-10-20T09:34:21.916+0000"}, {"author": "Erik van Oosten", "body": "Please let me know if you need help interpreting the code.", "created": "2025-10-20T09:42:03.906+0000"}, {"author": "Andrew Schofield", "body": "I've had a quick look at the code, but I must admit that I'm a bit of a Scala amateur. I suggest using a ConsumerRebalanceListener to ensure that your test has been assigned partitions to consume. Given the symptom about a test failing to receive the first record fails to receive any of them, it sounds like its not been assigned the partition to subscribe from. If this has occurred since the move to Apache Kafka 4.x, it may be related to the difference in behaviour of the `poll(long)` method which was removed and its replacement with `poll(Duration)`.", "created": "2025-10-20T13:58:36.361+0000"}, {"author": "Erik van Oosten", "body": "The problem has been around for some time (before 4.x) although anecdotally it did start to occur more in this year. I will add some debug logging that triggers when no partitions were assigned.", "created": "2025-10-20T16:02:46.705+0000"}, {"author": "Erik van Oosten", "body": "We just had a consumer lockup, where even the 10ms delay after producing was not enough.", "created": "2025-10-23T10:35:05.556+0000"}, {"author": "Erik van Oosten", "body": "[~schofielaj] Under what circumstances could no partition be assigned, even though there is only one broker and one consumer?", "created": "2025-10-23T13:05:28.181+0000"}], "derived_tasks": {"summary": "Acked record on new topic not immediately visible to consumer", "classifications": ["feature", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19810", "title": "Kafka streams with chained emitStrategy(onWindowClose) example does not work", "description": "Hi, I got this example by using the following prompt in Google: # kafka streams unit testing with chained \"emitStrategy\" # Provide an example of testing chained suppress with different grace periods [https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d] Compiled and ran the example using latest kafka jars only to get It appears that the test is not able to drive the kafka stream to emit the 2nd event. Could be a bug in test code/test driver/kafka streams? Thanks in advance Greg", "status": "Resolved", "priority": "Major", "reporter": "Greg F", "assignee": null, "created": "2025-10-19T06:01:45.000+0000", "updated": "2025-10-25T22:06:00.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Matthias J. Sax", "body": "The record you use to close the \"second window\" (ie, `inputTopic.pipeInput(\"D\", \"value6\", secondWindowCloseTime);`) will be processed by the first window, and will get \"stuck there\". It does open a new 1-minute window [10:07; 10:08) which is still open, and thus no result is emitted. Hence, the time for the second windowed-aggregation does not advance to `10:07` and it's own window [10:00; 10:05) does not close yet.", "created": "2025-10-20T23:25:27.708+0000"}, {"author": "Greg F", "body": "Thanks. is this a bug or a feature? I don't want several 1-minute windows for this test How do I fix the test so the end result (finalOutputTopic) contains a single aggregated value?", "created": "2025-10-21T01:13:43.603+0000"}, {"author": "Matthias J. Sax", "body": "It's not a bug, but behavior by design. For this particular test, you would need to send one more even, with ts => 10:08:30, to close the [10:07; 10:08) window; when this window gets closed the result goes into the second window operator, advancing the time there accordingly emitting the result of window [10:00; 10:05)", "created": "2025-10-21T01:29:02.483+0000"}, {"author": "Greg F", "body": "Hi you may close this issue as you wish. I have another one that I am about to open, hope to get your help with it. https://issues.apache.org/jira/browse/KAFKA-19828 Thanks", "created": "2025-10-23T00:50:25.923+0000"}], "derived_tasks": {"summary": "Kafka streams with chained emitStrategy(onWindowClose) example does not work - Hi, I got this example by using the following prompt in Google: # ka...", "classifications": ["bug"], "qa_pairs": [{"question": "Could be a bug in test code/test driver/kafka streams?", "answer": "The record you use to close the \"second window\" (ie, `inputTopic.pipeInput(\"D\", \"value6\", secondWindowCloseTime);`) will be processed by the first window, and will get \"stuck there\". It does open a new 1-minute window [10:07; 10:08) which is still open, and thus no result is emitted. Hence, the time for the second windowed-aggregation does not advance to `10:07` and it's own window [10:00; 10:05) does not close yet."}]}}
{"id": "KAFKA-19809", "title": "CheckStyle version upgrade: 10 -->> 12", "description": "*Task:* upgrade checkstyle version from *10.20.2* to *12.0.1* *Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1] *Note:*\u00a0difference between versions is quite big: * version *10.20.2* (published in Novemeber 2024) * version *12.0.1* (published in Octoober 2025) * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]", "status": "Patch Available", "priority": "Minor", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Dejan Stojadinovi\u0107", "created": "2025-10-18T13:16:55.000+0000", "updated": "2025-10-20T18:38:40.000+0000", "labels": ["checkstyle", "update", "updates", "upgrade", "upgrades"], "components": [], "comments": [{"author": "Dejan Stojadinovi\u0107", "body": "@Anyone: feel free to check PR: [https://github.com/apache/kafka/pull/20726]", "created": "2025-10-19T09:16:22.181+0000"}], "derived_tasks": {"summary": "CheckStyle version upgrade: 10 -->> 12 - *Task:* upgrade checkstyle version from *10", "classifications": ["task"], "qa_pairs": []}}
{"id": "KAFKA-19808", "title": "Handle batch alignment when share partition is at capacity", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Apoorv Mittal", "created": "2025-10-17T22:40:10.000+0000", "updated": "2025-10-21T14:42:53.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Handle batch alignment when share partition is at capacity", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19807", "title": "Add RPC-level integration tests for StreamsGroupHeartbeat", "description": "Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHeartbeatRequestTest` for `StreamsGroupHeartbeat`", "status": "Open", "priority": "Major", "reporter": "Lucy Liu", "assignee": "Lucy Liu", "created": "2025-10-17T20:09:30.000+0000", "updated": "2025-10-24T12:38:38.000+0000", "labels": [], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Add RPC-level integration tests for StreamsGroupHeartbeat - Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHear...", "classifications": ["feature", "test"], "qa_pairs": []}}
{"id": "KAFKA-19806", "title": "Hook to enable / disable multi-partition remote fetch feature", "description": "", "status": "Patch Available", "priority": "Major", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "created": "2025-10-17T08:58:11.000+0000", "updated": "2025-10-21T06:19:48.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Hook to enable / disable multi-partition remote fetch feature", "classifications": ["feature", "task"], "qa_pairs": []}}
{"id": "KAFKA-19804", "title": "Improve heartbeat request manager initial HB interval", "description": "With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it. That 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure). Probably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766] * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L255] We should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought) High level goals here would be to: * maintain the behaviour of sending a first HB without delay * ensure no unneeded activity on the HB mgr poll in the background, in tight loop, while we're just waiting for the first HB response with an interval * ensure the app thread poll timeout is not affected", "status": "Open", "priority": "Major", "reporter": "Lianet Magrans", "assignee": "Kuan Po Tseng", "created": "2025-10-16T20:49:52.000+0000", "updated": "2025-10-23T10:47:41.000+0000", "labels": [], "components": ["clients", "consumer"], "comments": [{"author": "Kuan Po Tseng", "body": "[~lianetm], are you planning to work on this issue? If not, I\u2019d be happy to take it over. Thanks!", "created": "2025-10-17T00:39:13.636+0000"}, {"author": "Lianet Magrans", "body": "Sure, feel free to take it and I can help with reviews. Thanks for your help!", "created": "2025-10-17T00:48:55.852+0000"}, {"author": "Arpit Goyal", "body": "[~brandboat]\u00a0 It seems you are already working on other issues .Can i take this up ? .It would be my first hand on the new consumer group protocol.", "created": "2025-10-17T06:01:23.901+0000"}, {"author": "Kuan Po Tseng", "body": "[~goyarpit] Thanks for checking! I\u2019ve already started exploring this one. If you are interested, you can help review once I have a PR ready.", "created": "2025-10-17T12:13:49.079+0000"}, {"author": "Kuan Po Tseng", "body": "Hi [~lianetm], I took a closer look at this issue and wanted to share a few thoughts. {quote} That 0 ms initial interval causes the HB manager poll to run continuously in a tight loop, executing logic that may not really be needed\u2014it mostly just waits for a response or failure. {quote} This actually shouldn\u2019t happen because we already check whether there\u2019s any in-flight heartbeat request before sending a new one: https://github.com/apache/kafka/blob/1330870efbb4efd9fd394ec5cb8a0fecf8e69b24/clients/src/main/java/org/apache/kafka/clients/consumer/internals/HeartbeatRequestState.java#L96 https://github.com/apache/kafka/blob/80f31224aad543dbfc892bce1ad73b6bb693855a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestState.java#L79 {quote} We should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought) {quote} It\u2019s also worth noting that if we set a non-zero initial interval, the first poll() call will block until the timeout because no partitions are assigned yet. That means no Fetch requests are sent and the fetchBuffer remains empty until the poll timeout expires. This \"tight loop\" behavior on the application thread is actually intentional per PR https://github.com/apache/kafka/pull/14835 to mitigate the case where Consumer.poll(Duration timeout) would otherwise block for the entire duration. I\u2019m thinking we could set a small non-zero initial heartbeat interval, what about 1s? to slightly loosen the tight loop. Of course, this means the first poll will still block for up to 1s. Using request.timeout.ms (default 30 s) with pollTimeout (e.g., 15 s) would make the first poll take too long before doing any useful work.", "created": "2025-10-20T16:51:25.807+0000"}], "derived_tasks": {"summary": "Improve heartbeat request manager initial HB interval - With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer H...", "classifications": ["improvement", "task"], "qa_pairs": []}}
{"id": "KAFKA-19803", "title": "Relax state directory file system restrictions", "description": "The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept. We could also make this configurable, which would require a KIP. KIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]", "status": "Resolved", "priority": "Minor", "reporter": "Matthias J. Sax", "assignee": "Nikita Shupletsov", "created": "2025-10-16T18:36:27.000+0000", "updated": "2025-10-25T16:14:42.000+0000", "labels": ["kip"], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Relax state directory file system restrictions - The implementation of permission restriction by https://issues", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19802", "title": "Update ShareGroupCommand to use share partition lag information", "description": "", "status": "Open", "priority": "Minor", "reporter": "Chirag Wadhwa", "assignee": "Andrew Schofield", "created": "2025-10-16T08:29:07.000+0000", "updated": "2025-10-25T11:09:24.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Update ShareGroupCommand to use share partition lag information", "classifications": ["improvement", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19793", "title": "Disable topic autocreation for streams consumers.", "description": "Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.", "status": "In Progress", "priority": "Minor", "reporter": "Nikita Shupletsov", "assignee": "Arpit Goyal", "created": "2025-10-15T19:15:21.000+0000", "updated": "2025-10-25T05:16:02.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Arpit Goyal", "body": "Hi [~nikita-shupletsov]\u00a0 can i pick this up ?", "created": "2025-10-16T01:12:46.027+0000"}, {"author": "Nikita Shupletsov", "body": "Hi, [~goyarpit]. yes, please", "created": "2025-10-16T01:44:03.146+0000"}, {"author": "Arpit Goyal", "body": "[~nikita-shupletsov]\u00a0 I saw in main consumer we made this change after override . So should we allow the overrride for this field\u00a0 or not ?. I will make the changes accordingly.", "created": "2025-10-17T09:51:53.208+0000"}, {"author": "Nikita Shupletsov", "body": "I don't think we should allow override for this field. [~mjsax] what do you think?", "created": "2025-10-17T16:15:16.868+0000"}, {"author": "Matthias J. Sax", "body": "Yes, we should not allow users to set this config. So we should add it to \"consumer default overwrites\" which are applied to all consumers.", "created": "2025-10-17T23:26:34.566+0000"}, {"author": "Arpit Goyal", "body": "[~nikita-shupletsov] \u00a0[~mjsax]\u00a0 PR is open for revview https://github.com/apache/kafka/pull/20723", "created": "2025-10-18T08:36:29.939+0000"}], "derived_tasks": {"summary": "Disable topic autocreation for streams consumers. - Currently we disable it only for [the main consumer|https://github", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19791", "title": "Add Idle Thread Ratio Metric to MetadataLoader", "description": "KIP-1229: [https://cwiki.apache.org/confluence/x/ZQteFw]", "status": "Open", "priority": "Major", "reporter": "Mahsa Seifikar", "assignee": "Mahsa Seifikar", "created": "2025-10-15T16:09:22.000+0000", "updated": "2025-10-25T03:21:55.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Add Idle Thread Ratio Metric to MetadataLoader - KIP-1229: [https://cwiki", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "KAFKA-19789", "title": "Handle situations where broker responses appear logically incorrect", "description": "We have seen a situation in which the ShareFetch responses from the broker appeared logically incorrect. The consumer should be more defensive, and at least log the problem.", "status": "In Progress", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Shivsundar R", "created": "2025-10-14T10:01:53.000+0000", "updated": "2025-10-24T10:32:04.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Handle situations where broker responses appear logically incorrect - We have seen a situation in which the ShareFetch responses from the broker ap...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19784", "title": "Expose Rack ID in MemberDescription", "description": "Currently, the {{{}AdminClient{}}}\u2019s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field. This causes users to be unable to retrieve member rack information through the Admin API. Rack information is crucial for: * Monitoring and visualization tools * Operational analysis of rack distribution * Diagnosing rack-aware assignment issues In addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent. The PR: \u00a0[https://github.com/apache/kafka/pull/20691] The KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription", "status": "Open", "priority": "Major", "reporter": "fujian", "assignee": "fujian", "created": "2025-10-12T09:34:16.000+0000", "updated": "2025-10-22T13:55:42.000+0000", "labels": ["needs-kip"], "components": ["admin", "clients"], "comments": [], "derived_tasks": {"summary": "Expose Rack ID in MemberDescription - Currently, the {{{}AdminClient{}}}\u2019s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does...", "classifications": ["task"], "qa_pairs": []}}
{"id": "KAFKA-19779", "title": "Relax offset commit validation to allow member epochs since assignment", "description": "h2. Fencing offset commits In the Kafka protocol, when a consumer commits offsets or a producer tries to add offsets to a transaction, it includes its epoch/generation of the consumer group. The point of this is for the group coordinator to fence against zombie commit requests, that is, commit requests that include an offset for a partition that was since reassigned to a different member. If such a guard was not in place, a zombie offset commit may overwrite offsets of the new owner, or its offsets may be committed to the consumer offset topic but not be included in the result of the new owners offset fetch request. In KIP-848, when receiving an offset commit request that includes the client-side member epoch and a member ID, the group coordinator performs the check {{Client-Side Member Epoch == Broker-Side Member Epoch}} and, if the check fails, returns a {{STALE_MEMBER_EPOCH}} error for regular offset commits and a {{ILLEGAL_GENERATION}} for transactional offset commits. If the member epoch sent in the request is the current broker-side member epoch, KIP-848 guarantees that the partition cannot also be owned by a different member at the same or a larger epoch. Therefore, this is sufficient for fencing zombie commits. Note that we assume zombie commits will always contain offsets for partitions that were owned by the member at the member epoch sent in the request. Commit requests that commit offsets for partitions that are _not_ owned by the member in that epoch, are not possible in a correct client-side implementation of the protocol. Note that the broker-side member epoch is not the group epoch or the target assignment epoch. For details, see KIP-848. Note also that commits can also be fenced because a member falls out of the group (e.g. because it does not revoke partitions within the rebalance timeout). At this point, its commits will be fenced solely based on the member ID (which is not part of the group anymore). We therefore ignore this case in this document, and only consider zombie commits from members that are still part of the group. h2. Downsides of the current approach This fencing is, however, unnecessarily strict. Assume, for example, a member owns P1 at epoch 1. The broker-side member epoch is bumped to 2, but the member still has P1 assigned at epoch 2. The member may not learn about the new broker-side member epoch in time, and submit an offset commit commit for P1 with epoch 1. This is not a zombie commit request as define above (because P1 was not reassigned to a different member), but it will still be rejected by a KIP-848 group coordinator. The trouble with this fencing mechanism is that it is very difficult to avoid the broker-side member epoch being bumped concurrently with an offset commit. Seen from the client-side, the broker-side member epoch may be bumped at any time while a heartbeat to the group coordinator is in-flight. To make sure the member epoch sent in an offset commit request is up-to-date would require making sure that no consumer group or streams group heartbeat request is in-flight at the same time. h2. Why a broker-side fix is warranted This problem is particularly challenging to solve on the client side for transactional offset commits, since they are performed by the producer, not the consumer, and the producer has no way of knowing when a consumer group heartbeat or streams group heartbeat is in-flight. The member epoch is passed from the Java consumer to the Java producer using the {{ConsumerGroupMetadata}} object, which is passed into {{{}sendOffsetsToTransaction{}}}. By the time the transactional offset commit is sent, the member epoch may be stale, the broker will return an {{ILLEGAL_GENERATION}} exception. This will force the Java producer into an abortable error state, surfacing the error as a {{CommitFailedException}} to the user, the user has no other way to recover from this to abort the transaction. This may hurt in any Kafka client application, since aborting transactions means throwing away work and restarting from an earlier point. But it is a particularly big problem in Kafka Streams with exactly-once semantics, where aborting a transaction usually means wiping and restoring the state store, so each aborted transaction means some downtime for apps using state stores of non-negligible size. Furthermore, since Kafka Streams commits every 100ms by default in EOS, this is likely to happen fairly often. h1. Conceptual Design n this design document, we therefore propose to relax the condition for offset commit fencing. h2. Identifying zombies using the last epoch a partition was assigned to the member To derive a more relaxed check, we need to identify an epoch which separates zombie commits from commits of the current owner. As mentioned above, zombie commit requests are commit requests that include a partition, member ID and member epoch combination, so that the member owned the partition at that epoch. However, the partition has since been reassigned to a different member. Most precisely, on the level of a single partition, a relaxed offset commit check can be defined using a *assignment epoch* for each assigned partition and each member, which is the epoch at which the partition was assigned to that member. To fence from zombie commit requests, we can reject all offset commit requests from a member that either does not have the partition assigned, or that includes any member epoch that is smaller or equal than the assignment epoch for that member and that partition. Assignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch The correctness of this is obvious: all commits of the current partition owner will be accepted, since the Client-Side Member Epoch of the current owner must always have an epoch that is larger or equal than the assignment epoch (a partition that is revoked in one epoch is never reassigned in the same epoch). It will also correctly reject any zombie commits from that member, because if a partition was owned by the member A at Client-Side Member Epoch (which we assume for zombie commits), but it was reassigned to member B since, we have two possible cases: # Member A currently does not have the partition assigned # Member A does currently have the partition assigned, but then it must have been reassigned to member A after being assigned to member B. By KIP-848 this cannot all happen in the same epoch, so we must have Assignment Epoch > Client-Side Member Epoch. h3. Differences to the design above This design does not need to disable commits on the client-side. The need to disable commits came from the fact that we are tracking epochs \u201cimprecisely\u201d, on the member-level and not on the partition-level. So in the RevocationEpoch design, when we have just revoked P2 on the client, we may attempt to commit a partition P1, triggering the race condition because the broker can concurrently bump the revocation epoch for that member because of the revocation of P2. We prevent this by disabling commits while a partition is revoked, and by \u201cwhile a partition is revoked\u201d I mean the timeframe from executing the revocation on the client, and seeing the following epoch bump on the client. In the partition-level AssignmentEpoch design, if we are committing P1, we are still convinced that we own P1, so we must also still own it on the broker. At the same time, we may remove the assignment epoch for P2 on the broker, but it doesn\u2019t matter since this doesn\u2019t impact whether we can commit P1, and we are not going to try to commit P2 after having revoked it on the client side. h1. Proposed Changes h3. Introducing Per-Member and Per-Partition Assignment Epoch We extend the model of a consumer group / streams group member with one integer per assigned partition for each member of a group. This includes both partitions directly assigned to the member, and partitions pending revocation. The assignment epoch is set to the epoch in which the partition was assigned to the member, and we have the invariant Assignment Epoch <= MemberEpoch <= TargetAssignmentEpoch <= GroupEpoch. The AssignmentEpoch is added as a field to TopicPartitions in ConsumerGroupCurrentMemberAssignmentValue, so that it can be stored and replayed from the committed offsets topic. For streams groups, we will use the same logic but add assignment epochs only for active tasks in StreamsGroupCurrentMemberAssignmentValue, since only active tasks commit offsets in Kafka Streams. h3. Relaxing the offset commit validation We replace the current check offset commit validation check Client-Side Member Epoch == Broker-Side Member Epoch by Assignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch where, for simplicity, we can assume the assignment epoch of a partition that is not assigned to that member to be Integer.maxValue.", "status": "Patch Available", "priority": "Major", "reporter": "Lucas Brutschy", "assignee": "Lucas Brutschy", "created": "2025-10-10T12:30:03.000+0000", "updated": "2025-10-23T15:10:11.000+0000", "labels": [], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Relax offset commit validation to allow member epochs since assignment", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19778", "title": "Share Partition Lag Persistence and Retrieval", "description": "Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets", "status": "Open", "priority": "Minor", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "created": "2025-10-10T07:59:04.000+0000", "updated": "2025-10-19T19:31:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Jimmy Wang", "body": "Hi [~chiragwadhwa55]\u00a0\uff0c I could help with some of the sub-tasks if you'd like to share the workload, thanks~", "created": "2025-10-16T09:24:58.073+0000"}, {"author": "Arpit Goyal", "body": "Hi [~chiragwadhwa55]\u00a0 i am\u0964happy to help in any of the subtasks Let me know if i can be of any help", "created": "2025-10-19T19:31:16.905+0000"}], "derived_tasks": {"summary": "Share Partition Lag Persistence and Retrieval - Ticket for KIP-1226", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19777", "title": "Generator | Fix order of arguments to assertEquals in unit tests", "description": "This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.", "status": "Open", "priority": "Trivial", "reporter": "Ksolves India Limited", "assignee": "Ksolves India Limited", "created": "2025-10-10T07:27:28.000+0000", "updated": "2025-10-24T16:33:56.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Generator | Fix order of arguments to assertEquals in unit tests - This sub-task is intended to fix the order of arguments passed in assertions in ...", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19773", "title": "Include push interval in ClientTelemetryReceiver context", "description": "Jira for KIP-1217 https://cwiki.apache.org/confluence/display/KAFKA/KIP-1217%3A+Include+push+interval+in+ClientTelemetryReceiver+context", "status": "Open", "priority": "Major", "reporter": "Mickael Maison", "assignee": "Maros Orsak", "created": "2025-10-08T15:55:54.000+0000", "updated": "2025-10-24T12:59:09.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Include push interval in ClientTelemetryReceiver context - Jira for KIP-1217 https://cwiki", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "KAFKA-19771", "title": "Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25", "description": "*Prologue:* KAFKA-19664 *In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106 *Action points:* - upgrade SpotBugs version (use Java 25 compatible version) - enable SpotBugs checks for Java 25 Github actions build *Related links:\u00a0* - [https://issues.apache.org/jira/projects/BCEL/versions/12354966] - [https://github.com/spotbugs/spotbugs/issues/3564]", "status": "Resolved", "priority": "Major", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Chia-Ping Tsai", "created": "2025-10-08T09:06:12.000+0000", "updated": "2025-10-19T11:36:40.000+0000", "labels": ["Java25", "build", "spotbugs", "update", "upgrade"], "components": ["build"], "comments": [{"author": "Dejan Stojadinovi\u0107", "body": "*{{Apache commons-bcel}}* Java 25 compatible version should be released soon: https://github.com/apache/commons-bcel/releases/tag/commons-bcel-6.11.0-RC1", "created": "2025-10-10T08:06:26.403+0000"}, {"author": "Dejan Stojadinovi\u0107", "body": "*Apache* *{{commons-bcel}}* new version has been released; Spotbugs version should be released soon: - [https://github.com/spotbugs/spotbugs/pull/3763] - [https://github.com/spotbugs/spotbugs/milestone/31?closed=1] - [https://github.com/spotbugs/spotbugs/discussions/3771]", "created": "2025-10-13T07:35:21.309+0000"}, {"author": "Dejan Stojadinovi\u0107", "body": "GitHub PR is created here: https://github.com/apache/kafka/pull/20704", "created": "2025-10-15T06:17:02.751+0000"}, {"author": "Dejan Stojadinovi\u0107", "body": "[~chia7712] Please review when you find some time: [https://github.com/apache/kafka/pull/20704]", "created": "2025-10-17T08:49:41.022+0000"}], "derived_tasks": {"summary": "Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25 - *Prologue:* KAFKA-19664 *In more details:* Apache commons bcel Java 25 compat...", "classifications": ["improvement", "task", "bug"], "qa_pairs": [{"question": "org/jira/browse/BCEL-377?", "answer": "*{{Apache commons-bcel}}* Java 25 compatible version should be released soon: https://github.com/apache/commons-bcel/releases/tag/commons-bcel-6.11.0-RC1"}]}}
{"id": "KAFKA-19768", "title": "`ci-complete` needs to work with active branches after the JDK is updated", "description": "The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Ming-Yen Chung", "created": "2025-10-07T16:31:26.000+0000", "updated": "2025-10-24T18:31:13.000+0000", "labels": ["build", "github-actions"], "components": ["build"], "comments": [{"author": "Chia-Ping Tsai", "body": "Perhasp we could use multiple branches to handle this case.  jobs: upload-build-scan-main: if: github.event.workflow_run.head_branch == 'trunk' runs-on: ubuntu-latest strategy: matrix: # ... rest of the steps upload-build-scan-other: if: github.event.workflow_run.head_branch != 'trunk' runs-on: ubuntu-latest strategy: matrix: # ... rest of the steps", "created": "2025-10-07T16:38:33.961+0000"}, {"author": "Dejan Stojadinovi\u0107", "body": "Another idea to be considered (on): https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/github-code-transformation-workflow-advanced.html FYI [~mingyen066]", "created": "2025-10-07T22:03:01.032+0000"}, {"author": "David Arthur", "body": "Do we know if it's possible to upload a build scan using Gradle version 9 that was produced using Gradle 8? I expect it might not work since the build scan data format may change. One thing we could try is to include a file in the build scan archive that indicates what version of Gradle and JDK were used. We can load these properties before running setup-gradle so we can select the correct version. There's nothing built in to GHA that helps here (as far as i know), maybe we can use JSON and \"jq\"? I won't be able to work on this unfortunately, but I'm happy to help with reviews.", "created": "2025-10-14T00:03:32.595+0000"}, {"author": "Ming-Yen Chung", "body": "Thanks for the suggestions,\u00a0[~dejan2609] and [~davidarthur] After understanding the situation, I think the problem is not just the JDK version \u2013 the artifact names are also different. The newer archive uses the names with suffix ['flaky' | 'noflaky']-['new'|'nonew'], which 4.0 does not have.", "created": "2025-10-24T18:29:16.100+0000"}], "derived_tasks": {"summary": "`ci-complete` needs to work with active branches after the JDK is updated - The JDK version used by `ci-complete` is inconsistent with other active...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19767", "title": "Improve handling of long processing times", "description": "If the share consumer application takes a long time to process records, the share consume request manager can do a better job of time-outs and liveness.", "status": "In Progress", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-10-07T16:15:01.000+0000", "updated": "2025-10-23T17:36:17.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Improve handling of long processing times - If the share consumer application takes a long time to process records, the share consume request manag...", "classifications": ["improvement", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19765", "title": "Store the last used assignment configuration in the group metadata", "description": "We should store the last used assignment configuration in the group metadata and bump the group epoch if the assignment configuration is changed.", "status": "Resolved", "priority": "Major", "reporter": "Lucas Brutschy", "assignee": "Lucy Liu", "created": "2025-10-07T14:50:58.000+0000", "updated": "2025-10-24T16:00:35.000+0000", "labels": [], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Store the last used assignment configuration in the group metadata - We should store the last used assignment configuration in the group metadata a...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19764", "title": "KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator", "description": "Ticket for KIP-1224. * Add a new allowed value for {{group.coordinator.append.linger.ms}} and {{share.coordinator.append.linger.ms}} of -1. * When {{append.linger.ms}} is set to -1, use the flush strategy outlined in the KIP. * Add batch-linger-time metrics. * Add batch-flush-time metrics.", "status": "Open", "priority": "Minor", "reporter": "Sean Quah", "assignee": "Sean Quah", "created": "2025-10-07T14:20:27.000+0000", "updated": "2025-10-22T13:49:09.000+0000", "labels": [], "components": ["group-coordinator"], "comments": [], "derived_tasks": {"summary": "KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator - Ticket for KIP-1224", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "KAFKA-19763", "title": "Parallel remote reads causes memory leak in broker", "description": "This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915 Broker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. Steps to reproduce: 1. Start a one node broker and configure LocalTieredStorage as remote storage. 2. Create a topic with 5 partitions. 3. Produce message and ensure that few segments are uploaded to remote. 4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log. 5. The broker crashes with the OOM error. 6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes. cc [~showuon] [~satish.duggana]", "status": "Resolved", "priority": "Blocker", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "created": "2025-10-07T13:53:55.000+0000", "updated": "2025-10-20T00:15:56.000+0000", "labels": [], "components": [], "comments": [{"author": "Luke Chen", "body": "[~ckamal] , I tired to reproduce it using your steps, but the heap only increase ~ 100 MB, not a big issue IMO. Have you figured it out where do we leak the memory?", "created": "2025-10-08T06:33:23.072+0000"}, {"author": "Kamal Chandraprakash", "body": "[~showuon] Able to reproduce the issue consistently. Uploaded the RemoteReadMemoryLeakReproducer. The leak was due to that the DelayedRemoteFetchPurgatory holding the references of previously completed DelayedRemoteFetch objects. DelayedRemoteFetch contains the RemoteReadResult internally. > Have you figured it out where do we leak the memory? In a given FETCH request, if 1 out of 5 partition, read the data from local log, then the watcherKey for that partition holds the reference of the DelayedRemoteFetch in the purgatory; if there are no other remote-read happens for that partition, then it won't get removed until the reaper thread cleans it up after the purgeInterval (entries) of 1000.  % sh kafka-topics.sh --create --topic apple --partitions 5 --replication-factor 1 --bootstrap-server localhost:9092 --config remote.storage.enable=true --config local.retention.ms=60000 --config retention.ms=7200000 --config segment.bytes=104857600 --config file.delete.delay.ms=1000 % for i in `seq 1 100`; do echo $i; sleep 1; sh kafka-producer-perf-test.sh --topic apple --num-records 1200000000 --record-size 1024 --throughput 1000 --producer-props bootstrap.servers=localhost:9092; done", "created": "2025-10-08T09:12:17.837+0000"}, {"author": "Luke Chen", "body": "This patch fixes the problem. But it slows down the read throughput because it takes time to clone the buffer. There should be other better solutions.  --- a/core/src/main/scala/kafka/server/DelayedRemoteFetch.scala +++ b/core/src/main/scala/kafka/server/DelayedRemoteFetch.scala @@ -22,6 +22,7 @@ import kafka.utils.Logging import org.apache.kafka.common.TopicIdPartition import org.apache.kafka.common.errors._ import org.apache.kafka.common.protocol.Errors +import org.apache.kafka.common.record.MemoryRecords import org.apache.kafka.server.LogReadResult import org.apache.kafka.server.metrics.KafkaMetricsGroup import org.apache.kafka.server.purgatory.DelayedOperation @@ -121,7 +122,8 @@ class DelayedRemoteFetch(remoteFetchTasks: util.Map[TopicIdPartition, Future[Voi result.error, result.highWatermark, result.leaderLogStartOffset, - \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0info.records, + \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// clone the record buffer to release the memory + \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0MemoryRecords.readableRecords(info.records.asInstanceOf[MemoryRecords].buffer()), Optional.empty(), if (result.lastStableOffset.isPresent) OptionalLong.of(result.lastStableOffset.getAsLong) else OptionalLong.empty(), info.abortedTransactions, @@ -132,7 +134,8 @@ class DelayedRemoteFetch(remoteFetchTasks: util.Map[TopicIdPartition, Future[Voi tp -> result.toFetchPartitionData(false) } } - + \u00a0 \u00a0// clear the map to avoid memory leak + \u00a0 \u00a0remoteFetchResults.clear() responseCallback(fetchPartitionData) } }", "created": "2025-10-08T10:16:34.982+0000"}, {"author": "Satish Duggana", "body": "This issue is fixed with https://github.com/apache/kafka/pull/20654 and https://github.com/apache/kafka/pull/20706", "created": "2025-10-20T00:13:21.724+0000"}], "derived_tasks": {"summary": "Parallel remote reads causes memory leak in broker - This issue is caused with changes from https://issues", "classifications": ["task"], "qa_pairs": []}}
{"id": "KAFKA-19762", "title": "Turn on Gradle reproducible builds feature", "description": "*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] *Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future) *Related Gradle issues and links:* * \u00a0[https://github.com/gradle/gradle/issues/34643] * [https://github.com/gradle/gradle/issues/30871] * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives] * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps]", "status": "Open", "priority": "Minor", "reporter": "Dejan Stojadinovi\u0107", "assignee": null, "created": "2025-10-07T11:23:45.000+0000", "updated": "2025-10-19T06:20:10.000+0000", "labels": ["Gradle", "build", "gradle"], "components": ["build"], "comments": [{"author": "Naveen", "body": "Hi [~dejan2609] Per the docs\u00a0reproducible builds is enabled by deafult on gradle 9  Starting with Gradle 9, archives are reproducible by default.  is this code block no longer required?  tasks.withType(AbstractArchiveTask).configureEach { reproducibleFileOrder = false preserveFileTimestamps = true useFileSystemPermissions() }  Sources: [Gradle Docks|https://docs.gradle.org/current/userguide/working_with_files.html#sec:reproducible_archives] [Gradle Reproducible Plugin|https://gradlex.org/reproducible-builds/]", "created": "2025-10-19T06:20:10.402+0000"}], "derived_tasks": {"summary": "Turn on Gradle reproducible builds feature - *Prologue:* [https://github", "classifications": ["feature", "task"], "qa_pairs": []}}
{"id": "KAFKA-19761", "title": "Reorder Gradle tasks (in order to bump Shadow plugin version)", "description": "*Prologue:* * JIRA ticket: KAFKA-19174 * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027] *Scenario:* * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+ * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x) *Action points (what needs to be done):* * use `com.gradleup.shadow` recent version (9+) * reorder Gradle tasks so that Gradle command mentioned above can work", "status": "Open", "priority": "Minor", "reporter": "Dejan Stojadinovi\u0107", "assignee": null, "created": "2025-10-07T11:13:51.000+0000", "updated": "2025-10-21T13:48:32.000+0000", "labels": ["Gradle", "build", "gradle"], "components": ["build"], "comments": [], "derived_tasks": {"summary": "Reorder Gradle tasks (in order to bump Shadow plugin version) - *Prologue:* * JIRA ticket: KAFKA-19174 * GitHub PR comment: [https://github", "classifications": ["task"], "qa_pairs": []}}
{"id": "KAFKA-18679", "title": "KafkaRaftMetrics metrics are exposing doubles instead of integers", "description": "The following metrics are being exposed as floating point doubles instead of ints/longs: * log-end-offset * log-end-epoch * number-unkown-voter-connections * current-leader * current-vote * current-epoch * high-watermark This issue extends to a lot of other metrics, which may be intending to report only integer/long values, but are instead reporting doubles. Link to GH discussion detailing issue further: https://github.com/apache/kafka/pull/18304#discussion_r1934364595", "status": "Open", "priority": "Major", "reporter": "Kevin Wu", "assignee": "TaiJuWu", "created": "2025-01-30T16:41:17.000+0000", "updated": "2025-10-25T03:41:22.000+0000", "labels": [], "components": ["kraft"], "comments": [{"author": "TaiJuWu", "body": "Hi [~kevinwu2412] , may I pick this up?", "created": "2025-01-31T09:52:46.979+0000"}], "derived_tasks": {"summary": "KafkaRaftMetrics metrics are exposing doubles instead of integers - The following metrics are being exposed as floating point doubles instead of in...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-18477", "title": "remove usage of OffsetForLeaderEpochRequest in AbstractFetcherThread", "description": "This is because of the base MV in 4.0 is 3.0.", "status": "Open", "priority": "Major", "reporter": "Jun Rao", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-01-11T00:26:28.000+0000", "updated": "2025-10-24T03:20:56.000+0000", "labels": [], "components": ["core"], "comments": [{"author": "\u9ec3\u7ae3\u967d", "body": "Hello [~junrao], If you wont work on this, may I take it?", "created": "2025-01-11T00:28:06.226+0000"}, {"author": "Jun Rao", "body": "[~m1a2st] : Thanks for helping out. Feel free to take this.", "created": "2025-01-11T01:33:27.458+0000"}, {"author": "\u9ec3\u7ae3\u967d", "body": "Hello [~junrao] I have a question, There are two request in AbstractFetcherThread, one is `OffsetsForLeaderEpochRequest` and the other is `OffsetForLeaderEpochRequestData`, I found in `OffsetForLeaderEpochRequestData.json` only remove the version 0-1 in 4.0, thus Why we should remove `OffsetForLeaderEpochRequestData` this? I think I missed something, Thanks for helping :)", "created": "2025-02-04T13:31:57.426+0000"}, {"author": "Jun Rao", "body": "[~m1a2st] : In AbstractFetcherThread, we have the following code.  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = { if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) { currentState } else if (initialFetchState.initOffset < 0) { fetchOffsetAndTruncate(tp, initialFetchState.topicId, initialFetchState.currentLeaderEpoch) } else if (leader.isTruncationOnFetchSupported) { // With old message format, `latestEpoch` will be empty and we use Truncating state // to truncate to high watermark. val lastFetchedEpoch = latestEpoch(tp) val state = if (lastFetchedEpoch.nonEmpty) Fetching else Truncating PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state, lastFetchedEpoch) } else { PartitionFetchState(initialFetchState.topicId, initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch, state = Truncating, lastFetchedEpoch = None) } }  leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state.", "created": "2025-02-07T01:07:35.539+0000"}, {"author": "\u9ec3\u7ae3\u967d", "body": "{quote}leader.isTruncationOnFetchSupported is always true in 4.0. latestEpoch(tp) should always be available since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} `leader.isTruncationOnFetchSupported` is not alaways true, There are two implement in production code, one is RemoteLeaderEndPoint and another is LocalLeaderEndPoint, the properties isTruncationOnFetchSupported[1] in LocalLeaderEndPoint is `false`, It seens we only can simplified the logic on `latestEpoch(tp)` [1] https://github.com/apache/kafka/blob/e53af1a48974926d2e671b2d02f6bedf0394d3d6/core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala#L58", "created": "2025-02-10T14:41:40.640+0000"}, {"author": "Jun Rao", "body": "[~m1a2st] : Sorry for the late reply. We can change LocalLeaderEndPoint.isTruncationOnFetchSupported() to true since in 4.0, every replica supports lastFetchedEpoch in the fetch request. So, we could handle truncation in both LocalLeaderEndPoint and RemoteLeaderEndPoint in the same way based on the DivergingEpoch field in FetchResponse. A couple of more details. 1. Currently, in LocalLeaderEndPoint, we don't call partitionData.setDivergingEpoch() in processResponseCallback(). So we need to add that. 2. When an empty replica is added, it doesn't have an initial latest epoch. In that case we can transition to the Fetching state with -1 as the latest epoch. The server will just bypass the latest epoch validation. This is ok since the replica has no existing data.", "created": "2025-02-20T20:05:07.916+0000"}, {"author": "Chia-Ping Tsai", "body": "{quote} since we only have V2 message format in 4.0. So, we only need to transition to the Fetching state and not the Truncating state. OffsetsForLeaderEpochRequest is only needed in the Truncating state. {quote} [~junrao] Excuse me, since 4.0 can still \"read\" v0/v1 messages, can it fetch a replica containing only v0/v1 messages? For example, a topic has existed for a long time and users haven't performed compaction or retention. One day, users want to create more replicas for it.", "created": "2025-03-01T06:39:32.050+0000"}, {"author": "Ismael Juma", "body": "It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. And direct upgrades to 4.0 have to be done from 3.3. It is possible for compacted topics to have some V0/V1 records though.", "created": "2025-03-01T16:25:02.894+0000"}, {"author": "Chia-Ping Tsai", "body": "{quote} It's unlikely that any topic would only have V0/V1 records since any writes with IBP >= 3.0 would result in a V2 record being written. {quote} I understand it's an edge case, but a topic with only v0/v1 records is still valid in 4.x, right? {quote} latestEpoch(tp) should always be available since we only have V2 message format in 4.0 {quote} Regardless of record version, the leaderEpochCache is updated when appending records. Therefore, the follower fetcher doesn't know the latestEpoch when starting the fetch thread. However, this looks like a no-op because it truncates the offset to 0.  [2025-03-03 09:21:43,792] INFO [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition 33opic-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread) [2025-03-03 09:21:43,792] INFO [UnifiedLog partition=33opic-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)", "created": "2025-03-03T09:33:11.221+0000"}, {"author": "Jun Rao", "body": "[~chia7712] : To send an OffsetForLeaderRequest, we need the leader epoch. If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. When a new replica is first created, it creates an empty log with offset 0. If the leader still contains offset 0, the replica will catch up through the fetched data. If the leader no longer contains offset 0, it will reply with an OffsetOutOfRangeException. The replica will then truncate to the earliest offset in the leader by issuing a ListOffset request. So, all the processes in the follower can be done without issuing an OffsetForLeaderRequest.", "created": "2025-03-03T18:40:54.573+0000"}, {"author": "Chia-Ping Tsai", "body": "[~junrao] thanks for your explanation. {quote} If there is no leader epoch, we could just fall back to sending a FetchRequest with -1 as LastFetchedEpoch to achieve the same behavior. {quote} Pardon me, do you mean the truncation logic using the high watermark can be removed?", "created": "2025-03-04T17:03:44.059+0000"}, {"author": "Jun Rao", "body": "{quote}do you mean the truncation logic using the high watermark can be removed? {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs. Then, we will never need to call truncateToHighWatermark().  private def maybeTruncate(): Unit = { val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions() if (partitionsWithEpochs.nonEmpty) { truncateToEpochEndOffsets(partitionsWithEpochs) } if (partitionsWithoutEpochs.nonEmpty) { truncateToHighWatermark(partitionsWithoutEpochs) } }", "created": "2025-03-04T19:54:10.473+0000"}, {"author": "Jun Rao", "body": "So, to summarize, we can remove the usage of OffsetForLeaderEpochRequest and only do follower truncation if FetchReponse.DivergingEpoch is not empty or the fetch offset is out of range.", "created": "2025-03-04T20:12:18.720+0000"}, {"author": "Chia-Ping Tsai", "body": "[~junrao] thanks for your patience!. {quote} If the PartitionState never enters the Truncating state, fetchTruncatingPartitions() will always return empty for both partitionsWithEpochs and partitionsWithoutEpochs {quote} I was considering what would happen if we don't truncate the v0/v1 replica using the high watermark. For instance, if a restarted 4.0 broker has a v0/v1 replica with [LEO=11, HW=10], it must truncate to HW before sending a fetch, right? I know this case is extremely rare, as the v0/v1 replica in an upgraded 4.0 broker should have HW=LEO. Also, if we never need to call truncateToHighWatermark(), should we change the initial offset from HW to LEO?  protected def initialFetchOffset(log: UnifiedLog): Long = { if (log.latestEpoch.nonEmpty) log.logEndOffset else log.highWatermark }  https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L2430", "created": "2025-03-05T04:11:14.564+0000"}, {"author": "Jun Rao", "body": "[~chia7712] : Good point. If the follower doesn't have the latest epoch, currently we fall back to truncating based on HWM. So we can still keep that part of the logic to support v0/v1 records. However, there is no need to ever issue OffsetForLeaderEpochRequests.", "created": "2025-03-05T18:36:55.218+0000"}, {"author": "Chia-Ping Tsai", "body": "[~junrao] Thank you for all your responses. I finally understand this issue :)", "created": "2025-03-06T11:36:42.219+0000"}, {"author": "Mickael Maison", "body": "Moving to the next release as we're now in code freeze for 4.1.0.", "created": "2025-06-25T09:31:20.106+0000"}], "derived_tasks": {"summary": "remove usage of OffsetForLeaderEpochRequest in AbstractFetcherThread - This is because of the base MV in 4", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-18379", "title": "Enforce resigned cannot transition to any other state in same epoch", "description": "", "status": "Open", "priority": "Major", "reporter": "Alyssa Huang", "assignee": "TengYao Chi", "created": "2024-12-30T19:20:53.000+0000", "updated": "2025-10-19T03:28:58.000+0000", "labels": [], "components": [], "comments": [{"author": "TengYao Chi", "body": "Hi [~alyssahuang] I would take over this issue, thanks :)", "created": "2024-12-30T23:05:18.202+0000"}, {"author": "Alyssa Huang", "body": "Thanks [~frankvicky]! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket.", "created": "2025-01-02T07:11:40.905+0000"}, {"author": "TengYao Chi", "body": "Hi [~alyssahuang] Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)", "created": "2025-01-02T07:23:21.907+0000"}], "derived_tasks": {"summary": "Enforce resigned cannot transition to any other state in same epoch", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-18376", "title": "High CPU load when AsyncKafkaConsumer uses a small max poll value", "description": "We stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally high CPU usage.\u00a0 Under normal usage (with defaults), the consumers on average use around 10% of the CPU with 20mb/s byte rate, which is aligned with the ClassicKafkaConsumer.\u00a0 As we tested the consumer with a small max poll value, we observed the CPU usage spikes to > 50% while the classic consumer stays at around 10%. _note: percentage of CPU usage may depend on the running pod hardware._ The profiling results shows two major contributors to the CPU cycles # AsyncKafkaConsumer.updateFetchPosition (addAndGet & new CheckAndUpdatePositionEvent()) # AbstractFetch.fetchablePartitions from the fetchrequestmanager for AsyncKafkaConsumer.updateFetchPosition - It seems like * UUID generation can become quite expensive. This is particularly noticeable when creating large number of events * ConsumerUtils.getResult, which uses future.get() also consumes quite a bit of CPU cycles for fetchablePartitions, FetchBuffer.bufferedPartitions which uses Java ConcurrentLinkedQueue.forEach also consumes quite a bit of CPUs.", "status": "Patch Available", "priority": "Major", "reporter": "Philip Nee", "assignee": "Kirk True", "created": "2024-12-30T17:37:31.000+0000", "updated": "2025-10-23T13:31:21.000+0000", "labels": ["async-kafka-consumer-performance", "consumer-threading-refactor", "performance"], "components": ["clients", "consumer"], "comments": [], "derived_tasks": {"summary": "High CPU load when AsyncKafkaConsumer uses a small max poll value - We stress tested the AsyncConsumer using maxPoll = 5 and observed abnormally hi...", "classifications": ["bug", "performance"], "qa_pairs": []}}
{"id": "KAFKA-17901", "title": "Handle fenced exception while acquiring fetch lock", "description": "https://github.com/apache/kafka/pull/16842#discussion_r1821661392", "status": "Resolved", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Apoorv Mittal", "created": "2024-10-30T17:55:37.000+0000", "updated": "2025-10-21T11:16:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Arpit Goyal", "body": "[~apoorvmittal10]\u00a0 can i pick this up , if you have not started yet ?", "created": "2025-10-21T09:34:53.503+0000"}, {"author": "Apoorv Mittal", "body": "This ticket is meant to be closed as the code we have do not currently need the fix. [~goyarpit] Thanks for reaching out though. Please let me know if you find something else to be picked.", "created": "2025-10-21T09:42:20.570+0000"}, {"author": "Arpit Goyal", "body": "[~apoorvmittal10]\u00a0 it seems all ticket has neen assigned , is it possible for you to assign me something to start on queues.Happy to pick up", "created": "2025-10-21T11:16:03.133+0000"}], "derived_tasks": {"summary": "Handle fenced exception while acquiring fetch lock - https://github", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-17895", "title": "Expose KeyValueStore's approximateNumEntries as a metric", "description": "Tracking the evolution of a state store's size is often useful. For example, we often use state store to persist pending work and set alert on maximum size because it means the process is falling behind. While KafkaStreams exposes many generic state store related or RocksDB specificif metrics; is does not expose KeyValueStore#approximateNumEntries which is a key information. Is it an oversight or was it a deliberate choice? I would be great if this metrics could be added. I assume that both in-memory & rocksdb implementation of approximateNumEntries are fast enought to be used in metrics.", "status": "Open", "priority": "Minor", "reporter": "Cl\u00e9ment MATHIEU", "assignee": "Evan Zhou", "created": "2024-10-29T16:41:00.000+0000", "updated": "2025-10-23T03:43:32.000+0000", "labels": ["needs-kip"], "components": ["streams"], "comments": [{"author": "A. Sophie Blee-Goldman", "body": "My guess is that this wasn't exposed as a metric because it's only an approximation and has no guarantee of being correct. For in-memory stores it's probably going to be accurate but I have no idea how accurate the rocksdb metric will be and would imagine it can be quite misleading. It's based on the property estimate-num-keys if you want to do some research there. That said, there's no reason not to expose this as a metric I suppose. We accept KIPs if you'd like to pick this up this yourself!", "created": "2024-10-29T20:05:10.987+0000"}, {"author": "Matthias J. Sax", "body": "Are you sure it's not collected? My understanding is, that it should be there at DEBUG level? [https://kafka.apache.org/documentation/#kafka_streams_rocksdb_monitoring] It's called `estimate-num-keys` \u2013 should be the same thing?", "created": "2024-10-30T06:03:46.257+0000"}, {"author": "Cl\u00e9ment MATHIEU", "body": "> My guess is that this wasn't exposed as a metric because it's only an approximation and has no guarantee of being correct. As [~mjsax] said, it\u2019s already exposed for rocksdb. Indeed, it\u2019s an approximation. Personally, I mostly use in-memory state stores. But anyone using rocksdb can decide if the approximation is good enough for their use case. > It's called `estimate-num-keys` \u2013 should be the same thing? It is; but it\u2019s rocksdb specific. It would be great to have it exposed for in-memory state stores too. Ideally, metric name would be the same for both types of state stores. If this is difficult, having a different name for the two types is better than not having the information or having to add the metric yourself. To give you some context; I use Micrometer to collect Kafka Streams metrics, export them to a Prometheus and then set a rule to fire an alert if the store size exceeds a certain size. It works well for use cases where state stores are used to persist pending work item (ex. retries of failures). In steady state, store size is mostly linear to input load. A growing size signals there is a problem.", "created": "2024-11-07T14:11:12.081+0000"}, {"author": "Matthias J. Sax", "body": "Thanks for clarification. - I have no objection to add a corresponding metric for in-memory stores. {quote}Ideally, metric name would be the same for both types of state stores. {quote} Not sure if that's possible in general. Store metrics are naturally very store specific IMHO, and only a few might apply to all stores...", "created": "2024-11-07T20:44:43.708+0000"}], "derived_tasks": {"summary": "Expose KeyValueStore's approximateNumEntries as a metric - Tracking the evolution of a state store's size is often useful", "classifications": ["improvement"], "qa_pairs": [{"question": "Is it an oversight or was it a deliberate choice?", "answer": "My guess is that this wasn't exposed as a metric because it's only an approximation and has no guarantee of being correct. For in-memory stores it's probably going to be accurate but I have no idea how accurate the rocksdb metric will be and would imagine it can be quite misleading. It's based on the property estimate-num-keys if you want to do some research there. That said, there's no reason not to expose this as a metric I suppose. We accept KIPs if you'd like to pick this up this yourself!"}]}}
{"id": "KAFKA-17853", "title": "Console share consumer is not terminated immediately", "description": "Even when share groups is not enabled , if pressed ctrl+c, then client doesn't immediately shutsdown.  ./bin/kafka-console-share-consumer.sh --bootstrap-server localhost:9092 --topic T1 --group SG1 --consumer-property client.id=share-consumer-2 [2024-10-22 17:01:07,958] ERROR [ShareConsumer clientId=share-consumer-2, groupId=SG1] ShareGroupHeartbeatRequest failed due to fatal error: The node does not support SHARE_GROUP_HEARTBEAT (org.apache.kafka.clients.consumer.internals.ShareHeartbeatRequestManager) [2024-10-22 17:01:07,958] ERROR [ShareConsumer clientId=share-consumer-2, groupId=SG1] Member <no ID> with epoch 0 transitioned to fatal state (org.apache.kafka.clients.consumer.internals.ShareMembershipManager) [2024-10-22 17:01:12,763] ERROR Error processing message, terminating consumer process: \u00a0(org.apache.kafka.tools.consumer.ConsoleShareConsumer) ^C^C Processed a total of 0 messages", "status": "Reopened", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Shivsundar R", "created": "2024-10-22T16:04:59.000+0000", "updated": "2025-10-23T07:08:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Jimmy Wang", "body": "Hi [~apoorvmittal10] , could I work on this?", "created": "2025-01-13T16:57:42.245+0000"}, {"author": "Apoorv Mittal", "body": "Please go ahead, also it might be that the issue is now fixed as part of other ongoing work for KIP-932. Incase you couldn't reproduce the issue, then please drop a not and I ll re-verify.", "created": "2025-01-14T21:40:21.877+0000"}, {"author": "Jimmy Wang", "body": "[~apoorvmittal10] It seems that the consumer will exit as soon as the UnsupportedVersionException raised, and I can't reproduce this.", "created": "2025-01-22T09:42:58.575+0000"}, {"author": "Apoorv Mittal", "body": "[~jimmywang611] Thanks for looking into it, I re-verified and the issue seems to be fixed by other onngoing fixes. I cannot re-produce hence closing the ticket.", "created": "2025-01-22T09:48:25.800+0000"}, {"author": "Andrew Schofield", "body": "The issue still occurs if the broker is not available. If you run `kafka-console-consumer.sh` when the broker is not running, it prints out lots of error messages but can be terminated immediately with CTRL-C. If you run `kafka-console-share-consumer.sh` when the broker is not running, CTRL-C does work, but it takes maybe 30 seconds to terminate.", "created": "2025-02-18T10:53:56.500+0000"}, {"author": "Jimmy Wang", "body": "[~schofielaj] Thanks for your feedback, I will try to reproduce that and figure out the\u00a0underlying issue.", "created": "2025-02-18T10:57:53.096+0000"}, {"author": "Apoorv Mittal", "body": "[~jimmywang611] Just following up, are you still able to work on it?", "created": "2025-05-06T09:06:25.860+0000"}, {"author": "Arpit Goyal", "body": "[~apoorvmittal10] \u00a0[~shivsundar]\u00a0 Can i pick this up , if this still needs to work on", "created": "2025-10-23T06:56:29.337+0000"}, {"author": "Shivsundar R", "body": "Hi [~goyarpit] , the PR for this is up - [https://github.com/apache/kafka/pull/19886.] and is under review. Once it is reviewed and merged, the issue should be solved, and we can close the ticket.", "created": "2025-10-23T07:06:19.582+0000"}, {"author": "Arpit Goyal", "body": "Thanks [~shivsundar] . If there is anything i can pick up, Please feel free to assign. I am happy to pick up.", "created": "2025-10-23T07:08:16.659+0000"}], "derived_tasks": {"summary": "Console share consumer is not terminated immediately - Even when share groups is not enabled , if pressed ctrl+c, then client doesn't immediately s...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-17243", "title": "MetadataQuorumCommand describe to include CommittedVoters", "description": "kafka-metadata-quorum describe --status output should include CommittedVoters information, formatted similarly to Voters and Observers KIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1204%3A+MetadataQuorumCommand+describe+to+include+CommittedVoters]", "status": "In Progress", "priority": "Minor", "reporter": "Alyssa Huang", "assignee": "TaiJuWu", "created": "2024-08-01T20:34:44.000+0000", "updated": "2025-10-24T03:20:49.000+0000", "labels": ["kip-required"], "components": [], "comments": [{"author": "\u9ec3\u7ae3\u967d", "body": "[~alyssahuang], Im interesting in this issue, if you don't work at this? could you assign to me, thank you", "created": "2024-08-02T12:07:33.645+0000"}, {"author": "Jos\u00e9 Armando Garc\u00eda Sancio", "body": "[~m1a2st] , note that this change is quite involved. Right now the KafkaRaftClient and KRaftControlRecordStateMachine only track the latest value for the voter set. It doesn't know which voters set is the latest committed voter set. We need to extend those types to keep track of both the latest voter set and the latest committed voter set.", "created": "2024-08-02T13:32:08.095+0000"}, {"author": "\u9ec3\u7ae3\u967d", "body": "[~jsancio], Thanks for your reminder, I will trace these code. If I have the ability, I will take on this issue.", "created": "2024-08-02T13:50:48.149+0000"}, {"author": "Chu Cheng Li", "body": "[~m1a2st] How is this going? Are you working on this? If you have other tickets with higher priority, I think I can take this! Thanks!", "created": "2024-10-12T07:59:27.966+0000"}, {"author": "\u9ec3\u7ae3\u967d", "body": "[~peterxcli], If you want do it, you can take it.", "created": "2024-10-12T08:48:08.853+0000"}, {"author": "Chu Cheng Li", "body": "thanks!", "created": "2024-10-12T08:50:23.216+0000"}, {"author": "Chu Cheng Li", "body": "[~alyssahuang] [~jsancio] To support this additional *_committedVoters_* info returned to client(kafka-metadata-quorum describe), it is necessary to add a new field in RPC message in DescribeQuorumResponse.json. Should I add a new KIP for this field addition? Thanks!", "created": "2024-10-12T14:19:49.687+0000"}, {"author": "Chu Cheng Li", "body": "Hi, if there is any update on this, please let me know. Thanks!", "created": "2024-11-23T17:47:46.691+0000"}, {"author": "Chia-Ping Tsai", "body": "[~peterxcli] have you created the KIP?", "created": "2025-02-11T17:20:11.003+0000"}], "derived_tasks": {"summary": "MetadataQuorumCommand describe to include CommittedVoters - kafka-metadata-quorum describe --status output should include CommittedVoters informati...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-17019", "title": "Producer TimeoutException should include root cause", "description": "With KAFKA-16965 we added a \"root cause\" to some `TimeoutException` thrown by the producer. However, it's only a partial solution to address a specific issue. We should consider to add the \"root cause\" for _all_ `TimeoutException` cases and unify/cleanup the code to get an holistic solution to the problem.", "status": "Open", "priority": "Major", "reporter": "Matthias J. Sax", "assignee": "sanghyeok An", "created": "2024-06-21T18:54:17.000+0000", "updated": "2025-10-24T10:23:56.000+0000", "labels": [], "components": ["clients", "producer "], "comments": [{"author": "sanghyeok An", "body": "Hi, [~mjsax] ! I take a look into producer code to contribute this issue. There are only a few remaining code paths that still raise\u202f{{{}TimeoutException{}}}. In most of them, the exception is thrown by the code itself while it is waiting for a\u202f{{{}ProducerBatch{}}}; in that case, the {{TimeoutException}} *is* the root cause, so it has nothing further to wrap or include. The scenario you describe would occur if an exception were thrown while the {{ProducerBatch}} is executing and, as a result, {{ProducerRequestResult.done()}} is never called. But in that case, hasn\u2019t the {{ProducerBatch}} already raised the real exception, so nothing is actually \u201cmissing\u201d? Or do you think the exception raised inside {{ProducerBatch}} should instead be set as the root cause of the subsequent {{{}TimeoutException{}}}?", "created": "2025-05-05T01:12:03.223+0000"}, {"author": "Matthias J. Sax", "body": "{quote}I take a look into producer code to contribute this issue. {quote} Nice! Thank you. {quote}the\u00a0{{TimeoutException}}\u00a0*is*\u00a0the root cause {quote} A timeout only happens when we try to do something, but cannot complete what we want to do within a timeout. So there must be a root cause why we could not complete what we tried to do? \u2013 I guess the only real timeout as a root cause is, if we send a request and don't get any response at all, ie, an actual request timeout. {quote}But in that case, hasn\u2019t the\u00a0{{ProducerBatch}}\u00a0already raised the real exception, so nothing is actually \u201cmissing\u201d? {quote} This depends. If the error is not retriable, yes. The error would be directly re-thrown into the application. However, if the error is retriable, the producer would, well, retry internally (and only log the error), and eventually might give up if some high level timeout expires. For example, I believe `ProducerBatch` could return \"not enough replicas\" exception, which we would retry internally, until eventually `max.block.ms` expires. {quote}Or do you think the exception raised inside {{ProducerBatch}}\u00a0should instead be set as the root cause of the subsequent\u00a0{{{}TimeoutException{}}}? {quote} Yes, that is the idea. I don't know all scenarios from top of my head, and I guess we need to take it on a case-by-case basis. But most `TimeoutException` should have some actual root cause I believe.", "created": "2025-05-05T01:52:57.777+0000"}, {"author": "sanghyeok An", "body": "[~mjsax] Thanks for your comments! Would it be okay for me to dig into this issue? If so, should I write KIP? Because `KafkaProducer` class have `TimeoutException` in their private method, but it can be thrown to user side. Also, `KafkaProducer` class is not in internal package. What do you think? Please let me know. Thanks in advance!", "created": "2025-05-05T05:29:38.803+0000"}, {"author": "Matthias J. Sax", "body": "I don't think we need a KIP \u2013 adding a \"cause\" is not a public API change. We can just handle it via a PR.", "created": "2025-05-06T00:59:48.975+0000"}, {"author": "sanghyeok An", "body": "[~mjsax] Thanks a lot. Let me try to take a crack! Could you assign this ticket to me?", "created": "2025-05-07T06:24:06.109+0000"}, {"author": "Matthias J. Sax", "body": "Just changed your permissions \u2013 you should be able to self-assign now.", "created": "2025-05-08T22:08:41.143+0000"}, {"author": "sanghyeok An", "body": "Thanks a lot! I assigned this issue to myself! If I give up because It is hard for me to handle, I will let you know!", "created": "2025-05-10T11:53:17.531+0000"}], "derived_tasks": {"summary": "Producer TimeoutException should include root cause - With KAFKA-16965 we added a \"root cause\" to some `TimeoutException` thrown by the producer", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-16926", "title": "Optimize BeginQuorumEpoch heartbeat", "description": "Instead of sending out BeginQuorum requests to every voter on a cadence, we can save on some requests by only sending to those which have not fetched within the fetch timeout. Split from KAFKA-16536", "status": "Open", "priority": "Minor", "reporter": "Alyssa Huang", "assignee": "TaiJuWu", "created": "2024-06-10T16:54:39.000+0000", "updated": "2025-10-25T04:32:12.000+0000", "labels": [], "components": ["kraft"], "comments": [{"author": "xiaochen.zhou", "body": "I would like to give a try on this, can I take this tickets?", "created": "2024-07-19T07:25:28.930+0000"}, {"author": "TaiJuWu", "body": "Hi [~phong260702] , do you working on this? If not, may I pick it up?", "created": "2025-08-05T23:21:03.226+0000"}], "derived_tasks": {"summary": "Optimize BeginQuorumEpoch heartbeat - Instead of sending out BeginQuorum requests to every voter on a cadence, we can save on some requests by only...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-16876", "title": "TaskManager.handleRevocation doesn't handle errors thrown from task.prepareCommit", "description": "`TaskManager.handleRevocation` does not handle exceptions thrown by `task.prepareCommit`. In the particular instance I observed, `pepareCommit` flushed caches which led to downstream `producer.send` calls that threw a `TaskMigratedException`. This means that the tasks that need to be revoked are not suspended by `handleRevocation`. `ConsumerCoordinator` stores the thrown exception and then moves on to the other task assignment callbacks. One of these - `StreamsPartitionAssigner.onCommit` tries to close the tasks and raises an `IllegalStateException`. Fortunately, it dirty-closes the tasks if close fails so we don't leak any tasks. I think there's maybe two bugs here: # `TaskManager.handleRevocation` should handle errors from `prepareCommit`. It should try not to leave any revoked tasks in an unsuspended state. # The `ConsumerCoordinator` just throws the first exception that it sees. But it seems bad to throw the `TaskMigratedException` and drop the `IllegalStateException` (though in this case I think its relatively benign). I think on `IllegalStateException` we really want the streams thread to exit. One idea here is to have `ConsumerCoordinator` throw an exception type that includes the other exceptions that it has seen in another field. But this breaks the contract for clients that catch specific exceptions. I'm not sure of a clean solution, but I think its at least worth recording that it would be preferable to have the caller of `poll` handle all the thrown exceptions rather than just the first one. Here is the IllegalStateException stack trace I observed:  [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] [ERROR] 2024-05-30 06:35:04.556 [e2e-c0a9810b-8b09-46bd-a6d0-f2678ce0a1f3-StreamThread-1] TaskManager - stream-thread [e2e-c0a9810b-8b09-46bd-a6d0-f2678ce0a1f3-St reamThread-1] Failed to close task 0_3 cleanly. Attempting to close remaining tasks before re-throwing: [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] java.lang.IllegalStateException: Illegal state RUNNING while closing active task 0_3 [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamTask.close(StreamTask.java:673) ~[kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamTask.closeClean(StreamTask.java:546) ~[kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.TaskManager.closeTaskClean(TaskManager.java:1295) ~[kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.TaskManager.closeAndRecycleTasks(TaskManager.java:630) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:350) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1381) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment(ConsumerCoordinator.java:315) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:469) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:478) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:389) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:564) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1220) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1179) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1159) [kafka-clients-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at dev.responsive.kafka.internal.clients.DelegatingConsumer.poll(DelegatingConsumer.java:94) [kafka-client-0.24.0-dc9acd1.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:1014) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:954) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:766) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:617) [kafka-streams-3.6.0.jar:?] [ \u00a0 \u00a0 \u00a0 508.535] [service_application2] [inf] at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:579) [kafka-streams-3.6.0.jar:?]", "status": "In Progress", "priority": "Minor", "reporter": "Rohan Desai", "assignee": "Lianet Magrans", "created": "2024-06-01T05:41:09.000+0000", "updated": "2025-10-21T20:10:13.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Ganesh Sadanala", "body": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method.", "created": "2024-06-03T03:33:58.647+0000"}, {"author": "Rohan Desai", "body": "[~ganesh_6] I don't have that stack trace unfortunately, but I'm pretty certain that the error is being thrown by `ProcessorStateManager#flushCache`. The problem is that `ProcessorStateManager#flushCache` throws the exception that was thrown inside the Producer's io thread and returned in the future, instead of re-wrapping it, so the stack trace is the Producer thread stack trace instead of the stream thread stack trace (which is where `flushCache` is called from).", "created": "2024-06-03T04:10:58.616+0000"}, {"author": "Ganesh Sadanala", "body": "[~rohanpd] Thank your for confirming it! I see the flow is from `TaskManager#handleRevocation` -> `TaskManager#prepareCommitAndAddOffsetsToMap` -> `StreamTask#prepareCommit` -> `StreamTask#flush` -> `ProcessorStateManager#flushCache` and registered state stores are iterated inside it, I see that in your case TaskMigratedException is thrown and is caught inside here:  catch (final RuntimeException exception) { if (firstException == null) { // do NOT wrap the error if it is actually caused by Streams itself if (exception instanceof StreamsException) { firstException = exception; }  and finally it is thrown to `StreamTask#flush` method, where I see it is not caught/handled. Hence, the entire flow leads to Runtime errors and all the active tasks are not revoked. Please correct me if I am wrong. So you would want it to be handled inside the `StreamTask#flush` method appropriately? Also could you guide me how you produced those exceptions, I want to produce them in my local to get a better picture. Anything else you want to share will be beneficial. Thank you!", "created": "2024-06-03T04:20:37.070+0000"}], "derived_tasks": {"summary": "TaskManager.handleRevocation doesn't handle errors thrown from task.prepareCommit - `TaskManager", "classifications": ["bug"], "qa_pairs": [{"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}, {"question": "jar:?", "answer": "[~rohanpd] Can you share the `TaskMigratedException` stack trace as well? I assume that it is thrown from the `ProcessorStateManager#flushCache` method."}]}}
{"id": "KAFKA-16505", "title": "KIP-1034: Dead letter queue in Kafka Streams", "description": "See KIP: KIP-1034: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1034%3A+Dead+letter+queue+in+Kafka+Streams", "status": "Resolved", "priority": "Major", "reporter": "Damien Gasparina", "assignee": "Damien Gasparina", "created": "2024-04-10T09:04:38.000+0000", "updated": "2025-10-22T19:17:00.000+0000", "labels": ["KIP"], "components": ["streams"], "comments": [{"author": "Swikar Patel", "body": "Hello, Can I also help in contributing and resolving this issue? I recently visited streams module while working on updating RocksDB version. I am curious to learn streams module and this issue as well. Thanks Swikar", "created": "2024-12-20T03:01:46.156+0000"}, {"author": "Matthias J. Sax", "body": "I guess it's up to [~sebviale] and [~Dabz] who did the KIP and working already on PRs. \u2013 You can for sure help to review the revised KIP (we needed to make changes and are currently re-voting), and help with PR reviews. If Sebastiean and Damien are willing to break down the KIP into smaller pieces and are ok that you pickup part of the implementation, this might also work, but that's up to Sebastien and Damien to decide. If you are looking for other things to work on, I would have a personal interest to get https://issues.apache.org/jira/browse/KAFKA-15307 done. Also KIP-770 was never finished, and it would be great to complete it: [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186878390]. Of course, there is many other interesting Jiras.", "created": "2024-12-20T03:30:24.639+0000"}, {"author": "Matthias J. Sax", "body": "Is this completed? Can we close this ticket?", "created": "2025-09-29T22:47:17.190+0000"}, {"author": "Matthias J. Sax", "body": "Converted the open sub-task into a standalone ticket. Resolving, so it's properly tracked for upcoming 4.2.0 release.", "created": "2025-10-22T19:17:00.496+0000"}], "derived_tasks": {"summary": "KIP-1034: Dead letter queue in Kafka Streams - See KIP: KIP-1034: https://cwiki", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-16024", "title": "SaslPlaintextConsumerTest#testCoordinatorFailover is flaky", "description": "The test is flaky with the async consumer as we are observing  I was not able to replicate this on my local machine easily.", "status": "In Progress", "priority": "Major", "reporter": "Philip Nee", "assignee": "Phuc Hong Tran", "created": "2023-12-17T17:37:51.000+0000", "updated": "2025-10-25T14:45:33.000+0000", "labels": ["flaky-test", "integration-tests"], "components": ["clients", "consumer"], "comments": [{"author": "Colin McCabe", "body": "Changing target fix version to 4.0 since this is not a blocker and we are past code freeze", "created": "2024-08-29T17:30:19.597+0000"}, {"author": "David Jacot", "body": "Changing target fix version to 4.1 since this is not a blocker and we are past code freeze.", "created": "2025-01-20T12:52:47.565+0000"}, {"author": "Mickael Maison", "body": "Moving to the next release as we're now in code freeze for 4.1.0.", "created": "2025-06-26T13:07:26.704+0000"}, {"author": "Kirk True", "body": "Per KAFKA-7605, this test has been flaky since 2018, so it's not specific to the \"new\" consumer. Per [Develocity|https://develocity.apache.org/scans/tests?search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=kafka.api.SaslSslConsumerTest], it's flaky for the {{SaslSslConsumerTest}} version of the test, too. I have to run many iterations of the test to see it fail:  COUNT=500 for ((i=1; i<=$COUNT; i++)); do ./gradlew -q \\ :core:test \\ --tests \"kafka.api.SaslSslConsumerTest.testCoordinatorFailover\" \\ --rerun \\ --fail-fast \\ -x spotbugsMain \\ -x spotbugsTest \\ -x checkstyleMain \\ -x checkstyleTest echo \"Completed run: $i\" sleep 1 done", "created": "2025-10-24T17:41:44.232+0000"}], "derived_tasks": {"summary": "SaslPlaintextConsumerTest#testCoordinatorFailover is flaky - The test is flaky with the async consumer as we are observing  I was not able to repli...", "classifications": ["test", "bug"], "qa_pairs": []}}
{"id": "KAFKA-15818", "title": "Implement max poll interval", "description": "The consumer needs to be polled at a candance lower than MAX_POLL_INTERVAL_MAX otherwise the consumer should try to leave the group.\u00a0 Currently, we send an acknowledgment event to the network thread per poll.\u00a0 The event only triggers update on autocommit state, we need to implement updating the poll timer so that the consumer can leave the group when the timer expires. The current logic looks like this:  if (heartbeat.pollTimeoutExpired(now)) { // the poll timeout has expired, which means that the foreground thread has stalled // in between calls to poll(). log.warn(\"consumer poll timeout has expired. This means the time between subsequent calls to poll() \" + \"was longer than the configured max.poll.interval.ms, which typically implies that \" + \"the poll loop is spending too much time processing messages. You can address this \" + \"either by increasing max.poll.interval.ms or by reducing the maximum size of batches \" + \"returned in poll() with max.poll.records.\"); maybeLeaveGroup(\"consumer poll timeout has expired.\"); }", "status": "Resolved", "priority": "Blocker", "reporter": "Philip Nee", "assignee": "Philip Nee", "created": "2023-11-13T22:29:09.000+0000", "updated": "2025-10-20T11:28:55.000+0000", "labels": ["consumer-threading-refactor", "kip-848-client-support", "kip-848-e2e", "kip-848-preview"], "components": ["clients", "consumer"], "comments": [{"author": "Philip Nee", "body": "Hi Ashwini - Sorry I just saw this. Would you be interested in reviewing this PR? https://github.com/apache/kafka/pull/14873 On Mon, Dec 4, 2023 at 10:29\u202fAM Ashwini Sharma (Jira) <jira@apache.org>", "created": "2023-12-04T18:31:00.025+0000"}, {"author": "Stanislav Kozlovski", "body": "[~pnee] this blocker bug seems merged. Are we OK to close it?", "created": "2023-12-20T12:13:41.419+0000"}, {"author": "Philip Nee", "body": "hi stan - [KAFKA-16026|https://github.com/apache/kafka/pull/15035] is also part of it.\u00a0 will close the issue after 16026 is merged.", "created": "2023-12-20T14:45:01.602+0000"}, {"author": "Stanislav Kozlovski", "body": "seems merged! closing this issue", "created": "2023-12-26T09:05:00.430+0000"}, {"author": "Philip Nee", "body": "thank u stan.\u00a0 it was merged.", "created": "2023-12-26T15:08:40.315+0000"}], "derived_tasks": {"summary": "Implement max poll interval - The consumer needs to be polled at a candance lower than MAX_POLL_INTERVAL_MAX otherwise the consumer should try to l...", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-15283", "title": "Client support for OffsetFetch and OffsetCommit with topic ID", "description": "Currently, {{KafkaConsumer}} keeps track of topic IDs in the in-memory {{ConsumerMetadata}} object, and they are provided to the {{FETCH}} and {{METADATA}} RPC calls. With KIP-848 the OffsetFetch and OffsetCommit will start using topic IDs in the same way, so the new client implementation will provide it when issuing those requests. Topic names should continue to be supported as needed by the {{{}AdminClient{}}}. We should also review/clean-up the support for topic names in requests such as the {{METADATA}} request (currently supporting topic names as well as topic IDs on the client side). Tasks include: * Introduce Topic ID in existing OffsetFetch and OffsetCommit API that will be upgraded on the server to support topic ID * Check topic ID propagation internally in the client based on RPCs including it. * Review existing support for topic name for potential clean if not needed.", "status": "In Progress", "priority": "Critical", "reporter": "Kirk True", "assignee": "Lan Ding", "created": "2023-08-01T00:48:02.000+0000", "updated": "2025-10-24T07:05:20.000+0000", "labels": ["kip-848-client-support", "newbie", "offset"], "components": ["clients", "consumer"], "comments": [{"author": "Kirk True", "body": "[~lianetm] is this covered by your work?", "created": "2023-12-19T19:49:22.977+0000"}, {"author": "Lianet Magrans", "body": "Not covered by my work for OffsetFetch and OffsetCommit v9 support, given that v9 APIs introduced on the server side did not include topic ID as it was initially planned. This task is still needed, and it should be done in sync with the broker side, when it introduces topic ID in these API calls (probably a v10 version)", "created": "2024-01-08T16:20:06.040+0000"}, {"author": "Lan Ding", "body": "Hi [~lianetm] , are you still working on this? May I take over?", "created": "2025-02-24T01:18:04.673+0000"}, {"author": "Lianet Magrans", "body": "Hi [~isding_l] , not working on it because the broker-side changes are not in place yet. Once it supports topic IDs on the OffsetFetch/Commit request, then we need to take on this one for the client side support. That being said, feel free to take it if you're interested, this will probably be for 4.1 depending on the broker.", "created": "2025-02-26T14:09:51.974+0000"}, {"author": "Mickael Maison", "body": "Moving to the next release as we're now in code freeze for 4.1.0.", "created": "2025-06-25T10:34:17.114+0000"}], "derived_tasks": {"summary": "Client support for OffsetFetch and OffsetCommit with topic ID - Currently, {{KafkaConsumer}} keeps track of topic IDs in the in-memory {{ConsumerMe...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-14405", "title": "Log a warning when users attempt to set a config controlled by Streams", "description": "Related to https://issues.apache.org/jira/browse/KAFKA-14404 It's too easy for users to try overriding one of the client configs that Streams hardcodes, and since we just silently ignore it there's no good way for them to tell their config is not being used. Sometimes this may be harmless but in cases like the Producer's partitioner, there could be important application logic that's never being invoked. When processing user configs in StreamsConfig, we should check for all these configs and log a warning when any of them have been set", "status": "Patch Available", "priority": "Major", "reporter": "A. Sophie Blee-Goldman", "assignee": "Shashank", "created": "2022-11-19T01:24:27.000+0000", "updated": "2025-10-19T03:28:37.000+0000", "labels": ["newbie"], "components": ["streams"], "comments": [{"author": "Shashank", "body": "Hi [~ableegoldman], [~mjsax] , I would like to start working on this. Can this be assigned to me?", "created": "2025-07-10T18:43:28.937+0000"}, {"author": "Matthias J. Sax", "body": "Thanks. You can now also self assign tickets.", "created": "2025-07-16T23:09:16.557+0000"}], "derived_tasks": {"summary": "Log a warning when users attempt to set a config controlled by Streams - Related to https://issues", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-12999", "title": "NPE when accessing RecordHeader.key() concurrently", "description": "h2. Summary After upgrading clients to {{2.8.0}}, reading {{ConsumerRecord}}'s header keys started resulting in occasional {{java.lang.NullPointerException}} in case of concurrent access from multiple(2) threads. h2. Where NPE happens here [RecordHeader.java:45|https://github.com/apache/kafka/blob/2.8.0/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L45]:  public String key() { if (key == null) { key = Utils.utf8(keyBuffer, keyBuffer.remaining()); // NPE here keyBuffer = null; } return key; }  h2. When/why Cause of issue is introduced by changes of KAFKA-10438 to avoid unnecessary creation of key's *{color:#0747a6}{{String}}{color}* when it might never be used. It is good optimization but this *lazy* initialization of field {{RecordHeader.key}} creates a problem if being accessed/initialized by 2 threads concurrently since it's now no longer read-only operation and there is race between initializing {color:#0747a6}*{{key}}*{color} and nullifying {color:#0747a6}*{{keyBuffer}}*{color} h2. Simple workaround Upon consuming record(s) and before passing {color:#0747a6}*{{ConsumerRecord}}*{color} to multiple processing threads, eagerly initialize all header keys by iterating through headers and invoking {color:#0747a6}*{{key()}}*{color} or even {color:#0747a6}*{{ConsumerRecord.headers().hashCode()}}*{color} which will initialize all keys (and header values too) h2. Consequences Current implementation renders RecordHeader not thread-safe for read-only access. h2. Reproducibility With enough iterations it's always possible to reproduce (at least on my local) Here is minimal snippet to reproduce:  @Test public void testConcurrentKeyInit() throws ExecutionException, InterruptedException { ByteBuffer keyBuffer = ByteBuffer.wrap(\"key\".getBytes(StandardCharsets.UTF_8)); ByteBuffer valueBuffer = ByteBuffer.wrap(\"value\".getBytes(StandardCharsets.UTF_8)); ExecutorService executorService = Executors.newSingleThreadExecutor(); try { for (int i = 0; i < 1_000_000; i++) { RecordHeader header = new RecordHeader(keyBuffer, valueBuffer); Future<String> future = executorService.submit(header::key); assertEquals(\"key\", header.key()); assertEquals(\"key\", future.get()); } } finally { executorService.shutdown(); } }  h2. Possible solution #1 Leave implementation as-is but somehow document this to users. h2. Possible solution #2 Add some concurrency primitives to current implementation * simply adding {color:#0747a6}*{{synchronized}}*{color} on method *{color:#0747a6}{{key()}}{color}* (and on *{color:#0747a6}{{value()}}{color}* too) gives correct behaviour avoiding race-conditions. * JMH benchmark comparing *{color:#0747a6}{{key()}}{color}* with and without {color:#0747a6}*{{synchronized}}*{color} showed no significant performance penalty  Benchmark Mode Cnt Score Error Units RecordHeaderBenchmark.key avgt 15 31.308 \u00b1 7.862 ns/op RecordHeaderBenchmark.synchronizedKey avgt 15 31.853 \u00b1 7.096 ns/op", "status": "Open", "priority": "Minor", "reporter": "Antonio Tomac", "assignee": "Ming-Yen Chung", "created": "2021-06-27T00:33:23.000+0000", "updated": "2025-10-25T08:41:06.000+0000", "labels": [], "components": ["clients"], "comments": [{"author": "Ismael Juma", "body": "Thanks for the ticket. This class was never meant to be thread safe, the value was initialized lazily previously and it just so happened the key was not. Since the consumer is single threaded, this was deemed ok. For now, the easiest path is for you to copy the data you care about to your own thread safe class. We could consider making ConsumerRecord thread safe, but it would require a KIP and it would probably only be done in a feature release.", "created": "2021-06-27T00:40:27.539+0000"}, {"author": "Ismael Juma", "body": "cc [~chia7712]", "created": "2021-06-27T00:41:49.584+0000"}, {"author": "Antonio Tomac", "body": "{quote}but it would require a KIP and it would probably only be done in a feature release.{quote} [~ijuma] What do you suggest for me to do? Should I then write a KIP? Cancel my [PR|https://github.com/apache/kafka/pull/10933]? {quote}For now, the easiest path is for you to copy the data you care about to your own thread safe class{quote} Yes, this is obvious once you hit NPE and only then realise class is not thread-safe", "created": "2021-06-27T01:15:56.894+0000"}, {"author": "Ismael Juma", "body": "A KIP seems reasonable. The class could be made thread safe at low cost by using a volatile field and synchronization during initialization only.", "created": "2021-06-27T01:42:24.598+0000"}, {"author": "Chia-Ping Tsai", "body": "Thanks for this ticket. As juma explained, the class is not designed for thread-safe. As modern Java make optimization for single thread in sync block, adding sync to make it thread-safe seems to be fine to me. Of course, it needs KIP :)", "created": "2021-06-27T01:54:05.086+0000"}, {"author": "Ismael Juma", "body": "Note that biased locking has been deprecated and will be removed in JDK 18 or soon after. So, synchronized blocks have a cost. We should do what I suggested and have the synchronized _only_ during initialization.", "created": "2021-06-27T03:44:47.343+0000"}], "derived_tasks": {"summary": "NPE when accessing RecordHeader.key() concurrently", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-10683", "title": "Consumer.position() Ignores Transaction Marker with read_uncommitted", "description": "The workaround for https://issues.apache.org/jira/browse/KAFKA-6607# Says: {quote} or use `consumer.position()` that takes the commit marker into account and would \"step over it\") {quote} Note that this problem occurs with all consumers, not just Streams. We have implemented this solution in our project (as an option for those users concerned about the pseudo lag). We have discovered that this technique will only work with isolation.level=read_committed Otherwise, the position() call does not include the marker \"record\". https://github.com/spring-projects/spring-kafka/issues/1587#issuecomment-721899560", "status": "Resolved", "priority": "Minor", "reporter": "Gary Russell", "assignee": null, "created": "2020-11-04T19:59:48.000+0000", "updated": "2025-10-22T17:48:29.000+0000", "labels": [], "components": ["clients", "core"], "comments": [{"author": "Gary Russell", "body": "In addition, position() does not take the marker into account when  max.poll.records=1 Even with read_committed isolation.", "created": "2020-11-05T15:34:48.231+0000"}, {"author": "Ruslan Gryn", "body": "The issue with\u00a0max.poll.record is not only related to value 1. I'm able to reproduce the issue if the count of records in a single transaction equals the value in max.poll.record then consumer.position() will not return the next offset after the transaction marker. It just will not include a transaction marker.", "created": "2020-11-10T09:05:04.640+0000"}, {"author": "Gary Russell", "body": "In my opinion, Kafka should \"hide\" this pseudo lag - requiring the application to do this position()/commit() hack is not the correct solution, even if it worked in all circumstances, which it doesn't. It also adds overhead.", "created": "2020-11-11T14:33:12.603+0000"}, {"author": "Timur", "body": "[~grussell], absolutely - from the consumer side this marker is completely hidden. This behavior looks like broker implementation details exposed to the outside world IMHO.", "created": "2020-11-16T09:16:59.184+0000"}, {"author": "Benoit Delbosc", "body": "I agree these internal [control batch |https://kafka.apache.org/documentation/#controlbatch]records should be completely hidden. Today the lag between producers and consumers is impossible to get in a reliable way, this is a big regression\u00a0for any system that relies on the lag to detect consumer failures.", "created": "2021-04-13T08:51:58.726+0000"}, {"author": "Matthias J. Sax", "body": "I am just getting aware of this ticket \u2013 it's behavior by design, and the application layer need to solve it: the consumer need to keep calling `poll()` to maintain group membership, and thus, if a batch or record really does not include the TX marker, and current `position() == lastRecord.offset() + 1`, this would resolve with the consecutive `poll()` call, which will advance `position` across the TX marker. Thus, the application code must buffer the last committed offset, and if `poll()` returns no data, compare it to `position()`, and if `position()` advanced (compare to the last committed offset), the app should do another offset commit. (We have the same implementation inside Kafka Streams.) As a matter of fact, we actually found some other issue with using `position()` which does not return the partition-leader epoch. This was addressed via https://issues.apache.org/jira/browse/KAFKA-17600 and applications should use the newly added `ConsumerRecords#nextOffsets()` API, to commit offset correctly. We also wrote a detailed blog post about this issue [https://www.confluent.io/blog/guide-to-consumer-offsets/]", "created": "2025-10-22T17:44:11.759+0000"}], "derived_tasks": {"summary": "Consumer.position() Ignores Transaction Marker with read_uncommitted - The workaround for https://issues", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-10486", "title": "Adding cluster with ssl to kafka-manager", "description": "I'm trying to add cluster in kafka which is ssl enabled with port 9093. below is the issue i'm facing ''' yikes! ask timed out on [actorselection[anchor(akka://kafka-manager-system/), path(/user/kafka-manager/q2/kafka-state)]] after [2000 ms]. message of type [kafka.manager.model.actormodel$ksgetbrokers$] was sent by [actor[akka://kafka-manager-system/user/kafka-manager/test#-20444736]|#-20444736]]. a typical reason for `asktimeoutexception` is that the recipient actor didn't send a reply. ``` I tried by creating <servername:2181>as zookeeper host and also <servername:9093> Both give the same above error. How to add the server in kafka-manager which is ssl enabled?", "status": "Open", "priority": "Major", "reporter": "vodevops", "assignee": null, "created": "2020-09-15T17:43:35.000+0000", "updated": "2025-10-23T17:12:33.000+0000", "labels": ["beginner", "security"], "components": ["security"], "comments": [], "derived_tasks": {"summary": "Adding cluster with ssl to kafka-manager - I'm trying to add cluster in kafka which is ssl enabled with port 9093", "classifications": ["feature", "security", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19760", "title": "RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used", "description": "The coordinator runtime can be overly optimistic about compression ratios when filling up batches and can often overflow `max.message.bytes` of the topic. When this happens, all the group coordinator requests that wrote to the batch fail with an UNKNOWN_SERVER_ERROR. This error is non-retriable and disruptive to client applications.", "status": "Resolved", "priority": "Blocker", "reporter": "Sean Quah", "assignee": "Izzy Harker", "created": "2025-10-07T07:27:55.000+0000", "updated": "2025-10-21T16:55:35.000+0000", "labels": [], "components": ["group-coordinator"], "comments": [], "derived_tasks": {"summary": "RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used - The coordinator runtime can be overly optimistic about...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19758", "title": "Weird behavior on Kafka Connect 4.1 class loading", "description": "I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\" Then there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector. Now I'm running the following image  FROM quay.io/debezium/connect:3.3.0.Final ENV MAVEN_REPO=\"https://repo1.maven.org/maven2\" ENV GROUP_ID=\"io/debezium\" ENV DEBEZIUM_VERSION=\"3.3.0.Final\" ENV ARTIFACT_ID=\"debezium-openlineage-core\" ENV CLASSIFIER=\"-libs\" COPY log4j.properties /kafka/config/log4j.properties Add OpenLineage RUN mkdir -p /tmp/openlineage-libs && \\ curl \"$MAVEN_REPO/$GROUP_ID/$ARTIFACT_ID/$DEBEZIUM_VERSION/$ARTIFACT_ID-${DEBEZIUM_VERSION}${CLASSIFIER}.tar.gz\" -o /tmp/debezium-openlineage-core-libs.tar.gz && \\ tar -xzvf /tmp/debezium-openlineage-core-libs.tar.gz -C /tmp/openlineage-libs --strip-components=1 RUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-postgres/ RUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-mongodb/ ADD openlineage.yml /kafka/  So is practically debezium connect image with just openlineage jars copied into postgres and mongodb connector folders. When I register the PostgreSQL connector  { \"name\": \"inventory-connector-postgres\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"postgres\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"postgres\", \"database.server.id\": \"184054\", \"database.dbname\": \"postgres\", \"topic.prefix\": \"inventory\", \"snapshot.mode\": \"initial\", \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\", \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\", \"slot.name\": \"postgres\", \"openlineage.integration.enabled\": \"true\", \"openlineage.integration.config.file.path\": \"/kafka/openlineage.yml\", \"openlineage.integration.job.description\": \"This connector does cdc for products\", \"openlineage.integration.tags\": \"env=prod,team=cdc\", \"openlineage.integration.owners\": \"Mario=maintainer,John Doe=Data scientist,IronMan=superero\", \"transforms\": \"openlineage\", \"transforms.openlineage.type\": \"io.debezium.transforms.openlineage.OpenLineage\" } }  I get the following error  2025-10-03T14:22:09,761 ERROR \u00a0|| \u00a0WorkerSourceTask{id=inventory-connector-postgres-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted \u00a0 [org.apache.kafka.connect.runtime.WorkerTask] Full logs [^connect-service.log] This is evidence that the emitters map is not shared between the connector and the SMT. The situation becomes weirder if I remove all connectors from the image except PostgreSQL and MongoDB. In that case, the PostgreSQL connector works perfectly. The plugins are in the folder */kafka/connect* (that is, the only `plugin.path` configured folder), each under a dedicated folder with their dependencies. I then started to add more connectors, and it continued to work until I added the SQL Server\u00a0connector. To summarize, the problem arises when I put one or all of [sqlserver, spanner,vitess]. The commonality for these connectors seems to be that they support multi-task. The others don't. Am I correct that Kafka Connect guarantees that each connector is loaded with an isolated class loader with its dependencies so that the static emitters should be shared between the Connector and the SMT? To add more, if I run the image from 3.2.0.Final (so Kafka 4.0.0) with all connectors, it works fine. I did other tests, and things are more and more weird. All tests were done with *{{plugin.path=/kafka/connect}}*\u00a0and *KC 4.1* My original tests were with this directory structure  /kafka/connect |___ debezium-connector-postgres |___ debezium-connector-mongodb |___ debezium-connector-sqlserver In this case, each connector should be isolated from each others (having a dedicated class loader). In that case, the sharing between the connector and SMT does not work for KC 4.0 Then I tried with  /kafka/connect |___ debezium-connectors |___ debezium-connector-postgres |___ debezium-connector-mongodb |___ debezium-connector-sqlserver So all connectors are not isolated and share the same class loader. In this case, no issue. And I'll say that this is expected. Then I tried with  /kafka/connect |___ debezium-connectors | |___ debezium-connector-postgres | |___ debezium-connector-mongodb |___ debezium-connector-sqlserver where\u00a0*{{postgres}}*\u00a0and\u00a0*{{mongodb}}*\u00a0are not isolated (same classloader) and\u00a0*{{sqlserver}}* is isolated (different classloader), and in this case, it still works. I expected this to fail as with the first setup. The SMT is in the *debezium-core* jar that and each connector has its own copy So in each connector folder, there are:  debezium-api-3.3.0.Final.jar debezium-common-3.3.0.Final.jar debezium-connector-[connectorName]-3.3.0.Final.jar debezium-core-3.3.0.Final.jar debezium-openlineage-api-3.3.0.Final.jar", "status": "Resolved", "priority": "Blocker", "reporter": "Mario Fiore Vitale", "assignee": "Mickael Maison", "created": "2025-10-06T13:26:46.000+0000", "updated": "2025-10-23T05:40:37.000+0000", "labels": [], "components": ["connect"], "comments": [{"author": "Mickael Maison", "body": "After some investigations, it looks like KIP-891 broke plugin isolation. For example if we have the same transformation under multiple directories, when Connect tries to execute it, it doesn't call apply with the classloader from the right connector. The issue seems to be in DelegatingClassLoader.findPluginLoader() where it always loops through all directories to find the transformation (or predicate) and keeps the last instance. It then uses the classloader of that instance which may not be the same one as the connector. Adding custom tracing I see:  ConnectorConfig.getTransformationOrPredicate connector=io.debezium.connector.postgresql.PostgresConnector connectorRange=null range=[3.4.0-SNAPSHOT,3.4.0-SNAPSHOT] ConnectorConfig.getTransformationOrPredicate classloader=PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} OpenLineage instantiated io.debezium.transforms.openlineage.OpenLineage@2675b74a from /tmp/plugins/debezium-connector-sqlserver/debezium-core-3.4.0-SNAPSHOT.jar classloader null PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/}  The connector is running from PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} and trying to run its transformation using PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/}. The OpenLineage transformation exists in both directories but the DelegatingClassLoader.findPluginLoader() fails to find the appropriate one. We should clarify the expected behavior when a plugins exists under multiple directories. Having multiple times the same connector with the same version should probably be rejected as from the connector configuration, where you have the name and version, you can't decide which one to run. On the other hands, the other connector plugins can exists in multiple copies as long as they are in different directories, thus isolated, as they are always associated with a connector so we can identify a preferred copy. Another inconsistency found while looking at this is that we have default versions for transformations and predicates but the other plugins default to null. For example, the connector configuration submitted via the REST API does not include any version but the computed configuration injects 3.4.0-SNAPSHOT for the transformation.  config.action.reload = restart connector.class = io.debezium.connector.postgresql.PostgresConnector connector.plugin.version = null errors.log.enable = false errors.log.include.messages = false errors.retry.delay.max.ms = 60000 errors.retry.timeout = 0 errors.tolerance = none header.converter = null header.converter.plugin.version = null key.converter = null key.converter.plugin.version = null name = inventory-connector-postgres predicates = [] tasks.max = 1 tasks.max.enforce = true transforms = [openlineage] transforms.openlineage.negate = false transforms.openlineage.plugin.version = 3.4.0-SNAPSHOT transforms.openlineage.predicate = null transforms.openlineage.type = class io.debezium.transforms.openlineage.OpenLineage value.converter = null value.converter.plugin.version = null", "created": "2025-10-07T17:00:46.783+0000"}, {"author": "Mickael Maison", "body": "[~gharris] [~snehashisp] WDYT?", "created": "2025-10-07T17:02:29.198+0000"}, {"author": "Greg Harris", "body": "I have not had a chance to inspect the code or reproduce this for myself, but my first impressions: # I don't think communication via static fields is good practice, and should be avoided if possible. I personally have been on the receiving end of bugs caused by static fields within a single plugin; I also think I personally would not like to debug static fields shared across plugins. # In deployments which make use of the plugin.version configs, changing the pinned version will necessarily change which instance of a static variable is used. So even if the isolation is bug-free, it is trivial for a user or automated tooling to foot-gun a plugin which uses static fields by changing the plugin.version # The concern about the backwards-compatibility when plugin.version is unset is valid. We made some attempts to use the connector's class loader or delegating class loader like in the old implementation, so perhaps the version being explicitly injected is not just a cosmetic problem. IMHO we should investigate the backwards-compatibility problem and try and make a minimal fix to preserve the old behavior when plugin.version is unset. And the affected plugins should eliminate their usage of static variables in order to be well behaved when the plugin.version is set.", "created": "2025-10-07T20:48:22.413+0000"}, {"author": "Mario Fiore Vitale", "body": "??I don't think communication via static fields is good practice, and should be avoided if possible. I personally have been on the receiving end of bugs caused by static fields within a single plugin; I also think I personally would not like to debug static fields shared across plugins.?? I agree that this is not the best approach, but the problem here is to understand what is possible and what is not. AFAIK, until KC 4.0, this was possible due to plugin isolation, as the same classloader was used to load the connector and its dependencies. Even I think that plugin versioning is compatible also with the isolation (and this seems to be the goal of [KIP-891|https://cwiki.apache.org/confluence/display/KAFKA/KIP-891%3A+Running+multiple+versions+of+Connector+plugins]), if this is something that is not true anymore, I would have expected it to be changed in a major release and not in a minor one. ??In deployments which make use of the plugin.version configs, changing the pinned version will necessarily change which instance of a static variable is used. So even if the isolation is bug-free, it is trivial for a user or automated tooling to foot-gun a plugin which uses static fields by changing the plugin.version?? Can you better clarify this point? ??IMHO we should investigate the backwards-compatibility problem and try and make a minimal fix to preserve the old behavior when plugin.version is unset?? Another thing to say here is that the `DelegatingClassLoader.findPluginLoader()` is not only used for transformations and predicates but also for loading the connectors themselves. So to me, this also seems to break the versioning of the connectors. ??And the affected plugins should eliminate their usage of static variables in order to be well behaved when the plugin.version is set?? Honestly, I'm not getting the point here. I expect that even with a correct management of `plugin.version`, the isolation must be preserved. What happens in the following situation where `plugin.path=opt/plugins/`:  opt/ plugins/ blue/ foo-connector-1.8/ foo-connector-1.8.jar foo-dependencies-1.0.jar foo-connector-1.9/ foo-connector-1.9.jar foo-dependencies-1.1.jar  And I register a connector `foo-connector` with version 1.8 and SMT (suppose it is in the foo-dependencies) with version 1.1? Is this possible? In the same way, this breaks the isolation. Isn't it?", "created": "2025-10-08T07:57:55.409+0000"}, {"author": "Mickael Maison", "body": "1. I agree, I don't think it's a good pattern. But I think this is only a symptom that highlights the non-deterministic, and IMHO unsafe, way plugins are loaded as I try to explain in 2. / 3. 2. / 3. When looking for a plugin that matches the requested name and version, I think we should prioritize the instance in the classloader of the current connector. If multiple copies of the same plugin exists, the current logic loads the instance from the last plugin path directory ordered alphabetically, which to me is pretty much a random one as it depends on which plugins (and their names) you have installed. If you have a class in your JAR file, you don't want that class to be loaded from another JAR file.", "created": "2025-10-08T07:58:34.694+0000"}, {"author": "Greg Harris", "body": "We did/should continue to have a policy of choosing the co-located plugin when all else is equal, see KAFKA-8819. This has unintentionally regressed with KIP-891 and should be fixed. However my point was that this is a very brittle setup that will only work as long as a plugin and it's connector are not isolated from one another (in the same PluginClassLoader). As soon as they are isolated, or once someone uses plugin.version to select another instance, or perhaps once a newer version is installed, I would expect this to break again. Mario, your example plugin path is an incorrect installation, because the plugins are all in the same class loader (corresponding to the blue directory). Only one version will be discovered, and it will be arbitrary based on filesystem order. It is very likely that the different versions of the classes collide and cause more serious problems, so Connect does not consider those cases (and it's also distinct from the bug being discussed here). That directly structure is meant to be used with `plugin.path=opt/plugins/blue/`.", "created": "2025-10-08T12:36:49.557+0000"}, {"author": "Mario Fiore Vitale", "body": "> That directly structure is meant to be used with `plugin.path=opt/plugins/blue/`. Sorry, I wrongly typed the plugin.path. I intend to write `plugin.path=opt/plugins/blue/` > As soon as they are isolated, or once someone uses plugin.version to select another instance, or perhaps once a newer version is installed, I would expect this to break again. I still didn't get your point here. If a connector is isolated, then all other plugins that are listed in the same directly should be loaded with the same classloader. Isn't it?", "created": "2025-10-08T12:48:35.847+0000"}, {"author": "Greg Harris", "body": "In your example, foo-connector-1.8 and foo-connector-1.9 are isolated from one another. They are each co-located/not-isolated from their dependencies and any contained SMTs. If you dont specify plugin.version at all, you should get the 1.9 (latest) version of the connector, the co-located version of the dependencies (non-plugins), and the 1.1 (latest) version of the SMT. This is the good path, because the latest plugins are all present in the same location. But there are several bad paths where static variables cannot be shared: * If you reference an SMT that is not co-located (installed in some other directory) * If you specify the 1.0 version of the SMT, the 1.9 version of the connector will be loaded because it's latest * If you specify version 1.8 of the connector, the 1.1 version of the SMT will be loaded because it's latest * Another plugin is installed with a later version (1.2) of the SMT", "created": "2025-10-08T13:20:27.450+0000"}, {"author": "Mario Fiore Vitale", "body": "Now it's clear. Thanks for the detailed explanation. I think this needs to be reported in the documentation since it's crucial. Well, in that case, I have to say that the new versioning feature introduced more flexibility, which is good, but at the same time, you can have the same \"strict\" level of isolation as before, if you configure your versions to maintain it. Previously the isolation was achieved with the directory structure on the file system. In my example, if I want the full isolation for both connector versions, I need to specify for  1.8: connector verions = 1.8 and SMT version = 1.0 19.: connector version = 1.9 and SMT version = 1.1  That said. I dunno if this full flexibility will be used so often since it can lead to strange behavior, but maybe this is just my limited experience view.", "created": "2025-10-08T13:36:42.533+0000"}, {"author": "Greg Harris", "body": "I don't think you and I are using the word \"isolation\" in the same way, and maybe that's the cause of some of the friction here. The boundaries of isolation are still specified at installation time in the same way as they were before, only now users can choose older versions for each plugin. It feels strange (that 1.8+1.1 and 1.9+1.0 are broken) because static field sharing adds context that operators have to keep in mind when doing upgrades. Users are going to want to focus on data-flow/semantics when performing upgrades, not debugging static field aliasing. FWIW, static field sharing between plugins has never been in the mental model for Connect plugins, and the threading model/initialization order/lifecycle of plugins isn't well defined enough for static fields to be safe to use. That is why I want to push back strongly on this practice, while still admitting that we broke backwards compat and need to fix it.", "created": "2025-10-08T14:25:18.451+0000"}, {"author": "Mario Fiore Vitale", "body": "> I don't think you and I are using the word \"isolation\" in the same way, and maybe that's the cause of some of the friction here. The boundaries of isolation are still specified at installation time in the same way as they were before, only now users can choose older versions for each plugin. For me, isolation means loading with the same classLoader. > It feels strange (that 1.8+1.1 and 1.9+1.0 are broken) because static field sharing adds context that operators have to keep in mind when doing upgrades. This could happen not only for static field sharing but for any breaking change. For example, imagine a source connector that renames some field in a non-backward-compatible manner (in a major release) and and SMT should adapt to that change. An operator has to keep in mind this right? > FWIW, static field sharing between plugins has never been in the mental model for Connect plugins, and the threading model/initialization order/lifecycle of plugins isn't well defined enough for static fields to be safe to use. That is why I want to push back strongly on this practice, As said previously, also before this issue, I had a bad feeling about this static field sharing, and now I have the proof that it was a design flaw. I'm currently looking for a different way to achieve it. That said, here the problem is another. If my understanding is correct, please correct me if I'm wrong, in KC version until 4.0 the isolation of the connector was done just with directory separation inside the `plugin.path` folder. Level 1 directories are isolated from each other. This is similar to what happens to war files in application servers, where each war is loaded with a different class loader. With KC 4.1 this is no more true, since all is guided by version and your level 1 directory has no more importance for class loader isolation since you could have a connector in a level 0 folder, let's say connector a, that could then use a plugin (SMT, Converter, etc) that is in another lavel 1 folder, so loaded with a different class loader, just because it is the one thath mathes the desidered version. If my understanding of the past and current status for me, this is a huge behavior change that shouldn't have happened in a minor release, and so we need to be as backward compatible as possible.", "created": "2025-10-08T15:02:37.977+0000"}, {"author": "Greg Harris", "body": "For me, two plugins are \"isolated\" when they have different classloaders. This allows them to pull in other dependencies completely independently. You're right that this generalizes to any binary compatibility between classes. If you have classes in the same class loader, they need to be binary compatible and loadable together. Because this is difficult to manage, plugin isolation was implemented to permit plugins to be distributed independently from one another. No your understanding of isolation is not correct, as the directory structure has not changed. A plugin path (config) is made up of plugin path elements (level 0). Each plugin path element contains multiple plugin locations (level 1). Each plugin location may contain multiple jars (level 2+). PluginClassLoaders are instantiated at level 1, and different jar files at level 2+ are included together in the same loader. What has changed in a backwards-incompatible way is that when plugin.version is not specified, the copy in the connector is preferred. Now an arbitrary copy of the latest version is preferred. This should go back to how it was before.", "created": "2025-10-08T15:58:21.044+0000"}, {"author": "Mickael Maison", "body": "I think we now all agree on the issue. It's clear there's a regression in 4.1.0. We found it via some questionable logic used in Debezium but this is clearly a change in behavior that is not correct. I've started working on a fix, unfortunately so far I've not found a quick/small fix. I hope to have something ready to review later this week.", "created": "2025-10-08T16:11:03.881+0000"}, {"author": "Mario Fiore Vitale", "body": "> PluginClassLoaders are instantiated at level 1, and different jar files at level 2+ are included together in the same loader. [~gharris1727] This matches my understanding. But now, apart from when `plugin.version` is not specified, this will no more true. If you have this situation where the `plugin.path` is `opt/ plugins/ blue/`:  opt/ plugins/ blue/ foo-connector-1.8/ foo-connector-1.8.jar foo-dependencies-1.0.jar foo-connector-1.9/ foo-connector-1.9.jar foo-dependencies-1.1.jar And based on what you said, in KC 4.0, I'll have a classloader for : * foo-connector-1.8 that loads the foo-connector-1.8.jar and foo-dependencies-1.0.jar * foo-connector-1.9 that loads the foo-connector-1.9.jar foo-dependencies-1.1.jar With KC 4.1, if I do not specify any version, the expected behavior will be to have foo-connector-1.9.jar and foo-dependencies-1.1.jar used, and by coincidence, they are in the same level 1 folder. But I can set the version in a way that I could use foo-connector-1.8.jar and foo-dependencies-1.1.jar( suppose here there are SMT or any other plugin), leading to having a plugin loaded from a different class loader. Isn't it?", "created": "2025-10-09T07:51:55.466+0000"}, {"author": "Mario Fiore Vitale", "body": "[~mimaison] Thanks for working on it.", "created": "2025-10-09T07:52:28.614+0000"}, {"author": "Snehashis Pal", "body": "Pointing out some more details here. While classloading transformations or predicates we did make a conscious effort to ensure that, in the absence of any provided version, we try to load them using the classloader of the connector it is associated with. This happens [here|https://github.com/apache/kafka/blob/59f51fb3cac66d8096e647eb90be6c5e4ba9f485/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java#L418] where we pass in the plugin loader for the connector class to the [newPlugin|https://github.com/apache/kafka/blob/59f51fb3cac66d8096e647eb90be6c5e4ba9f485/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java#L376] method, which should load the transformation using the PluginClassLoader of the connector. If the connector and transformation is colocated in the same path this should have ensured the same isolation behaviour as the previous connect version.", "created": "2025-10-10T14:08:15.133+0000"}, {"author": "Mickael Maison", "body": "Yes we have all the data in getTransformationOrPredicate() but it's not used correctly in newPlugin(). From this path, range is never null, even if it's not set in the configuration because Connect always injects a default version for transformations and predicates in EnrichablePlugin.enrich(). So newPlugin() ends up not using the classloader passed from getTransformationOrPredicate().", "created": "2025-10-10T14:15:38.054+0000"}, {"author": "Mickael Maison", "body": "[~snehashisp] It would be great if you could take a look at https://github.com/apache/kafka/pull/20675. Thanks", "created": "2025-10-10T14:28:36.991+0000"}, {"author": "Lianet Magrans", "body": "Hey [~mimaison] , just touching base regarding the 4.1.1 cut. Do we have an idea of how far are we with the PR/fix? Thanks!", "created": "2025-10-15T14:26:01.776+0000"}, {"author": "Mickael Maison", "body": "[~lianetm] I put a PR out (https://github.com/apache/kafka/pull/20675) last week. Now we're waiting for reviews.", "created": "2025-10-15T15:00:24.464+0000"}, {"author": "Mickael Maison", "body": "[~lianetm] Sorry for the delay. The fix is merged on trunk and I backported it to 4.1 too.", "created": "2025-10-20T17:56:08.610+0000"}, {"author": "Lianet Magrans", "body": "Great, thanks!", "created": "2025-10-20T23:32:27.531+0000"}], "derived_tasks": {"summary": "Weird behavior on Kafka Connect 4.1 class loading - I have the [DebeziumOpenLineageEmitter|https://github", "classifications": ["bug"], "qa_pairs": [{"question": "Am I correct that Kafka Connect guarantees that each connector is loaded with an isolated class loader with its dependencies so that the static emitters should be shared between the Connector and the SMT?", "answer": "After some investigations, it looks like KIP-891 broke plugin isolation. For example if we have the same transformation under multiple directories, when Connect tries to execute it, it doesn't call apply with the classloader from the right connector. The issue seems to be in DelegatingClassLoader.findPluginLoader() where it always loops through all directories to find the transformation (or predicate) and keeps the last instance. It then uses the classloader of that instance which may not be the same one as the connector. Adding custom tracing I see: ConnectorConfig.getTransformationOrPredicate connector=io.debezium.connector.postgresql.PostgresConnector connectorRange=null range=[3.4.0-SNAPSHOT,3.4.0-SNAPSHOT] ConnectorConfig.getTransformationOrPredicate classloader=PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} OpenLineage instantiated io.debezium.transforms.openlineage.OpenLineage@2675b74a from /tmp/plugins/debezium-connector-sqlserver/debezium-core-3.4.0-SNAPSHOT.jar classloader null PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/} The connector is running from PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} and trying to run its transformation using PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/}. The OpenLineage transformation exists in both directories but the DelegatingClassLoader.findPluginLoader() fails to find the appropriate one. We should clarify the expected behavior when a plugins exists under multiple directories. Having multiple times the same connector with the same version should probably be rejected as from the connector configuration, where you have the name and version, you can't decide which one to run. On the other hands, the other connector plugins can exists in multiple copies as long as they are in different directories, thus isolated, as they are always associated with a connector so we can identify a preferred copy. Another inconsistency found while looking at this is that we have default versions for transformations and predicates but the other plugins default to null. For example, the connector configuration submitted via the REST API does not include any version but the computed configuration injects 3.4.0-SNAPSHOT for the transformation. config.action.reload = restart connector.class = io.debezium.connector.postgresql.PostgresConnector connector.plugin.version = null errors.log.enable = false errors.log.include.messages = false errors.retry.delay.max.ms = 60000 errors.retry.timeout = 0 errors.tolerance = none header.converter = null header.converter.plugin.version = null key.converter = null key.converter.plugin.version = null name = inventory-connector-postgres predicates = [] tasks.max = 1 tasks.max.enforce = true transforms = [openlineage] transforms.openlineage.negate = false transforms.openlineage.plugin.version = 3.4.0-SNAPSHOT transforms.openlineage.predicate = null transforms.openlineage.type = class io.debezium.transforms.openlineage.OpenLineage value.converter = null value.converter.plugin.version = null"}]}}
{"id": "KAFKA-19756", "title": "Write down the steps for upgrading Gradle", "description": "Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below. # upgrade gradle-wrapper.properties to the latest gradle # upgrade dependencies.gradle as well # use latest gradle to run command `gradle wrapper`to update gradlew # update wrapper.gradle to ensure the generated \"download command\" works well", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Chih-Yuan Chien", "created": "2025-10-06T09:59:20.000+0000", "updated": "2025-10-24T09:19:15.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Write down the steps for upgrading Gradle - Normally, we only update `gradle-wrapper", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19755", "title": "Move `KRaftClusterTest` from core module to server module", "description": "It should include following tasks # rewrite by java # move to server module", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Lan Ding", "created": "2025-10-04T14:33:51.000+0000", "updated": "2025-10-24T03:20:44.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Move `KRaftClusterTest` from core module to server module - It should include following tasks # rewrite by java # move to server module", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19747", "title": "Improve handling of failed push telemetry request", "description": "When a failure occurs with a push telemetry request, any exception is treated as fatal, causing telemetry to be turned off. We can enhance this error handling to check if the exception is a transient one with expected recovery, and not turn off telemetry in those cases.", "status": "Resolved", "priority": "Major", "reporter": "Bill Bejeck", "assignee": "Bill Bejeck", "created": "2025-09-30T17:01:01.000+0000", "updated": "2025-10-21T00:38:12.000+0000", "labels": [], "components": ["clients"], "comments": [], "derived_tasks": {"summary": "Improve handling of failed push telemetry request - When a failure occurs with a push telemetry request, any exception is treated as fatal, causing...", "classifications": ["improvement", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19746", "title": "Tools | Fix order of arguments to assertEquals in unit test", "description": "This sub-task is intended to fix the order of arguments passed in assertions in test cases within the tools package.", "status": "Resolved", "priority": "Trivial", "reporter": "Ksolves India Limited", "assignee": "Ksolves India Limited", "created": "2025-09-30T09:28:57.000+0000", "updated": "2025-10-24T15:30:25.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Tools | Fix order of arguments to assertEquals in unit test - This sub-task is intended to fix the order of arguments passed in assertions in test ...", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19736", "title": "KAFKA-15665 prevents safe reassignments from completing", "description": "The implementation for KAFKA-15665 is a little too strict - it requires that all replicas in the target replica set be in ISR. This means if we have a reassignment like [1, 2, 3] -> [2, 3, 4] where broker 2 was unhealthy and lagging in replication, reassignment would not complete. I believe it should work to instead just check the following: - Added replicas are in ISR (newTargetISR) - Resulting ISR (newTargetISR) is not under-min-isr", "status": "Open", "priority": "Major", "reporter": "Alyssa Huang", "assignee": "Yu Chia Ma", "created": "2025-09-25T18:00:05.000+0000", "updated": "2025-10-24T16:14:07.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "KAFKA-15665 prevents safe reassignments from completing - The implementation for KAFKA-15665 is a little too strict - it requires that all replicas...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19734", "title": "Add application-id as a tag to the ClientState JMX metric", "description": "As a follow-on to the improvements introduced in [KIP-1091|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1091%3A+Improved+Kafka+Streams+operator+metrics] it would be useful to add the application-id as a tag to the `client-state` metric. This allows Kafka Streams developers and operators to connect metrics containing a `thread-id` (which embeds the `application-id`) across separate deployments of Kafka Streams instances, which are members of the same logical application. KIP-1221 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1221%3A+Add+application-id+tag+to+Kafka+Streams+state+metric]", "status": "In Progress", "priority": "Major", "reporter": "Bill Bejeck", "assignee": "Genseric Ghiro", "created": "2025-09-24T22:54:42.000+0000", "updated": "2025-10-25T23:55:10.000+0000", "labels": ["kip"], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Add application-id as a tag to the ClientState JMX metric - As a follow-on to the improvements introduced in [KIP-1091|https://cwiki", "classifications": ["feature", "task"], "qa_pairs": []}}
{"id": "KAFKA-19720", "title": "Regex subscription should be empty for classic members joining mixed group", "description": "We don't recompute the assignment on consumer -> classic member replacement when the consumer member had a regex subscription and the classic member does not. We should explicitly set regex subscription to empty in classicGroupJoinToConsumerGroup", "status": "Open", "priority": "Major", "reporter": "Dongnuo Lyu", "assignee": "Dongnuo Lyu", "created": "2025-09-17T18:01:25.000+0000", "updated": "2025-10-22T04:11:16.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Regex subscription should be empty for classic members joining mixed group - We don't recompute the assignment on consumer -> classic member replac...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19691", "title": "Add metrics corresponding to consumer rebalance listener metrics", "description": "The consumer provides metrics for execution of the consumer rebalance listener: * Consumer PartitionsLost Latency * Consumer PartitionsAssigned Latency * Consumer PartitionsRevoked Latency It would be useful for the StreamsRebalanceListener to replicate semilar metrics for TasksLost, TasksAssigned and TasksRevoked.", "status": "Resolved", "priority": "Major", "reporter": "travis", "assignee": "travis", "created": "2025-09-09T00:49:57.000+0000", "updated": "2025-10-22T18:14:17.000+0000", "labels": ["kip"], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Add metrics corresponding to consumer rebalance listener metrics - The consumer provides metrics for execution of the consumer rebalance listener: ...", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "KAFKA-19685", "title": "Revoked partitions are included in currentOffsets passed to preCommit on task stop", "description": "When a task stops, [{{WorkerSinkTask#closeAllPartitions()}}|https://github.com/apache/kafka/blob/84caaa6e9da06435411510a81fa321d4f99c351f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L664] runs and the [task\u2019s {{preCommit}}\u00a0is invoked|https://github.com/apache/kafka/blob/3c7f99ad31397a6a7a4975d058891f236f37d02d/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L444]. At that time, the {{currentOffsets}} argument may include {*}revoked partitions{*}. This appears to be caused by: * {{WorkerSinkTask}} does not remove revoked partitions from {{currentOffsets}} * {{WorkerSinkTask#closeAllPartitions()}} passes its {{currentOffsets}} to {{SinkTask#preCommit(...)}} _as-is_ (i.e., without filtering). During normal iterations, {{SinkTask#preCommit(...)}} receives {{{}KafkaConsumer#assignment(){}}}, so revoked partitions are *not* included. Having revoked partitions included *only* at stop is confusing behavior. If this behavior is specified, Could we add a brief note to the {{SinkTask#preCommit(...)}} Javadoc to clarify this behavior?", "status": "Open", "priority": "Minor", "reporter": "Toshiki Murakami", "assignee": null, "created": "2025-09-08T01:36:10.000+0000", "updated": "2025-10-23T04:29:52.000+0000", "labels": [], "components": ["connect"], "comments": [{"author": "Guang Zhao", "body": "Added clarification in https://github.com/apache/kafka/pull/20756.", "created": "2025-10-23T04:29:52.142+0000"}], "derived_tasks": {"summary": "Revoked partitions are included in currentOffsets passed to preCommit on task stop - When a task stops, [{{WorkerSinkTask#closeAllPartitions()}}|ht...", "classifications": ["bug"], "qa_pairs": [{"question": ")}} Javadoc to clarify this behavior?", "answer": "Added clarification in https://github.com/apache/kafka/pull/20756."}]}}
{"id": "KAFKA-19683", "title": "Clean up TaskManagerTest", "description": "See https://github.com/apache/kafka/pull/20392#issuecomment-3241457533", "status": "In Progress", "priority": "Blocker", "reporter": "Shashank", "assignee": "Shashank", "created": "2025-09-07T16:14:32.000+0000", "updated": "2025-10-26T00:51:28.000+0000", "labels": [], "components": ["streams", "unit tests"], "comments": [{"author": "Shashank", "body": "Hi [~lucasbru], since the tests in the cleanup of this file are many, I would like to propose to make incremental changes to this cleanup. - Removal of dead tests and address these 3 comments - [#1|https://github.com/apache/kafka/pull/19275#discussion_r2107811068], [#2|https://github.com/apache/kafka/pull/19275#discussion_r2107814832] and [#3|https://github.com/apache/kafka/pull/19275#discussion_r2107828813] made in the previous stale PR ([#19275|https://github.com/apache/kafka/pull/19275]). - Identify and replace tryToCompleteRestoration() with checkStateUpdater() in all tests that require no additional mocking - Modify tests that may require to be rewritten (I still am doing my analysis of this and may need some help) Steps 1 and 2 seem to be straightforward and also I think splitting would make it easier for you to review. Step 3 is where I may need some guidance and help. Do you think this is a good approach?", "created": "2025-09-07T22:30:25.754+0000"}], "derived_tasks": {"summary": "Clean up TaskManagerTest - See https://github", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19682", "title": "Improve logging about task readiness", "description": "Kafka Streams processes its assigned tasks in a round robin fashion, however, if a task is considered not ready for processing, it might get skipped. We have observed cases, in which Kafka Streams seems to get stuck on a partition, and restarting the application instance resolves this issue. We suspect, it's related to considering the task for the stuck partition as not ready for processing. (As the ready/not-ready decision is based on in-memory state of KS runtime, bouncing the instance would reset KS runtime into a clean state, unblocking the stuck task.) However, we currently don't have sufficient logging to reason about this case, and to understand if and why a task might be skipped. We should add more log statement (DEBUG and/or TRACE) to get better visibility. There might be a lurking bug in this logic that we cannot narrow down w/o the corresponding information logging could provide.", "status": "Patch Available", "priority": "Minor", "reporter": "Matthias J. Sax", "assignee": "Lucy Liu", "created": "2025-09-06T17:32:31.000+0000", "updated": "2025-10-22T20:54:36.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Lucy Liu", "body": "Hi [~qyte] , I saw you picked ticket and wanted to check if you are actively working on it? I'm doing a patch for this but forgot to assign this to myself \u2013 my apologies for the confusion. If you haven't had a chance to start, I have some capacity and would be happy to pick this up. Please let me know what works for you. Thanks!", "created": "2025-10-10T22:44:50.381+0000"}, {"author": "Tanmay Datey", "body": "Sure, you can pick it up. I haven't had a chance to start yet to work on it yet.", "created": "2025-10-11T09:03:49.942+0000"}], "derived_tasks": {"summary": "Improve logging about task readiness - Kafka Streams processes its assigned tasks in a round robin fashion, however, if a task is considered not re...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19678", "title": "Streams open iterator tracking has high contention on metrics lock", "description": "We run Kafka Streams 4.1.0 with custom processors that heavily use state store range iterators. While investigating disappointing performance, we found a surprising source of lock contention. Over the course of about a 1 minute profiler sample, the {{org.apache.kafka.common.metrics.Metrics}} lock is taken approximately 40,000 times and blocks threads for about 1 minute. This appears to be because our state stores generally have no iterators open, except when their processor is processing a record, in which case it opens an iterator (taking the lock through {{OpenIterators.add}} into {{{}Metrics.registerMetric{}}}), does a tiny bit of work, and then closes the iterator (again taking the lock through {{OpenIterators.remove}} into {{{}Metrics.removeMetric{}}}). So, stream processing threads takes a globally shared lock twice per record, for this subset of our data. I've attached a profiler thread state visualization with our findings - the red bar indicates the thread was blocked during the sample on this lock. As you can see, this lock seems to be severely hampering our performance. !image-2025-09-05-12-13-24-910.png!", "status": "Open", "priority": "Major", "reporter": "Steven Schlansker", "assignee": "Bill Bejeck", "created": "2025-09-05T19:31:43.000+0000", "updated": "2025-10-21T20:23:10.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Matthias J. Sax", "body": "Thanks for reporting this issue. \u2013 I am wondering what your application is doing exactly, and if it might be possible to avoid creating an iterator per record? One hacky work-around I could think of, would be to create a \"dummy iterator\", so you always have at least one-open iterators, and the metric won't be removed/added over and over again?", "created": "2025-10-09T23:54:34.472+0000"}, {"author": "Steven Schlansker", "body": "Thanks [~mjsax] for taking a look. We have a product requirement to compute a streaming-min and streaming-max operation over a grouped aggregate. For example, \"earliest record due date for each user\" or \"latest record created date for each user\". To do this, we take the input stream,  K1 = U1 V1 K2 = U1 V2 K3 = U2 V3 K4 = U2 V4  and reorganize the records so the group-key and value are the key prefix, like  U1 V1 K1 = K1 U1 V2 K2 = K2 U2 V3 K3 = K3 U2 V4 K4 = K4 and put it in a state store. Then, to determine the minimum or maximum, we do a prefix range scan to take the first or last record for the group U1 or U2. It might be possible to reduce the number of range scans by caching the minimum and maximum values by key, to know if the max or min possibly changed and skip the iterator if not, but then we need a second state store duplicating the winning record per user. We assumed the cost of opening an iterator is roughly equal to the cost of a key lookup, but maybe this is not a good assumption. Regardless, to me, the current semantics for this metric seems wrong. If the store is open, with no iterators currently, the correct value for the metric is explicitly \"0\" not \"null / unregister\". The current setup makes it difficult to graph, since our dashboards will interpret \"null\" as \"missing data\" which is distinct from a present 0. I would expect the metric to be unregistered only when the state store is closed or otherwise we are sure no new iterators will ever be created.", "created": "2025-10-13T18:02:07.784+0000"}, {"author": "Matthias J. Sax", "body": "This metric is a little bit tricky... (for context [KIP-989|https://cwiki.apache.org/confluence/display/KAFKA/KIP-989%3A+Improved+StateStore+Iterator+metrics+for+detecting+leaks]) \u2013 if we would report `0` (or `-1`), the issue is, that if you setup an alert that computes \"currentTime minus metricValue\" you get false-positives, as the iterator open time computation would report a high value (many years). Your alert would need to be conditional, what is a struggle as far as I know. While a dashboard can render `0` it would blow out your \"y-axis\" on the dashboard to a very high value, too, and it seems it would make it very hard to actually read the dashboard? We actually reported `null` originally, but this also caused issues: https://issues.apache.org/jira/browse/KAFKA-17954 \u2013 so we decided to de-register the metric when it becomes empty. {quote}\u00a0otherwise we are sure no new iterators will ever be created. {quote} Not sure what you mean by this? For your use case: how many values per group do you get? Would it be possible to do an `aggregation` per group, and compute a `List` over all values per group? This would allow you to maintain this list with a key-lookup per update, avoiding a range scan (of course, this only works if the list is small enough, to avoid too large records...)", "created": "2025-10-13T20:14:38.434+0000"}, {"author": "Steven Schlansker", "body": "Thanks for the context, this does sound tricky :( Unfortunately, some degenerate groups can have upwards of 0.5M entries (of at least 16 bytes each), so I'm concerned the list approach would quickly run into maximum-record-size problems, as well as expensive serialization and deserialization costs. For now, we run a patched kafka client which intentionally leaks these metrics, which is far from a long term solution but at least keeps us running at the moment.", "created": "2025-10-13T20:23:00.873+0000"}, {"author": "Matthias J. Sax", "body": "0.5M entries... yeah, that won't work with the list approach... Did you consider the \"dummy iterator\" idea? Something like, create an `all()` iterator at startup, and every X second, you first create a new all() iterator, and close the old one? This way, you have at least one open iterator all the time \u2013 you still want to close and replace it periodically, to not get an very old open iterator. Could this work? Leaking the metrics sounds like a bad idea, as it will consume a lot a resources inside RocksDB...", "created": "2025-10-13T20:32:28.656+0000"}, {"author": "Steven Schlansker", "body": "Yes, we can explore the dummy iterator approach. That said, is this not a problem also for built in processors, like the ForeignTableJoinProcessor? It also seems to use a range scan per record.", "created": "2025-10-13T20:39:55.781+0000"}, {"author": "Matthias J. Sax", "body": "We did not observe any regression in our test/benchmark setup with regard to throughput (but maybe our setup is just not catching it), and nobody reported anything about it yet (well, beside you :))... But yes, might be worth to look into. It's not just FK join, but also session-windows, sliding-windows, and stream-stream join that use range scans... (maybe also others \u2013 would need to double check the code).", "created": "2025-10-13T22:36:54.353+0000"}, {"author": "Steven Schlansker", "body": "I believe I can observe a similar performance bottleneck where the `ForeinTableJoinProcessorSupplier$KTableKTableJoinProcessor` is less performant than it could be due to repeated registering and unregistering metrics with this lock, so while I am happy to test workaround on our custom processor, I have increased confidence that ideally there would be a fix outside of each individual processor having workarounds: !image-2025-10-20-13-36-54-857.png!", "created": "2025-10-20T20:37:14.412+0000"}, {"author": "Steven Schlansker", "body": "Ok, I think part of the reason why the monitor contention is so high, is because we have a lot of metrics registered, and the storage is a LinkedList inside a CHM. I'm not sure yet if 9.1M(!!) metrics is a leak or something we're doing wrong... !image-2025-10-21-09-24-02-505.png!", "created": "2025-10-21T16:23:30.217+0000"}, {"author": "Steven Schlansker", "body": "Ok, this might be an error on our end. I (too eagerly?) picked the fix to KAFKA-19748 before it was finished, looks like the version I have is incomplete, and thought the metrics leak was fixed. I will re-apply the final version and hope it fixes the leak properly this time.", "created": "2025-10-21T17:03:41.539+0000"}, {"author": "Steven Schlansker", "body": "Re-picking the fix to KAFKA-19748 made the situation much, much better. But, I am still seeing elevated levels of contention, with the leak fixed.", "created": "2025-10-21T18:18:41.302+0000"}, {"author": "Steven Schlansker", "body": "[~mjsax] , would it make sense to move the state store oldest open iterator metric from current INFO to only register when metrics is set to DEBUG level? That would resolve the issue as far as we are concerned, we are happy to accept this kind of overhead when debugging (now that the leak is fixed).", "created": "2025-10-21T19:26:12.939+0000"}, {"author": "Matthias J. Sax", "body": "{quote}ideally there would be a fix outside of each individual processor having workarounds {quote} I never disagreed about this \u2013 just wanted to get you out of the ditch, until we find a fix :) Glad you figures out the memory/metric leak thing, and happy to hear that the fix improves the situation... AK 4.1.1 should do out soon.... Interesting idea about making it a DEBUG level metric \u2013 could be a good solution in case we cannot figure out anything better. But would require a KIP I assume? [~bbejeck] wanted to work on this ticket. Let's hear from him. \u2013 Personally I would hope that we just find a good fix, even if I am not 100% sure what it could be \u2013 maybe something a lazy/delayed removal of the metric, that we would cancel if a new iterator comes in again?", "created": "2025-10-21T20:23:10.667+0000"}], "derived_tasks": {"summary": "Streams open iterator tracking has high contention on metrics lock - We run Kafka Streams 4", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19662", "title": "Support resetting offsets in kafka-share-groups.sh for topics which are not currently subscribed", "description": "The kafka-share-groups.sh tool can be used to reset the start offset for consumption if the share group is empty. One use for this is to initialise the start offset before starting to use the share group. Currently, the tool only lets the user reset the start offset for a topic which already has offsets in the share group. Instead, it should permit setting the SPSO for any topic, subject to auth checking. This brings the tool's behaviour in line with kafka-consumer-groups.sh.", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-09-01T16:58:01.000+0000", "updated": "2025-10-23T17:37:53.000+0000", "labels": [], "components": ["tools"], "comments": [], "derived_tasks": {"summary": "Support resetting offsets in kafka-share-groups.sh for topics which are not currently subscribed - The kafka-share-groups", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19648", "title": "KIP-1170: Unify cluster metadata bootstrapping", "description": "Implementation for https://cwiki.apache.org/confluence/display/KAFKA/KIP-1170%3A+Unify+cluster+metadata+bootstrapping", "status": "Open", "priority": "Major", "reporter": "Jos\u00e9 Armando Garc\u00eda Sancio", "assignee": "Jos\u00e9 Armando Garc\u00eda Sancio", "created": "2025-08-27T20:25:31.000+0000", "updated": "2025-10-24T17:34:14.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "KIP-1170: Unify cluster metadata bootstrapping - Implementation for https://cwiki", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "KAFKA-19638", "title": "NPE in `Processor#init()` accessing state store", "description": "As reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release. Stacktrace:  15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now. Thanks [~eduwerc] for reporting the issue. We clearly have a testing gap, not trying to use a state store within `Processor#init()`. \u2013 We need to close this gap. However, there is also the question if using zero as surrogate ts for this case (as the old code does), is a good solution or not? \u2013 We could try to use stream-time, but for the very first startup of an application, we also do not have stream-time established yet, so we kinda push the can down the road.", "status": "Patch Available", "priority": "Blocker", "reporter": "Matthias J. Sax", "assignee": "Eduwer Camacaro", "created": "2025-08-22T22:33:33.000+0000", "updated": "2025-10-25T06:07:10.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Jainil Rana", "body": "Hi @mjsax, I\u2019d like to work on this issue and submit a patch, can you please assign it to me?", "created": "2025-09-01T17:48:09.511+0000"}], "derived_tasks": {"summary": "NPE in `Processor#init()` accessing state store - As reported on the dev mailing list, we introduced a regression bug via https://issues", "classifications": ["bug"], "qa_pairs": [{"question": "However, there is also the question if using zero as surrogate ts for this case (as the old code does), is a good solution or not?", "answer": "Hi @mjsax, I\u2019d like to work on this issue and submit a patch, can you please assign it to me?"}]}}
{"id": "KAFKA-19634", "title": "document the encoding of nullable struct", "description": "In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.", "status": "Open", "priority": "Major", "reporter": "Jun Rao", "assignee": "Lan Ding", "created": "2025-08-22T00:05:00.000+0000", "updated": "2025-10-25T07:12:47.000+0000", "labels": [], "components": [], "comments": [{"author": "Lan Ding", "body": "Hi [~junrao], if you are not working on this, may i take it over, thanks.", "created": "2025-08-22T00:27:18.343+0000"}, {"author": "Jun Rao", "body": "[~isding_l] : Thanks for your interest. Feel free to take it.", "created": "2025-08-22T16:42:36.564+0000"}, {"author": "Jun Rao", "body": "[~isding_l]: Any progress on this? Thanks.", "created": "2025-09-26T21:48:07.306+0000"}, {"author": "Lan Ding", "body": "Hi [~junrao], I apologize, this slipped through the cracks. Thanks for the ping. I'll handle this within the next three days. Appreciate your patience.", "created": "2025-09-26T22:00:07.993+0000"}, {"author": "Lan Ding", "body": "Hi [~junrao], I have a question regarding the nullability of Struct. Under what circumstances could a Struct potentially be null? Looking at the current implementation, a Struct holds a reference to a Schema, and the Schema's isNullable() method returns false by default. This suggests that Struct should not be nullable at all from a design perspective. What confuses me is how this aligns with handling nullable structs at the protocol level, especially when compared to other nullable types like NullableString where null handling is embedded within the type definition itself. Could you help clarify if I'm missing something here? Looking forward to your insights.", "created": "2025-09-28T08:12:38.885+0000"}, {"author": "Jun Rao", "body": "[~isding_l]: Here is an example. ConsumerGroupHeartbeatResponse has a field Assignment, which is a nullable struct.  { \"name\": \"Assignment\", \"type\": \"Assignment\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\", \"about\": \"null if not provided; the assignment otherwise.\", \"fields\": [ { \"name\": \"TopicPartitions\", \"type\": \"[]TopicPartitions\", \"versions\": \"0+\", \"about\": \"The partitions assigned to the member that can be used immediately.\" } ]}  The generated class ConsumerGroupHeartbeatResponseData has the following code for serialization related to a null value.  if (assignment == null) { _writable.writeByte((byte) -1); } else { _writable.writeByte((byte) 1); assignment.write(_writable, _cache, _version); }", "created": "2025-09-29T17:48:17.309+0000"}, {"author": "Lan Ding", "body": "Thanks for the helpful details! I've opened a PR with the nullable struct documentation. [KAFKA-19634|https://github.com/apache/kafka/pull/20614] PTAL when you get a chance, thanks in advince.", "created": "2025-09-30T01:09:11.236+0000"}], "derived_tasks": {"summary": "document the encoding of nullable struct - In [https://kafka", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19606", "title": "Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode", "description": "JMX metrics RequestHandlerAvgIdlePercent reports a value close to 2 in combined kraft mode but it's expected to be b/w 0 and 1. This is an issue with combined mode specifically because both controller + broker are using the same Meter object in combined mode, defined in {{{}RequestThreadIdleMeter#requestThreadIdleMeter{}}}, but the controller and broker are using separate\u00a0{{KafkaRequestHandlerPool}} objects, where each object's {{{}threadPoolSize == KafkaConfig.numIoThreads{}}}. This means when calculating idle time, each pool divides by its own {{numIoThreads}}\u00a0value before reporting to the shared meter and\u00a0\u00a0{{RequestHandlerAvgIdlePercent}}\u00a0calculates the final result by accumulating all the values reported by all threads. However, since there are actually 2 \u00d7 numIoThreads total threads contributing to the metric, the denominator should be doubled to get the correct average.", "status": "Open", "priority": "Major", "reporter": "tony tang", "assignee": "tony tang", "created": "2025-08-14T20:23:09.000+0000", "updated": "2025-10-25T16:11:24.000+0000", "labels": [], "components": ["core"], "comments": [], "derived_tasks": {"summary": "Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode - JMX metrics RequestHandlerAvgIdlePercent reports a value close to 2 in...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19542", "title": "Consumer.close() does not remove all added sensors from Metrics", "description": "When a Consumer is created, it adds sensors to the Metrics object. Some of the classes that manage metrics implement AutoCloseable and diligently remove any Sensors it added from the Metrics object. However, not all of the sensors are removed. It feels like we should be consistent.", "status": "In Progress", "priority": "Minor", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-07-23T23:10:38.000+0000", "updated": "2025-10-24T00:57:16.000+0000", "labels": [], "components": ["clients", "consumer", "metrics"], "comments": [{"author": "Matthias J. Sax", "body": "Wondering if there is any relationship to https://issues.apache.org/jira/browse/KAFKA-19529 ?", "created": "2025-07-24T17:07:08.461+0000"}], "derived_tasks": {"summary": "Consumer.close() does not remove all added sensors from Metrics - When a Consumer is created, it adds sensors to the Metrics object", "classifications": ["feature", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19531", "title": "Add an end-to-end integration test for the DLQ feature", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Lucas Brutschy", "assignee": "Sebastien Viale", "created": "2025-07-21T11:47:10.000+0000", "updated": "2025-10-22T19:15:47.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Mahesh Sambaram", "body": "Hi[~lucasbru] , is this \u00a0a good first issue to work on?Could you please guide me on this?", "created": "2025-07-21T11:51:51.450+0000"}], "derived_tasks": {"summary": "Add an end-to-end integration test for the DLQ feature", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19521", "title": "Fix consumption and leaving group when source topic is deleted", "description": "h2. Issue Description In the integration test `HandlingSourceTopicDeletionIntegrationTest`, we observe significant differences in application exit time between old and new protocols when source topic is deleted: * Old Protocol: ~10 seconds * New Protocol: ~100 seconds h2. Observed Behavior # State Transition Discrepancy: ## One stream transitions to ERROR state quickly ## Another stream remains in PENDING_ERROR state for an extended period # Critical Log Evidence: **  Leader for partition inputTopic-0 is unknown for fetching offset -1 Building ListOffsets request for partitions {inputTopic-0=-1}  Consumer attempted to leave the group but couldn't complete it within 30000 ms. It will proceed to close. # Timing Analysis: * ** 60s: AsyncConsumer continues attempting to consume while leader is unknown ** 30s: Failed attempts to leave consumer group ** Total: ~100s vs ~10s in old protocol h2. Root Cause Analysis # Extended Polling Phase (60s): ** AsyncConsumer continues polling despite unknown leader ** No early termination mechanism when topic is deleted ** Waits for full timeout before shutdown # Group Leave Issues (30s): ** New protocol struggles with group leave operation ** Requires full timeout before forced close ** Potential coordination issues in new protocol implementation h2. Related Code *  HandlingSourceTopicDeletionIntegrationTest * AsyncConsumer implementation * New protocol group management code", "status": "Resolved", "priority": "Major", "reporter": "travis", "assignee": "travis", "created": "2025-07-18T18:06:59.000+0000", "updated": "2025-10-22T13:03:07.000+0000", "labels": [], "components": ["streams"], "comments": [], "derived_tasks": {"summary": "Fix consumption and leaving group when source topic is deleted", "classifications": ["improvement", "bug"], "qa_pairs": []}}
{"id": "KAFKA-19513", "title": "Flaky test: AclControlManagerTest.testDeleteExceedsMaxRecords()", "description": "[https://github.com/apache/kafka/actions/runs/16310027994/job/46064346395?pr=20164]", "status": "Open", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Qiang Liu", "created": "2025-07-16T08:54:52.000+0000", "updated": "2025-10-20T14:14:04.000+0000", "labels": [], "components": [], "comments": [{"author": "Lianet Magrans", "body": "Hey [~qiangmliu7] , just FYI, I've notice the flakiness on this one has increased lately, so just marked it as Flaky for now to reduce noise (https://github.com/apache/kafka/pull/20713), do you have bandwidth to take a look? cc. [~alyssahuang] in case you may have thoughts on what could be behind the flakiness here? (I see you introduced the test on [https://github.com/apache/kafka/pull/19974)] Thanks!", "created": "2025-10-16T13:32:08.110+0000"}, {"author": "Lianet Magrans", "body": "Here is the graph showing how flakiness has increased lately https://develocity.apache.org/scans/tests?search.relativeStartTime=P90D&search.rootProjectNames=kafka&search.tags=trunk&search.timeZoneId=America%2FToronto&tests.container=org.apache.kafka.controller.AclControlManagerTest&tests.sortField=FLAKY&tests.test=testDeleteExceedsMaxRecords()", "created": "2025-10-16T13:33:47.532+0000"}, {"author": "Qiang Liu", "body": "Hi, [~lianetm] I'll take a look at the recent builds.", "created": "2025-10-19T12:38:13.500+0000"}, {"author": "Qiang Liu", "body": "hello [~isding_l] and [~alyssahuang] Did some tests on my local and seems what takes time is AclControlManager.deleteAcls, the iteration will be execute over 10k * 10k times * iterate filters in deleteAcls and iterate idToAcl(items are added in the replay) in deleteAclsForFilter The number of execution is not the same everytime on my local testing + the resource on CI might cause the flaky. We didn't print log in deleteAcls, so # Maybe we can add some log in AclControlManager.deleteAcls to verify. # Confirm if AclControlManager.deleteAcls can be optimized.", "created": "2025-10-20T14:14:04.506+0000"}], "derived_tasks": {"summary": "Flaky test: AclControlManagerTest.testDeleteExceedsMaxRecords() - [https://github", "classifications": ["bug"], "qa_pairs": [{"question": "com/apache/kafka/actions/runs/16310027994/job/46064346395?", "answer": "Hey [~qiangmliu7] , just FYI, I've notice the flakiness on this one has increased lately, so just marked it as Flaky for now to reduce noise (https://github.com/apache/kafka/pull/20713), do you have bandwidth to take a look? cc. [~alyssahuang] in case you may have thoughts on what could be behind the flakiness here? (I see you introduced the test on [https://github.com/apache/kafka/pull/19974)] Thanks!"}]}}
{"id": "KAFKA-19493", "title": "Incorrect rate metric with larger window size", "description": "The Kafka Rate metric gives incorrect rate when the window size is larger say hourly. I suspect the code for calculating rate is incorrect for metrics which are not emitting rate per second or millisecond. The code for time unit calculation seems wrong: [convert(windowSize(config, now), unit)|https://github.com/apache/kafka/blob/da4fbba2793528e283458e080a690ad141857b0b/clients/src/main/java/org/apache/kafka/common/metrics/stats/Rate.java#L67] i.e. the convert code divides the elapsed time in window to the time unit. The problem can be easily reporduced by looking at `rebalance-rate-per-hour` metric.", "status": "Open", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Lan Ding", "created": "2025-07-10T13:13:39.000+0000", "updated": "2025-10-21T01:27:31.000+0000", "labels": [], "components": ["clients", "metrics"], "comments": [{"author": "Lan Ding", "body": "Hi [~apoorvmittal10], if you're not working on this, may I take it? Thanks.", "created": "2025-07-10T13:36:28.779+0000"}, {"author": "Apoorv Mittal", "body": "[~isding_l] thanks, please go ahead.", "created": "2025-07-10T15:59:18.025+0000"}], "derived_tasks": {"summary": "Incorrect rate metric with larger window size - The Kafka Rate metric gives incorrect rate when the window size is larger say hourly", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19486", "title": "Always use the latest version of kafka-topics.sh to create topics in system tests", "description": "Using \"old\" kafka-topics.sh to create topics on \"old\" brokers is stable, but it also has some disadvantages. 1. E2E does not cover the case of using \"new\" kafka-topics.sh on \"old\" brokers 2. it requires a bunch of conditions for \"zk\", since some old kafka-topics.sh require using zk connection In short, we should always use latest kafka-topics.sh to create topics on \"old\" brokers", "status": "Open", "priority": "Major", "reporter": "Chia-Ping Tsai", "assignee": "Chih-Yuan Chien", "created": "2025-07-09T13:44:51.000+0000", "updated": "2025-10-25T03:41:26.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Always use the latest version of kafka-topics.sh to create topics in system tests - Using \"old\" kafka-topics", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19479", "title": "at_least_once mode in Kafka Streams silently drops messages when the producer fails with MESSAGE_TOO_LARGE, violating delivery guarantees", "description": "*Description* It appears there is a scenario where Kafka Streams running with {{processing.guarantee=at_least_once}} does {*}not uphold its delivery guarantees{*}, resulting in *message loss.* *Reproduction Details* We run a simple Kafka Streams topology like the following:  props[StreamsConfig.APPLICATION_ID_CONFIG] = \"poc-at-least-once\" props[StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG] = Serdes.String().javaClass.name props[StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG] = Serdes.String().javaClass.name props[StreamsConfig.PROCESSING_GUARANTEE_CONFIG] = StreamsConfig.AT_LEAST_ONCE // Large producer batch size to induce MESSAGE_TOO_LARGE props[ProducerConfig.LINGER_MS_CONFIG] = \"300000\" props[ProducerConfig.BATCH_SIZE_CONFIG] = \"33554432\" /** * a custom ProductionExceptionHandler is registered to demonstrate that it is not triggered in this scenario. * in fact, neither the ProductionExceptionHandler nor the StreamsUncaughtExceptionHandler are invoked during this failure */ props[StreamsConfig.PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG] = \"poc.MyProductionExceptionHandler\" val stream = streamsBuilder.stream<String, String>(\"input.topic\") stream.peek { key, value -> println(\"$key:$value\") } .to(\"output.topic\")*  *What we observe:* * Records from {{input.topic}} are consumed and buffered at producer side * After some time (likely based on {{{}commit.interval.ms{}}}), the *consumer offset is committed* * Producer records *flush* is triggered * The sendind of records to kafka broker fails with {{{}MESSAGE_TOO_LARGE{}}}{*}{*} * As a result, the application {*}commits offsets without actually producing the records{*}, which leads to *silent message loss* *Steps to Reproduce* # Generate ~50,000 records (sized similarly to the sample project) in {{input.topic to induce MESSAGE_TOO_LARGE}} # Start the topology with the configuration above # Wait for all messages to be consumed # Observe: * ** Offsets are committed * ** Output topic receives no messages * ** Log shows repeated {{MESSAGE_TOO_LARGE}} error:  11:50:30.695 [kafka-producer-network-thread | kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer] WARN \u00a0o.a.k.c.producer.internals.Sender - [Producer clientId=kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer] Got error produce response in correlation id 255 on topic-partition output.topic-0, splitting and retrying (2147483647 attempts left). Error: MESSAGE_TOO_LARGE  *Reproduced*\u00a0with : * kafka-client-3.7.0, kafka-streams-3.7.0 * kafka-client-4.-.0, kafka-streams-4.0.0 * kafka-client-7.9.2-ccs, kafka-streams-7.9.2-ccs * kafka-client-8.0.0-ccs, kafka-streams-8.0.0-ccs *Expected Behavior* In {{at_least_once}} mode, Kafka Streams should *not commit offsets* unless records are {*}successfully produced{*}. *Attached* * configs for stream, producer, consumer * sample project used to replicate the issue", "status": "Resolved", "priority": "Critical", "reporter": "Mihai Lucian", "assignee": "Shashank", "created": "2025-07-08T09:23:07.000+0000", "updated": "2025-10-21T18:18:43.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Mihai Lucian", "body": "*Clarification* The configuration settings described above are not used in any of our actual projects, and the issue demonstrated in this example is not the root cause of the original problem we encountered. Our real concern involves message loss during the scaling of stateful Kafka Streams applications. In the process of investigating whether there are scenarios in which at_least_once processing might fail to uphold its delivery guarantees, we developed this proof of concept to explore and replicate such edge cases.", "created": "2025-07-08T10:57:58.374+0000"}, {"author": "Mihai Lucian", "body": "Additional comments on the delivery.timeout,ms: Upon reviewing the streamsd-config.txt, it can be observed the following log entry, which indicates that the Kafka producer is intelligent enough to automatically adjust the\u00a0[delivery.timeout.ms|http://delivery.timeout.ms/]\u00a0setting to ensure it aligns with the expected behavior: 11:47:50.649 [main] WARN\u00a0 o.a.k.clients.producer.KafkaProducer - [Producer clientId=kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer]\u00a0[delivery.timeout.ms|http://delivery.timeout.ms/]\u00a0should be equal to or larger than\u00a0[linger.ms|http://linger.ms/]\u00a0\u00a0+[request.timeout.ms|http://request.timeout.ms/]. Setting it to 330000.+ +Subsequently, I conducted additional tests in which I explicitly set\u00a0[delivery.timeout.ms|http://delivery.timeout.ms/]\u00a0to 400000 (i.e., greater than\u00a0[linger.ms|http://linger.ms/]+\u00a0\u00a0[request.timeout.ms|http://request.timeout.ms/]) and also disabled the ProductionExceptionHandler. Despite these changes, the issue continues to persist.", "created": "2025-07-15T07:04:24.861+0000"}, {"author": "Matthias J. Sax", "body": "Thanks for following up with details... I hope we find time quickly to look into this. Sounds concerning.", "created": "2025-07-16T23:05:49.890+0000"}, {"author": "Shashank", "body": "Hi [~mjsax], I would like to start working on this issue. I have reviewed this issue and was able to reproduce the behaviour locally with the attached poc. My initial plan is to create an integration test that reproduces the issue within streams and fails accordingly. I don't yet have a fix in mind, but do you think it would be wise for me to start by writing the failing integration test and then opening a PR with this failing test? That way, you can review it to confirm whether this is indeed a bug. Would opening such a PR affect any existing workflows, or is that approach okay? Once we align on that and confirm that this is a bug, I can proceed with the investigation and work on a possible solution.", "created": "2025-07-21T21:12:56.092+0000"}, {"author": "Matthias J. Sax", "body": "Thanks a lot [~shashankhs]. {quote}My initial plan is to create an integration test that reproduces the issue within streams and fails accordingly. {quote} This sounds like an excellent idea. I like the idea to start with a PR, that only adds this test (and let the test fail during initial review, and disable it before we merge the PR). If we can merge a test PR, it's easier for others to help looking into the issue, by enabling the test in a local branch.", "created": "2025-07-27T22:32:17.907+0000"}, {"author": "Shashank", "body": "Hi [~mjsax], I pushed the integration test and is ready to be reviewed here - [#20254|https://github.com/apache/kafka/pull/20254]", "created": "2025-07-29T01:10:09.033+0000"}, {"author": "Matthias J. Sax", "body": "Thanks for the test case. I believe it's actually a bug in the producer: https://github.com/apache/kafka/pull/20254#issuecomment-3134139853", "created": "2025-07-29T21:32:46.167+0000"}, {"author": "Shashank", "body": "Thank you [~mjsax]. Since you confirmed that it is indeed a bug in KafkaProducer, should we go ahead and disable the test case and merge the testcase, like you mentioned earlier, so that anyone can help looking into this issue? Next, I am planning on tracing the bug and understanding it better, so that I can write more specific failing tests.", "created": "2025-07-30T02:50:42.984+0000"}, {"author": "Jun Rao", "body": "Merged [https://github.com/apache/kafka/pull/20285] to trunk.", "created": "2025-10-16T22:28:00.774+0000"}], "derived_tasks": {"summary": "at_least_once mode in Kafka Streams silently drops messages when the producer fails with MESSAGE_TOO_LARGE, violating delivery guarantees - *Descri...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19431", "title": "Stronger assignment consistency with subscription for consumer groups", "description": "Currently, consumer group assignments are eventually consistent with subscriptions: when a member has unrevoked partitions, it is not allowed to reconcile with the latest target assignment. If a member with unrevoked partitions shrinks its subscription, it may observe assignments from the broker containing topics it is no longer subscribed to. If we wanted to, we could tighten this up at the cost of extra CPU time. Note that it's not feasible to close the gap for regex subscriptions, since there will always be a window when the regex is not yet resolved and we cannot tell whether a topic is part of the subscription. One way to do this would be to update {{CurrentAssignmentBuilder}} and * Add a {{MetadataImage}} and map of resolved regexes * Define the set of subscribed topic uuids as the union of the topic name subscription and resolved regex topic names, like how {{TargetAssignmentBuilder}} does it. ** When the regex is unresolved, we can\u2019t know which topics are part of the subscription. We treat unresolved regexes as matching no topics, to be conservative. This way, the assignment is always consistent with the subscription. * Update the loop over topics in {{computeNextAssignment}} to treat the assigned partitions as an empty set when the topic is not part of the subscription. * Do not advance the member epoch past the target assignment epoch when exiting the {{UNREVOKED_PARTITIONS}} state. * Define an {{updateCurrentAssignment}} method that drops any unsubscribed topics from the member\u2019s current assignment and transitions the member to {{UNREVOKED_PARTITIONS}} if any topics were dropped. * Use {{updateCurrentAssignment}} on the other {{UNREVOKED_PARTITIONS}} path. Additionally, if we ever end up with asynchronous assignors (such as client-computed ones), * We add a new flag to {{maybeReconcile}} called {{{}hasSubscriptionChanged{}}}. When the flag is set, we run the {{CurrentAssignmentBuilder}} even when reconciled to the target assignment, since the target assignment can lag behind the group epoch. * Use {{updateCurrentAssignment}}\u00a0on all {{CurrentAssignmentBuilder}} paths that do not use {{{}computeNextAssignment{}}}.", "status": "Resolved", "priority": "Major", "reporter": "Sean Quah", "assignee": "Sean Quah", "created": "2025-06-24T14:24:14.000+0000", "updated": "2025-10-19T14:57:07.000+0000", "labels": [], "components": ["group-coordinator"], "comments": [], "derived_tasks": {"summary": "Stronger assignment consistency with subscription for consumer groups - Currently, consumer group assignments are eventually consistent with subscr...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19426", "title": "TopicBasedRemoteLogMetadataManager's initial should happen after the broker ready", "description": "currently. the default value don't make sure the retry can always happen with successful For example: There are possible two timeout (2 * 60s) happen at TopicBasedRemoteLogMetadataManager#initializeResources [2025-06-03 21:57:21,151] INFO Topic __remote_log_metadata does not exist. Error: Timed out waiting for a node assignment. Call: listNodes at [2025-06-03 21:58:21,153] ERROR Encountered error while creating __remote_log_metadata topic. java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics at this original default configure value request the server must be ready within 2 minutes. Not every broker can reach this requirement. So propose the [KIP-1197| https://cwiki.apache.org/confluence/display/KAFKA/KIP-1197%3A+Introduce+new+method+to+improve+the+TopicBasedRemoteLogMetadataManager%27s+initialization]", "status": "Open", "priority": "Major", "reporter": "fujian", "assignee": null, "created": "2025-06-21T11:38:51.000+0000", "updated": "2025-10-22T22:03:30.000+0000", "labels": [], "components": ["Tiered-Storage"], "comments": [], "derived_tasks": {"summary": "TopicBasedRemoteLogMetadataManager's initial should happen after the broker ready - currently", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19345", "title": "Use ShareFetchUtils mock for DelayedShareFetchTest tests", "description": "", "status": "Open", "priority": "Major", "reporter": "Abhinav Dixit", "assignee": "Kuan Po Tseng", "created": "2025-05-28T08:14:14.000+0000", "updated": "2025-10-25T03:15:44.000+0000", "labels": [], "components": [], "comments": [{"author": "Kuan Po Tseng", "body": "Hi [~adixitconfluent] , may I take this one?", "created": "2025-05-28T08:28:36.812+0000"}, {"author": "Abhinav Dixit", "body": "I have assigned it to you [~brandboat] , thanks!", "created": "2025-05-28T09:57:26.198+0000"}], "derived_tasks": {"summary": "Use ShareFetchUtils mock for DelayedShareFetchTest tests", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19340", "title": "Move DelayedRemoteFetch to the storage module", "description": "Move DelayedRemoteFetch to storage module and rewrite it to java.", "status": "Resolved", "priority": "Minor", "reporter": "Lan Ding", "assignee": "Lan Ding", "created": "2025-05-27T12:08:39.000+0000", "updated": "2025-10-25T08:43:10.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Move DelayedRemoteFetch to the storage module - Move DelayedRemoteFetch to storage module and rewrite it to java", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19262", "title": "Add a test to verify all metrics naming pattern", "description": "We should add an integration test for KafkaYammerMetrics to ensure all metric names conform to the expected naming convention. see disscussion: [https://lists.apache.org/thread/5wx7b724v9yhqytonbvc3vbyf5fsbsrp]", "status": "Open", "priority": "Minor", "reporter": "\u9ec3\u7ae3\u967d", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-05-11T08:27:36.000+0000", "updated": "2025-10-24T03:21:12.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Add a test to verify all metrics naming pattern - We should add an integration test for KafkaYammerMetrics to ensure all metric names conform to th...", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "KAFKA-19255", "title": "KRaft request manager should support one in-flight request per request type", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Kevin Wu", "assignee": "Kevin Wu", "created": "2025-05-08T18:56:37.000+0000", "updated": "2025-10-23T15:53:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Mickael Maison", "body": "Moving to the next release as we're now in code freeze for 4.1.0.", "created": "2025-06-25T10:16:27.639+0000"}], "derived_tasks": {"summary": "KRaft request manager should support one in-flight request per request type", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19153", "title": "Add OAuth integration tests", "description": "From [~omkreddy] on the KIP-1139 discussion thread: {quote}4. I recommend including Keycloak-based integration tests to ensure compatibility with standard OAuth providers.{quote} I don't know if this requires Testcontainers or not, but this is a big gap.", "status": "Resolved", "priority": "Major", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-04-15T18:29:45.000+0000", "updated": "2025-10-23T23:48:40.000+0000", "labels": ["OAuth2", "integration-tests", "kip-1139"], "components": ["clients"], "comments": [{"author": "Lianet Magrans", "body": "Merged to trunk and cherry-picked to 4.1 https://github.com/apache/kafka/commit/923b6c3fea1fa7c9f06ebf01440dd5c12fac58db", "created": "2025-06-12T20:22:49.565+0000"}], "derived_tasks": {"summary": "Add OAuth integration tests - From [~omkreddy] on the KIP-1139 discussion thread: {quote}4", "classifications": ["feature", "test"], "qa_pairs": []}}
{"id": "KAFKA-19132", "title": "Move FetchSession and related classes to server module", "description": "https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/FetchSession.scala", "status": "Patch Available", "priority": "Minor", "reporter": "Dmitry Werner", "assignee": "Dmitry Werner", "created": "2025-04-12T07:32:57.000+0000", "updated": "2025-10-26T03:44:04.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Move FetchSession and related classes to server module - https://github", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19123", "title": "the buffer of forwarded request should be released after receiving the reponse from controller", "description": "the forwarded requests are async processed by forwarded manager, but the processor release the buffer of forwarded requests after the requests are enqueued. That could corrupt the buffer, but it does not cause bug for now as the memory pool does not reuse the buffer.", "status": "Open", "priority": "Major", "reporter": "Chia-Ping Tsai", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-04-10T06:49:42.000+0000", "updated": "2025-10-24T03:21:17.000+0000", "labels": [], "components": [], "comments": [{"author": "Chia-Ping Tsai", "body": "Maybe we should release the buffer by `processNewResponses`", "created": "2025-04-10T06:50:40.616+0000"}], "derived_tasks": {"summary": "the buffer of forwarded request should be released after receiving the reponse from controller - the forwarded requests are async processed by forw...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19117", "title": "Client Throttling Log messages should be of log level - WARN (Java client)", "description": "We experienced an outage in our application due to a built up Kafka lag, which we eventually discovered to be the problem with Kafka Clients being throttled. Even when using the confluent cluster, the dashboard does point to exceeding the quota of the principal. It will be nice, if the log message on the client side is a WARNING. This helps those who don't have visibility to cluster, and just look at the problem from the client side, setup alerts etc. (instead of enabling TRACE to figure out where the problem is) NetworkClient#maybeThrottle  log.trace(\"Connection to node {} is throttled for {} ms until timestamp {}\", nodeId, throttleTimeMs, now + throttleTimeMs);", "status": "Open", "priority": "Minor", "reporter": "Ravi Kalasapur", "assignee": null, "created": "2025-04-09T09:42:54.000+0000", "updated": "2025-10-22T03:27:35.000+0000", "labels": [], "components": ["clients", "logging"], "comments": [{"author": "Siddhartha Devineni", "body": "Hello [~rkalasapur], I have changed the log level to warn in the NetworkClient as follows: https://github.com/apache/kafka/pull/19456 Please have a look. Thank you", "created": "2025-04-12T17:45:55.926+0000"}], "derived_tasks": {"summary": "Client Throttling Log messages should be of log level - WARN (Java client) - We experienced an outage in our application due to a built up Kafka la...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19089", "title": "Gradle :test task failing unexpectedly", "description": "In this build, we had two flaky tests in the \":core:test\" task, but we ended up with a failure and an exit code 1 [https://develocity.apache.org/s/3zz2cie6j76hc/tests/task/:core:test/details/kafka.api.SaslPlainPlaintextConsumerTest?top-execution=1] This was not expected since we have retries set to 1. I suspect there may be an issue with this test's generated test name. In the JUnit XML, we see > testCoordinatorFailover(String).groupProtocol=consumer but in the build scan, it appears as > \u00a0testCoordinatorFailover(String)[2]", "status": "Open", "priority": "Major", "reporter": "David Arthur", "assignee": "Chia-Ping Tsai", "created": "2025-04-04T15:05:26.000+0000", "updated": "2025-10-24T17:44:31.000+0000", "labels": [], "components": ["build"], "comments": [], "derived_tasks": {"summary": "Gradle :test task failing unexpectedly - In this build, we had two flaky tests in the \":core:test\" task, but we ended up with a failure and an exit...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-19042", "title": "Move kafka.api test cases to clients-integration-tests module", "description": "This is an umbrella Jira about moving integration tests from kafka.api module to clients-integration-tests module and rewrite them with ClusterTestExtensions. h2. AbstractConsumerTest * ConsumerBounceTest [~taijuwu] * ConsumerWithLegacyMessageFormatIntegrationTest [~lansg] * PlaintextConsumerAssignTest [~taijuwu] * PlaintextConsumerAssignorsTest [~taijuwu] * PlaintextConsumerCallbackTest [~m1a2st] * PlaintextConsumerCommitTest [~m1a2st] * PlaintextConsumerFetchTest [~m1a2st] * PlaintextConsumerPollTest\u00a0[~m1a2st] * PlaintextConsumerSubscriptionTest\u00a0[~m1a2st] h3. BaseConsumerTest [~m1a2st] * PlaintextConsumerTest [~m1a2st] * SaslMultiMechanismConsumerTest * SaslPlainPlaintextConsumerTest [~m1a2st] * SaslPlaintextConsumerTest * SaslSslConsumerTest * SslConsumerTest h3. RebootstrapTest * ConsumerRebootstrapTest ---- h2. AbstractSaslTest * SaslClientsWithInvalidCredentialsTest ---- h2. AbstractAuthorizerIntegrationTest * kafka.api.AuthorizerIntegrationTest * org.apache.kafka.tools.consumer.group.AuthorizerIntegrationTest ---- h2. BaseAdminIntegrationTest * PlaintextAdminIntegrationTest * SaslSslAdminIntegrationTest * SslAdminIntegrationTest ---- h2. BaseProducerSendTest * PlaintextProducerSendTest * SslProducerSendTest ---- h2. BaseQuotaTest * ClientIdQuotaTest * UserClientIdQuotaTest * UserQuotaTest ---- h2. EndToEndAuthorizationTest * DelegationTokenEndToEndAuthorizationTest * DelegationTokenEndToEndAuthorizationWithOwnerTest * PlaintextEndToEndAuthorizationTest * SslEndToEndAuthorizationTest h3. SaslEndToEndAuthorizationTest * GroupEndToEndAuthorizationTest * SaslGssapiSslEndToEndAuthorizationTest * SaslOAuthBearerSslEndToEndAuthorizationTest * SaslScramSslEndToEndAuthorizationTest ---- h2. Others * AdminClientWithPoliciesIntegrationTest [~yung] * ConsumerTopicCreationTest [~frankvicky] * CustomQuotaCallbackTest * GroupAuthorizerIntegrationTest [~lansg] * GroupCoordinatorIntegrationTest [~yung] * MetricsTest [~yung] * ProducerCompressionTest [~gongxuanzhang] * ProducerFailureHandlingTest [~gongxuanzhang] * ProducerIdExpirationTest [~gongxuanzhang] * ProducerSendWhileDeletionTest\u00a0[~m1a2st] * TransactionsBounceTest [~yangpoan] * TransactionsExpirationTest [~yangpoan] * TransactionsTest [~m1a2st] * TransactionsWithMaxInFlightOneTest [~yangpoan]", "status": "Open", "priority": "Major", "reporter": "PoAn Yang", "assignee": "PoAn Yang", "created": "2025-03-25T10:45:00.000+0000", "updated": "2025-10-23T23:45:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Jhen-Yung Hsu", "body": "Nit: client-integration-tests -> clients-integration-tests I noticed that both the Jira and the current PR title misspell it.", "created": "2025-04-05T15:06:06.259+0000"}, {"author": "PoAn Yang", "body": "Thanks. Updated it.", "created": "2025-04-06T03:30:07.233+0000"}, {"author": "Lianet Magrans", "body": "Hi [~yangpoan], long time no see! :) Is it still the intention to migrate the remaining consumer/producer/admin client integration tests from core to clients-integration-tests and to the new test infra? And if so, is this the right Jira to follow for updates? (Just trying to know where we are on this). Thanks!", "created": "2025-08-18T19:37:29.646+0000"}, {"author": "PoAn Yang", "body": "Hi [~lianetm], yes, this Jira lists all test cases which will be migrated to clients-integration-tests module. I think most of cases in AbstractConsumerTest and Others sections are finished, because they don't have inherited test cases. For other test cases like BaseConsumerTest, BaseAdminIntegrationTest, etc., we need to find a way to handle inherited test cases. I think we can follow what [~m1a2st] did, using a static function like\u00a0testSimpleConsumption to reuse the test case in different security protocols. cc [~chia7712]", "created": "2025-08-19T03:19:52.443+0000"}], "derived_tasks": {"summary": "Move kafka.api test cases to clients-integration-tests module - This is an umbrella Jira about moving integration tests from kafka", "classifications": ["test"], "qa_pairs": []}}
{"id": "KAFKA-19035", "title": "Ensure DLQ can be tested using TTD", "description": "", "status": "Open", "priority": "Major", "reporter": "Matthias J. Sax", "assignee": null, "created": "2025-03-24T21:24:59.000+0000", "updated": "2025-10-22T19:14:14.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Ensure DLQ can be tested using TTD", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-19020", "title": "Handle strict max fetch records in share fetch", "description": "", "status": "In Progress", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Jimmy Wang", "created": "2025-03-20T11:25:07.000+0000", "updated": "2025-10-25T07:58:15.000+0000", "labels": [], "components": [], "comments": [{"author": "Jimmy Wang", "body": "Hi [~apoorvmittal10]\u00a0\uff0c I wonder did you already start to handle this issue? If not, I think maybe I can help.", "created": "2025-06-17T07:23:54.573+0000"}, {"author": "Apoorv Mittal", "body": "[~jimmywang611] Are we able to make progress on it or shall I re-assign to myself?", "created": "2025-07-18T12:40:48.257+0000"}, {"author": "Jimmy Wang", "body": "[~apoorvmittal10]\u00a0 Sorry for the delay. I've been a bit occupied these days. I'll start to work on this issue this weekend. Thank you so much for asking.", "created": "2025-07-18T12:51:46.921+0000"}, {"author": "Apoorv Mittal", "body": "Thanks [~jimmywang611].", "created": "2025-07-18T13:03:13.951+0000"}, {"author": "Jimmy Wang", "body": "[~apoorvmittal10]\u00a0 The current\u00a0{{maxFetchRecords}}\u00a0limit isn't strict because\u00a0{{lastOffsetFromBatchWithRequestOffset()}} may return oversized offsets. I think tightening this could cause batch splits, am I on the right track? Additionally, do you think it is necessary to forcefully complete the DelayedShareFetch when the maxFetchRecords limit is satisfied (similar to the logic in isMinBytesSatisfied())? These are my early thoughts\u2014I\u2019ll run some tests to see how it works.", "created": "2025-07-19T19:29:13.877+0000"}], "derived_tasks": {"summary": "Handle strict max fetch records in share fetch", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "KAFKA-19008", "title": "Remove scala version from artifacts", "description": "We can remove it from the tarball. I think that we can also consider removing it from the jars.", "status": "Open", "priority": "Major", "reporter": "David Jacot", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-03-18T14:38:05.000+0000", "updated": "2025-10-24T03:21:23.000+0000", "labels": [], "components": [], "comments": [{"author": "\u9ec3\u7ae3\u967d", "body": "Hello [~dajac], If you won't work on this, may I take this issue?", "created": "2025-03-18T14:41:14.374+0000"}, {"author": "David Jacot", "body": "go for it.", "created": "2025-03-18T14:43:43.388+0000"}, {"author": "Ismael Juma", "body": "There were some concerns when it comes to removing it from the Maven artifacts (since some build tools handle that automatically). I think it's safe to do it for the tarball though. I suggest we stick to the latter for this Jira ticket.", "created": "2025-03-18T16:14:57.065+0000"}], "derived_tasks": {"summary": "Remove scala version from artifacts - We can remove it from the tarball", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-18995", "title": "MirrorMaker2 does not replicate from secondary to primary DC", "description": "I have a the following setup: {+}Kafka cluster DC1{+}: TopicA {+}Kafka cluster DC2{+}: DC1.TopicA (created by mirror maker in DC2) A producer always runs in DC1. Consumer consumes and commits in DC1 up to 100 messages. Consumer stops in DC1. Then it fails over to start in DC2. It now consumes and commits in DC2 from topic DC1.TopicA in the same consumer group. It consumes and commits 200 additional messages in DC2 from replicated topic (DC1.TopicA). The consumer stops and fails back to DC1. {*}The expectation is to start from message 301{*}. But this is not the case (It starts from 101). The committed offsets in replicated topic never syncs back to primary topic in the same consumer group (It used to work fine in 3.2.3). I could not find a bug / feature reported along these lines hence I am flagging it as a new one.", "status": "Open", "priority": "Major", "reporter": "Prashanth", "assignee": null, "created": "2025-03-15T12:13:00.000+0000", "updated": "2025-10-23T03:13:23.000+0000", "labels": [], "components": ["mirrormaker"], "comments": [], "derived_tasks": {"summary": "MirrorMaker2 does not replicate from secondary to primary DC - I have a the following setup: {+}Kafka cluster DC1{+}: TopicA {+}Kafka cluster DC2{+...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "KAFKA-18973", "title": "Review MetadataSchemaCheckerToolTest.testVerifyEvolutionGit requiring git project", "description": "While testing a release candidate we noticed that the fact that the test testVerifyEvolutionGit requires a git directory, causes that running tests on a release source folder after download doesn't work anymore (as it used to in previous versions). As of 4.0 it fails with: {_}java.lang.RuntimeException: Invalid directory, need to be within a Git repository{_}. This started failing on 4.0 which is the first release to include this test.", "status": "Resolved", "priority": "Major", "reporter": "Lianet Magrans", "assignee": "Arpit Goyal", "created": "2025-03-13T00:04:22.000+0000", "updated": "2025-10-22T15:20:23.000+0000", "labels": [], "components": [], "comments": [{"author": "PoAn Yang", "body": "Hi [~lianetm], if you're not working on this, may I take it? Thanks.", "created": "2025-03-13T00:27:00.753+0000"}, {"author": "Lianet Magrans", "body": "Sure, thanks!", "created": "2025-03-13T01:34:34.977+0000"}, {"author": "Mickael Maison", "body": "Moving to the next release as we're now in code freeze for 4.1.0.", "created": "2025-06-25T09:25:08.644+0000"}, {"author": "Arpit Goyal", "body": "[~yangpoan]\u00a0 I am unasisgning it , as i see no activity in the last 2 month.", "created": "2025-10-15T04:44:59.334+0000"}, {"author": "Arpit Goyal", "body": "https://github.com/apache/kafka/pull/20703", "created": "2025-10-15T05:41:02.333+0000"}, {"author": "Arpit Goyal", "body": "[~lianetm]\u00a0 Please review it", "created": "2025-10-15T05:41:23.442+0000"}], "derived_tasks": {"summary": "Review MetadataSchemaCheckerToolTest.testVerifyEvolutionGit requiring git project - While testing a release candidate we noticed that the fact that...", "classifications": ["test"], "qa_pairs": []}}
{"id": "KAFKA-18913", "title": "Consider removing state-updater feature flag", "description": "We did enable the new StateUpdated thread with 3.8 release. We should consider removing the internal feature flag {{__state.updater.enabled__}}, and drop the old code that restores state stores in the poll loop by the stream thread. The entry point of the old restoration code is the following: https://github.com/apache/kafka/blob/138c2a211ad90e3966b01b99c4570df50ec48ea2/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L1100", "status": "In Progress", "priority": "Blocker", "reporter": "Matthias J. Sax", "assignee": "Shashank", "created": "2025-03-03T16:53:31.000+0000", "updated": "2025-10-22T19:32:28.000+0000", "labels": [], "components": ["streams"], "comments": [{"author": "Janindu Pathirana", "body": "Hi [~mjsax] , I'd like to work on this, but might need some pointers and more details to work on this. Shall I assign this to me then?", "created": "2025-03-06T12:55:42.432+0000"}, {"author": "Matthias J. Sax", "body": "[~cadonna] \u2013 WDYT? Should we do this for 4.1 release? Or do you think we should wait for 4.2? I believe it's time to rip out the old code.", "created": "2025-03-07T00:53:34.763+0000"}, {"author": "Bruno Cadonna", "body": "Thank you for your interest, [~janchilling]! [~mjsax], yes, let's move forward and rip out the old code.", "created": "2025-03-07T08:40:34.155+0000"}, {"author": "Janindu Pathirana", "body": "Hi [~mjsax] , Would you please be able to give me a more detailed overview on what should be done? Still new to streams so need more info as to what should be done and the purpose!:D", "created": "2025-03-09T19:30:08.167+0000"}, {"author": "Bruno Cadonna", "body": "[~janchilling], since I mainly worked on the state updater, I will give you an overview. First of all, if you want to know more about the state updater, you can have a look into the slides this talk: https://www.slideshare.net/slideshow/restoring-restorations-reputation-in-kafka-streams-with-bruno-cadonna-lucas-brutschy/258205394 or watch the recordings of the talk: https://www.confluent.io/events/kafka-summit-london-2023/restoring-restorations-reputation-in-kafka-streams/. I added more details to the description of this ticket. Basically, you need to find the usages of the internal feature flag {{__state.updater.enabled__}} and remove the code that is guarded by its negation. An example is provided in the description of the ticket. Of course in the end, you also need to remove the internal feature flag itself. Additionally, you also need to get rid of the tests that verify the old code. Hope that helps, otherwise let me know.", "created": "2025-03-11T09:10:31.825+0000"}, {"author": "Janindu Pathirana", "body": "Hi [~cadonna] , Can you check on this [https://github.com/apache/kafka/pull/19275] please?", "created": "2025-03-24T19:07:48.803+0000"}, {"author": "Bruno Cadonna", "body": "Hi [~janchilling], I saw your PR. Thank you! Sorry for being silent, but I was out last week. I will look at it as soon as I find time.", "created": "2025-03-24T20:17:56.949+0000"}, {"author": "Mickael Maison", "body": "Moving to the next release as we're now in code freeze for 4.1.0.", "created": "2025-06-25T09:57:12.534+0000"}, {"author": "Lucas Brutschy", "body": "[~shashankhs] are you still on this? Would be nice to get it into 4.2", "created": "2025-10-22T18:19:05.139+0000"}, {"author": "Shashank", "body": "Hi [~lucasbru], yes! I haven't had the chance to work on the remaining tests since our last PR, though. I'll aim to make some progress by the end of this month.", "created": "2025-10-22T19:32:28.546+0000"}], "derived_tasks": {"summary": "Consider removing state-updater feature flag - We did enable the new StateUpdated thread with 3", "classifications": ["feature", "improvement", "task"], "qa_pairs": []}}
{"id": "KAFKA-18896", "title": "Add support for Login", "description": "", "status": "Open", "priority": "Major", "reporter": "Mickael Maison", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-02-27T17:36:07.000+0000", "updated": "2025-10-21T10:16:43.000+0000", "labels": [], "components": [], "comments": [], "derived_tasks": {"summary": "Add support for Login", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-18894", "title": "Add support for ConfigProvider", "description": "", "status": "Open", "priority": "Major", "reporter": "Mickael Maison", "assignee": "Jhen-Yung Hsu", "created": "2025-02-27T17:35:45.000+0000", "updated": "2025-10-25T03:34:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Jhen-Yung Hsu", "body": "I'm working on this, thanks :)", "created": "2025-02-27T21:02:54.228+0000"}], "derived_tasks": {"summary": "Add support for ConfigProvider", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "KAFKA-18836", "title": "Make ConsumerGroupMetadata an interface", "description": "This ticket is based on feedback from the user mailing list: [https://lists.apache.org/thread/bw0ycbmtxlrfd3bsh9kk41xjdq9dhmnz] Users should not create instances of `ConsumerGroupMetadata`, and this class should not have any public constructor. Ideally, it would only be an interface. We should do a KIP an deprecate both constructors, and make it an interface in AK 5.0 release. KIP https://cwiki.apache.org/confluence/display/KAFKA/KIP-1136%3A+Make+ConsumerGroupMetadata+an+interface", "status": "Open", "priority": "Minor", "reporter": "Matthias J. Sax", "assignee": "Pawe\u0142 Szymczyk", "created": "2025-02-20T02:48:53.000+0000", "updated": "2025-10-20T15:25:31.000+0000", "labels": ["KIP-1136"], "components": ["clients", "consumer"], "comments": [{"author": "Ukpa Uchechi", "body": "Hi [~mjsax] , can I pick this up?", "created": "2025-02-20T03:29:02.019+0000"}, {"author": "Matthias J. Sax", "body": "Sure. :)", "created": "2025-02-20T05:03:54.097+0000"}, {"author": "Pawe\u0142 Szymczyk", "body": "Hi [~graceu] , I am an author of the feedback from mailing list, please let me finish this contribution :)", "created": "2025-02-20T10:31:57.536+0000"}, {"author": "Ukpa Uchechi", "body": "[~pawelszymczyk] sure", "created": "2025-02-20T10:45:50.761+0000"}, {"author": "Pawe\u0142 Szymczyk", "body": "I am waiting for Confluence account approval, when this will be approved I will provide KIP.", "created": "2025-02-20T13:45:32.786+0000"}, {"author": "Matthias J. Sax", "body": "Thanks for for agree on who works on this. [~pawelszymczyk] you should be all set with the Confluence account. Did assign the ticket to you. Will also leave a few comments on your PR, that you should consider for the KIP.", "created": "2025-02-20T17:37:39.413+0000"}, {"author": "Pawe\u0142 Szymczyk", "body": "KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1136%3A+Make+ConsumerGroupMetadata+an+interface", "created": "2025-10-02T19:46:41.990+0000"}, {"author": "Andrew Schofield", "body": "[https://github.com/apache/kafka/pull/20728] is the PR for the deprecation part of this issue which is in Apache Kafka 4.2. To complete the issue, the class needs to be converted to an interface.", "created": "2025-10-20T08:52:45.054+0000"}], "derived_tasks": {"summary": "Make ConsumerGroupMetadata an interface - This ticket is based on feedback from the user mailing list: [https://lists", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "KAFKA-18812", "title": "Improve new consumer API errors upon background thread failures", "description": "The new consumer background thread could fail to start, in which case it will log an error and die. At the moment, the expectation is that consumer api calls will\u00a0 fail with TimeoutException (KafkaException) when sending events to the background for processing and wait for responses (that will never be received). We should review to consider improving, given that seems pointless to wait for an event to be processed if we know the background thread is not running. Maybe we can propagate a clearer error message better Note that once in this area, we should also review the errors propagated to the API if the background fails on a single run (runOnce), which also logs but the thread does not die. The expectation in this case is that individual API calls will fail when trying to access resources that may not be available (ex. a request manager). But each process within ApplicationEventProcessor handles this kind of situation differently, so we should improve it if we consider, consistently for all APIs", "status": "Open", "priority": "Major", "reporter": "Lianet Magrans", "assignee": "Genseric Ghiro", "created": "2025-02-17T15:36:27.000+0000", "updated": "2025-10-20T14:51:37.000+0000", "labels": ["consumer-threading-refactor"], "components": ["clients", "consumer"], "comments": [{"author": "TengYao Chi", "body": "Hi [~lianetm] May I take over this issue?", "created": "2025-02-17T15:39:38.704+0000"}, {"author": "Lianet Magrans", "body": "Sure, thanks!", "created": "2025-02-17T15:42:09.108+0000"}, {"author": "Lianet Magrans", "body": "Hey [~frankvicky], is this something you still plan to work on? If not let me know to take over (too late for 4.1 but would be important to sort this one out for 4.2). Thanks!", "created": "2025-06-16T16:40:34.031+0000"}, {"author": "TengYao Chi", "body": "As offline discussion, I have assigned to you :)", "created": "2025-06-17T02:54:18.249+0000"}], "derived_tasks": {"summary": "Improve new consumer API errors upon background thread failures - The new consumer background thread could fail to start, in which case it will log...", "classifications": ["feature", "improvement", "bug"], "qa_pairs": []}}
{"id": "KAFKA-18786", "title": "Enable returnning supported features from a specific broker", "description": "Currently, the Admin#describeFeatures method sends an API request to an arbitrary broker or controller within the cluster. This approach has the limitation that it does not provide information about the supported features of a specific broker or controller. Furthermore, it may inadvertently lead users to believe that feature support is a cluster-wide attribute", "status": "In Progress", "priority": "Major", "reporter": "Chia-Ping Tsai", "assignee": "PoAn Yang", "created": "2025-02-13T03:40:58.000+0000", "updated": "2025-10-22T13:57:55.000+0000", "labels": ["need-kip"], "components": [], "comments": [{"author": "PoAn Yang", "body": "Hi [~chia7712], if you're not working on this, may I take it? Thank you.", "created": "2025-02-13T03:45:28.747+0000"}, {"author": "PoAn Yang", "body": "KIP: https://cwiki.apache.org/confluence/x/5gnXF Discussion thread: https://lists.apache.org/thread/mbjlgbv2q4x5ml7mx93ls77bg93wtrfy", "created": "2025-04-15T14:32:41.210+0000"}, {"author": "PoAn Yang", "body": "Vote thread: [https://lists.apache.org/thread/czmbljctdkrl6s834gmrv4kwn8d6oq44]", "created": "2025-06-25T06:20:03.198+0000"}], "derived_tasks": {"summary": "Enable returnning supported features from a specific broker - Currently, the Admin#describeFeatures method sends an API request to an arbitrary bro...", "classifications": ["feature", "new feature"], "qa_pairs": []}}
{"id": "KAFKA-18709", "title": "Move DynamicConfigPublisher to metadata module", "description": "", "status": "Open", "priority": "Major", "reporter": "\u9ec3\u7ae3\u967d", "assignee": "Jimmy Wang", "created": "2025-02-03T10:08:00.000+0000", "updated": "2025-10-24T08:05:48.000+0000", "labels": [], "components": [], "comments": [{"author": "Jimmy Wang", "body": "Hi [~m1a2st]\u00a0\uff0c If you haven't started to handle this ticket yet, maybe I could help:)", "created": "2025-10-21T09:04:22.765+0000"}, {"author": "\u9ec3\u7ae3\u967d", "body": "Sure, go ahead, I will assign to you!", "created": "2025-10-21T09:59:20.618+0000"}, {"author": "Jimmy Wang", "body": "[~m1a2st] Thanks~", "created": "2025-10-24T08:05:48.935+0000"}], "derived_tasks": {"summary": "Move DynamicConfigPublisher to metadata module", "classifications": ["sub-task"], "qa_pairs": []}}
