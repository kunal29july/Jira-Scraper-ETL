{"id": "HADOOP-19736", "title": "ABFS: Support for new auth type: User-bound SAS", "description": "Adding support for new authentication type: user bound SAS", "status": "Open", "priority": "Major", "reporter": "Manika Joshi", "assignee": "Manika Joshi", "created": "2025-10-24T09:31:18.000+0000", "updated": "2025-10-24T11:45:38.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "manika137 opened a new pull request, #8051: URL: https://github.com/apache/hadoop/pull/8051 ### Description of PR Adding support for new authentication type: user bound SAS ### How was this patch tested? Test suite will be run for the patch", "created": "2025-10-24T09:44:17.799+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8051: URL: https://github.com/apache/hadoop/pull/8051#issuecomment-3442714485 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 38s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 50s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 23s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 26m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 34s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 36 new + 4 unchanged - 0 fixed = 40 total (was 4) | | -1 :x: | mvnsite | 0m 37s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 32s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 2 new + 1472 unchanged - 0 fixed = 1474 total (was 1472) | | -1 :x: | javadoc | 0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 2 new + 1413 unchanged - 0 fixed = 1415 total (was 1413) | | -1 :x: | spotbugs | 0m 34s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 29m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 41s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 119m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8051 | | JIRA Issue | HADOOP-19736 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 88abe69cd96c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 05b52e40f1bc99edc039fc0d2ab5f83d1ceb0da9 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 779 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-24T11:45:38.208+0000"}], "derived_tasks": {"summary": "ABFS: Support for new auth type: User-bound SAS - Adding support for new authentication type: user bound SAS", "classifications": ["feature", "task"], "qa_pairs": []}}
{"id": "HADOOP-19735", "title": "ABFS: Adding request priority for prefetches", "description": "Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "status": "Open", "priority": "Major", "reporter": "Manika Joshi", "assignee": "Manika Joshi", "created": "2025-10-24T04:46:13.000+0000", "updated": "2025-10-24T05:00:42.000+0000", "labels": [], "components": ["fs/azure"], "comments": [], "derived_tasks": {"summary": "ABFS: Adding request priority for prefetches - Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server d...", "classifications": ["feature", "task"], "qa_pairs": []}}
{"id": "HADOOP-19734", "title": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"", "description": "Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests. Hypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs. Outcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover. Proposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue.", "status": "Resolved", "priority": "Minor", "reporter": "Steve Loughran", "assignee": null, "created": "2025-10-23T15:42:38.000+0000", "updated": "2025-10-24T13:48:33.000+0000", "labels": [], "components": ["fs/s3"], "comments": [{"author": "Steve Loughran", "body": "[INFO] [INFO]  This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h When these uploads fail we do leave incomplete uploads in progress:  Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found.  Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed.   Yet the uploads list afterwards finds it  job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4", "created": "2025-10-23T15:47:44.415+0000"}, {"author": "Steve Loughran", "body": "And stack on a write failure.   we'd have to map 400 + the error text to a \"MultipartUploadCompleteFailed\" exception and add a policy for it, leaving other 400s as unrecoverable.", "created": "2025-10-23T15:52:03.066+0000"}, {"author": "Steve Loughran", "body": "+ any tracking in block output stream should record when the POST to initiate the MPU was issued. That way if an error still surfaces but the output stream has been open for three days, we have a good cause \"stream open too long\"", "created": "2025-10-23T15:57:00.573+0000"}, {"author": "Steve Loughran", "body": "this is actually me making a mess of checksum config if the sdk checksum clalculation is set to \"always\" then the user MUST choose a checksum algorithm for s3 uploads (proposed: CRC32). I\"m going to leave checksum calculation off by default for performance and compatibility", "created": "2025-10-24T13:48:33.158+0000"}], "derived_tasks": {"summary": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\" - Experienced transient failure in test run of https:/...", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19733", "title": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`", "description": "HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found. I think the cause is: * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration` * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR. And the fix seems small: * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.", "status": "Open", "priority": "Minor", "reporter": "Brandon", "assignee": "Brandon", "created": "2025-10-22T19:56:55.000+0000", "updated": "2025-10-23T19:44:03.000+0000", "labels": ["pull-request-available"], "components": ["fs/s3"], "comments": [{"author": "Brandon", "body": "I haven't contributed to Hadoop or other Apache projects before, but this approachable for a first contribution. I'll open a PR.", "created": "2025-10-22T20:36:34.829+0000"}, {"author": "ASF GitHub Bot", "body": "brandonvin opened a new pull request, #8048: URL: https://github.com/apache/hadoop/pull/8048 \u2026lassloader <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR Follow-up to [HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) and [HADOOP-19733](https://issues.apache.org/jira/browse/HADOOP-19733) before it. With `fs.s3a.classloader.isolation` set to `false` in a Spark application, it was still impossible to load a credentials provider class from the Spark application jar. `fs.s3a.classloader.isolation` works by saving a reference to the intended classloader in the `Configuration`. However, loading credentials providers goes through `S3AUtils#getInstanceFromReflection`, which always used the classloader that loaded `S3AUtils`. With this patch, credentials providers will be loaded using the `Configuration`'s classloader. ### How was this patch tested? Unit tests in `org.apache.hadoop.fs.s3a.ITestS3AFileSystemIsolatedClassloader`. Manual testing in a Spark application. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-23T04:13:15.711+0000"}, {"author": "ASF GitHub Bot", "body": "brandonvin commented on code in PR #8048: URL: https://github.com/apache/hadoop/pull/8048#discussion_r2453905622 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS } } - private Map<String, String> mapOf() { - return new HashMap<>(); - } - - private Map<String, String> mapOf(String key, String value) { - HashMap<String, String> m = new HashMap<>(); - m.put(key, value); - return m; - } Review Comment: Since I added test cases that set 2 key-value pairs, I switched to `Map.of` instead of extending these. Not sure if there was a reason to avoid `Map.of` here.", "created": "2025-10-23T04:43:50.648+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8048: URL: https://github.com/apache/hadoop/pull/8048#issuecomment-3435132657 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 19s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html]([CI_URL] | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 51s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 31 new + 4 unchanged - 0 fixed = 35 total (was 4) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-aws.html]([CI_URL] | hadoop-tools/hadoop-aws generated 2 new + 188 unchanged - 0 fixed = 190 total (was 188) | | +1 :green_heart: | shadedclient | 15m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 1m 57s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 63m 28s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-aws | | | Nullcheck of conf at line 655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:[line 645] | | | Non-virtual method call in org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(String, String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:[line 125] | | Failed junit tests | hadoop.fs.s3a.auth.TestSignerManager | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8048 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 573c49df2825 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 032c335082f24aef12ee3e002ae1cfd9c5f40507 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 610 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T05:17:39.742+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8048: URL: https://github.com/apache/hadoop/pull/8048#issuecomment-3435205771 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html]([CI_URL] | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 12 new + 4 unchanged - 0 fixed = 16 total (was 4) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 46s | [/new-spotbugs-hadoop-tools_hadoop-aws.html]([CI_URL] | hadoop-tools/hadoop-aws generated 2 new + 188 unchanged - 0 fixed = 190 total (was 188) | | +1 :green_heart: | shadedclient | 14m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 0s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 63m 47s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-aws | | | Nullcheck of conf at line 655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:655 of value previously dereferenced in org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At S3AUtils.java:[line 645] | | | Non-virtual method call in org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(String, String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:String) passes null for non-null parameter of org.apache.hadoop.fs.s3a.S3AUtils.getInstanceFromReflection(String, Configuration, URI, Class, String, String) At SignerFactory.java:[line 125] | | Failed junit tests | hadoop.fs.s3a.auth.TestSignerManager | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8048 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 55f7cbac0d20 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 249aef5213fa039d252e7f7ae03c060b6c87d94f | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 616 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T05:46:12.703+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #8048: URL: https://github.com/apache/hadoop/pull/8048#discussion_r2455036391 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -37,10 +46,33 @@ */ public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + private static String customClassName = \"custom.class.name\"; + + private static class CustomCredentialsProvider implements AwsCredentialsProvider { + + public CustomCredentialsProvider() { + } + + @Override + public AwsCredentials resolveCredentials() { + return null; + } + + } + private static class CustomClassLoader extends ClassLoader { } - private final ClassLoader customClassLoader = new CustomClassLoader(); + private final ClassLoader customClassLoader = spy(new CustomClassLoader()); + { + try { Review Comment: this is a nice way to simulate classloader pain. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -28,6 +29,14 @@ import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.s3a.impl.InstantiationIOException; + +import software.amazon.awssdk.auth.credentials.AwsCredentials; Review Comment: nit: put the amazon imports in the same group as the junit ones ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -100,11 +122,26 @@ public void defaultIsolatedClassloader() throws IOException { .isEqualTo(fs.getClass().getClassLoader()) .describedAs(\"the classloader that loaded the fs\"); }); + + Throwable thrown = Assertions.catchThrowable(() -> { Review Comment: Use our `LambdaTestUtils.intercept()`; it's like the spark one and does the casting checks ``` InstantiationIOException ex = intercept(InstantiationIOException.class, () -> { assert...}) ``` we have a `assertExceptionContains` to look at the inner stuff, but the assert of L136 is fine. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -115,11 +152,31 @@ public void isolatedClassloader() throws IOException { .isEqualTo(fs.getClass().getClassLoader()) .describedAs(\"the classloader that loaded the fs\"); }); + + Throwable thrown = Assertions.catchThrowable(() -> { Review Comment: again `intercept()` and cut the assert at L163 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS } } - private Map<String, String> mapOf() { - return new HashMap<>(); - } - - private Map<String, String> mapOf(String key, String value) { - HashMap<String, String> m = new HashMap<>(); - m.put(key, value); - return m; - } Review Comment: It's because we only switched to java17 yesterday! And in trunk only. If you want to see this change in Hadoop 3.4.3 it'll still need to be java8 code, so this needs to be restored. Otherwise: trunk/3.5.0 only", "created": "2025-10-23T13:03:14.097+0000"}, {"author": "Brandon", "body": "Ok, will also update the custom signer loading to use the configuration, for consistency.", "created": "2025-10-23T16:33:11.604+0000"}, {"author": "ASF GitHub Bot", "body": "brandonvin commented on code in PR #8048: URL: https://github.com/apache/hadoop/pull/8048#discussion_r2456994430 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map<String, String> confToSet, Consumer<FileS } } - private Map<String, String> mapOf() { - return new HashMap<>(); - } - - private Map<String, String> mapOf(String key, String value) { - HashMap<String, String> m = new HashMap<>(); - m.put(key, value); - return m; - } Review Comment: Thanks, makes sense!", "created": "2025-10-23T19:44:03.691+0000"}], "derived_tasks": {"summary": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false` - HADOOP-18993 added the option `fs", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-19732", "title": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)", "description": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone. After the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.  ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:51)", "status": "Resolved", "priority": "Major", "reporter": "Karthik Palanisamy", "assignee": null, "created": "2025-10-21T18:43:38.000+0000", "updated": "2025-10-23T17:47:42.000+0000", "labels": [], "components": [], "comments": [{"author": "Steve Loughran", "body": "looks a duplicate of HDFS-17138. [~kpalanisamy] please set the hadoop version you saw it with. If it is a version without HDFS-17138 -please upgrade. closing as a duplicate. If it surfaces on branches with HDFS-17138, re-open", "created": "2025-10-23T12:41:51.201+0000"}, {"author": "Karthik Palanisamy", "body": "You\u2019re right [~stevel@apache.org].\u00a0My user version is 3.1.1, so it is missing this HDFS-17138 fix, which clearly addresses my user scenarios. Thanks for checking so quickly. I should have checked it, but unfortunately I taken your time :)", "created": "2025-10-23T17:47:42.603+0000"}], "derived_tasks": {"summary": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule) - The delegation token renewer enters runtime exit when a _NoMatchingRule_ e...", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "HADOOP-19731", "title": "Fix SpotBugs warnings introduced after SpotBugs version upgrade.", "description": "Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged. We plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "status": "In Progress", "priority": "Major", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "created": "2025-10-19T09:57:26.000+0000", "updated": "2025-10-23T12:26:45.000+0000", "labels": [], "components": ["common", "hdfs", "mapreduce", "yarn"], "comments": [{"author": "Anuj Modi", "body": "Hi [~slfan1989] Thanks for tracking this. We do have a bunch of PRs open that are facing issue reported here. What are the expectations here? Do we need to address all warnings with the PR itself or they can be ignored and taken later as part of this Jira? I think later would be better. It will help keep the changes in PR limited to what is being done and will ease the review process.", "created": "2025-10-23T12:10:33.527+0000"}, {"author": "Shilun Fan", "body": "I agree with your point. We\u2019ll work on submitting a common PR that includes a SpotBugs rule to temporarily suppress the new static analysis warnings and restore the state back to the 4.2.0 level.", "created": "2025-10-23T12:20:20.684+0000"}, {"author": "Anuj Modi", "body": "Sounds awesome. Thanks for all the efforts.", "created": "2025-10-23T12:26:45.915+0000"}], "derived_tasks": {"summary": "Fix SpotBugs warnings introduced after SpotBugs version upgrade. - Following the upgrade to SpotBugs {*}4", "classifications": ["improvement", "bug"], "qa_pairs": []}}
{"id": "HADOOP-19730", "title": "upgrade bouncycastle to 1.82 due to CVE-2025-8916", "description": "https://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.", "status": "Open", "priority": "Major", "reporter": "PJ Fanning", "assignee": null, "created": "2025-10-19T09:09:36.000+0000", "updated": "2025-10-23T17:26:09.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "pjfanning opened a new pull request, #8039: URL: https://github.com/apache/hadoop/pull/8039 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR HADOOP-19730 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-19T09:15:30.542+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8039: URL: https://github.com/apache/hadoop/pull/8039#issuecomment-3420366545 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 28m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 12s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 28s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | mvnsite | 8m 58s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 36s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 36s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 43m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 27m 23s | | the patch passed | | +1 :green_heart: | compile | 14m 49s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 49s | | the patch passed | | +1 :green_heart: | compile | 15m 37s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 7m 1s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 42s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 45m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 807m 40s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 36s | | The patch does not generate ASF License warnings. | | | | 1064m 10s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.TestRollingUpgrade | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | | hadoop.yarn.service.TestYarnNativeServices | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8039 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs | | uname | Linux 66cf96c27f49 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 695a0a30232b143ec8837d6a6648344ffd4efec0 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 4498 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-20T03:00:51.681+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #8039: URL: https://github.com/apache/hadoop/pull/8039", "created": "2025-10-21T01:42:32.195+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8039: URL: https://github.com/apache/hadoop/pull/8039#issuecomment-3424343993 @pjfanning Thanks for the contribution! Merged into trunk. Could we also open a PR for branch-3.4?", "created": "2025-10-21T01:44:38.858+0000"}, {"author": "ASF GitHub Bot", "body": "pjfanning opened a new pull request, #8047: URL: https://github.com/apache/hadoop/pull/8047 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR backport #6976 * HADOOP-19730. Upgrade Bouncycastle to 1.82 due to CVE-2025-8916 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-23T00:19:48.036+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8047: URL: https://github.com/apache/hadoop/pull/8047#issuecomment-3438232725 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 2m 54s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 39m 30s | | branch-3.4 passed | | +1 :green_heart: | compile | 18m 44s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 41s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 5s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 9m 20s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 50m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 29m 11s | | the patch passed | | +1 :green_heart: | compile | 17m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 19s | | the patch passed | | +1 :green_heart: | compile | 15m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 13s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 11s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 52m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 720m 55s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 49s | | The patch does not generate ASF License warnings. | | | | 1024m 55s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSubmission | | | hadoop.mapred.gridmix.TestLoadJob | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8047 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs | | uname | Linux d69a46a8ee67 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / c8b8fb4e82d33a470f10a447f4799cc872fb3c01 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3660 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T17:26:09.708+0000"}], "derived_tasks": {"summary": "upgrade bouncycastle to 1.82 due to CVE-2025-8916 - https://github", "classifications": ["task"], "qa_pairs": []}}
{"id": "HADOOP-19727", "title": "Release hadoop-thirdparty 1.5.0", "description": "Release hadoop-thirdparty 1.5.0", "status": "In Progress", "priority": "Major", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "created": "2025-10-14T14:34:32.000+0000", "updated": "2025-10-20T16:40:40.000+0000", "labels": [], "components": ["hadoop-thirdparty"], "comments": [], "derived_tasks": {"summary": "Release hadoop-thirdparty 1.5.0 - Release hadoop-thirdparty 1", "classifications": ["task"], "qa_pairs": []}}
{"id": "HADOOP-19726", "title": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module", "description": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:  This error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class. To resolve this issue, JDK 17 compile options have been added to ensure the maven-surefire-plugin works correctly in a JDK 17 environment. This PR adds the necessary compile options for maven-surefire-plugin to support JDK 17, fixing the error and ensuring that unit tests can run smoothly.", "status": "Resolved", "priority": "Major", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "created": "2025-10-12T23:47:15.000+0000", "updated": "2025-10-19T03:00:16.000+0000", "labels": ["pull-request-available"], "components": ["hadoop-tos"], "comments": [{"author": "ASF GitHub Bot", "body": "slfan1989 opened a new pull request, #8029: URL: https://github.com/apache/hadoop/pull/8029 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19726. [JDK17] Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module. ### How was this patch tested? CI ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-12T23:48:43.587+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3395542255 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | mvnsite | 0m 21s | | trunk passed | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 42m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 13s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | javadoc | 0m 12s | [/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tos in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | shadedclient | 1m 41s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 13s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in the patch failed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 55m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux dd7bc96b56b5 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f188198cdec1613ae955ea31795a8cb1ca496139 | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 576 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-13T00:44:42.818+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411382684 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 30s | | trunk passed | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | mvnsite | 0m 21s | | trunk passed | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 40m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 14m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 26s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 58m 21s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream | | | hadoop.fs.tosfs.object.TestObjectRangeInputStream | | | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider | | | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain | | | hadoop.fs.tosfs.object.TestObjectOutputStream | | | hadoop.fs.tosfs.commit.TestMagicOutputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 3f25ded462da 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2bd00ae481cc8a6aeb977fef2700e684236375a4 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 616 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-16T15:09:25.152+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411690102 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 6s | | trunk passed | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 24s | | trunk passed | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 35s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html]([CI_URL] | hadoop-cloud-storage-project/hadoop-tos in trunk has 56 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 12s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 12s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 8s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 0m 31s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 27s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 60m 57s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream | | | hadoop.fs.tosfs.object.TestObjectRangeInputStream | | | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider | | | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain | | | hadoop.fs.tosfs.object.TestObjectOutputStream | | | hadoop.fs.tosfs.commit.TestMagicOutputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 9e3548929018 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9ba2bc5bf9cda429475a820bfcf689479e7fdc2d | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 639 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-16T16:27:26.865+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3412145538 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 44s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 9s | | trunk passed | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 20s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 34s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos-warnings.html]([CI_URL] | hadoop-cloud-storage-project/hadoop-tos in trunk has 56 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 8s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 14s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 0m 31s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 58s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 60m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8029 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 6a680a7543dc 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1b29c9884dbd1895ea8116fe1c0cf495ce03d39f | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 643 (vs. ulimit of 5500) | | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-16T18:01:30.696+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3413251437 @wojiaodoubao Could you please help review this PR again? Thanks a lot!", "created": "2025-10-16T23:43:30.763+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8029: URL: https://github.com/apache/hadoop/pull/8029#issuecomment-3418122640 I plan to merge this PR, as the unit test errors in TOS have been resolved. If further optimization is needed later, we can submit a separate PR for improvements. cc: @steveloughran @wojiaodoubao", "created": "2025-10-18T09:40:49.588+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #8029: URL: https://github.com/apache/hadoop/pull/8029", "created": "2025-10-19T02:54:48.791+0000"}], "derived_tasks": {"summary": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module - Currently, the {{hadoop-tos}} module does not have the JDK 17 compile o...", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "HADOOP-19719", "title": "Upgrade to wildfly version with support for openssl 3", "description": "Wildfly 2.1.4 * doesn't work with openssl 3 (that symbol change...why did they do that?) we need a version with https://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d 2.2.5.Final does the openssl 3 support.", "status": "Resolved", "priority": "Major", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "created": "2025-10-08T10:52:33.000+0000", "updated": "2025-10-20T12:47:50.000+0000", "labels": ["pull-request-available"], "components": ["build", "fs/azure", "fs/s3"], "comments": [{"author": "ASF GitHub Bot", "body": "steveloughran opened a new pull request, #8019: URL: https://github.com/apache/hadoop/pull/8019 ### How was this patch tested? Going to see if it works on a mac... ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-08T10:59:16.505+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #8019: URL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381330281 the test which is parameterized on ssl (and storediag when a store is forced to OpenSSL) ``` [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractSeek.testReadFullyZeroBytebufferPastEOF", "created": "2025-10-08T12:41:04.887+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #8019: URL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381381104 s3a tests all good, s3 london `-Dparallel-tests -DtestsThreadCount=8`", "created": "2025-10-08T12:55:21.261+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #8019: URL: https://github.com/apache/hadoop/pull/8019#issuecomment-3405882774 OK, 2.2.5 doesn't include the arm linux binaries. It does in our private builds, which is why I was confused.", "created": "2025-10-15T11:07:38.888+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #8019: URL: https://github.com/apache/hadoop/pull/8019", "created": "2025-10-20T12:45:49.133+0000"}], "derived_tasks": {"summary": "Upgrade to wildfly version with support for openssl 3 - Wildfly 2", "classifications": ["bug"], "qa_pairs": [{"question": "why did they do that?", "answer": "steveloughran opened a new pull request, #8019: URL: https://github.com/apache/hadoop/pull/8019 ### How was this patch tested? Going to see if it works on a mac... ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}]}}
{"id": "HADOOP-19717", "title": "Resolve build error caused by missing Checker Framework (NonNull not recognized)", "description": "In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.  I checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.", "status": "Resolved", "priority": "Major", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "created": "2025-10-07T03:49:55.000+0000", "updated": "2025-10-20T16:32:20.000+0000", "labels": ["pull-request-available"], "components": ["hdfs", "tos"], "comments": [{"author": "ASF GitHub Bot", "body": "slfan1989 opened a new pull request, #8015: URL: https://github.com/apache/hadoop/pull/8015 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19717. Resolve build error caused by missing Checker Framework (NonNull not recognized). ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-07T04:01:33.037+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2409341008 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: I suspect this makes the `Nullable` useless, I don't think the static analyzer tools can recognize such a relocated annotation.", "created": "2025-10-07T04:48:15.069+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3375421893 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 35s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 22m 41s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 5m 25s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 4m 39s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | -1 :x: | mvnsite | 0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | mvnsite | 0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | -1 :x: | javadoc | 0m 27s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | +1 :green_heart: | shadedclient | 24m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 23s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | -1 :x: | compile | 5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 11s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 5m 10s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 41s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 16s | | hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 21s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | javadoc | 0m 19s | | hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | spotbugs | 1m 33s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 38m 54s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 54s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 154m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8015 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1ad3180c6b2e 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 4610 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-07T06:36:36.034+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2410285097 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: I think what you said makes some sense, but there are similar users in AzureBFS as well. https://github.com/apache/hadoop/blob/1566613c725979d0ccda45822dfa275cbd97467a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java#L38", "created": "2025-10-07T11:19:14.896+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2410285097 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: I think what you said makes some sense, but there are similar users in AzureBFS as well. https://github.com/apache/hadoop/blob/1566613c725979d0ccda45822dfa275cbd97467a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java#L38 @steveloughran I\u2019d like to hear your thoughts \u2014 do you think we should reintroduce a new dependency to resolve the issue where org.checkerframework.checker.nullness.qual.Nullable cannot be found? cc: @szetszwo", "created": "2025-10-07T11:20:49.833+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3376863181 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 12s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 22m 36s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 5m 21s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 4m 43s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | -1 :x: | mvnsite | 0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | mvnsite | 0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | -1 :x: | javadoc | 0m 18s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 1m 4s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 22s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | spotbugs | 0m 17s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | +1 :green_heart: | shadedclient | 21m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 22s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 4s | | the patch passed | | -1 :x: | compile | 5m 49s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 5m 49s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 5s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 5m 5s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | hadoop-yarn-server-resourcemanager in the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 0m 20s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 17s | | hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 27s | | hadoop-yarn-server-resourcemanager in the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | javadoc | 0m 22s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | javadoc | 0m 19s | | hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | spotbugs | 2m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 90m 40s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 38m 2s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 53s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 24s | | The patch does not generate ASF License warnings. | | | | 241m 25s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8015 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux de756f6ac704 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / aca0e73a716b49f41b6eb3e0a57c876842a258a8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 4761 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-07T13:18:47.987+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377007084 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 46s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 26m 13s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 12s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 18s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | checkstyle | 1m 19s | | trunk passed | | -1 :x: | mvnsite | 0m 25s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | mvnsite | 0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | -1 :x: | javadoc | 0m 23s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 17s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | +1 :green_heart: | shadedclient | 26m 8s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 41s | | the patch passed | | -1 :x: | compile | 6m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 6m 23s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 5m 50s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 5m 50s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 24s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 45s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 16s | | hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1) | | +1 :green_heart: | javadoc | 0m 18s | | hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | javadoc | 0m 17s | | hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2) | | +1 :green_heart: | spotbugs | 1m 28s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 54s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 39m 4s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 54s | | hadoop-tos in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 158m 17s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8015 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 30e4a84a468c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 4202 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-07T13:54:35.628+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377166843 @szetszwo @pan3793 My thought is that since `AzureBFS` already uses this approach, we should be able to apply the same solution in other places as well. For now, we can use`org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable` instead of `org.checkerframework.checker.nullness.qual.Nullable` to unblock the trunk build issue first. A follow-up PR can be submitted later to fully resolve this dependency problem in a cleaner way. Currently, the build result is as expected \u2014 before applying this patch, the trunk could not compile successfully, but after merging it, the build now passes under both JDK 8 and JDK 11.", "created": "2025-10-07T14:28:09.076+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on code in PR #8015: URL: https://github.com/apache/hadoop/pull/8015#discussion_r2411050812 ########## hadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java: ########## @@ -19,7 +19,7 @@ package org.apache.hadoop.fs.tosfs.util; import org.apache.hadoop.util.Preconditions; -import org.checkerframework.checker.nullness.qual.Nullable; +import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable; Review Comment: > ... this makes the Nullable useless, ... Making it useless seems better than breaking the build. Unforturately, the the builds after this remain failing.", "created": "2025-10-07T15:30:58.356+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377457340 @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing.", "created": "2025-10-07T15:32:12.017+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377471954 > @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing. @szetszwo A new issue occurred during the compilation of yarn-ui. The log output is as follows: ``` [INFO] [2/4] Fetching packages... [INFO] error color@5.0.2: The engine \"node\" is incompatible with this module. Expected version \">=18\". Got \"12.22.1\" [INFO] error Found incompatible module. ```", "created": "2025-10-07T15:36:23.621+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377538969 > > @slfan1989 , if it can fix the build, then it is fine. But the builds after this remain failing. > > @szetszwo A new issue occurred during the compilation of yarn-ui. The log output is as follows: > > ``` > [INFO] [2/4] Fetching packages... > [INFO] error color@5.0.2: The engine \"node\" is incompatible with this module. Expected version \">=18\". Got \"12.22.1\" > [INFO] error Found incompatible module. > ``` > > I tried to apply a local fix for this issue. I manually specified `color@^3.1.3` in the package.json, and it took effect successfully. I will submit a PR to fix this issue. ``` [INFO] -", "created": "2025-10-07T15:55:40.527+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #8015: URL: https://github.com/apache/hadoop/pull/8015", "created": "2025-10-07T15:58:33.738+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377550294 @szetszwo Thank you very much for the review!", "created": "2025-10-07T15:59:04.147+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on PR #8015: URL: https://github.com/apache/hadoop/pull/8015#issuecomment-3377632827 @slfan1989 , thanks for fixing it!", "created": "2025-10-07T16:20:51.703+0000"}, {"author": "Steve Loughran", "body": "this is complicating the new thirdparty release FWIW. this should all be using the unshaded javax. Nullable/nonnull. And the hadoop-thirdparty release needs to address this stuff getting left out so 1.5.0 can be a drop-in replacement for 1.4.0", "created": "2025-10-20T16:32:20.848+0000"}], "derived_tasks": {"summary": "Resolve build error caused by missing Checker Framework (NonNull not recognized) - In the recent build, we encountered the following issue: *org", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-19709", "title": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default", "description": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "status": "Open", "priority": "Major", "reporter": "Vinayakumar B", "assignee": "Vinayakumar B", "created": "2025-09-27T04:38:55.000+0000", "updated": "2025-10-21T16:28:27.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "vinayakumarb opened a new pull request, #8001: URL: https://github.com/apache/hadoop/pull/8001 This commit introduces support for Debian 12 (Bookworm) and Debian 13 (Trixie) as build platforms, following the approach established for Ubuntu 24. Key changes include: - Creation of `Dockerfile_debian_12` and `Dockerfile_debian_13` based on `Dockerfile_ubuntu_24`, with appropriate base images and package resolver arguments. - Updates to `dev-support/docker/pkg-resolver/packages.json` to include package definitions for `debian:12` and `debian:13`. - Addition of `debian:12` and `debian:13` to `dev-support/docker/pkg-resolver/platforms.json`. - Modification of `BUILDING.txt` to list `debian_12` and `debian_13` as supported OS platforms.", "created": "2025-09-27T04:42:39.625+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3341301544 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 23m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 37m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | shadedclient | 34m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 53s | | The patch does not generate ASF License warnings. | | | | 98m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 01b5b1df2935 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ccea846897ea6a8209a2238c06933afb4c489bc | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | [CI_URL] | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-27T06:22:34.799+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3342170970 Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly?", "created": "2025-09-28T01:26:24.689+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3349642263 > Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly? @vinayakumarb Thank you very much for your contribution. However, I still have some concerns. Expanding support to many operating systems could be a rather heavy undertaking, since it requires us to pay closer attention to their EOL and version lifecycles. I'm wondering if it might be more sustainable to maintain a smaller subset of supported systems instead. If users have other requirements, they could always try customizing the build themselves. cc: @pan3793 @ayushtkn @cnauroth", "created": "2025-09-30T01:48:27.552+0000"}, {"author": "ASF GitHub Bot", "body": "vinayakumarb commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376374939 > > Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly? > > @vinayakumarb Thank you very much for your contribution. However, I still have some concerns. Expanding support to many operating systems could be a rather heavy undertaking, since it requires us to pay closer attention to their EOL and version lifecycles. I'm wondering if it might be more sustainable to maintain a smaller subset of supported systems instead. If users have other requirements, they could always try customizing the build themselves. > > cc: @pan3793 @ayushtkn @cnauroth I understand the concern. Directly upgrading the debian:10 to debian:13 may break existing pipelines. However, having a Dockerfiles for various platforms provides the developers to build an environment as per their choice. It not necessarily means Hadoop binaries (jars and tar) are compiled in these. if users are interested in building Hadoop in their own choice of environment, these Dockerfiles will be a good starting point.", "created": "2025-10-07T10:59:13.217+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376469046 @vinayakumarb Thank you for the clarification \u2014 I agree (+1). However, given the complexity of operating system EOL management, I would carefully evaluate the introduction of Docker support for new systems in the future, considering both maintenance costs and long-term sustainability.", "created": "2025-10-07T11:28:11.135+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3376715593 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 31m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 40m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | shadedclient | 37m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 113m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 9e7987a7030b 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 92bd7478449829a0e7b987157945cfd72199e4ac | | Max. process+thread count | 647 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | [CI_URL] | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-07T12:40:28.529+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2412409276 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -263,6 +352,14 @@ \"openjdk-11-jdk\", \"openjdk-17-jdk\" ], + \"debian:12\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" + ], + \"debian:13\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" Review Comment: temurin-25 is out BTW, I think we should prefer to use the JDK provided by official APT repo if possible, Debian 13 already has `openjdk-25-jdk`", "created": "2025-10-08T03:28:03.011+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2412409614 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -353,26 +472,34 @@ }, \"software-properties-common\": { \"debian:11\": \"software-properties-common\", + + Review Comment: ?", "created": "2025-10-08T03:28:27.870+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2412425091 ########## dev-support/docker/Dockerfile_debian_13: ########## @@ -0,0 +1,110 @@ +# Licensed to the Apache Software Foundation (ASF) under one +# or more contributor license agreements. See the NOTICE file +# distributed with this work for additional information +# regarding copyright ownership. The ASF licenses this file +# to you under the Apache License, Version 2.0 (the +# \"License\"); you may not use this file except in compliance +# with the License. You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, software +# distributed under the License is distributed on an \"AS IS\" BASIS, +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. +# See the License for the specific language governing permissions and +# limitations under the License. + +# Dockerfile for installing the necessary dependencies for building Hadoop. +# See BUILDING.txt. + +FROM debian:13 + +WORKDIR /root + +SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] + +##### +# Disable suggests/recommends +##### +RUN echo 'APT::Install-Recommends \"0\";' > /etc/apt/apt.conf.d/10disableextras +RUN echo 'APT::Install-Suggests \"0\";' >> /etc/apt/apt.conf.d/10disableextras + +ENV DEBIAN_FRONTEND=noninteractive +ENV DEBCONF_TERSE=true + +###### +# Platform package dependency resolver +###### +COPY pkg-resolver pkg-resolver +RUN chmod a+x pkg-resolver/*.sh pkg-resolver/*.py \\ + && chmod a+r pkg-resolver/*.json + +###### +# Install packages from apt +###### +# hadolint ignore=DL3008,SC2046 +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends wget apt-transport-https gpg gpg-agent gawk ca-certificates +RUN apt-get -q install -y --no-install-recommends python3 +RUN echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" > /etc/apt/sources.list.d/adoptium.list +RUN wget -q -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/trusted.gpg.d/adoptium.asc +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:13) +RUN apt-get clean +RUN update-java-alternatives -s temurin-17-jdk-amd64 +RUN rm -rf /var/lib/apt/lists/* Review Comment: each RUN produces one image layer, you should concat those shell commands by && instead", "created": "2025-10-08T03:44:34.377+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3379465889 @vinayakumarb, in addition to creating a dev container from the Dockerfile, have you verified that Hadoop can build successfully with native and frontend components in the created dev container?", "created": "2025-10-08T03:48:46.924+0000"}, {"author": "ASF GitHub Bot", "body": "vinayakumarb commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2422824903 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -263,6 +352,14 @@ \"openjdk-11-jdk\", \"openjdk-17-jdk\" ], + \"debian:12\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" + ], + \"debian:13\": [ + \"temurin-17-jdk\", + \"temurin-24-jdk\" Review Comment: Done. Using openjdk-25-jdk in debian-13 and temurin-25-jdk in debian 12 as openjdk is not available in debian repository for bookworm.", "created": "2025-10-11T13:07:53.269+0000"}, {"author": "ASF GitHub Bot", "body": "vinayakumarb commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2422825814 ########## dev-support/docker/Dockerfile_debian_13: ########## @@ -0,0 +1,110 @@ +# Licensed to the Apache Software Foundation (ASF) under one +# or more contributor license agreements. See the NOTICE file +# distributed with this work for additional information +# regarding copyright ownership. The ASF licenses this file +# to you under the Apache License, Version 2.0 (the +# \"License\"); you may not use this file except in compliance +# with the License. You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, software +# distributed under the License is distributed on an \"AS IS\" BASIS, +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. +# See the License for the specific language governing permissions and +# limitations under the License. + +# Dockerfile for installing the necessary dependencies for building Hadoop. +# See BUILDING.txt. + +FROM debian:13 + +WORKDIR /root + +SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] + +##### +# Disable suggests/recommends +##### +RUN echo 'APT::Install-Recommends \"0\";' > /etc/apt/apt.conf.d/10disableextras +RUN echo 'APT::Install-Suggests \"0\";' >> /etc/apt/apt.conf.d/10disableextras + +ENV DEBIAN_FRONTEND=noninteractive +ENV DEBCONF_TERSE=true + +###### +# Platform package dependency resolver +###### +COPY pkg-resolver pkg-resolver +RUN chmod a+x pkg-resolver/*.sh pkg-resolver/*.py \\ + && chmod a+r pkg-resolver/*.json + +###### +# Install packages from apt +###### +# hadolint ignore=DL3008,SC2046 +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends wget apt-transport-https gpg gpg-agent gawk ca-certificates +RUN apt-get -q install -y --no-install-recommends python3 +RUN echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" > /etc/apt/sources.list.d/adoptium.list +RUN wget -q -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/trusted.gpg.d/adoptium.asc +RUN apt-get -q update +RUN apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:13) +RUN apt-get clean +RUN update-java-alternatives -s temurin-17-jdk-amd64 +RUN rm -rf /var/lib/apt/lists/* Review Comment: Done.", "created": "2025-10-11T13:08:23.085+0000"}, {"author": "ASF GitHub Bot", "body": "vinayakumarb commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393325468 > @vinayakumarb, in addition to creating a dev container from the Dockerfile, have you verified that Hadoop can build successfully with native and frontend components in the created dev container? Yes. I have verified building both native and frontend.", "created": "2025-10-11T13:21:27.822+0000"}, {"author": "ASF GitHub Bot", "body": "vinayakumarb commented on code in PR #8001: URL: https://github.com/apache/hadoop/pull/8001#discussion_r2422836979 ########## dev-support/docker/pkg-resolver/packages.json: ########## @@ -353,26 +472,34 @@ }, \"software-properties-common\": { \"debian:11\": \"software-properties-common\", + + Review Comment: Forgot to remove empty lines. `software-properties-common` not available for debian 12 and 13. Also does not look like it was needed.", "created": "2025-10-11T13:23:14.869+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393364407 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 24m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shadedclient | 21m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 61m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 5a71556ea849 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | [CI_URL] | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-11T14:07:48.325+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393395506 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 14m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 1s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 13m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 40m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 4e9b089f3372 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 | | Max. process+thread count | 575 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | [CI_URL] | | versions | git=2.30.2 maven=3.9.11 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-11T14:48:41.044+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393395990 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at [CI_URL] in case of problems.", "created": "2025-10-11T14:49:36.386+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8001: URL: https://github.com/apache/hadoop/pull/8001#issuecomment-3393432943 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | shadedclient | 19m 49s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | hadolint | 0m 2s | | No new issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 19m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 49m 15s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8001 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint | | uname | Linux 167189e0eac1 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e371f08075c145585c0d620978733fceb30c93b0 | | Max. process+thread count | 568 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-11T15:38:02.482+0000"}, {"author": "ASF GitHub Bot", "body": "vinayakumarb merged PR #8001: URL: https://github.com/apache/hadoop/pull/8001", "created": "2025-10-21T16:28:26.949+0000"}], "derived_tasks": {"summary": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default - Add a new Dockerfiles to compile Hadoop on latest Debian:12 and De...", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19696", "title": "hadoop binary distribution to move cloud connectors to hadoop common/lib", "description": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed. * filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. * Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. This adds a lot more stuff into the distribution, so I'm doing the following design * all hadoop-* modules in common/lib * minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!) * hadoop-aws: everything except bundle.jar * other connectors: only included with explicit profiles. ASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything One concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup. Noticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here.", "status": "Open", "priority": "Major", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "created": "2025-09-17T13:36:12.000+0000", "updated": "2025-10-21T22:22:37.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure", "fs/gcs", "fs/huawei", "fs/s3"], "comments": [{"author": "ASF GitHub Bot", "body": "steveloughran opened a new pull request, #7980: URL: https://github.com/apache/hadoop/pull/7980 * new assembly for hadoop cloud storage * hadoop-cloud-storage does the assembly on -Pdist * layout stitching to move into share/hadoop/common/lib * remove connectors from hadoop-tools-dist * cut old jackson version from huawaei cloud dependency -even though it was being upgraded by our own artifacts, it was a complication. ### How was this patch tested? Manual build, review, storediag, hadoop fs commands ### For code changes: - [=] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-09-17T13:51:39.228+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303127263 * This puts the hadoop-azure, hadoop-aws &c binaries into common/lib and so on the classpath everywhere * some problem with gcs instantiation during enum (will file later, as while it surfaces here, I think it's unrelated) * my local builds end up (today) with some versioned jars as well as the -SNAPSHOT. I think this is from me tainting my maven repo, would like to see what others see ``` total 1401704 -rw-r--r--@ 1 stevel staff 106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar -rw-r--r--@ 1 stevel staff 163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar -rw-r--r--@ 1 stevel staff 220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar -rw-r--r--@ 1 stevel staff 928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar -rw-r--r--@ 1 stevel staff 2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar -rw-r--r--@ 1 stevel staff 27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar -rw-r--r--@ 1 stevel staff 20891 Sep 17 13:11 audience-annotations-0.12.0.jar -rw-r--r--@ 1 stevel staff 651391 Sep 17 13:11 avro-1.11.4.jar -rw-r--r--@ 1 stevel staff 113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar -rw-r--r--@ 1 stevel staff 10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar -rw-r--r--@ 1 stevel staff 815331 Sep 17 12:57 azure-storage-7.0.1.jar -rw-r--r--@ 1 stevel staff 8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar -rw-r--r--@ 1 stevel staff 641534749 Sep 17 12:57 bundle-2.29.52.jar -rw-r--r--@ 1 stevel staff 223979 Sep 17 13:11 checker-qual-3.33.0.jar -rw-r--r--@ 1 stevel staff 75479 Sep 17 13:11 commons-cli-1.9.0.jar -rw-r--r--@ 1 stevel staff 353793 Sep 17 13:11 commons-codec-1.15.jar -rw-r--r--@ 1 stevel staff 751914 Sep 17 13:11 commons-collections4-4.4.jar -rw-r--r--@ 1 stevel staff 1079377 Sep 17 13:11 commons-compress-1.26.1.jar -rw-r--r--@ 1 stevel staff 657516 Sep 17 13:11 commons-configuration2-2.10.1.jar -rw-r--r--@ 1 stevel staff 24239 Sep 17 13:11 commons-daemon-1.0.13.jar -rw-r--r--@ 1 stevel staff 508826 Sep 17 13:11 commons-io-2.16.1.jar -rw-r--r--@ 1 stevel staff 673587 Sep 17 13:11 commons-lang3-3.17.0.jar -rw-r--r--@ 1 stevel staff 70816 Sep 17 13:11 commons-logging-1.3.0.jar -rw-r--r--@ 1 stevel staff 2213560 Sep 17 13:11 commons-math3-3.6.1.jar -rw-r--r--@ 1 stevel staff 316431 Sep 17 13:11 commons-net-3.9.0.jar -rw-r--r--@ 1 stevel staff 238400 Sep 17 13:11 commons-text-1.10.0.jar -rw-r--r--@ 1 stevel staff 8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar -rw-r--r--@ 1 stevel staff 2983237 Sep 17 13:11 curator-client-5.2.0.jar -rw-r--r--@ 1 stevel staff 336384 Sep 17 13:11 curator-framework-5.2.0.jar -rw-r--r--@ 1 stevel staff 315569 Sep 17 13:11 curator-recipes-5.2.0.jar -rw-r--r--@ 1 stevel staff 583996 Sep 17 13:11 dnsjava-3.6.1.jar -rw-r--r--@ 1 stevel staff 324655 Sep 17 12:57 dom4j-2.1.4.jar -rw-r--r--@ 1 stevel staff 670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar -rw-r--r--@ 1 stevel staff 4617 Sep 17 13:11 failureaccess-1.0.1.jar -rw-r--r--@ 1 stevel staff 249277 Sep 17 13:11 gson-2.9.0.jar -rw-r--r--@ 1 stevel staff 3037368 Sep 17 13:11 guava-32.0.1-jre.jar -rw-r--r--@ 1 stevel staff 94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar -rw-r--r--@ 1 stevel staff 827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar -rw-r--r--@ 1 stevel staff 138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar -rw-r--r--@ 1 stevel staff 1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar -rw-r--r--@ 1 stevel staff 4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar -rw-r--r--@ 1 stevel staff 200223 Sep 17 13:11 hk2-api-2.6.1.jar -rw-r--r--@ 1 stevel staff 203358 Sep 17 13:11 hk2-locator-2.6.1.jar -rw-r--r--@ 1 stevel staff 131590 Sep 17 13:11 hk2-utils-2.6.1.jar -rw-r--r--@ 1 stevel staff 780321 Sep 17 13:11 httpclient-4.5.13.jar -rw-r--r--@ 1 stevel staff 328593 Sep 17 13:11 httpcore-4.4.13.jar -rw-r--r--@ 1 stevel staff 102220 Sep 17 12:57 ini4j-0.5.4.jar -rw-r--r--@ 1 stevel staff 29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar -rw-r--r--@ 1 stevel staff 9301 Sep 17 13:11 j2objc-annotations-2.8.jar -rw-r--r--@ 1 stevel staff 76636 Sep 17 13:11 jackson-annotations-2.14.3.jar -rw-r--r--@ 1 stevel staff 473081 Sep 17 13:11 jackson-core-2.14.3.jar -rw-r--r--@ 1 stevel staff 1617187 Sep 17 13:11 jackson-databind-2.14.3.jar -rw-r--r--@ 1 stevel staff 68453 Sep 17 13:11 jakarta.activation-1.2.2.jar -rw-r--r--@ 1 stevel staff 44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar -rw-r--r--@ 1 stevel staff 25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar -rw-r--r--@ 1 stevel staff 18140 Sep 17 13:11 jakarta.inject-2.6.1.jar -rw-r--r--@ 1 stevel staff 82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar -rw-r--r--@ 1 stevel staff 53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar -rw-r--r--@ 1 stevel staff 91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar -rw-r--r--@ 1 stevel staff 140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar -rw-r--r--@ 1 stevel staff 115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar -rw-r--r--@ 1 stevel staff 7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 18432 Sep 17 12:57 java-xmlbuilder-1.2.jar -rw-r--r--@ 1 stevel staff 794714 Sep 17 13:11 javassist-3.30.2-GA.jar -rw-r--r--@ 1 stevel staff 95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar -rw-r--r--@ 1 stevel staff 1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar -rw-r--r--@ 1 stevel staff 4722 Sep 17 13:11 jcip-annotations-1.0-1.jar -rw-r--r--@ 1 stevel staff 327806 Sep 17 12:57 jdom2-2.0.6.1.jar -rw-r--r--@ 1 stevel staff 311826 Sep 17 13:11 jersey-client-2.46.jar -rw-r--r--@ 1 stevel staff 1267957 Sep 17 13:11 jersey-common-2.46.jar -rw-r--r--@ 1 stevel staff 32929 Sep 17 13:11 jersey-container-servlet-2.46.jar -rw-r--r--@ 1 stevel staff 75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar -rw-r--r--@ 1 stevel staff 80272 Sep 17 13:11 jersey-hk2-2.46.jar -rw-r--r--@ 1 stevel staff 964550 Sep 17 13:11 jersey-server-2.46.jar -rw-r--r--@ 1 stevel staff 90184 Sep 17 12:57 jettison-1.5.4.jar -rw-r--r--@ 1 stevel staff 249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 282591 Sep 17 13:11 jsch-0.1.55.jar -rw-r--r--@ 1 stevel staff 19936 Sep 17 13:11 jsr305-3.0.2.jar -rw-r--r--@ 1 stevel staff 4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 223129 Sep 17 13:11 kerb-core-2.0.3.jar -rw-r--r--@ 1 stevel staff 115065 Sep 17 13:11 kerb-crypto-2.0.3.jar -rw-r--r--@ 1 stevel staff 36361 Sep 17 13:11 kerb-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 100095 Sep 17 13:11 kerby-asn1-2.0.3.jar -rw-r--r--@ 1 stevel staff 30190 Sep 17 13:11 kerby-config-2.0.3.jar -rw-r--r--@ 1 stevel staff 200581 Sep 17 13:11 kerby-pkix-2.0.3.jar -rw-r--r--@ 1 stevel staff 40787 Sep 17 13:11 kerby-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar -rw-r--r--@ 1 stevel staff 136314 Sep 17 13:11 metrics-core-3.2.4.jar -rw-r--r--@ 1 stevel staff 4554 Sep 17 13:11 netty-all-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 719225 Sep 17 13:11 netty-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar -rw-r--r--@ 1 stevel staff 36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar -rw-r--r--@ 1 stevel staff 40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar -rw-r--r--@ 1 stevel staff 6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar -rw-r--r--@ 1 stevel staff 425763 Sep 17 12:57 okhttp-3.14.2.jar -rw-r--r--@ 1 stevel staff 91980 Sep 17 12:57 okio-1.17.2.jar -rw-r--r--@ 1 stevel staff 141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar -rw-r--r--@ 1 stevel staff 47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar -rw-r--r--@ 1 stevel staff 18189 Sep 17 12:57 opentracing-api-0.33.0.jar -rw-r--r--@ 1 stevel staff 10542 Sep 17 12:57 opentracing-noop-0.33.0.jar -rw-r--r--@ 1 stevel staff 7504 Sep 17 12:57 opentracing-util-0.33.0.jar -rw-r--r--@ 1 stevel staff 281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar -rw-r--r--@ 1 stevel staff 19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar -rw-r--r--@ 1 stevel staff 128414 Sep 17 13:11 re2j-1.1.jar -rw-r--r--@ 1 stevel staff 11369 Sep 17 12:57 reactive-streams-1.0.3.jar -rw-r--r--@ 1 stevel staff 332398 Sep 17 13:11 reload4j-1.2.22.jar -rw-r--r--@ 1 stevel staff 41125 Sep 17 13:11 slf4j-api-1.7.36.jar -rw-r--r--@ 1 stevel staff 9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar -rw-r--r--@ 1 stevel staff 195909 Sep 17 13:11 stax2-api-4.2.1.jar -rw-r--r--@ 1 stevel staff 72007 Sep 17 13:11 txw2-2.3.9.jar -rw-r--r--@ 1 stevel staff 443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar -rw-r--r--@ 1 stevel staff 522679 Sep 17 13:11 woodstox-core-5.4.0.jar -rw-r--r--@ 1 stevel staff 1323991 Sep 17 13:11 zookeeper-3.8.4.jar -rw-r--r--@ 1 stevel staff 254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar ```", "created": "2025-09-17T13:56:19.944+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303133677 for contrast, here's 3.4.2 ``` ../hadoop-3.4.2/: total 272 drwxr-xr-x@ 13 stevel staff 416 Aug 7 12:58 bin -rw-r--r--@ 1 stevel staff 824 Aug 27 16:58 binding.xml drwxr-xr-x@ 3 stevel staff 96 Aug 25 17:09 downloads drwxr-xr-x@ 3 stevel staff 96 Aug 7 12:16 etc drwxr-xr-x@ 7 stevel staff 224 Aug 7 12:58 include drwxr-xr-x@ 3 stevel staff 96 Aug 7 12:58 lib drwxr-xr-x@ 14 stevel staff 448 Aug 7 12:58 libexec -rw-r--r--@ 1 stevel staff 23682 Aug 7 10:40 LICENSE-binary -rw-r--r--@ 1 stevel staff 15791 Aug 7 10:39 LICENSE.txt drwxr-xr-x@ 45 stevel staff 1440 Aug 7 12:58 licenses-binary -rw-r--r--@ 1 stevel staff 45514 Aug 27 17:12 log.txt -rw-r--r--@ 1 stevel staff 27373 Aug 7 10:39 NOTICE-binary -rw-r--r--@ 1 stevel staff 1541 Aug 7 10:39 NOTICE.txt -rw-r--r--@ 1 stevel staff 175 Aug 7 10:39 README.txt drwxr-xr-x@ 29 stevel staff 928 Aug 7 12:16 sbin -rw-r--r--@ 1 stevel staff 438 Aug 25 16:59 secrets.bin drwxr-xr-x@ 4 stevel staff 128 Aug 7 13:23 share -rw-r--r--@ 1 stevel staff 275 Aug 27 17:12 system.properties share/hadoop/common/lib: total 1401704 -rw-r--r--@ 1 stevel staff 106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar -rw-r--r--@ 1 stevel staff 163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar -rw-r--r--@ 1 stevel staff 220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar -rw-r--r--@ 1 stevel staff 928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar -rw-r--r--@ 1 stevel staff 2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar -rw-r--r--@ 1 stevel staff 27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar -rw-r--r--@ 1 stevel staff 20891 Sep 17 13:11 audience-annotations-0.12.0.jar -rw-r--r--@ 1 stevel staff 651391 Sep 17 13:11 avro-1.11.4.jar -rw-r--r--@ 1 stevel staff 113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar -rw-r--r--@ 1 stevel staff 10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar -rw-r--r--@ 1 stevel staff 815331 Sep 17 12:57 azure-storage-7.0.1.jar -rw-r--r--@ 1 stevel staff 8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar -rw-r--r--@ 1 stevel staff 641534749 Sep 17 12:57 bundle-2.29.52.jar -rw-r--r--@ 1 stevel staff 223979 Sep 17 13:11 checker-qual-3.33.0.jar -rw-r--r--@ 1 stevel staff 75479 Sep 17 13:11 commons-cli-1.9.0.jar -rw-r--r--@ 1 stevel staff 353793 Sep 17 13:11 commons-codec-1.15.jar -rw-r--r--@ 1 stevel staff 751914 Sep 17 13:11 commons-collections4-4.4.jar -rw-r--r--@ 1 stevel staff 1079377 Sep 17 13:11 commons-compress-1.26.1.jar -rw-r--r--@ 1 stevel staff 657516 Sep 17 13:11 commons-configuration2-2.10.1.jar -rw-r--r--@ 1 stevel staff 24239 Sep 17 13:11 commons-daemon-1.0.13.jar -rw-r--r--@ 1 stevel staff 508826 Sep 17 13:11 commons-io-2.16.1.jar -rw-r--r--@ 1 stevel staff 673587 Sep 17 13:11 commons-lang3-3.17.0.jar -rw-r--r--@ 1 stevel staff 70816 Sep 17 13:11 commons-logging-1.3.0.jar -rw-r--r--@ 1 stevel staff 2213560 Sep 17 13:11 commons-math3-3.6.1.jar -rw-r--r--@ 1 stevel staff 316431 Sep 17 13:11 commons-net-3.9.0.jar -rw-r--r--@ 1 stevel staff 238400 Sep 17 13:11 commons-text-1.10.0.jar -rw-r--r--@ 1 stevel staff 8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar -rw-r--r--@ 1 stevel staff 2983237 Sep 17 13:11 curator-client-5.2.0.jar -rw-r--r--@ 1 stevel staff 336384 Sep 17 13:11 curator-framework-5.2.0.jar -rw-r--r--@ 1 stevel staff 315569 Sep 17 13:11 curator-recipes-5.2.0.jar -rw-r--r--@ 1 stevel staff 583996 Sep 17 13:11 dnsjava-3.6.1.jar -rw-r--r--@ 1 stevel staff 324655 Sep 17 12:57 dom4j-2.1.4.jar -rw-r--r--@ 1 stevel staff 670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar -rw-r--r--@ 1 stevel staff 4617 Sep 17 13:11 failureaccess-1.0.1.jar -rw-r--r--@ 1 stevel staff 249277 Sep 17 13:11 gson-2.9.0.jar -rw-r--r--@ 1 stevel staff 3037368 Sep 17 13:11 guava-32.0.1-jre.jar -rw-r--r--@ 1 stevel staff 94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar -rw-r--r--@ 1 stevel staff 827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar -rw-r--r--@ 1 stevel staff 70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar -rw-r--r--@ 1 stevel staff 138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar -rw-r--r--@ 1 stevel staff 3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar -rw-r--r--@ 1 stevel staff 1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar -rw-r--r--@ 1 stevel staff 4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar -rw-r--r--@ 1 stevel staff 200223 Sep 17 13:11 hk2-api-2.6.1.jar -rw-r--r--@ 1 stevel staff 203358 Sep 17 13:11 hk2-locator-2.6.1.jar -rw-r--r--@ 1 stevel staff 131590 Sep 17 13:11 hk2-utils-2.6.1.jar -rw-r--r--@ 1 stevel staff 780321 Sep 17 13:11 httpclient-4.5.13.jar -rw-r--r--@ 1 stevel staff 328593 Sep 17 13:11 httpcore-4.4.13.jar -rw-r--r--@ 1 stevel staff 102220 Sep 17 12:57 ini4j-0.5.4.jar -rw-r--r--@ 1 stevel staff 29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar -rw-r--r--@ 1 stevel staff 9301 Sep 17 13:11 j2objc-annotations-2.8.jar -rw-r--r--@ 1 stevel staff 76636 Sep 17 13:11 jackson-annotations-2.14.3.jar -rw-r--r--@ 1 stevel staff 473081 Sep 17 13:11 jackson-core-2.14.3.jar -rw-r--r--@ 1 stevel staff 1617187 Sep 17 13:11 jackson-databind-2.14.3.jar -rw-r--r--@ 1 stevel staff 68453 Sep 17 13:11 jakarta.activation-1.2.2.jar -rw-r--r--@ 1 stevel staff 44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar -rw-r--r--@ 1 stevel staff 25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar -rw-r--r--@ 1 stevel staff 18140 Sep 17 13:11 jakarta.inject-2.6.1.jar -rw-r--r--@ 1 stevel staff 82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar -rw-r--r--@ 1 stevel staff 53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar -rw-r--r--@ 1 stevel staff 91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar -rw-r--r--@ 1 stevel staff 140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar -rw-r--r--@ 1 stevel staff 115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar -rw-r--r--@ 1 stevel staff 7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar -rw-r--r--@ 1 stevel staff 18432 Sep 17 12:57 java-xmlbuilder-1.2.jar -rw-r--r--@ 1 stevel staff 794714 Sep 17 13:11 javassist-3.30.2-GA.jar -rw-r--r--@ 1 stevel staff 95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar -rw-r--r--@ 1 stevel staff 1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar -rw-r--r--@ 1 stevel staff 4722 Sep 17 13:11 jcip-annotations-1.0-1.jar -rw-r--r--@ 1 stevel staff 327806 Sep 17 12:57 jdom2-2.0.6.1.jar -rw-r--r--@ 1 stevel staff 311826 Sep 17 13:11 jersey-client-2.46.jar -rw-r--r--@ 1 stevel staff 1267957 Sep 17 13:11 jersey-common-2.46.jar -rw-r--r--@ 1 stevel staff 32929 Sep 17 13:11 jersey-container-servlet-2.46.jar -rw-r--r--@ 1 stevel staff 75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar -rw-r--r--@ 1 stevel staff 80272 Sep 17 13:11 jersey-hk2-2.46.jar -rw-r--r--@ 1 stevel staff 964550 Sep 17 13:11 jersey-server-2.46.jar -rw-r--r--@ 1 stevel staff 90184 Sep 17 12:57 jettison-1.5.4.jar -rw-r--r--@ 1 stevel staff 249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 282591 Sep 17 13:11 jsch-0.1.55.jar -rw-r--r--@ 1 stevel staff 19936 Sep 17 13:11 jsr305-3.0.2.jar -rw-r--r--@ 1 stevel staff 4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 223129 Sep 17 13:11 kerb-core-2.0.3.jar -rw-r--r--@ 1 stevel staff 115065 Sep 17 13:11 kerb-crypto-2.0.3.jar -rw-r--r--@ 1 stevel staff 36361 Sep 17 13:11 kerb-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 100095 Sep 17 13:11 kerby-asn1-2.0.3.jar -rw-r--r--@ 1 stevel staff 30190 Sep 17 13:11 kerby-config-2.0.3.jar -rw-r--r--@ 1 stevel staff 200581 Sep 17 13:11 kerby-pkix-2.0.3.jar -rw-r--r--@ 1 stevel staff 40787 Sep 17 13:11 kerby-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar -rw-r--r--@ 1 stevel staff 136314 Sep 17 13:11 metrics-core-3.2.4.jar -rw-r--r--@ 1 stevel staff 4554 Sep 17 13:11 netty-all-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 719225 Sep 17 13:11 netty-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar -rw-r--r--@ 1 stevel staff 36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar -rw-r--r--@ 1 stevel staff 40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar -rw-r--r--@ 1 stevel staff 6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar -rw-r--r--@ 1 stevel staff 425763 Sep 17 12:57 okhttp-3.14.2.jar -rw-r--r--@ 1 stevel staff 91980 Sep 17 12:57 okio-1.17.2.jar -rw-r--r--@ 1 stevel staff 141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar -rw-r--r--@ 1 stevel staff 47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar -rw-r--r--@ 1 stevel staff 18189 Sep 17 12:57 opentracing-api-0.33.0.jar -rw-r--r--@ 1 stevel staff 10542 Sep 17 12:57 opentracing-noop-0.33.0.jar -rw-r--r--@ 1 stevel staff 7504 Sep 17 12:57 opentracing-util-0.33.0.jar -rw-r--r--@ 1 stevel staff 281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar -rw-r--r--@ 1 stevel staff 19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar -rw-r--r--@ 1 stevel staff 128414 Sep 17 13:11 re2j-1.1.jar -rw-r--r--@ 1 stevel staff 11369 Sep 17 12:57 reactive-streams-1.0.3.jar -rw-r--r--@ 1 stevel staff 332398 Sep 17 13:11 reload4j-1.2.22.jar -rw-r--r--@ 1 stevel staff 41125 Sep 17 13:11 slf4j-api-1.7.36.jar -rw-r--r--@ 1 stevel staff 9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar -rw-r--r--@ 1 stevel staff 195909 Sep 17 13:11 stax2-api-4.2.1.jar -rw-r--r--@ 1 stevel staff 72007 Sep 17 13:11 txw2-2.3.9.jar -rw-r--r--@ 1 stevel staff 443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar -rw-r--r--@ 1 stevel staff 522679 Sep 17 13:11 woodstox-core-5.4.0.jar -rw-r--r--@ 1 stevel staff 1323991 Sep 17 13:11 zookeeper-3.8.4.jar -rw-r--r--@ 1 stevel staff 254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar ```", "created": "2025-09-17T13:57:33.303+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303139199 by contrast: 3.4.2 ``` total 98048 -rw-r--r--@ 1 stevel staff 3448 Aug 7 12:16 animal-sniffer-annotations-1.17.jar -rw-r--r--@ 1 stevel staff 20891 Aug 7 12:16 audience-annotations-0.12.0.jar -rw-r--r--@ 1 stevel staff 651391 Aug 7 12:16 avro-1.11.4.jar -rw-r--r--@ 1 stevel staff 8324412 Aug 7 12:15 bcprov-jdk18on-1.78.1.jar -rw-r--r--@ 1 stevel staff 193322 Aug 7 12:16 checker-qual-2.5.2.jar -rw-r--r--@ 1 stevel staff 75479 Aug 7 12:16 commons-cli-1.9.0.jar -rw-r--r--@ 1 stevel staff 353793 Aug 7 12:16 commons-codec-1.15.jar -rw-r--r--@ 1 stevel staff 751914 Aug 7 12:16 commons-collections4-4.4.jar -rw-r--r--@ 1 stevel staff 1079377 Aug 7 12:16 commons-compress-1.26.1.jar -rw-r--r--@ 1 stevel staff 657516 Aug 7 12:16 commons-configuration2-2.10.1.jar -rw-r--r--@ 1 stevel staff 24239 Aug 7 12:16 commons-daemon-1.0.13.jar -rw-r--r--@ 1 stevel staff 508826 Aug 7 12:16 commons-io-2.16.1.jar -rw-r--r--@ 1 stevel staff 673587 Aug 7 12:16 commons-lang3-3.17.0.jar -rw-r--r--@ 1 stevel staff 70816 Aug 7 12:16 commons-logging-1.3.0.jar -rw-r--r--@ 1 stevel staff 2213560 Aug 7 12:16 commons-math3-3.6.1.jar -rw-r--r--@ 1 stevel staff 316431 Aug 7 12:16 commons-net-3.9.0.jar -rw-r--r--@ 1 stevel staff 238400 Aug 7 12:16 commons-text-1.10.0.jar -rw-r--r--@ 1 stevel staff 2983237 Aug 7 12:16 curator-client-5.2.0.jar -rw-r--r--@ 1 stevel staff 336384 Aug 7 12:16 curator-framework-5.2.0.jar -rw-r--r--@ 1 stevel staff 315569 Aug 7 12:16 curator-recipes-5.2.0.jar -rw-r--r--@ 1 stevel staff 583996 Aug 7 12:16 dnsjava-3.6.1.jar -rw-r--r--@ 1 stevel staff 3727 Aug 7 12:16 failureaccess-1.0.jar -rw-r--r--@ 1 stevel staff 249277 Aug 7 12:16 gson-2.9.0.jar -rw-r--r--@ 1 stevel staff 2747878 Aug 7 12:16 guava-27.0-jre.jar -rw-r--r--@ 1 stevel staff 25517 Aug 7 12:16 hadoop-annotations-3.4.2.jar -rw-r--r--@ 1 stevel staff 110106 Aug 7 12:16 hadoop-auth-3.4.2.jar -rw-r--r--@ 1 stevel staff 810477 Aug 7 12:39 hadoop-azure-3.4.2.jar -rw-r--r--@ 1 stevel staff 3519516 Aug 7 12:16 hadoop-shaded-guava-1.4.0.jar -rw-r--r--@ 1 stevel staff 1952967 Aug 7 12:16 hadoop-shaded-protobuf_3_25-1.4.0.jar -rw-r--r--@ 1 stevel staff 780321 Aug 7 12:16 httpclient-4.5.13.jar -rw-r--r--@ 1 stevel staff 328593 Aug 7 12:16 httpcore-4.4.13.jar -rw-r--r--@ 1 stevel staff 8782 Aug 7 12:16 j2objc-annotations-1.1.jar -rw-r--r--@ 1 stevel staff 75705 Aug 7 12:16 jackson-annotations-2.12.7.jar -rw-r--r--@ 1 stevel staff 365538 Aug 7 12:16 jackson-core-2.12.7.jar -rw-r--r--@ 1 stevel staff 1512418 Aug 7 12:16 jackson-databind-2.12.7.1.jar -rw-r--r--@ 1 stevel staff 44399 Aug 7 12:16 jakarta.activation-api-1.2.1.jar -rw-r--r--@ 1 stevel staff 95806 Aug 7 12:16 javax.servlet-api-3.1.0.jar -rw-r--r--@ 1 stevel staff 102244 Aug 7 12:16 jaxb-api-2.2.11.jar -rw-r--r--@ 1 stevel staff 890168 Aug 7 12:16 jaxb-impl-2.2.3-1.jar -rw-r--r--@ 1 stevel staff 4722 Aug 7 12:16 jcip-annotations-1.0-1.jar -rw-r--r--@ 1 stevel staff 436731 Aug 7 12:16 jersey-core-1.19.4.jar -rw-r--r--@ 1 stevel staff 158890 Aug 7 12:16 jersey-json-1.22.0.jar -rw-r--r--@ 1 stevel staff 705276 Aug 7 12:16 jersey-server-1.19.4.jar -rw-r--r--@ 1 stevel staff 128990 Aug 7 12:16 jersey-servlet-1.19.4.jar -rw-r--r--@ 1 stevel staff 90184 Aug 7 12:16 jettison-1.5.4.jar -rw-r--r--@ 1 stevel staff 249911 Aug 7 12:16 jetty-http-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 183011 Aug 7 12:16 jetty-io-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 118496 Aug 7 12:16 jetty-security-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 739348 Aug 7 12:16 jetty-server-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 146064 Aug 7 12:16 jetty-servlet-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 588962 Aug 7 12:16 jetty-util-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 66643 Aug 7 12:16 jetty-util-ajax-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 140308 Aug 7 12:16 jetty-webapp-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 68894 Aug 7 12:16 jetty-xml-9.4.57.v20241219.jar -rw-r--r--@ 1 stevel staff 282591 Aug 7 12:16 jsch-0.1.55.jar -rw-r--r--@ 1 stevel staff 100636 Aug 7 12:15 jsp-api-2.1.jar -rw-r--r--@ 1 stevel staff 19936 Aug 7 12:16 jsr305-3.0.2.jar -rw-r--r--@ 1 stevel staff 46367 Aug 7 12:16 jsr311-api-1.1.1.jar -rw-r--r--@ 1 stevel staff 4519 Aug 7 12:16 jul-to-slf4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 223129 Aug 7 12:16 kerb-core-2.0.3.jar -rw-r--r--@ 1 stevel staff 115065 Aug 7 12:16 kerb-crypto-2.0.3.jar -rw-r--r--@ 1 stevel staff 36361 Aug 7 12:16 kerb-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 100095 Aug 7 12:16 kerby-asn1-2.0.3.jar -rw-r--r--@ 1 stevel staff 30190 Aug 7 12:16 kerby-config-2.0.3.jar -rw-r--r--@ 1 stevel staff 200581 Aug 7 12:16 kerby-pkix-2.0.3.jar -rw-r--r--@ 1 stevel staff 40787 Aug 7 12:16 kerby-util-2.0.3.jar -rw-r--r--@ 1 stevel staff 2199 Aug 7 12:16 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar -rw-r--r--@ 1 stevel staff 136314 Aug 7 12:16 metrics-core-3.2.4.jar -rw-r--r--@ 1 stevel staff 4554 Aug 7 12:15 netty-all-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 339045 Aug 7 12:16 netty-buffer-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 355199 Aug 7 12:16 netty-codec-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 67192 Aug 7 12:15 netty-codec-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37789 Aug 7 12:15 netty-codec-haproxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 674362 Aug 7 12:15 netty-codec-http-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 490985 Aug 7 12:15 netty-codec-http2-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 44736 Aug 7 12:15 netty-codec-memcache-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 113699 Aug 7 12:15 netty-codec-mqtt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 46015 Aug 7 12:15 netty-codec-redis-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 21344 Aug 7 12:15 netty-codec-smtp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 121032 Aug 7 12:15 netty-codec-socks-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 34636 Aug 7 12:15 netty-codec-stomp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19823 Aug 7 12:15 netty-codec-xml-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 719225 Aug 7 12:16 netty-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 580162 Aug 7 12:16 netty-handler-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25650 Aug 7 12:15 netty-handler-proxy-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 26833 Aug 7 12:15 netty-handler-ssl-ocsp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 37842 Aug 7 12:16 netty-resolver-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 188360 Aug 7 12:15 netty-resolver-dns-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 9145 Aug 7 12:15 netty-resolver-dns-classes-macos-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 19825 Aug 7 12:15 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 19629 Aug 7 12:15 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 521428 Aug 7 12:16 netty-transport-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 147621 Aug 7 12:16 netty-transport-classes-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 108558 Aug 7 12:15 netty-transport-classes-kqueue-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 42321 Aug 7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar -rw-r--r--@ 1 stevel staff 36594 Aug 7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar -rw-r--r--@ 1 stevel staff 40644 Aug 7 12:15 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar -rw-r--r--@ 1 stevel staff 6193 Aug 7 12:16 netty-transport-native-epoll-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 25741 Aug 7 12:15 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar -rw-r--r--@ 1 stevel staff 25170 Aug 7 12:15 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar -rw-r--r--@ 1 stevel staff 44157 Aug 7 12:16 netty-transport-native-unix-common-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 18241 Aug 7 12:15 netty-transport-rxtx-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 50814 Aug 7 12:15 netty-transport-sctp-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 32189 Aug 7 12:15 netty-transport-udt-4.1.118.Final.jar -rw-r--r--@ 1 stevel staff 779369 Aug 7 12:16 nimbus-jose-jwt-9.37.2.jar -rw-r--r--@ 1 stevel staff 128414 Aug 7 12:16 re2j-1.1.jar -rw-r--r--@ 1 stevel staff 332398 Aug 7 12:16 reload4j-1.2.22.jar -rw-r--r--@ 1 stevel staff 41125 Aug 7 12:15 slf4j-api-1.7.36.jar -rw-r--r--@ 1 stevel staff 9824 Aug 7 12:15 slf4j-reload4j-1.7.36.jar -rw-r--r--@ 1 stevel staff 2112099 Aug 7 12:16 snappy-java-1.1.10.4.jar -rw-r--r--@ 1 stevel staff 195909 Aug 7 12:16 stax2-api-4.2.1.jar -rw-r--r--@ 1 stevel staff 522679 Aug 7 12:16 woodstox-core-5.4.0.jar -rw-r--r--@ 1 stevel staff 1323991 Aug 7 12:16 zookeeper-3.8.4.jar -rw-r--r--@ 1 stevel staff 254932 Aug 7 12:16 zookeeper-jute-3.8.4.jar ```", "created": "2025-09-17T13:58:49.196+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303435683 Having audited the files coming off the cloud connectors, we have about a dozen whose licenses aren't in the binary ``` analyticsaccelerator-s3-1.3.0.jar cos_api-bundle-5.6.19.jar dom4j-2.1.4.jar esdk-obs-java-3.20.4.2.jar java-trace-api-0.2.11-beta.jar java-xmlbuilder-1.2.jar opentracing-api-0.33.0.jar opentracing-noop-0.33.0.jar opentracing-util-0.33.0.jar reactive-streams-1.0.3.jar ve-tos-java-sdk-hadoop-2.8.9.jar ``` the analyticsaccelerator is @ahmarsuhail 's work to add to the license, not sure about the others. Proposed: identify which connector the unacknowledged artifacts are coming from, create homework for each team.", "created": "2025-09-17T15:03:51.648+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303936232 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 13m 4s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 43m 58s | | trunk passed | | +1 :green_heart: | compile | 16m 16s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 2m 57s | | trunk passed | | +1 :green_heart: | javadoc | 2m 53s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 43s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 40m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 10s | | the patch passed | | +1 :green_heart: | compile | 14m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 58s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 2m 55s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 2m 47s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 41s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 42m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 39s | | hadoop-assemblies in the patch passed. | | +1 :green_heart: | unit | 0m 41s | | hadoop-tools-dist in the patch passed. | | +1 :green_heart: | unit | 0m 44s | | hadoop-huaweicloud in the patch passed. | | +1 :green_heart: | unit | 0m 41s | | hadoop-cloud-storage in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 214m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 604f3bcd050e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c59e35149ea17b7cea37be9203a265dcbff118fe | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 548 (vs. ulimit of 5500) | | modules | C: hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-cloud-storage U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-17T17:27:14.500+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3311933628 not very familiar with how the packaging stuff works, so finding this a bit difficult to review. How are testing the packaging, I just ran `mvn package -Pdist -DskipTests -Dmaven.javadoc.skip=true -DskipShade`, but the outputs in : `hadoop-cloud-storage-project/hadoop-cloud-storage/target/hadoop-cloud-storage-3.5.0-SNAPSHOT/share/hadoop/common/lib`, `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/tools/lib` `hadoop/hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib` are all the same before and after your changes, so I must be doing something wrong.", "created": "2025-09-19T12:03:19.259+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3312222391 did you do a `mvn clean package`? `hadoop-cloud-storage-project/hadoop-cloud-storage/target/hadoop-cloud-storage-3.5.0-SNAPSHOT/share/hadoop/common/lib` -new, contains all cloud stuff we want in; should cut stuff already going to be there just to reduce copying `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/share/hadoop/tools/lib` should remove hadoop-azure, hadoop-aws, hadoop-gcs, bundle.jar... the big distro created under `hadoop-dist/target/hadoop-3.5.0-SNAPSHOT/` is what is shipped.", "created": "2025-09-19T13:33:05.736+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3327784078 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 12s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 27s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 56s | | trunk passed | | +1 :green_heart: | compile | 16m 7s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 20m 36s | | trunk passed | | +1 :green_heart: | javadoc | 9m 11s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 18m 3s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 38m 59s | | the patch passed | | +1 :green_heart: | compile | 15m 25s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 25s | | the patch passed | | +1 :green_heart: | compile | 13m 56s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 56s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 55s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 9s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5526 unchanged - 10 fixed = 5536 total (was 5536) | | -1 :x: | javadoc | 7m 34s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1418 unchanged - 10 fixed = 1428 total (was 1428) | | -1 :x: | shadedclient | 19m 38s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 800m 7s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 46s | | The patch does not generate ASF License warnings. | | | | 1046m 13s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 271d5e8aaf6a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0aaa6ce66e8a8fc7fc04fa4e2650badf956d531a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 4938 (vs. ulimit of 5500) | | modules | C: hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-cloud-storage . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-24T11:03:18.734+0000"}, {"author": "Steve Loughran", "body": "I've just discovered quite how many things google-cloud-storage jar pulls in if you don't build a shaded release. Nowhere as big as the aws sdk, but it it is still significant. # I'm going to exclude hadoop-gcp dependencies by default in a build, so if you build hadoop distro with -DskipShade you don't get these in common-lib unless you have a -Dhadoop-gcp-package. # Most of these aren't in our published LICENSE-binary file. some are, but not all. # the opentelemetry/census artifacts are newer than those from one of the other projects; build both and you get conflict (joy!). # a protobuf 2.5 comes in from somewhere I think for now I'd say \"don't make issues 2-3 blockers on merging the PR\" because they're independent. But ideally the gcs imports should be tuned down and we should go for consistent opentelemetry/opencensus versions wherever imported.  3.0K animal-sniffer-annotations-1.24.jar 3.0K annotations-4.1.1.4.jar 49K api-common-2.47.2.jar 7.3K auto-value-annotations-1.11.0.jar 232K checker-qual-3.49.0.jar 4.3M conscrypt-openjdk-uber-2.5.2.jar 18K detector-resources-support-0.33.0.jar 19K error_prone_annotations-2.36.0.jar 39K exporter-metrics-0.33.0.jar 4.6K failureaccess-1.0.2.jar 52K gapic-google-cloud-storage-v2-2.52.0.jar 424K gax-2.64.2.jar 154K gax-grpc-2.64.2.jar 162K gax-httpjson-2.64.2.jar 295K google-api-client-2.7.2.jar 252K google-api-services-storage-v1-rev20250420-2.0.0.jar 8.2K google-auth-library-credentials-1.33.1.jar 294K google-auth-library-oauth2-http-1.33.1.jar 137K google-cloud-core-2.54.2.jar 16K google-cloud-core-grpc-2.54.2.jar 15K google-cloud-core-http-2.54.2.jar 249K google-cloud-monitoring-3.52.0.jar 1.3M google-cloud-storage-2.52.0.jar 289K google-http-client-1.46.3.jar 11K google-http-client-apache-v2-1.46.3.jar 19K google-http-client-appengine-1.46.3.jar 13K google-http-client-gson-1.46.3.jar 9.4K google-http-client-jackson2-1.46.3.jar 80K google-oauth-client-1.37.0.jar 316K grpc-alts-1.70.0.jar 316K grpc-api-1.70.0.jar 14K grpc-auth-1.70.0.jar 293B grpc-context-1.70.0.jar 639K grpc-core-1.70.0.jar 30K grpc-google-cloud-storage-v2-2.52.0.jar 15K grpc-googleapis-1.70.0.jar 175K grpc-grpclb-1.70.0.jar 39K grpc-inprocess-1.70.0.jar 9.3M grpc-netty-shaded-1.70.0.jar 67K grpc-opentelemetry-1.70.0.jar 5.2K grpc-protobuf-1.70.0.jar 7.7K grpc-protobuf-lite-1.70.0.jar 248K grpc-rls-1.70.0.jar 928K grpc-services-1.70.0.jar 59K grpc-stub-1.70.0.jar 98K grpc-util-1.70.0.jar 9.4M grpc-xds-1.70.0.jar 243K gson-2.9.0.jar 2.9M guava-33.4.8-jre.jar 12K j2objc-annotations-3.0.0.jar 462K jackson-core-2.14.3.jar 26K javax.annotation-api-1.3.2.jar 3.7K jspecify-1.0.0.jar 19K jsr305-3.0.2.jar 2.1K listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar 347K opencensus-api-0.31.1.jar 23K opencensus-contrib-http-util-0.31.1.jar 155K opentelemetry-api-1.47.0.jar 48K opentelemetry-context-1.47.0.jar 8.1K opentelemetry-gcp-resources-1.37.0-alpha.jar 6.6K opentelemetry-sdk-1.47.0.jar 54K opentelemetry-sdk-common-1.47.0.jar 20K opentelemetry-sdk-extension-autoconfigure-spi-1.47.0.jar 53K opentelemetry-sdk-logs-1.47.0.jar 322K opentelemetry-sdk-metrics-1.47.0.jar 129K opentelemetry-sdk-trace-1.47.0.jar 73K opentelemetry-semconv-1.29.0-alpha.jar 6.8K perfmark-api-0.27.0.jar 1.9M proto-google-cloud-monitoring-v3-3.52.0.jar 980K proto-google-cloud-storage-v2-2.52.0.jar 2.6M proto-google-common-protos-2.55.2.jar 182K proto-google-iam-v1-1.50.2.jar 521K protobuf-java-2.5.0.jar 71K protobuf-java-util-3.25.5.jar 125K re2j-1.1.jar 91K shared-resourcemapping-0.33.0.jar 40K slf4j-api-1.7.36.jar 503K threetenbp-1.7.0.jar", "created": "2025-10-09T17:04:53.181+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3387174385 Latest build generates stack traces from gcs and obs filesystem incomplete CP in service loader. Both need to move to core-default.xml *only* which is faster anyway. ``` 2025-10-09 20:06:44,452 [main] WARN fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem", "created": "2025-10-09T19:09:29.393+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3387715974 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 3s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 50s | | trunk passed | | +1 :green_heart: | compile | 16m 3s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 6s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 21m 2s | | trunk passed | | +1 :green_heart: | javadoc | 9m 0s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 29s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 49m 55s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 40s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 41m 14s | | the patch passed | | +1 :green_heart: | compile | 15m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 33s | | the patch passed | | +1 :green_heart: | compile | 13m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 48s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 58s | [/results-javadoc-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 10 new + 5526 unchanged - 10 fixed = 5536 total (was 5536) | | -1 :x: | javadoc | 8m 2s | [/results-javadoc-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 10 new + 1418 unchanged - 10 fixed = 1428 total (was 1428) | | +1 :green_heart: | shadedclient | 63m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 22s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +0 :ok: | asflicense | 0m 32s | | ASF License check generated no output? | | | | 309m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 01b725e22dc7 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 76b4eae78d738ee73cf68118a87c9204d120d752 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 699 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-assemblies hadoop-tools/hadoop-tools-dist hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos hadoop-cloud-storage-project/hadoop-cloud-storage hadoop-cloud-storage-project/hadoop-cloud-storage-dist hadoop-cloud-storage-project . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-09T22:50:28.141+0000"}, {"author": "Steve Loughran", "body": "and full jars if you pull in everything. aws bundle.jar dominates; the rest adds up.  104K aliyun-java-core-0.2.11-beta.jar 190K aliyun-java-sdk-core-4.5.10.jar 160K aliyun-java-sdk-kms-2.11.0.jar 216K aliyun-java-sdk-ram-3.1.0.jar 907K aliyun-sdk-oss-3.18.1.jar 2.4M analyticsaccelerator-s3-1.3.0.jar 3.0K animal-sniffer-annotations-1.24.jar 3.0K annotations-4.1.1.4.jar 49K api-common-2.47.2.jar 7.3K auto-value-annotations-1.11.0.jar 111K azure-data-lake-store-sdk-2.3.9.jar 10K azure-keyvault-core-1.0.0.jar 796K azure-storage-7.0.1.jar 612M bundle-2.29.52.jar 232K checker-qual-3.49.0.jar 346K commons-codec-1.15.jar 69K commons-logging-1.3.0.jar 4.3M conscrypt-openjdk-uber-2.5.2.jar 8.3M cos_api-bundle-5.6.19.jar 18K detector-resources-support-0.33.0.jar 317K dom4j-2.1.4.jar 19K error_prone_annotations-2.36.0.jar 39K exporter-metrics-0.33.0.jar 4.6K failureaccess-1.0.2.jar 52K gapic-google-cloud-storage-v2-2.52.0.jar 424K gax-2.64.2.jar 154K gax-grpc-2.64.2.jar 162K gax-httpjson-2.64.2.jar 295K google-api-client-2.7.2.jar 252K google-api-services-storage-v1-rev20250420-2.0.0.jar 8.2K google-auth-library-credentials-1.33.1.jar 294K google-auth-library-oauth2-http-1.33.1.jar 137K google-cloud-core-2.54.2.jar 16K google-cloud-core-grpc-2.54.2.jar 15K google-cloud-core-http-2.54.2.jar 249K google-cloud-monitoring-3.52.0.jar 1.3M google-cloud-storage-2.52.0.jar 289K google-http-client-1.46.3.jar 11K google-http-client-apache-v2-1.46.3.jar 19K google-http-client-appengine-1.46.3.jar 13K google-http-client-gson-1.46.3.jar 9.4K google-http-client-jackson2-1.46.3.jar 80K google-oauth-client-1.37.0.jar 316K grpc-alts-1.70.0.jar 316K grpc-api-1.70.0.jar 14K grpc-auth-1.70.0.jar 293B grpc-context-1.70.0.jar 639K grpc-core-1.70.0.jar 30K grpc-google-cloud-storage-v2-2.52.0.jar 15K grpc-googleapis-1.70.0.jar 175K grpc-grpclb-1.70.0.jar 39K grpc-inprocess-1.70.0.jar 9.3M grpc-netty-shaded-1.70.0.jar 67K grpc-opentelemetry-1.70.0.jar 5.2K grpc-protobuf-1.70.0.jar 7.7K grpc-protobuf-lite-1.70.0.jar 248K grpc-rls-1.70.0.jar 928K grpc-services-1.70.0.jar 59K grpc-stub-1.70.0.jar 98K grpc-util-1.70.0.jar 9.4M grpc-xds-1.70.0.jar 243K gson-2.9.0.jar 2.9M guava-33.4.8-jre.jar 92K hadoop-aliyun-3.5.0-SNAPSHOT.jar 910K hadoop-aws-3.5.0-SNAPSHOT.jar 810K hadoop-azure-3.5.0-SNAPSHOT.jar 33K hadoop-azure-datalake-3.5.0-SNAPSHOT.jar 68K hadoop-cos-3.5.0-SNAPSHOT.jar 135K hadoop-gcp-3.5.0-SNAPSHOT.jar 142K hadoop-huaweicloud-3.5.0-SNAPSHOT.jar 250K hadoop-tos-3.5.0-SNAPSHOT.jar 762K httpclient-4.5.13.jar 933K httpclient5-5.5.jar 321K httpcore-4.4.13.jar 888K httpcore5-5.3.6.jar 236K httpcore5-h2-5.3.4.jar 100K ini4j-0.5.4.jar 12K j2objc-annotations-3.0.0.jar 462K jackson-core-2.14.3.jar 7.6K java-trace-api-0.2.11-beta.jar 26K javax.annotation-api-1.3.2.jar 320K jdom2-2.0.6.1.jar 88K jettison-1.5.4.jar 575K jetty-util-9.4.57.v20241219.jar 65K jetty-util-ajax-9.4.57.v20241219.jar 3.7K jspecify-1.0.0.jar 19K jsr305-3.0.2.jar 2.1K listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar 347K opencensus-api-0.31.1.jar 23K opencensus-contrib-http-util-0.31.1.jar 155K opentelemetry-api-1.47.0.jar 48K opentelemetry-context-1.47.0.jar 8.1K opentelemetry-gcp-resources-1.37.0-alpha.jar 6.6K opentelemetry-sdk-1.47.0.jar 54K opentelemetry-sdk-common-1.47.0.jar 20K opentelemetry-sdk-extension-autoconfigure-spi-1.47.0.jar 53K opentelemetry-sdk-logs-1.47.0.jar 322K opentelemetry-sdk-metrics-1.47.0.jar 129K opentelemetry-sdk-trace-1.47.0.jar 73K opentelemetry-semconv-1.29.0-alpha.jar 275K org.jacoco.agent-0.8.5-runtime.jar 6.8K perfmark-api-0.27.0.jar 1.9M proto-google-cloud-monitoring-v3-3.52.0.jar 980K proto-google-cloud-storage-v2-2.52.0.jar 2.6M proto-google-common-protos-2.55.2.jar 182K proto-google-iam-v1-1.50.2.jar 521K protobuf-java-2.5.0.jar 71K protobuf-java-util-3.25.5.jar 125K re2j-1.1.jar 11K reactive-streams-1.0.3.jar 91K shared-resourcemapping-0.33.0.jar 40K slf4j-api-1.7.36.jar 503K threetenbp-1.7.0.jar 980K ve-tos-java-sdk-hadoop-2.8.9.jar 433K wildfly-openssl-2.1.4.Final.jar  The default settings will produce something a lot leaner  2.4M analyticsaccelerator-s3-1.3.0.jar 10K azure-keyvault-core-1.0.0.jar 796K azure-storage-7.0.1.jar 346K commons-codec-1.15.jar 69K commons-logging-1.3.0.jar 910K hadoop-aws-3.5.0-SNAPSHOT.jar 810K hadoop-azure-3.5.0-SNAPSHOT.jar 33K hadoop-azure-datalake-3.5.0-SNAPSHOT.jar 68K hadoop-cos-3.5.0-SNAPSHOT.jar 135K hadoop-gcp-3.5.0-SNAPSHOT.jar 142K hadoop-huaweicloud-3.5.0-SNAPSHOT.jar 250K hadoop-tos-3.5.0-SNAPSHOT.jar 762K httpclient-4.5.13.jar 321K httpcore-4.4.13.jar 575K jetty-util-9.4.57.v20241219.jar 65K jetty-util-ajax-9.4.57.v20241219.jar 433K wildfly-openssl-2.1.4.Final.jar", "created": "2025-10-13T16:07:33.413+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3402138222 shaded test failures * some in yarn which are presumably unrelated...let's see * lots of more skipped tests in hadoop-azure, hadoop-azuredatalake, such as `TestAbfsInputStreamStatistics` which is skipping .... Looks like maven is back to running these tests and skipping where they don't have the credentials", "created": "2025-10-14T14:23:01.303+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3405523026 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 30s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 15s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 35m 25s | | trunk passed | | +1 :green_heart: | compile | 17m 37s | | trunk passed | | +1 :green_heart: | checkstyle | 3m 40s | | trunk passed | | -1 :x: | mvnsite | 11m 2s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 44s | | trunk passed | | +0 :ok: | spotbugs | 0m 27s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 22s | | branch/hadoop-assemblies no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 19s | [/branch-spotbugs-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in trunk failed. | | -1 :x: | spotbugs | 0m 32s | [/branch-spotbugs-hadoop-tools_hadoop-gcp.txt]([CI_URL] | hadoop-gcp in trunk failed. | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-tools/hadoop-tools-dist no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 32s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt]([CI_URL] | hadoop-huaweicloud in trunk failed. | | -1 :x: | spotbugs | 0m 37s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in trunk failed. | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-cloud-storage-project/hadoop-cloud-storage no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 31s | [/branch-spotbugs-hadoop-cloud-storage-project.txt]([CI_URL] | hadoop-cloud-storage-project in trunk failed. | | -1 :x: | spotbugs | 0m 30s | [/branch-spotbugs-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | shadedclient | 30m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 43m 52s | | the patch passed | | +1 :green_heart: | compile | 17m 5s | | the patch passed | | +1 :green_heart: | javac | 17m 5s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 39s | | the patch passed | | -1 :x: | mvnsite | 7m 36s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 41s | [/results-javadoc-javadoc-root.txt]([CI_URL] | root generated 30 new + 42985 unchanged - 30 fixed = 43015 total (was 43015) | | +0 :ok: | spotbugs | 0m 22s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 22s | | hadoop-assemblies has no data from spotbugs | | -1 :x: | spotbugs | 1m 19s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | +0 :ok: | spotbugs | 0m 24s | | hadoop-tools/hadoop-tools-dist has no data from spotbugs | | -1 :x: | spotbugs | 0m 33s | [/patch-spotbugs-hadoop-tools_hadoop-gcp.txt]([CI_URL] | hadoop-gcp in the patch failed. | | -1 :x: | spotbugs | 0m 32s | [/patch-spotbugs-hadoop-cloud-storage-project_hadoop-huaweicloud.txt]([CI_URL] | hadoop-huaweicloud in the patch failed. | | -1 :x: | spotbugs | 0m 36s | [/patch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt]([CI_URL] | hadoop-tos in the patch failed. | | +0 :ok: | spotbugs | 0m 22s | | hadoop-cloud-storage-project/hadoop-cloud-storage has no data from spotbugs | | +0 :ok: | spotbugs | 0m 23s | | hadoop-cloud-storage-project/hadoop-cloud-storage-dist has no data from spotbugs | | -1 :x: | spotbugs | 0m 31s | [/patch-spotbugs-hadoop-cloud-storage-project.txt]([CI_URL] | hadoop-cloud-storage-project in the patch failed. | | -1 :x: | spotbugs | 0m 30s | [/patch-spotbugs-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shadedclient | 31m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 801m 10s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | -1 :x: | asflicense | 1m 45s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 1072m 30s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.fs.tosfs.object.TestObjectOutputStream | | | hadoop.fs.tosfs.commit.TestMagicOutputStream | | | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider | | | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain | | | hadoop.fs.tosfs.object.TestObjectRangeInputStream | | | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 209e4e5576fc 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0254bb430e1623793f30744d7b32a37202f2d586 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 3665 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-assemblies hadoop-common-project/hadoop-common hadoop-tools/hadoop-tools-dist hadoop-tools/hadoop-gcp hadoop-cloud-storage-project/hadoop-huaweicloud hadoop-cloud-storage-project/hadoop-tos hadoop-cloud-storage-project/hadoop-cloud-storage hadoop-cloud-storage-project/hadoop-cloud-storage-dist hadoop-cloud-storage-project . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-15T09:44:55.130+0000"}, {"author": "ASF GitHub Bot", "body": "cnauroth commented on code in PR #7980: URL: https://github.com/apache/hadoop/pull/7980#discussion_r2446383325 ########## hadoop-cloud-storage-project/pom.xml: ########## @@ -34,6 +34,7 @@ <module>hadoop-cos</module> <module>hadoop-huaweicloud</module> <module>hadoop-tos</module> + <module>hadoop-cloud-storage-dist</module> Review Comment: It seems like we would never need to enter execution of `hadoop-cloud-storage-dist` unless we are building a distro (activating `-Pdist`). Should we also wrap inclusion of the sub-module here behind activation of the `dist` profile? ########## BUILDING.txt: ########## @@ -388,6 +388,58 @@ Create a local staging version of the website (in /tmp/hadoop-site) Note that the site needs to be built in a second pass after other artifacts. +---------------------------------------------------------------------------------- +Including Cloud Connector Dependencies in Distributions: + +Hadoop distributions include the hadoop modules need to work with data and services Review Comment: Nitpick: \"modules needed\".", "created": "2025-10-20T23:40:44.245+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #7980: URL: https://github.com/apache/hadoop/pull/7980#discussion_r2447670994 ########## hadoop-cloud-storage-project/pom.xml: ########## @@ -34,6 +34,7 @@ <module>hadoop-cos</module> <module>hadoop-huaweicloud</module> <module>hadoop-tos</module> + <module>hadoop-cloud-storage-dist</module> Review Comment: valid point. Will do, as it'll save on disk space as well as time.", "created": "2025-10-21T10:47:12.420+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7980: URL: https://github.com/apache/hadoop/pull/7980#issuecomment-3425948823 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 16s | | https://github.com/apache/hadoop/pull/7980 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7980 | | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-21T10:57:22.714+0000"}, {"author": "ASF GitHub Bot", "body": "cnauroth commented on code in PR #7980: URL: https://github.com/apache/hadoop/pull/7980#discussion_r2449857037 ########## BUILDING.txt: ########## @@ -388,6 +388,57 @@ Create a local staging version of the website (in /tmp/hadoop-site) Note that the site needs to be built in a second pass after other artifacts. +---------------------------------------------------------------------------------- +Including Cloud Connector Dependencies in Distributions: + +Hadoop distributions include the hadoop modules needed to work with data and services +on cloud infrastructure + +However, dependencies are omitted for all cloud connectors except hadoop-azure +(abfs:// and wasb://) and possibly hadoop-gcp (gs://) and hadoop-tos (tos://). +For the latter two modules, it depends on shading options. + +For hadoop-aws the AWS SDK bundle.jar is omitted, but everything else is included. + +Excluding the extra binaries: +* Keeps release artifact size below the limit of the ASF distribution network. +* Reduces download and size overhead in docker usage. +* Reduces the CVE attack surface and audit-related complaints about those same ScVES. Review Comment: Nitpick: \"CVEs.\"", "created": "2025-10-21T22:22:36.924+0000"}], "derived_tasks": {"summary": "hadoop binary distribution to move cloud connectors to hadoop common/lib - Place all the cloud connector hadoop-* artifacts and dependencies into h...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19694", "title": "Bump guava to 33.4.8-jre due to EOL", "description": "We can use the latest 33.4.8-jre version as the current one is quite old.", "status": "Resolved", "priority": "Major", "reporter": "Rohit Kumar", "assignee": "Rohit Kumar", "created": "2025-09-17T11:01:22.000+0000", "updated": "2025-10-20T16:41:38.000+0000", "labels": ["pull-request-available"], "components": ["hadoop-thirdparty"], "comments": [{"author": "Steve Loughran", "body": "When I do a thirdparty build I now get a warning of duplicate module 9 info.  [INFO] No artifact matching filter org.checkerframework:checker-qual [WARNING] error_prone_annotations-2.36.0.jar, guava-33.4.8-jre.jar, failureaccess-1.0.3.jar, jspecify-1.0.0.jar, j2objc-annotations-3.0.0.jar define 1 overlapping classes: [WARNING] - META-INF.versions.9.module-info [WARNING] maven-shade-plugin has detected that some class files are [WARNING] present in two or more JARs. When this happens, only one [WARNING] single version of the class is copied to the uber jar. [WARNING] Usually this is not harmful and you can skip these warnings, [WARNING] otherwise try to manually exclude artifacts based on [WARNING] mvn dependency:tree -Ddetail=true and the above output. [WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/  This is new. given we want this to work on java17+ , we need to come up with a way of resolving the conflict, at the very least by making the guava one dominant.", "created": "2025-09-22T13:37:32.907+0000"}, {"author": "Steve Loughran", "body": "ahh, the later versions of guava exclude a dependency on checkerframework. So any references in our code (there are four) fail. Which means we have to manually add it if we want a drop in replacement. PITA.", "created": "2025-10-20T16:41:38.036+0000"}], "derived_tasks": {"summary": "Bump guava to 33.4.8-jre due to EOL - We can use the latest 33", "classifications": ["task"], "qa_pairs": []}}
{"id": "HADOOP-19672", "title": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))", "description": "", "status": "Open", "priority": "Major", "reporter": "Manish Bhatt", "assignee": "Manish Bhatt", "created": "2025-09-01T10:54:52.000+0000", "updated": "2025-10-23T04:38:43.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "bhattmanish98 opened a new pull request, #7967: URL: https://github.com/apache/hadoop/pull/7967 JIRA \u2013 https://issues.apache.org/jira/browse/HADOOP-19672 In case of a network error while using the Apache client, we allow the client to switch over from Apache to JDK. This network fallback occurs in two scenarios: 1. During file system initialization \u2013 When warming up the cache, if no connection is created (indicating an issue with the Apache client), the system will fall back to the JDK. 2. During a network call \u2013 If an I/O or Unknown Host exception occurs for three consecutive retries, the system will fall back to the JDK. This fallback is applied at the JVM level, so all file system calls will use the JDK client once the switch occurs. There is also a possibility of recovery. During cache warmup, if connections are successfully created using the Apache client, the system will automatically switch back to the Apache client.", "created": "2025-09-15T14:55:52.052+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293191749 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 57m 10s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 59s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 144m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b7ef8b2a88ce 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1dad672faf42fbfb0ada2a95456b922a50b2becf | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 693 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-15T17:22:46.350+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293931499 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 45s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 6s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 156m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f1f4dba85ce3 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6faa4c9c8ad8083e0a636b6439146a24c66c0ce3 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-15T20:58:18.410+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381728892 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -73,18 +80,21 @@ static boolean usable() { } AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory, - final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache, - URL baseUrl) { + final AbfsConfiguration abfsConfiguration, + final KeepAliveCache keepAliveCache, + URL baseUrl, + final boolean isCacheWarmupNeeded) { final AbfsConnectionManager connMgr = new AbfsConnectionManager( createSocketFactoryRegistry( new SSLConnectionSocketFactory(delegatingSSLSocketFactory, getDefaultHostnameVerifier())), new AbfsHttpClientConnectionFactory(), keepAliveCache, - abfsConfiguration, baseUrl); + abfsConfiguration, baseUrl, isCacheWarmupNeeded); final HttpClientBuilder builder = HttpClients.custom(); builder.setConnectionManager(connMgr) .setRequestExecutor( - new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout())) + new AbfsManagedHttpRequestExecutor( Review Comment: As we were discussing last time, if we keep it to read timeout the 100 continue timeout would become 30 seconds, this should be another config for 100 continue timeout", "created": "2025-09-26T09:54:12.608+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381745959 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: Can you add some comments around this change as to how it would affect the need for cache warmup", "created": "2025-09-26T09:57:59.745+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381793492 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java: ########## @@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager { /** * The base host for which connections are managed. */ - private HttpHost baseHost; + private final HttpHost baseHost; AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry, - AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac, - final AbfsConfiguration abfsConfiguration, final URL baseUrl) { + AbfsHttpClientConnectionFactory connectionFactory, + KeepAliveCache kac, + final AbfsConfiguration abfsConfiguration, + final URL baseUrl, + final boolean isCacheWarmupNeeded) { this.httpConnectionFactory = connectionFactory; this.kac = kac; this.connectionOperator = new DefaultHttpClientConnectionOperator( socketFactoryRegistry, null, null); this.abfsConfiguration = abfsConfiguration; - if (abfsConfiguration.getApacheCacheWarmupCount() > 0 + this.baseHost = new HttpHost(baseUrl.getHost(), + baseUrl.getDefaultPort(), baseUrl.getProtocol()); + if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0 && kac.getFixedThreadPool() != null) { // Warm up the cache with connections. LOG.debug(\"Warming up the KeepAliveCache with {} connections\", abfsConfiguration.getApacheCacheWarmupCount()); - this.baseHost = new HttpHost(baseUrl.getHost(), - baseUrl.getDefaultPort(), baseUrl.getProtocol()); HttpRoute route = new HttpRoute(baseHost, null, true); - cacheExtraConnection(route, + int totalConnectionsCreated = cacheExtraConnection(route, abfsConfiguration.getApacheCacheWarmupCount()); + if (totalConnectionsCreated == 0) { Review Comment: even if we fail or catch rejected exception for any one of the tasks we want to register fallback as the successfully submitted tasks might have increased the count of totalConnectionsCreated", "created": "2025-09-26T10:08:30.641+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381815558 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestApacheClientConnectionPool.java: ########## @@ -118,6 +121,38 @@ public void testConnectedConnectionLogging() throws Exception { .isEqualTo(4); } + /** + * Test to verify that the ApacheHttpClient falls back to JDK client + * when connection warmup fails. + * This test is applicable only for ApacheHttpClient. + */ + @Test + public void testApacheClientFallbackDuringConnectionWarmup() + throws Exception { + try (KeepAliveCache keepAliveCache = new KeepAliveCache( + new AbfsConfiguration(new Configuration(), EMPTY_STRING))) { + // Create a connection manager with invalid URL to force fallback to JDK client + // during connection warmup. + // This is to simulate failure during connection warmup in the connection manager. + // The invalid URL will cause the connection manager to fail to create connections + // during warmup, forcing it to fall back to JDK client. + final AbfsConnectionManager connMgr = new AbfsConnectionManager( + RegistryBuilder.<ConnectionSocketFactory>create() + .register(HTTPS_SCHEME, new SSLConnectionSocketFactory( + DelegatingSSLSocketFactory.getDefaultFactory(), + getDefaultHostnameVerifier())) + .build(), + new AbfsHttpClientConnectionFactory(), keepAliveCache, + new AbfsConfiguration(new Configuration(), EMPTY_STRING), + new URL(\"https://test.com\"), true); + + Assertions.assertThat(AbfsApacheHttpClient.usable()) + .describedAs(\"Apache HttpClient should be not usable\") Review Comment: Can you make one http call and validate now jdk is being used, user agent can be used for validation", "created": "2025-09-26T10:13:16.449+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2382005373 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -68,13 +68,13 @@ public AbfsClientHandler(final URL baseUrl, final SASTokenProvider sasTokenProvider, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { + initServiceType(abfsConfiguration); Review Comment: Why this change? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -188,7 +189,7 @@ public AbfsBlobClient(final URL baseUrl, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { super(baseUrl, sharedKeyCredentials, abfsConfiguration, tokenProvider, - encryptionContextProvider, abfsClientContext); + encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB); Review Comment: Why this change? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -73,18 +80,21 @@ static boolean usable() { } AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory, - final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache, - URL baseUrl) { + final AbfsConfiguration abfsConfiguration, + final KeepAliveCache keepAliveCache, + URL baseUrl, + final boolean isCacheWarmupNeeded) { final AbfsConnectionManager connMgr = new AbfsConnectionManager( createSocketFactoryRegistry( new SSLConnectionSocketFactory(delegatingSSLSocketFactory, getDefaultHostnameVerifier())), new AbfsHttpClientConnectionFactory(), keepAliveCache, - abfsConfiguration, baseUrl); + abfsConfiguration, baseUrl, isCacheWarmupNeeded); final HttpClientBuilder builder = HttpClients.custom(); builder.setConnectionManager(connMgr) .setRequestExecutor( - new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout())) + new AbfsManagedHttpRequestExecutor( Review Comment: +1 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: +1 What are we trying to achieve here?", "created": "2025-09-26T11:06:00.544+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2381793492 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java: ########## @@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager { /** * The base host for which connections are managed. */ - private HttpHost baseHost; + private final HttpHost baseHost; AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry, - AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac, - final AbfsConfiguration abfsConfiguration, final URL baseUrl) { + AbfsHttpClientConnectionFactory connectionFactory, + KeepAliveCache kac, + final AbfsConfiguration abfsConfiguration, + final URL baseUrl, + final boolean isCacheWarmupNeeded) { this.httpConnectionFactory = connectionFactory; this.kac = kac; this.connectionOperator = new DefaultHttpClientConnectionOperator( socketFactoryRegistry, null, null); this.abfsConfiguration = abfsConfiguration; - if (abfsConfiguration.getApacheCacheWarmupCount() > 0 + this.baseHost = new HttpHost(baseUrl.getHost(), + baseUrl.getDefaultPort(), baseUrl.getProtocol()); + if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0 && kac.getFixedThreadPool() != null) { // Warm up the cache with connections. LOG.debug(\"Warming up the KeepAliveCache with {} connections\", abfsConfiguration.getApacheCacheWarmupCount()); - this.baseHost = new HttpHost(baseUrl.getHost(), - baseUrl.getDefaultPort(), baseUrl.getProtocol()); HttpRoute route = new HttpRoute(baseHost, null, true); - cacheExtraConnection(route, + int totalConnectionsCreated = cacheExtraConnection(route, abfsConfiguration.getApacheCacheWarmupCount()); + if (totalConnectionsCreated == 0) { Review Comment: even if we fail or catch rejected exception for any one of the tasks we want to register fallback ? as the successfully submitted tasks might have increased the count of totalConnectionsCreated", "created": "2025-09-26T12:01:54.816+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2387212747 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; Review Comment: Nit: We should change this in our md file as well", "created": "2025-09-29T09:02:01.277+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2390726345 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3; Review Comment: where are we checking after 3 retries we'll fallback to JDK?", "created": "2025-09-30T10:14:45.238+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2390726345 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3; Review Comment: where are we checking after 3 retries we'll fallback to JDK?", "created": "2025-09-30T10:15:32.677+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393513847 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsApacheHttpClient.java: ########## @@ -73,18 +80,21 @@ static boolean usable() { } AbfsApacheHttpClient(DelegatingSSLSocketFactory delegatingSSLSocketFactory, - final AbfsConfiguration abfsConfiguration, final KeepAliveCache keepAliveCache, - URL baseUrl) { + final AbfsConfiguration abfsConfiguration, + final KeepAliveCache keepAliveCache, + URL baseUrl, + final boolean isCacheWarmupNeeded) { final AbfsConnectionManager connMgr = new AbfsConnectionManager( createSocketFactoryRegistry( new SSLConnectionSocketFactory(delegatingSSLSocketFactory, getDefaultHostnameVerifier())), new AbfsHttpClientConnectionFactory(), keepAliveCache, - abfsConfiguration, baseUrl); + abfsConfiguration, baseUrl, isCacheWarmupNeeded); final HttpClientBuilder builder = HttpClients.custom(); builder.setConnectionManager(connMgr) .setRequestExecutor( - new AbfsManagedHttpRequestExecutor(abfsConfiguration.getHttpReadTimeout())) + new AbfsManagedHttpRequestExecutor( Review Comment: Sure, will create a new config for read timeout and use that when 100 continue is enabled.", "created": "2025-10-01T05:45:47.150+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393517614 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: The reason for this change: Since the keep alive cache is on client level and we were doing cache warmup for both the client separately. Now with this change, we will do cache warmup only for the default client, not for both the clients.", "created": "2025-10-01T05:49:04.577+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393517614 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -254,7 +259,8 @@ private AbfsClient(final URL baseUrl, abfsApacheHttpClient = new AbfsApacheHttpClient( DelegatingSSLSocketFactory.getDefaultFactory(), - abfsConfiguration, keepAliveCache, baseUrl); + abfsConfiguration, keepAliveCache, baseUrl, + abfsConfiguration.getFsConfiguredServiceType() == abfsServiceType); Review Comment: The reason for this change: Since the keep alive cache is on client level and we were doing cache warmup for both the client separately. Now with this change, we will do cache warmup only for the default client, not for both the clients. Will add the comment in the code as well", "created": "2025-10-01T05:49:59.660+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393520715 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestApacheClientConnectionPool.java: ########## @@ -118,6 +121,38 @@ public void testConnectedConnectionLogging() throws Exception { .isEqualTo(4); } + /** + * Test to verify that the ApacheHttpClient falls back to JDK client + * when connection warmup fails. + * This test is applicable only for ApacheHttpClient. + */ + @Test + public void testApacheClientFallbackDuringConnectionWarmup() + throws Exception { + try (KeepAliveCache keepAliveCache = new KeepAliveCache( + new AbfsConfiguration(new Configuration(), EMPTY_STRING))) { + // Create a connection manager with invalid URL to force fallback to JDK client + // during connection warmup. + // This is to simulate failure during connection warmup in the connection manager. + // The invalid URL will cause the connection manager to fail to create connections + // during warmup, forcing it to fall back to JDK client. + final AbfsConnectionManager connMgr = new AbfsConnectionManager( + RegistryBuilder.<ConnectionSocketFactory>create() + .register(HTTPS_SCHEME, new SSLConnectionSocketFactory( + DelegatingSSLSocketFactory.getDefaultFactory(), + getDefaultHostnameVerifier())) + .build(), + new AbfsHttpClientConnectionFactory(), keepAliveCache, + new AbfsConfiguration(new Configuration(), EMPTY_STRING), + new URL(\"https://test.com\"), true); + + Assertions.assertThat(AbfsApacheHttpClient.usable()) + .describedAs(\"Apache HttpClient should be not usable\") Review Comment: Sure, will do that.", "created": "2025-10-01T05:51:19.732+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393522594 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java: ########## @@ -188,7 +189,7 @@ public AbfsBlobClient(final URL baseUrl, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { super(baseUrl, sharedKeyCredentials, abfsConfiguration, tokenProvider, - encryptionContextProvider, abfsClientContext); + encryptionContextProvider, abfsClientContext, AbfsServiceType.BLOB); Review Comment: As described in the previous comment, we need to find out default client and for that we are comparing these values with the default value configured.", "created": "2025-10-01T05:52:49.824+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393523578 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -214,7 +214,7 @@ public final class FileSystemConfigurations { public static final long THOUSAND = 1000L; public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY - = HttpOperationType.JDK_HTTP_URL_CONNECTION; + = HttpOperationType.APACHE_HTTP_CLIENT; Review Comment: Sure, will make the change in .md file as well", "created": "2025-10-01T05:53:34.855+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2393620859 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsConnectionManager.java: ########## @@ -91,26 +92,34 @@ class AbfsConnectionManager implements HttpClientConnectionManager { /** * The base host for which connections are managed. */ - private HttpHost baseHost; + private final HttpHost baseHost; AbfsConnectionManager(Registry<ConnectionSocketFactory> socketFactoryRegistry, - AbfsHttpClientConnectionFactory connectionFactory, KeepAliveCache kac, - final AbfsConfiguration abfsConfiguration, final URL baseUrl) { + AbfsHttpClientConnectionFactory connectionFactory, + KeepAliveCache kac, + final AbfsConfiguration abfsConfiguration, + final URL baseUrl, + final boolean isCacheWarmupNeeded) { this.httpConnectionFactory = connectionFactory; this.kac = kac; this.connectionOperator = new DefaultHttpClientConnectionOperator( socketFactoryRegistry, null, null); this.abfsConfiguration = abfsConfiguration; - if (abfsConfiguration.getApacheCacheWarmupCount() > 0 + this.baseHost = new HttpHost(baseUrl.getHost(), + baseUrl.getDefaultPort(), baseUrl.getProtocol()); + if (isCacheWarmupNeeded && abfsConfiguration.getApacheCacheWarmupCount() > 0 && kac.getFixedThreadPool() != null) { // Warm up the cache with connections. LOG.debug(\"Warming up the KeepAliveCache with {} connections\", abfsConfiguration.getApacheCacheWarmupCount()); - this.baseHost = new HttpHost(baseUrl.getHost(), - baseUrl.getDefaultPort(), baseUrl.getProtocol()); HttpRoute route = new HttpRoute(baseHost, null, true); - cacheExtraConnection(route, + int totalConnectionsCreated = cacheExtraConnection(route, abfsConfiguration.getApacheCacheWarmupCount()); + if (totalConnectionsCreated == 0) { Review Comment: Yes, make sense. I have updated the returned value in case of rejected exception; other thing will remain the same. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -68,13 +68,13 @@ public AbfsClientHandler(final URL baseUrl, final SASTokenProvider sasTokenProvider, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { + initServiceType(abfsConfiguration); Review Comment: This will initialize the default and ingress service types. This is needed before crating the clients so that we can do cache warmup only for default client.", "created": "2025-10-01T07:11:40.076+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3378551948 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 54m 51s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 40s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 8s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 150m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 6778220a7832 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 44765c0ae41d29a78198cb113bc06780be7092af | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-07T20:12:06.452+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7967: URL: https://github.com/apache/hadoop/pull/7967#discussion_r2415677758 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientHandler.java: ########## @@ -68,6 +68,9 @@ public AbfsClientHandler(final URL baseUrl, final SASTokenProvider sasTokenProvider, final EncryptionContextProvider encryptionContextProvider, final AbfsClientContext abfsClientContext) throws IOException { + // This will initialize the default and ingress service types. + // This is needed before crating the clients so that we can do cache warmup Review Comment: nit: typo creating", "created": "2025-10-09T06:17:29.239+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3386392364 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 53m 46s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 7s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 150m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7967 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux a733619d79ed 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ff70868cc0483c55d875f66cf2379f04e75075e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 526 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-09T15:44:50.580+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3388294934 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 169 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 282 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 228 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 721, Failures: 0, Errors: 0, Skipped: 140 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 284 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 718, Failures: 0, Errors: 0, Skipped: 152 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 198 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 746, Failures: 0, Errors: 0, Skipped: 226 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 281 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24", "created": "2025-10-10T04:58:37.773+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 merged PR #7967: URL: https://github.com/apache/hadoop/pull/7967", "created": "2025-10-13T06:19:02.121+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3401193052 I understand why the startup mechanism makes sense, but I'm curious about doing it on a live connection. What problems were occurring with hostname lookup that changing client would fix? Do you want this in 3.4.3? if so do a backport PR ASAP", "created": "2025-10-14T10:43:56.693+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3432065456 @steveloughran We\u2019re performing cache refresh when the number of available connections in the cache drops below a certain threshold. The warmup normally happens during file system initialization, but if the file system is already active and making multiple parallel network calls, creating new connections on demand with the Apache client tends to take longer. To avoid delays in such cases, we asynchronously pre-create and cache a few additional connections whenever the pool runs low. This ensures that subsequent network calls can use already-established connections and spend time on actual data processing rather than connection setup.", "created": "2025-10-22T12:13:35.113+0000"}, {"author": "ASF GitHub Bot", "body": "bhattmanish98 commented on PR #7967: URL: https://github.com/apache/hadoop/pull/7967#issuecomment-3435042669 > Do you want this in 3.4.3? if so do a backport PR ASAP The previous PR that includes the Apache client changes was also not included in release 3.4.3. We\u2019re planning to include this change in the next major release.", "created": "2025-10-23T04:38:43.011+0000"}], "derived_tasks": {"summary": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19666", "title": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension", "description": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.", "status": "Open", "priority": "Major", "reporter": "Lei Wen", "assignee": null, "created": "2025-08-27T03:08:41.000+0000", "updated": "2025-10-26T05:02:39.000+0000", "labels": ["pull-request-available"], "components": ["native"], "comments": [{"author": "ASF GitHub Bot", "body": "leiwen2025 opened a new pull request, #7912: URL: https://github.com/apache/hadoop/pull/7912 This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over **3X** compared to the software implementation. <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-08-28T06:49:54.702+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3232576752 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 30s | | trunk passed | | +1 :green_heart: | compile | 7m 30s | | trunk passed | | -1 :x: | mvnsite | 1m 29s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 56m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 7m 3s | | the patch passed | | +1 :green_heart: | cc | 7m 3s | | the patch passed | | +1 :green_heart: | golang | 7m 3s | | the patch passed | | +1 :green_heart: | javac | 7m 3s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 20 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 2 line(s) with tabs. | | -1 :x: | mvnsite | 1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 22m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 44s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 123m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7912 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 3c797fab6900 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 67c832e1dc930b52b9a68c261f372b14e6cf1639 | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | [CI_URL] | | Max. process+thread count | 1262 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-28T08:54:04.337+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3233497529 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 26s | | trunk passed | | +1 :green_heart: | compile | 7m 37s | | trunk passed | | -1 :x: | mvnsite | 1m 30s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 57m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 6m 57s | | the patch passed | | +1 :green_heart: | cc | 6m 57s | | the patch passed | | +1 :green_heart: | golang | 6m 57s | | the patch passed | | +1 :green_heart: | javac | 6m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 23m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 38s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 111m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7912 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 9af82b84cde2 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / be2c9c5dfefc7f4ad7591c58f7857b177bef5b0e | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | [CI_URL] | | Max. process+thread count | 3150 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-28T13:24:50.096+0000"}, {"author": "ASF GitHub Bot", "body": "leiwen2025 commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3409341544 @steveloughran @ayushtkn Hi, this PR has been open for a while. Could you please take a look when you have time? Thanks!", "created": "2025-10-16T06:18:04.852+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3416334594 It's been a long time since I did C code, I'll have to stare at this a while. In #7903 that creation of the file bulk_crc32_riscv.c should be what the new code goes into; work with @PeterPtroc to get something you are both happy with *and tested*. Having two people who are set up to build and test this on riscv hardware is exactly what we need to get this done", "created": "2025-10-17T16:46:45.561+0000"}, {"author": "Steve Loughran", "body": "how does this differ from HADOOP-19655? I think that PR got there just before this one, so takes precedence number-wise; I'm not in a position to evaluate the code. Can you and [~peterptroc] collaborate on this? I'll give you both credit in the patches", "created": "2025-10-17T16:49:52.534+0000"}, {"author": "ASF GitHub Bot", "body": "leiwen2025 commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3448014826 @steveloughran Thanks for the review! I'll be happy to work with @PeterPtroc on this. If someone in the community could help connect me with Peter, that'd be much appreciated. I'd like to coordinate testing and integration on RISC-V.", "created": "2025-10-26T04:52:16.381+0000"}, {"author": "Lei Wen", "body": "Thanks! This patch implements CRC32 on RISC-V using Zvbc vector instructions, while HADOOP-19655 uses Zbc scalar ones. Happy to work together with [~peterptroc] to coordinate and test both implementations.", "created": "2025-10-26T05:02:39.383+0000"}], "derived_tasks": {"summary": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension - This PR implements vector-accelerated CRC32 using the RISC-V V,...", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19654", "title": "Upgrade AWS SDK to 2.35.4", "description": "Upgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "status": "In Progress", "priority": "Major", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "created": "2025-08-18T16:47:04.000+0000", "updated": "2025-10-25T09:26:53.000+0000", "labels": ["pull-request-available"], "components": ["build", "fs/s3"], "comments": [{"author": "ASF GitHub Bot", "body": "steveloughran opened a new pull request, #7882: URL: https://github.com/apache/hadoop/pull/7882 ### How was this patch tested? Testing in progress; still trying to get the ITests working. JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-08-18T18:33:54.643+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201364651 > JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. I found `hadoop-client-runtime` and `hadoop-client-minicluster` broken during integration with Spark, HADOOP-19652 plus YARN-11824 recovers that, is it the same issue?", "created": "2025-08-19T16:09:58.714+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201641390 @pan3793 maybe. what is unrelated is out the box the SDK doesn't do bulk delete with third party stores which support it (Dell ECS). ```", "created": "2025-08-19T17:45:40.582+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201646178 @pan3793 no, it's lifecycle related. Test needs to set up that minicluster before the test cases. and that's somehow not happening", "created": "2025-08-19T17:47:23.055+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3209266234 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 49s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 34m 10s | | trunk passed | | +1 :green_heart: | compile | 17m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 24s | | trunk passed | | +1 :green_heart: | javadoc | 9m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 65m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 15s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 7s | | the patch passed | | +1 :green_heart: | compile | 15m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 9s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 18s | | the patch passed | | +1 :green_heart: | mvnsite | 18m 45s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 66m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 368m 52s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 19s | | The patch does not generate ASF License warnings. | | | | 735m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux cb65e960fd1f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0d3f20b487ebe8cca5f4b91a3197d7e6cc639901 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3658 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-21T07:00:51.417+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3221833528 regressions ## everywhere No logging. Instead we get ``` SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\". SLF4J: Defaulting to no-operation MDCAdapter implementation. SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ``` `ITestS3AContractAnalyticsStreamVectoredRead` failures -stream closed. more on this once I've looked at it. If it is an SDK issue, major regression, though it may be something needing changes in the aal libary ## s3 express ``` [ERROR] ITestTreewalkProblems.testDistCp:319->lambda$testDistCp$3:320 [Exit code of distcp -useiterator -update -delete -direct s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/src s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/dest] ``` assumption: now that the store has lifecycle rules, you don't get prefix listings when there's an in-progress upload. Fix: change test but also path capability warning of inconsistency. this is good. Operation costs/auditing count an extra HTTP request, so cost tests fail. I suspect it is always calling CreateSession, but without logging can't be sure", "created": "2025-08-25T21:42:37.666+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3222650336 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 57s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 39s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 16s | | trunk passed | | +1 :green_heart: | javadoc | 9m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 65m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 3s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 0s | | the patch passed | | +1 :green_heart: | compile | 15m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 22s | | the patch passed | | +1 :green_heart: | compile | 13m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 52s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 11s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 18m 51s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 53s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 65m 59s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 371m 3s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 17s | | The patch does not generate ASF License warnings. | | | | 733m 32s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestRollingUpgrade | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 880f3cb624ae 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5b9a7e32525c27e876698f49e88ab520eae2d8c4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3821 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-26T05:14:11.359+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3296808329 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 31s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 24m 17s | | trunk passed | | +1 :green_heart: | compile | 9m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 0s | | trunk passed | | +1 :green_heart: | mvnsite | 19m 57s | | trunk passed | | +1 :green_heart: | javadoc | 5m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 11s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 40m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 40s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 52s | | the patch passed | | +1 :green_heart: | compile | 8m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 3s | | the patch passed | | +1 :green_heart: | compile | 7m 24s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 24s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 54s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 11m 32s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 26s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 7s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 39m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 678m 20s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 913m 40s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 113d355d9ed2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cc31e5be98b54ee418f5ddad4696de2d40e099a0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 4200 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-16T09:01:49.711+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3298415046 Thanks @steveloughran, PR looks good overall. Are then failures in `ITestS3AContractAnalyticsStreamVectoredRead` intermittent? I've not been able to reproduce, am running the test on this SDK upgrade branch.", "created": "2025-09-16T12:20:41.743+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on code in PR #7882: URL: https://github.com/apache/hadoop/pull/7882#discussion_r2352378026 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java: ########## @@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable { // close the stream, should throw RemoteFileChangedException RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close); - assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); + verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); Review Comment: do you know what the difference is with the other tests here? As in, why with S3 express is it ok to assert that we'll get a 412, whereas the others tests will throw a 200? ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java: ########## @@ -203,7 +206,7 @@ protected Configuration createValidRoleConf() throws JsonProcessingException { conf.set(ASSUMED_ROLE_SESSION_DURATION, \"45m\"); // disable create session so there's no need to // add a role policy for it. - disableCreateSession(conf); + //disableCreateSession(conf); Review Comment: nit: can just cut this instead of commenting it out, since we're skipping these tests if S3 Express is enabled", "created": "2025-09-16T12:42:45.837+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3301096683 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 23m 50s | | trunk passed | | +1 :green_heart: | compile | 8m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 30s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 14m 30s | | trunk passed | | +1 :green_heart: | javadoc | 5m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 5s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 38m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 26s | | the patch passed | | +1 :green_heart: | compile | 8m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 17s | | the patch passed | | +1 :green_heart: | compile | 7m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 15s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 58s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 12m 9s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 27s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 4s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 38m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 678m 56s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 11s | | The patch does not generate ASF License warnings. | | | | 905m 0s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 3b890eb50412 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3351e41830fbc9230ffe18bd88bfc0e2a60b20bd | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 4379 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-17T03:12:40.463+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3304013014 I've attached a log of a test run against an s3 express bucket where the test `ITestAWSStatisticCollection.testSDKMetricsCostOfGetFileStatusOnFile()` is failing because the AWS SDK stats report 2 http requests for the probe. I'd thought it was create-session related but it isn't: it looks like somehow the stream is broken. This happens reliably on every test runs. The relevant stuff is at line 564 where a HEAD request fails because the stream is broken \"end of stream\". ``` 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-request: attempt=1; max=3[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=228a46bb1d008468d38afd0da0ed7b4c354ab12631a63bf4283cb23dc02527a3[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Connection: Keep-Alive[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(87)) - http-outgoing-1 << \"end of stream\" 2025-09-17 18:43:49,314 [setup] DEBUG awssdk.request (LoggerAdapter.java:debug(125)) - Retryable error detected. Will retry in 51ms. Request attempt number 1", "created": "2025-09-17T17:51:56.355+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #7882: URL: https://github.com/apache/hadoop/pull/7882#discussion_r2356314622 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java: ########## @@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable { // close the stream, should throw RemoteFileChangedException RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close); - assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); + verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); Review Comment: Hey, it's your server code. Go see.", "created": "2025-09-17T17:59:23.011+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3305933937 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 12m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 29s | | trunk passed | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 0s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 21m 27s | | trunk passed | | +1 :green_heart: | javadoc | 9m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 58s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 66m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 3s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 40m 59s | | the patch passed | | +1 :green_heart: | compile | 15m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 18s | | the patch passed | | +1 :green_heart: | compile | 13m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 50s | | the patch passed | | -1 :x: | blanks | 0m 1s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 10s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47) | | +1 :green_heart: | mvnsite | 19m 25s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 66m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 450m 14s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 21s | | The patch does not generate ASF License warnings. | | | | 832m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 40fa101aa5ab 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 661dc6e3caa66f1218db70d8e6959c2ee3cb0a87 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3559 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-18T07:51:09.155+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306500643 @steveloughran discovered completely by accident, but it's something to do with the checksumming code. If you comment out these lines: ``` // builder.addPlugin(LegacyMd5Plugin.create()); // do not do request checksums as this causes third-party store problems. // builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED); // response checksum validation. Slow, even with CRC32 checksums. // if (parameters.isChecksumValidationEnabled()) { // builder.responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED); // } ``` the test will pass. Could be something to do with s3Express not supporting md5, will look into it.", "created": "2025-09-18T09:38:36.643+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306741287 Specifically, it's this line: `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED);` that causes this. Comment that out, or change it to `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_SUPPORTED)`, it passes. My guess is it's something to do with S3 express not supporting MD5, but for operations where `RequestChecksumCalculation.WHEN_REQUIRED` is true, SDK calculates the m5 and then S3 express rejects it. Have asked the SDK team.", "created": "2025-09-18T10:37:54.521+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3307416441 ok, so maybe for s3express stores we don't do legacy MD5 plugin stuff all is good? 1. Does imply the far end is breaking the connection when it is unhappy -at least our unit tests found this stuff before the cost of every HEAD doubles. 2. maybe we should make the choice of checksums an enum with md5 the default, so it is something that can be turned off/changed in future. While on the topic of S3 Express, is it now the case that because there's lifecycle rules for cleanup, LIST calls don't return prefixes of paths with incomplete uploads? If so I will need to change production code and the test -with a separate JIRA for that for completeness", "created": "2025-09-18T13:19:35.674+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3311549488 > for s3express stores we don't do legacy MD5 plugin stuff all is good @steveloughran confirming with the SDK team, since the MD5 plugin is supposed to restore previous behaviour, the server rejecting the first request seems wrong. let's see what they have to say. > LIST calls don't return prefixes of paths with incomplete uploads Will check with S3 express team on this", "created": "2025-09-19T10:04:35.168+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3312197290 thanks. I don't see it on tests against s3 with the 2.29.52 release, so something is changing with the requests made with new SDK + MD5 stuff.", "created": "2025-09-19T13:25:35.040+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3318915081 @steveloughran not able to narrow this error down just yet, it looks like it's a combination of S3A's configuration of the S3 client + these new Md5 changes. ``` @Test public void testHead() throws Throwable { // S3Client s3Client = getFileSystem().getS3AInternals().getAmazonS3Client(\"test instance\"); S3Client s3Client = S3Client.builder().region(Region.US_EAST_1) .addPlugin(LegacyMd5Plugin.create()) .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED) .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED) .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1))) .build(); s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"<>\").build()); } ``` I see the failure when the S3A client, and don't see it when I use a newly created client. So it's not just because of `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` Looking into it some more. S3 express team said there have been no changes in LIST behaviour.", "created": "2025-09-22T13:08:46.288+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3320014995 able to reproduce the issue outside of S3A. Basically did what would happen when you run a test in S3A: * a probe for the `test/` directory, and then create the `test/` directory, and then do the `headObject()` call. The head fails, but if you comment out `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` it works again. no idea what's going on. but have shared this local reproduction with SDK team. And rules out that it's something in the S3A code. ``` public class TestClass { S3Client s3Client; public TestClass() { this.s3Client = S3Client.builder().region(Region.US_EAST_1) .addPlugin(LegacyMd5Plugin.create()) .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED) .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED) .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1))) .build(); } public void testS3Express(String bucket, String key) { s3Client.listObjectsV2(ListObjectsV2Request.builder() .bucket(\"<>\") .maxKeys(2) .prefix(\"test/\") .build()); try { s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"test\") .build()); } catch (Exception e) { System.out.println(\"Exception thrown: \" + e.getMessage()); } s3Client.putObject(PutObjectRequest .builder() .bucket(\"<>\") .key(\"test/\").build(), RequestBody.empty()); s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"<>\") .build()); } ```", "created": "2025-09-22T16:22:45.459+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3325180154 well, nice and simple code snippet for the regression testing. Shows the value in having sdk metrics tied up...this is the only case which failed because it's the one asserting at the SDK level values.", "created": "2025-09-23T18:54:18.451+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3353451859 @ahmarsuhail is there a public sdk issue for this for me to link/track?", "created": "2025-09-30T19:06:46.002+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3355348352 Just created https://github.com/aws/aws-sdk-java-v2/issues/6459", "created": "2025-10-01T08:46:42.642+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3370618249 FYI @steveloughran , SDK team was able to root cause the issue, details here: https://github.com/aws/aws-sdk-java-v2/issues/6459#issuecomment-3362570846 Since it's a bit of an edge case, and the SDK retry means we recover from it anyway, you think we can go ahead with the upgrade or should we wait for the fix?", "created": "2025-10-06T09:07:35.668+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3405709735 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 27m 4s | | trunk passed | | +1 :green_heart: | compile | 15m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 3m 13s | | trunk passed | | -1 :x: | mvnsite | 10m 26s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 10s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 31m 2s | [/branch-spotbugs-root-warnings.html]([CI_URL] | root in trunk has 12 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 57m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 50s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 28m 35s | | the patch passed | | +1 :green_heart: | compile | 15m 6s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 6s | | the patch passed | | +1 :green_heart: | compile | 15m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 39s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 13s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47) | | -1 :x: | mvnsite | 7m 15s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 2s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 1 new + 43015 unchanged - 0 fixed = 43016 total (was 43015) | | +1 :green_heart: | javadoc | 8m 45s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 24s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 57m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 793m 21s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1110m 44s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 503d715744fe 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e319765ca591fc2a0968f3b2e900586bb46ce7c1 | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 3551 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-15T10:27:10.615+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3408678775 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 15 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 9s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 16m 19s | | trunk passed | | +1 :green_heart: | compile | 8m 32s | | trunk passed | | +1 :green_heart: | checkstyle | 1m 41s | | trunk passed | | -1 :x: | mvnsite | 6m 5s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 5s | | trunk passed | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 28s | [/branch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | shadedclient | 14m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 7s | | the patch passed | | +1 :green_heart: | compile | 8m 25s | | the patch passed | | +1 :green_heart: | javac | 8m 25s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 32s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 11 new + 47 unchanged - 6 fixed = 58 total (was 53) | | -1 :x: | mvnsite | 3m 45s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 4m 54s | [/results-javadoc-javadoc-root.txt]([CI_URL] | root generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 12s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 0m 26s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | spotbugs | 0m 17s | [/patch-spotbugs-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shadedclient | 13m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 227m 31s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 343m 23s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux f49b0547d834 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c2eb04aa497f8d4648a3b457a90843ce96abe7fe | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 5153 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-16T00:02:02.776+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412290514 @ahmarsuhail I'm handling the retries now by requiring the md5 plugin to be explicitly requested (i.e. third party stores); also making it easier to switch checksum generation from ALWAYS to WHEN_REQUESTED. So for AWS S3: stricter checksums, no md5. Other stores: configure it as needed. Still wondering if we should make this more automated, but not in a way which causes problems later. --- I am now seeing failings against s3 express ```", "created": "2025-10-16T18:32:37.931+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412377933 Now that 3rd party is good, I'm getting S3 express happy, mainly by test tuning. But many, many errors with vectored reads ``` [ERROR] Errors: [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead [INFO] Run 1: PASS [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAndReadFully [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220 \u00bb IO test/vectored_file.txt: Stream is closed! [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220 \u00bb IO test/vectored_file.txt: Stream is closed! [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadMultipleRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206 \u00bb Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206 \u00bb Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile \u00bb IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [INFO] ``` ``` [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection", "created": "2025-10-16T18:48:35.658+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412492676 @steveloughran just ran with the old 2.29.x SDK, failures there too. will look into it and fix", "created": "2025-10-16T19:12:49.318+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415388890 This is happening because those readVectored() tests create a new `vectored-read.txt` file on the setup() before each test. Since the tests are parameterized, they run twice, once for `direct-buffer` and then for `array-buffer`. On the first run for `direct-buffer`, a HEAD for the metadata is made and cached, and the data for `vectored-read.txt` is also cached. Then the stream is `closed()` and since the file ends in `.txt`, AAL clears the data cache. (since it's a sequential format, the chances there will be a backward seek and the same data will be accessed are low so it's better to clear the data cache). The metadata cache is not cleared here (it should be, and I will make that fix). On the second run for `array-buffer`, the `vectored-file` is written again. AAL will get the metadata from the metadata cache, and use that eTag when making the GETS for the block data. Since on S3 express, the eTag is no longer the md5 of the object content, even though the object content is the same, the eTag has changed. And hence the 412s on the GETS. On consistency with caching in general: * AAL provides a `metadatastore.ttl` config, set that to 0 and HEAD responses are never cached. This solves the caching issues we had when overwrite files before, as with that `ttl` 0 we will always get the latest version of the file. * Data blocks will be removed once memory usage is > defined memory threshold (2GB), and clean up happens every 5s by default. The edge case here is that what if data usage is always below 2GB, and data blocks never get evicted? This is why the `metadatastore.ttl` was introduced. * Our `BlockKey` which is the key under which file data is stored is a combination of the S3URI + eTag. If the eTag changes, then we'll have a different BlockKey, which means we don't have any data stored for it. For example: ``` * Data is written to A.parquet, etag is \"1234\". * A.parquet is read fully in to the cache, with key \"A.parquet + 1234\" * A.parquet is overwritten, etag is \"6789\". * A.parquet is opened for reading again: If metadata ttl has not yet expired, and metadata cache has eTag as `1234`, so AAL will return data from the data cache using key \"A.parquet + 1234\". If the requested data is not in the data cache, we'll make a GET with the outdated eTag as `1234` and this will fail with a 412. If metadata TTL has expired, a new HEAD request is made, and we now have the eTag `6789`, this will now create a new BlockKey \"A.parquet + 6789\", and since there is no data stored here, will make GETS for the data. ``` With this we ensure two things: 1/ Once a stream opened it will always serve bytes from the same object version, or fail. 2/ Data will be stale at maximum metadata.tll milliseconds, with the exception of stream's lifetime. Basically, if your data changes often, set the metadataTTL to 0, and AAL will always get the latest data. Otherwise we have eventually consistency.", "created": "2025-10-17T12:33:00.298+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415396379 The TLDR is: * I will make a small fix to clear the metadata cache on stream close for sequential formats (which fixes this issue) * Setting the `metadata.ttl` also fixes this issue. I've tested with both, and all `ITestS3AContractAnalyticsStreamVectoredRead` test cases pass.", "created": "2025-10-17T12:34:40.704+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416145759 good explanation. Though I would have expected a bit broader test coverage of your own stores; something to look for on the next library update. Can I also get improvements in error translation too -we need the error string including request IDs. Relying on the stack entry below to print it isn't enough, as deep exception nesting (hive, spark) can lose that.", "created": "2025-10-17T15:56:22.837+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416153341 one more thing here: make sure you can handle `null` as an etag in the cache. Not all stores have it, which is why it can be turned off for classic input stream version checking. You won't be able to detect overwrites, but we can just document having a short TTL here.", "created": "2025-10-17T15:59:00.358+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3422219254 @steveloughran updated exception handling: https://github.com/awslabs/analytics-accelerator-s3/pull/361, next release will have include the requestIDs in the message, eg: ```", "created": "2025-10-20T14:04:08.240+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428848006 latest iteration works with third party stores without MPU (so no magic or use of memory for upload buffering), or bulk delete. tested google gcs, only underful buffers which can be ignored. ``` [ERROR] Failures: [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:108->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <533> [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferAfterRead:61->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <533> [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferBeforeRead:71->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <539> [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferOnClosedFile:91->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: <1024> but was: <539> [INFO] [ERROR] Tests run: 1253, Failures: 4, Errors: 0, Skipped: 450 [INFO] ```", "created": "2025-10-21T18:51:58.062+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428880917 @ahmarsuhail I think Apache Ozone is the one. I just added an `etag` command to cloudstore to print this stuff out and experimented with various stores: https://github.com/steveloughran/cloudstore/blob/main/src/main/site/etag.md dell ECS and Google both supply etags. We don't retrieve them for directory markers anyway, which isn't an issue * I've updated the third-party docs to cover etags in more detail, and say \"switch to classic and disable version checking\" * I do think the cache needs to handle null/empty string tags, somehow. Certainly by not caching metadata.", "created": "2025-10-21T18:56:28.823+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3432106892 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 2s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 38 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 43s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 3s | | trunk passed | | +1 :green_heart: | compile | 15m 53s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 41s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 3m 15s | | trunk passed | | -1 :x: | mvnsite | 10m 51s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 36s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 31s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 18s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html]([CI_URL] | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 36m 27s | [/branch-spotbugs-root-warnings.html]([CI_URL] | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 61m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 48s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 27m 26s | | the patch passed | | +1 :green_heart: | compile | 14m 53s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 53s | | the patch passed | | +1 :green_heart: | compile | 15m 24s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 24s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 15s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85) | | -1 :x: | mvnsite | 7m 2s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 37s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184) | | -1 :x: | javadoc | 8m 41s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 22s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 62m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 741m 49s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1085m 5s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 8fbc6faf5962 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1cd8a2820c4aadeca61f3a7449c7d98fd34bb9d8 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 3717 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-22T12:25:28.756+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3434634222 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 39 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 18s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 15s | | trunk passed | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | -1 :x: | mvnsite | 6m 10s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 31s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html]([CI_URL] | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 18m 42s | [/branch-spotbugs-root-warnings.html]([CI_URL] | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 11s | | the patch passed | | +1 :green_heart: | compile | 7m 52s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 7m 52s | | the patch passed | | +1 :green_heart: | compile | 8m 17s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 17s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 33s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85) | | -1 :x: | mvnsite | 3m 43s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 5m 23s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184) | | -1 :x: | javadoc | 4m 38s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 12s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 32m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 594m 47s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 779m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.server.federation.router.async.TestRouterAsyncRpcClient | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux c90f744f2220 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 96c3f38ea5e033636e1acdb8fe2ed4b398bedb08 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 4624 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T00:16:22.819+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3437736945 fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`) ``` [ERROR] ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [INFO] [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13 [INFO] ``` This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h When these uploads fail we do leave incomplete uploads in progress: ``` Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found. ``` Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed. ``` [ERROR] ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190 \u00bb AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) ``` Yet the uploads list afterwards finds it ``` job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 ``` I have to conclude that the list of pending uploads was briefly offline/inconsistent. This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute. Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\".", "created": "2025-10-23T15:44:27.902+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3441063874 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 39 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 48s | | trunk passed | | +1 :green_heart: | compile | 8m 10s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 15s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 36s | | trunk passed | | -1 :x: | mvnsite | 6m 44s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 35s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 51s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html]([CI_URL] | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 19m 4s | [/branch-spotbugs-root-warnings.html]([CI_URL] | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 34m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 16m 48s | | the patch passed | | +1 :green_heart: | compile | 8m 49s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 49s | | the patch passed | | +1 :green_heart: | compile | 9m 26s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 41s | | root: The patch generated 0 new + 79 unchanged - 6 fixed = 79 total (was 85) | | -1 :x: | mvnsite | 4m 11s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 17s | | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184) | | +1 :green_heart: | javadoc | 4m 44s | | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015) | | +0 :ok: | spotbugs | 0m 11s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 34m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 591m 32s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 51s | | The patch does not generate ASF License warnings. | | | | 781m 38s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 54d25015775c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fa906dcf97ff8829f50184906bd7433bc2a0a73a | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 4675 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-24T04:55:19.546+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444243683 OK, this is all related to checksums on multipart puts. If you declare that checksums are always required on requests, you MUST define a checksum algorithm to use for multipart put, otherwise upload completions fail. I have no idea why, will file some SDK bug report to say \"this is wrong\" and simply change our settings to - checksums NOT always required - MD5 always enabled - checksum algorithm is CRC32C (will test with third party store) checksums in MPUs breaks a couple of the multipart uploader tests; more worried that about a ITestS3AOpenCost test failing with checksum verification being enabled (slow, expensive). I need to make sure that this is not an SDK regression.", "created": "2025-10-24T17:46:39.828+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444263138 Its a change in the default value: downloads have checksums verified unless you say \"no\". we say no.", "created": "2025-10-24T17:53:50.702+0000"}, {"author": "Steve Loughran", "body": "AWS SDK issue #6518 shows how checksum generation on uploaded data (fs.s3a.create.checksum) must be set if request checksum calculation is enabled (fs.s3a.checksum.generation) Checksum validation has also been enabled by default; {{ITestS3AOpenCost.testStreamIsNotChecksummed()}} caught that change. It looks like the SDK has really embraced checksums, which first broke compatibility with other stores, but which has also surfaced problems within their own code. All checksum logic will be off by default; MD5 headers will be attached now", "created": "2025-10-24T20:01:02.429+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3446382053 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 44 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 1s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 50s | | trunk passed | | +1 :green_heart: | compile | 8m 12s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 36s | | trunk passed | | -1 :x: | mvnsite | 5m 54s | [/branch-mvnsite-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 18s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 41s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 16s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 27s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html]([CI_URL] | hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings. | | -1 :x: | spotbugs | 0m 39s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html]([CI_URL] | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 18m 32s | [/branch-spotbugs-root-warnings.html]([CI_URL] | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 37s | | the patch passed | | +1 :green_heart: | compile | 8m 6s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 6s | | the patch passed | | +1 :green_heart: | compile | 8m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 20s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 6 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 30s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 6 new + 83 unchanged - 6 fixed = 89 total (was 89) | | -1 :x: | mvnsite | 3m 47s | [/patch-mvnsite-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 16s | | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184) | | +1 :green_heart: | javadoc | 4m 41s | | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015) | | +0 :ok: | spotbugs | 0m 11s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 32m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 587m 3s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 770m 3s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint shellcheck shelldocs | | uname | Linux cc0336ad4f43 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 149e98291ada965d1dc2b85b4214f235bb4b5c5d | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 4586 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-25T09:26:53.461+0000"}], "derived_tasks": {"summary": "Upgrade AWS SDK to 2.35.4 - Upgrade to a recent version of 2", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-19638", "title": "[JDK17] Set Up CI Support JDK17 & JDK21", "description": "Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project\u2019s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "status": "Resolved", "priority": "Major", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "created": "2025-07-23T13:49:51.000+0000", "updated": "2025-10-23T00:11:53.000+0000", "labels": ["pull-request-available"], "components": ["build"], "comments": [{"author": "ASF GitHub Bot", "body": "slfan1989 opened a new pull request, #7831: URL: https://github.com/apache/hadoop/pull/7831 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19638. [JDK17] Set Up CI Support JDK17. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-07-26T02:10:38.048+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121008019 @GauthamBanasandra I\u2019m working on adding a new pipeline in Trunk to support JDK17. While reviewing the history, I noticed you've done in-depth work on this area, particularly around the upgrade to `jenkins.sh` As part of this change, I reviewed the implementation in HADOOP-16888(#2012) to better understand how JDK11 support was introduced. From what I can tell, it seems that enabling JDK17 unit test support only requires configuring the appropriate JDK17-related variables in `jenkins.sh`, similar to how JDK11 was handled. Could you please confirm if that's sufficient? Let me know if there are any additional compatibility steps or considerations I should be aware of. Thank you very much!", "created": "2025-07-26T02:20:13.780+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121011939 @ayushtkn @aajisaka Could you kindly provide some help and guidance? I would really appreciate it.", "created": "2025-07-26T02:22:11.995+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121183709 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 30m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 30m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 30m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 105m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux b2225bbcc657 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0024f51e6ba35cbe31db45afab54cb9a1521708d | | Test Results | [CI_URL] | | Max. process+thread count | 548 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-07-26T03:57:51.871+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3122378936 @slfan1989 yes, what you've done in this PR should be sufficient to get the multi-JDK tests running. The multi-JDK unit tests are only run as part of the nightly CI and not for PRs. Could you please trigger on nightly CI run on ci-hadoop.apache.org? Let me know if you need me to do this.", "created": "2025-07-26T18:48:46.606+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3123880810 > @slfan1989 yes, what you've done in this PR should be sufficient to get the multi-JDK tests running. The multi-JDK unit tests are only run as part of the nightly CI and not for PRs. Could you please trigger on nightly CI run on ci-hadoop.apache.org? > > Let me know if you need me to do this. > @slfan1989 yes, what you've done in this PR should be sufficient to get the multi-JDK tests running. The multi-JDK unit tests are only run as part of the nightly CI and not for PRs. Could you please trigger on nightly CI run on ci-hadoop.apache.org? > > Let me know if you need me to do this. Thank you very much for your reply! I'm not very familiar with how to trigger a JDK17 build. If you happen to know how, would you mind helping trigger a JDK17 build for this PR? Also, if we want the PR builds to run under the JDK17 environment as well, do you know how we can configure that?", "created": "2025-07-27T04:25:19.923+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3123896957 @steveloughran @ayushtkn @Hexiaoqiao Is it possible to remove JDK 8 compilation support from the trunk? We\u2019d appreciate your input.", "created": "2025-07-27T04:35:36.862+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3127566790 @slfan1989 I just checked the nightly pipeline for Linux and it looks like it's isn't using `jenkins.sh` anymore. The `jenkins.sh` is currently used by the nightly pipeline for Windows and the pre-commit pipeline for Linux. Anyway, I just created a new pipeline that runs on JDK 17 - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java17-linux-x86_64/. You should be having the permission to trigger the builds. I've currently configured it to checkout your repo (I've mentioned the same in the pipeline's description page). I'll reconfigure it to github.com/apache/hadoop once you're done with the testing.", "created": "2025-07-28T14:43:44.604+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3127664208 I've started a run with JDK 17 on that pipeline - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java17-linux-x86_64/1/console.", "created": "2025-07-28T15:05:34.721+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3130322461 @GauthamBanasandra Thank you very much for your kind help in configuring this pipeline!", "created": "2025-07-29T02:07:13.039+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3131923151 Looks like the Jenkins CI had some trouble with the docker container when it was running the unit tests - ``` cd /home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java17-linux-x86_64/sourcedir/hadoop-common-project/hadoop-auth /usr/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java17-linux-x86_64/yetus-m2/hadoop-HADOOP-19638-full-2 -DskipTests test-compile spotbugs:spotbugs -DskipTests=true > /home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java17-linux-x86_64/out/branch-spotbugs-hadoop-common-project_hadoop-auth.txt 2>&1 Cleaning up docker image used for testing. Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? ============================================================================ ============================================================================ Finished build. ============================================================================ ============================================================================ ``` I've started another run - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java17-linux-x86_64/2/console", "created": "2025-07-29T10:59:28.458+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3133728324 > @steveloughran @ayushtkn @Hexiaoqiao @GauthamBanasandra Is it possible to remove JDK 8 compilation support from the trunk? We\u2019d appreciate your input. @slfan1989 I don't think we should remove JDK 8 support yet.", "created": "2025-07-29T19:13:09.848+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3135759641 I don't think we can maintain so many JDK versions. There would be some thirdparty dependencies which we might upgrade to & they would be compiled on higher JDK versions. So, the moment that happens, JDK-8 compilation will break. & Not upgrading to those dependencies or not using higher JDK features doesn't make sense. Hadoop in branch-3 dropped JDK-7 support as well, branch-2 only has JDK-7 IIRC. We haven't marked the lower branches which are JDK-8 compliant as EOL, so, it isn't a deal breaker for someone who wants to stick to JDK-8, Moreover we can't run tests & all on all JDK versions, We need to pick one only. I think most of the projects are dropping the JDK-8 support and chasing for higher versions. I know in Hive it is like 4.0 line -JDK-8, 4.1.0 line - JDK-17 & now master branch min JDK-21 compile time, same with Tez the master is on JDK-21 compile time support. I don't think sticking to any legacy stuff is worth it if it adds any technical debt for us, Should chase for a higher version. Just my opinion, other might feel differently", "created": "2025-07-30T10:41:37.713+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3137400811 @GauthamBanasandra 3.5 is java17+ only. there are things we depend on which are java11; there are places where we can't make use of the JDK features, let alone language changes. we have stayed on java8 for longer than we've ever been on any older java version. Time to move on. If you want to keep java8 support alive, then backport new features to branch-3.4. Me? I'm looking at bug fixes only there.", "created": "2025-07-30T18:24:14.531+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3137407351 ...assuming we switch now to java17 on trunk, all backports to branch-3.4 must be retested on that branch in case something in the JDK itself changed. With the JUnit move that's needed anyway, IMO", "created": "2025-07-30T18:26:29.242+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3138524338 @GauthamBanasandra @ayushtkn @steveloughran Thank you all for the feedback! From my perspective, it makes sense to drop support for JDK 8, and I also agree with using JDK 17 for the trunk branch. However, there are still some issues with the JDK 17 pipeline compilation, and I will work on fixing the related unit tests.", "created": "2025-07-31T04:28:44.124+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3146156723 I have adjusted the trunk branch's build to default to JDK 17 support.", "created": "2025-08-02T02:51:17.464+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3146204334 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 29s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 30m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 30m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 105m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 459b670a40fa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 577479ba84422a763054b1f4684b0a8fa23b3362 | | Test Results | [CI_URL] | | Max. process+thread count | 547 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-02T04:29:33.423+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3192520796 @slfan1989 could you please let us know at what point does the compilation with JDK 8 breaks?", "created": "2025-08-15T19:28:36.371+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3236498588 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 44s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 58s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 42s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 29m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 29m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 122m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 97acd1950f16 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f8e749693777c3816e6262928a717ccbf70921c4 | | Test Results | [CI_URL] | | Max. process+thread count | 611 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-29T10:10:25.350+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7831: URL: https://github.com/apache/hadoop/pull/7831#discussion_r2314272985 ########## dev-support/jenkins.sh: ########## @@ -124,9 +124,8 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 8 and 11 - YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-8-openjdk-amd64\") - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-11-openjdk-amd64\") + # test with Java 17 + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") Review Comment: now, all platforms on the trunk branch have `java-17-openjdk-amd64` installed in the docker image, it's time to move forward", "created": "2025-09-01T15:53:31.259+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3340964462 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 35m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 30m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 46s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 30m 5s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 109m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 8e7dd49e956b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / db6a4cb61b785de00ed82cd3e9957acab0746cc4 | | Test Results | [CI_URL] | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-27T01:26:52.273+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3393051785 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 20s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 35m 7s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 29m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 29m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 129m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7831 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux aefc94cb1d97 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3b2a01d785cffcf73c9ecca6a48caa45f9a101ae | | Test Results | [CI_URL] | | Max. process+thread count | 585 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-11T08:11:58.594+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #7831: URL: https://github.com/apache/hadoop/pull/7831", "created": "2025-10-12T00:55:03.019+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7831: URL: https://github.com/apache/hadoop/pull/7831#issuecomment-3393790285 @ayushtkn @steveloughran @GauthamBanasandra @cnauroth @pan3793 Merge this PR into the trunk branch to support pipeline compilation with JDK 17.", "created": "2025-10-12T00:58:42.839+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 opened a new pull request, #8030: URL: https://github.com/apache/hadoop/pull/8030 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR JIRA: HADOOP-19638. [Addendum] [JDK17] Set Up CI Support JDK17. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-13T01:56:45.031+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3395710867 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 9m 10s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 52s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 15m 51s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 13m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 19s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 1s | | No new issues. | | +1 :green_heart: | shadedclient | 13m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 62m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8030 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 8828e8951baa 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ac73fe197ca9f8418c9af6397d08903a130f4465 | | Test Results | [CI_URL] | | Max. process+thread count | 573 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-13T03:06:11.094+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3395798651 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 22m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 52s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 32m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 136m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8030 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux d7c4c36f06f4 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ac73fe197ca9f8418c9af6397d08903a130f4465 | | Test Results | [CI_URL] | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-13T04:15:02.274+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3395810701 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 38s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 43s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 13m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 19s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 13m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 54m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8030 | | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs | | uname | Linux 0bd5ffd3e04f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2c677aca06d2cab666b3cdc4fc5031e5c794f12c | | Test Results | [CI_URL] | | Max. process+thread count | 729 (vs. ulimit of 5500) | | modules | C: U: | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-13T04:23:13.889+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3397990368 @cnauroth @szetszwo Could you please help review this PR? Thank you very much! We have confirmed that the current Hadoop version can be successfully compiled with both JDK 17 and JDK 21. I plan to add support for these two JDK versions.", "created": "2025-10-13T15:27:41.697+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3401292933 @steveloughran Could you please review this PR? Thank you very much!", "created": "2025-10-14T11:14:22.562+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3405488556 @Hexiaoqiao Could you please review this PR? Thank you very much!", "created": "2025-10-15T09:36:30.655+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on code in PR #8030: URL: https://github.com/apache/hadoop/pull/8030#discussion_r2433036912 ########## dev-support/jenkins.sh: ########## @@ -124,8 +124,9 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 17 - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") + # test with Java 17 & Java 21 + YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-17-openjdk-amd64\") + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64\") Review Comment: Do we need two jdk dirs? ``` --multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64,/usr/lib/jvm/java-17-openjdk-amd64 ``` https://yetus.apache.org/documentation/0.15.1/precommit/usage-intro/#multijdk", "created": "2025-10-15T15:31:39.561+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on code in PR #8030: URL: https://github.com/apache/hadoop/pull/8030#discussion_r2433097240 ########## dev-support/jenkins.sh: ########## @@ -124,8 +124,9 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 17 - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") + # test with Java 17 & Java 21 + YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-17-openjdk-amd64\") + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64\") Review Comment: Thank you very much for helping with the review! Since I\u2019m not yet very familiar with Yetus, the current configuration was created by referring to and simulating the original JDK 8 and JDK 11 configurations. ``` # test with Java 8 and 11 YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-8-openjdk-amd64\") YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-11-openjdk-amd64\") YETUS_ARGS+=(\"--multijdktests=compile\") ```", "created": "2025-10-15T15:46:13.633+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on code in PR #8030: URL: https://github.com/apache/hadoop/pull/8030#discussion_r2433168273 ########## dev-support/jenkins.sh: ########## @@ -124,8 +124,9 @@ function run_ci() { YETUS_ARGS+=(\"--mvn-custom-repos\") YETUS_ARGS+=(\"--dockermemlimit=22g\") - # test with Java 17 - YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-17-openjdk-amd64\") + # test with Java 17 & Java 21 + YETUS_ARGS+=(\"--java-home=/usr/lib/jvm/java-17-openjdk-amd64\") + YETUS_ARGS+=(\"--multijdkdirs=/usr/lib/jvm/java-21-openjdk-amd64\") Review Comment: I see. Let's keep using it then.", "created": "2025-10-15T16:03:23.975+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #8030: URL: https://github.com/apache/hadoop/pull/8030", "created": "2025-10-16T14:09:04.163+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8030: URL: https://github.com/apache/hadoop/pull/8030#issuecomment-3411088399 @szetszwo @Hexiaoqiao @zhtttylz Thank you very much for reviewing the code!", "created": "2025-10-16T14:09:24.496+0000"}], "derived_tasks": {"summary": "[JDK17] Set Up CI Support JDK17 & JDK21 - Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to u...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19624", "title": "[Bug Report] Thread leak in ABFS AbfsClientThrottlingAnalyzer", "description": "Bug reported by Matt over common-dev discussion. > What seems to be the issue is that the timer tasks are cleaned up but > the timer threads themselves are never actually cleaned up. This will > eventually lead to an OOM since nothing is collecting these. I was > able to reproduce this locally in 3.3.6 and 3.4.1 but I believe that > it would affect any version that relies on autothrottling for ABFS. > > I was also able to make a quick fix as well as confirm a workaround -- > the long term fix would be to include `timer.cancel()` and > `timer.purge()` in a method for AbfsClientThrottlingAnalyzer.java. The > short term workaround is to disable autothrottling and rely on Azure > to throttle the connections as needed with the below configuration.", "status": "Open", "priority": "Major", "reporter": "Anuj Modi", "assignee": null, "created": "2025-07-15T12:51:29.000+0000", "updated": "2025-10-22T04:35:39.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "mattkduran opened a new pull request, #7852: URL: https://github.com/apache/hadoop/pull/7852 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR The ABFS driver's auto-throttling feature (`fs.azure.enable.autothrottling=true`) creates Timer threads in AbfsClientThrottlingAnalyzer that are never properly cleaned up, leading to a memory leak that eventually causes OutOfMemoryError in long-running applications like Hive Metastore. #### Impact: - Thread count grows indefinitely (observed >100,000 timer threads) - Affects any long-running service that creates multiple ABFS filesystem instances #### Root Cause: AbfsClientThrottlingAnalyzer creates Timer objects in its constructor but provides no mechanism to cancel them. When AbfsClient instances are closed, the associated timer threads continue running indefinitely. #### Solution Implement proper resource cleanup by making the throttling components implement Closeable and ensuring timers are cancelled when ABFS clients are closed. #### Changes Made 1. AbfsClientThrottlingAnalyzer.java - Added: implements Closeable - Added: close() method that calls timer.cancel() and timer.purge() - Purpose: Ensures timer threads are properly terminated when analyzer is no longer needed 2. AbfsThrottlingIntercept.java (Interface) - Added: extends Closeable - Added: close() method signature - Purpose: Establishes cleanup contract for all throttling intercept implementations 3. AbfsClientThrottlingIntercept.java - Added: close() method that closes both readThrottler and writeThrottler - Purpose: Coordinates cleanup of both read and write throttling analyzers 4. AbfsNoOpThrottlingIntercept.java - Added: No-op close() method - Purpose: Satisfies interface contract for no-op implementation 5. AbfsClient.java - Added: IOUtils.cleanupWithLogger(LOG, intercept) in existing close() method - Purpose: Integrates throttling cleanup into existing client resource management https://github.com/mattkduran/ABFSleaktest https://www.mail-archive.com/common-dev@hadoop.apache.org/msg43483.html ### How was this patch tested? #### Standalone Validation Tool This fix was validated using a standalone reproduction and testing tool that directly exercises the ABFS auto-throttling components outside of a full Hadoop deployment. Repository: [ABFSLeakTest](https://github.com/mattkduran/ABFSleaktest) #### Testing Scope - Problem reproduction confirmed - demonstrates the timer thread leak - Fix validation confirmed - proves close() method resolves the leak - Resource cleanup verified - shows proper timer cancellation - Limited integration testing - standalone tool, not full Hadoop test suite #### Test Results Leak Reproduction Evidence ``` # Without fix: Timer threads accumulate over filesystem creation cycles Cycle Total Threads ABFS Timer Threads Status 1 50->52 0->2 LEAK DETECTED 50 150->152 98->100 LEAK GROWING 200 250->252 398->400 LEAK CONFIRMED Final Analysis: 400 leaked timer threads named \"abfs-timer-client-throttling-analyzer-*\" Memory Impact: ~90MB additional heap usage # Direct analyzer testing: \ud83d\udd34 Without close(): +3 timer threads (LEAKED) \u2705 With close(): +0 timer threads (NO LEAK) ``` #### Test Environment - Java Version: OpenJDK 11.0.x - Hadoop Version: 3.3.6/3.4.1 (both affected) - Test Duration: 200 filesystem creation/destruction cycles - Thread Monitoring: JMX ThreadMXBean # Fix effectiveness: 100% - no threads leaked when close() called ### For code changes: - [ X ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-08-03T18:02:19.934+0000"}, {"author": "Matt Duran", "body": "[~anujmodi] Got everything set up! It looks like I don't have permission to assign this issue to myself. PR filed here: https://github.com/apache/hadoop/pull/7852", "created": "2025-08-03T18:02:48.722+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7852: URL: https://github.com/apache/hadoop/pull/7852#issuecomment-3148687502 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 41m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 21s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | -1 :x: | mvnsite | 0m 23s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 22s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 44m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 25s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 160m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7852 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 39b56ca5b682 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fee9861e3ff8d302a76812102c6c0cf38ebf37b5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-03T20:44:36.507+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #7852: URL: https://github.com/apache/hadoop/pull/7852#discussion_r2300374528 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingIntercept.java: ########## @@ -223,4 +224,18 @@ private static long getContentLengthIfKnown(String range) { } return contentLength; } + + /** + * Closes the throttling intercept and releases associated resources. + * This method closes both the read and write throttling analyzers. Review Comment: Nit: Javadoc to include @ throws ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsNoOpThrottlingIntercept.java: ########## @@ -40,4 +42,12 @@ public void updateMetrics(final AbfsRestOperationType operationType, public void sendingRequest(final AbfsRestOperationType operationType, final AbfsCounters abfsCounters) { } + +/** + * No-op implementation of close method. Review Comment: Nit: javadoc to include @ throws ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingAnalyzer.java: ########## @@ -172,6 +175,22 @@ public boolean suspendIfNecessary() { return false; } + /** + * Closes the throttling analyzer and releases associated resources. + * This method cancels the internal timer and cleans up any pending timer tasks. + * It is safe to call this method multiple times. Review Comment: Fix javadoc here a well. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsThrottlingIntercept.java: ########## @@ -26,7 +26,7 @@ */ @InterfaceAudience.Private @InterfaceStability.Unstable -public interface AbfsThrottlingIntercept { +public interface AbfsThrottlingIntercept extends Closable { Review Comment: Shouldn't this be extends `Closeable`?", "created": "2025-08-26T09:35:06.106+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7852: URL: https://github.com/apache/hadoop/pull/7852#issuecomment-3419108875 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 17s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 54s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 36s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 33m 4s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 46s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 42s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 42s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 25s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 6 new + 0 unchanged - 0 fixed = 6 total (was 0) | | +1 :green_heart: | mvnsite | 0m 47s | | the patch passed | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 1m 39s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 29s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 147m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7852 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b9c2f8e9e9ea 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 90abc25c0200fa1cb0252f1d34298ffc9fb1906b | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 585 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-19T01:01:03.096+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7852: URL: https://github.com/apache/hadoop/pull/7852#issuecomment-3419109315 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 42s | | trunk passed | | +1 :green_heart: | compile | 0m 46s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 50s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 54s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 51s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 42s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 42s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 26s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 6 new + 0 unchanged - 0 fixed = 6 total (was 0) | | +1 :green_heart: | mvnsite | 0m 45s | | the patch passed | | +1 :green_heart: | javadoc | 0m 34s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 1m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 31m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 30s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 148m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7852 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 940a3e5ad094 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 90abc25c0200fa1cb0252f1d34298ffc9fb1906b | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 585 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-19T01:02:04.471+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7852: URL: https://github.com/apache/hadoop/pull/7852#issuecomment-3419175534 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 47s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 26s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 31m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 6 new + 0 unchanged - 0 fixed = 6 total (was 0) | | +1 :green_heart: | mvnsite | 0m 41s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | spotbugs | 1m 26s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 21s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 123m 59s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7852 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e16a0317d906 5.15.0-157-generic #167-Ubuntu SMP Wed Sep 17 21:35:53 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 90b097b9584a889c2e0867851a6b6e489078d6aa | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 592 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-19T03:20:17.309+0000"}, {"author": "ASF GitHub Bot", "body": "matt-duran-starburst commented on code in PR #7852: URL: https://github.com/apache/hadoop/pull/7852#discussion_r2442738562 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingAnalyzer.java: ########## @@ -172,6 +175,22 @@ public boolean suspendIfNecessary() { return false; } + /** + * Closes the throttling analyzer and releases associated resources. + * This method cancels the internal timer and cleans up any pending timer tasks. + * It is safe to call this method multiple times. Review Comment: Updated! ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingIntercept.java: ########## @@ -223,4 +224,18 @@ private static long getContentLengthIfKnown(String range) { } return contentLength; } + + /** + * Closes the throttling intercept and releases associated resources. + * This method closes both the read and write throttling analyzers. Review Comment: Updated! ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsNoOpThrottlingIntercept.java: ########## @@ -40,4 +42,12 @@ public void updateMetrics(final AbfsRestOperationType operationType, public void sendingRequest(final AbfsRestOperationType operationType, final AbfsCounters abfsCounters) { } + +/** + * No-op implementation of close method. Review Comment: Updated!", "created": "2025-10-19T03:26:17.511+0000"}, {"author": "ASF GitHub Bot", "body": "matt-duran-starburst commented on code in PR #7852: URL: https://github.com/apache/hadoop/pull/7852#discussion_r2442738680 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsThrottlingIntercept.java: ########## @@ -26,7 +26,7 @@ */ @InterfaceAudience.Private @InterfaceStability.Unstable -public interface AbfsThrottlingIntercept { +public interface AbfsThrottlingIntercept extends Closable { Review Comment: Fixed", "created": "2025-10-19T03:26:37.568+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #7852: URL: https://github.com/apache/hadoop/pull/7852#issuecomment-3430462759 Spotbug issue might be due to this: https://issues.apache.org/jira/browse/HADOOP-19731 We can wait for the fix and merge with trunk once it gets resolved.", "created": "2025-10-22T04:35:39.440+0000"}], "derived_tasks": {"summary": "[Bug Report] Thread leak in ABFS AbfsClientThrottlingAnalyzer - Bug reported by Matt over common-dev discussion", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19622", "title": "ABFS: [ReadAheadV2] Implement Read Buffer Manager V2 with improved aggressiveness", "description": "", "status": "In Progress", "priority": "Major", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "created": "2025-07-15T05:42:16.000+0000", "updated": "2025-10-24T10:53:01.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "anujmodi2021 opened a new pull request, #7832: URL: https://github.com/apache/hadoop/pull/7832 ### Description of PR JIRA: https://issues.apache.org/jira/browse/HADOOP-19622 Implementing ReadBufferManagerV2 as per the new design document. Following capabilities are added to ReadBufferManager: 1. Configurable minimum and maximum number of prefetch threads. 2. Configurable minimum and maximum size of cached buffer pool 3. Dynamically adjusting thread pool size and buffer pool size based on workload requirement and resource utilization but within the limits defined by user. 4. Mapping prefetched data to file ETag so that multiple streams reading same file can share the cache and save TPS. For more details on design doc please refer to the design doc attached to parent JIRA: https://issues.apache.org/jira/browse/HADOOP-19596 ### How was this patch tested? TBA", "created": "2025-07-29T07:44:49.335+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3131423891 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 59s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 7 new + 3 unchanged - 9 fixed = 10 total (was 12) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 21m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 17s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 26s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 81m 35s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 06ad120d8f97 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 28cb97fde1eda2fa096401499e9ee1dbccbdef2b | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-07-29T09:07:28.936+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3145123984 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 5 new + 3 unchanged - 9 fixed = 8 total (was 12) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | shadedclient | 20m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 26s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 79m 28s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time Unsynchronized access at ReadBufferManagerV2.java:81% of time Unsynchronized access at ReadBufferManagerV2.java:[line 617] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c66f9b95f564 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7670846f293f7f629cacdd968ccc1cbd9e70ff25 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 709 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-01T16:26:49.551+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3148434367 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 10s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 25s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 51s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 7 new + 3 unchanged - 9 fixed = 10 total (was 12) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | spotbugs | 0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | shadedclient | 20m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 22s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 24s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 84m 42s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferPool; locked 66% of time Unsynchronized access at ReadBufferManagerV2.java:66% of time Unsynchronized access at ReadBufferManagerV2.java:[line 361] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time Unsynchronized access at ReadBufferManagerV2.java:81% of time Unsynchronized access at ReadBufferManagerV2.java:[line 634] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f39a03e570dd 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6a686866e701665a2cb897afdf6470fe9b636ef5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-03T13:45:54.518+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3148480004 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 24s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 8 new + 3 unchanged - 9 fixed = 11 total (was 12) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | javadoc | 0m 20s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | spotbugs | 0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | shadedclient | 20m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 21s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 25s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 76m 28s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferPool; locked 66% of time Unsynchronized access at ReadBufferManagerV2.java:66% of time Unsynchronized access at ReadBufferManagerV2.java:[line 359] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time Unsynchronized access at ReadBufferManagerV2.java:81% of time Unsynchronized access at ReadBufferManagerV2.java:[line 632] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6003c20c0979 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9552072e0e9e4addbb5f7ccb6f9109332fa47f2c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-03T15:00:22.121+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3150755978 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 9 fixed = 13 total (was 12) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 29s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | javadoc | 0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | spotbugs | 1m 11s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | shadedclient | 35m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 47s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 39s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 124m 32s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time Unsynchronized access at ReadBufferManagerV2.java:81% of time Unsynchronized access at ReadBufferManagerV2.java:[line 635] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 4fd3a0fe88be 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0c368d836c420693f2467e8aeb52f25a5f53e1bd | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 703 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-04T13:36:43.533+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3154838752 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 30m 26s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 19s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 18s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 18s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 16s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 16s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 9 fixed = 13 total (was 12) | | -1 :x: | mvnsite | 0m 18s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | spotbugs | 0m 19s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 22m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 26s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 79m 59s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 9480f2b5a408 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a894d06c0b1b783fa1228997c65f5b0544285662 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-05T11:34:35.034+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3159934862 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 11 new + 3 unchanged - 9 fixed = 14 total (was 12) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | javadoc | 0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10) | | -1 :x: | spotbugs | 1m 10s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 5 new + 0 unchanged - 0 fixed = 5 total (was 0) | | +1 :green_heart: | shadedclient | 35m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 47s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 124m 55s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time Unsynchronized access at ReadBufferManagerV2.java:81% of time Unsynchronized access at ReadBufferManagerV2.java:[line 640] | | | Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.cpuMonitorThread from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init() At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init() At ReadBufferManagerV2.java:[line 179] | | | Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.memoryMonitorThread from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init() At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init() At ReadBufferManagerV2.java:[line 157] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux f0bb66ee2300 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b4d81832a01395287563a3cba721b500dccfafa5 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 709 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-06T12:19:35.254+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3164166890 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 9 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 49s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 8 new + 5 unchanged - 9 fixed = 13 total (was 14) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 42s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | -1 :x: | shadedclient | 25m 8s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 24s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 25s | | ASF License check generated no output? | | | | 87m 38s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux ae06a4383cc7 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d258d90f06489e5e4f2316a000b9d985cd572577 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-07T13:17:11.341+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3168134181 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 22s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | trunk passed | | +1 :green_heart: | shadedclient | 34m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 9 new + 5 unchanged - 9 fixed = 14 total (was 14) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 1m 10s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 35m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 51s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 125m 1s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux f6be6af2509d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2444f92a353dacf5cced826c51e181b696e424d7 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 708 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-08T14:24:50.488+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3195330138 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 36s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 48s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 0s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 43s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 20m 54s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 33s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 24s | | The patch does not generate ASF License warnings. | | | | 86m 15s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5e04ca0b1e01 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a6064efe52a45fe05fe92bae1e97c5131085f74f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 567 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-18T06:42:06.039+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3201235322 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 5s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 55s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 8s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 21m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 79m 57s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ed73b0a40bbd 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 12d56c451aa41ac9ed1c6ec87e4c185f753ad956 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-19T15:30:21.139+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3219871750 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 23s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 28s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 23m 42s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | shadedclient | 21m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 80m 12s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Unread field:field be static? At ReadBufferManagerV2.java:[line 66] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f29ebc7d545e 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0376544b9ac0dc8a4741f6b6350b197b66d3e25f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-25T11:19:09.517+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3220266799 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 5s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 17s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 12 new + 5 unchanged - 9 fixed = 17 total (was 14) | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 43s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | shadedclient | 23m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 22s | | The patch does not generate ASF License warnings. | | | | 82m 52s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Unread field:field be static? At ReadBufferManagerV2.java:[line 66] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3532fd079953 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a203e3d3ed6df6cdcca26db4f21b012fe7511bb8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-25T13:24:50.834+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3220297099 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 29m 15s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 24s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 41s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 53s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 12 new + 5 unchanged - 9 fixed = 17 total (was 14) | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 43s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | shadedclient | 22m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 29s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 84m 6s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Unread field:field be static? At ReadBufferManagerV2.java:[line 66] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 235d7a28a991 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 08ea09b7d98df7e1b50b99b364b64d8958822cde | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-25T13:31:39.955+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3241001764 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 9 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 9s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 6s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 19s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 49s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | shadedclient | 20m 58s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 86m 33s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | | Unread field:field be static? At ReadBufferManagerV2.java:[line 66] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 445fc7cc9c6d 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 28a2c82ca44ef89a13a7ba7a1745a03a8c458950 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-01T06:21:16.329+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3242030746 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 9 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 49s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 8s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 21s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 34 new + 5 unchanged - 9 fixed = 39 total (was 14) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 20m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 29s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 81m 32s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 48c5c02b07a5 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cd8a2b9fa68545870c4b362191ab3999ecd28047 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 630 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-01T11:36:08.977+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3243169253 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 31s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 5s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 18s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 18 new + 5 unchanged - 9 fixed = 23 total (was 14) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 21m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 87m 35s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 094ad1010077 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a0f8021e0edaa0e5b62fbf27727a06c26e9cdc78 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-01T19:50:25.617+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3244159256 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 14s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 27s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 40s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 18 new + 5 unchanged - 9 fixed = 23 total (was 14) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 20m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 23s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 24s | | The patch does not generate ASF License warnings. | | | | 86m 48s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer At ReadBuffer.java:by returning ReadBuffer.buffer At ReadBuffer.java:[line 111] | | | org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer At ReadBuffer.java:[line 115] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d86dbba0e97b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7990ab04361b202bf6b731baf7194599beaa9101 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 567 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-02T07:39:43.382+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3245348841 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 6s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 8s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 20s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 2 new + 5 unchanged - 9 fixed = 7 total (was 14) | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 24s | | The patch does not generate ASF License warnings. | | | | 78m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 51751dc46640 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 98902b0ff5a75dccb704b866cdd12f40e5af7929 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 626 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-02T13:26:47.662+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3246049986 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 55s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 25s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 38s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 9 fixed = 11 total (was 14) | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 29s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 82m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2dca3625cc19 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c3acf9680516650709856c239f6c4a28d01ccd9e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-02T16:36:16.922+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7832: URL: https://github.com/apache/hadoop/pull/7832#issuecomment-3322556049 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 49m 55s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 37s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 35m 59s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 9 fixed = 11 total (was 14) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 138m 2s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7832 | | JIRA Issue | HADOOP-19622 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b9c041f528b7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a214133ad464435be643778dce6afff7f96b4d64 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 707 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-23T06:08:52.479+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459585691 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1469,6 +1504,18 @@ public boolean isReadAheadEnabled() { return this.enabledReadAhead; } + /** + * Checks if the read-ahead v2 feature is enabled by user. + * @return true if read-ahead v2 is enabled, false otherwise. + */ + public boolean isReadAheadV2Enabled() { + return this.isReadAheadV2Enabled; Review Comment: nit: this. not needed", "created": "2025-10-24T09:49:37.974+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459591012 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -128,13 +128,20 @@ public final class FileSystemConfigurations { public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120; public static final boolean DEFAULT_ENABLE_READAHEAD = true; - public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false; + public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true; Review Comment: should be made false as discussed for now", "created": "2025-10-24T09:51:08.760+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459592071 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -128,13 +128,20 @@ public final class FileSystemConfigurations { public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120; public static final boolean DEFAULT_ENABLE_READAHEAD = true; - public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false; + public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true; + public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true; Review Comment: same for this config", "created": "2025-10-24T09:51:28.270+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459660348 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -87,17 +111,53 @@ static ReadBufferManagerV2 getBufferManager() { } /** - * {@inheritDoc} + * Set the ReadBufferManagerV2 configurations based on the provided before singleton initialization. + * @param readAheadBlockSize the read-ahead block size to set for the ReadBufferManagerV2. + * @param abfsConfiguration the configuration to set for the ReadBufferManagerV2. + */ + public static void setReadBufferManagerConfigs(final int readAheadBlockSize, + final AbfsConfiguration abfsConfiguration) { + // Set Configs only before initializations. + if (bufferManager == null && !isConfigured) { + minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize(); + maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize(); + cpuMonitoringIntervalInMilliSec = abfsConfiguration.getReadAheadV2CpuMonitoringIntervalMillis(); + cpuThreshold = abfsConfiguration.getReadAheadV2CpuUsageThresholdPercent()/ ONE_HUNDRED; + threadPoolUpscalePercentage = abfsConfiguration.getReadAheadV2ThreadPoolUpscalePercentage(); + threadPoolDownscalePercentage = abfsConfiguration.getReadAheadV2ThreadPoolDownscalePercentage(); + executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMillis(); + + minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize(); Review Comment: As discussed earlier this should start with 8 threads which is the default in trunk today and can scale further", "created": "2025-10-24T10:11:43.289+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459676934 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { + int currentPoolSize = workerRefs.size(); + double cpuLoad = getCpuLoad(); + int requiredPoolSize = getRequiredThreadPoolSize(); + int newThreadPoolSize; + printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize); + if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) { + // Submit more background tasks. + newThreadPoolSize = Math.min(maxThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED)); + // Create new Worker Threads + for (int i = currentPoolSize; i < newThreadPoolSize; i++) { + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); + workerPool.submit(worker); + } + printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) { + newThreadPoolSize = Math.max(minThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED)); + // Signal the extra workers to stop + while (workerRefs.size() > newThreadPoolSize) { + ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1); + worker.stop(); + } + printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else { + printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize); + } + } + /** - * {@inheritDoc} + * Similar to System.currentTimeMillis, except implemented with System.nanoTime(). + * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization), + * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core. + * Note: it is not monotonic across Sockets, and even within a CPU, its only the + * more recent parts which share a clock across all cores. + * + * @return current time in milliseconds */ - @VisibleForTesting - @Override - public void callTryEvict() { - // TODO: To be implemented + private long currentTimeMillis() { + return System.nanoTime() / 1000 / 1000; + } + + private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) { + for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) { + ReadBuffer readBuffer = it.next(); + if (readBuffer.getStream() == stream) { + it.remove(); + // As failed ReadBuffers (bufferIndex = -1) are already pushed to free + // list in doneReading method, we will skip adding those here again. + if (readBuffer.getBufferindex() != -1) { + pushToFreeList(readBuffer.getBufferindex()); + } + } + } } /** - * {@inheritDoc} + * Test method that can clean up the current state of readAhead buffers and + * the lists. Will also trigger a fresh init. */ @VisibleForTesting @Override public void testResetReadBufferManager() { - // TODO: To be implemented + synchronized (this) { + ArrayList<ReadBuffer> completedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if (buf != null) { + completedBuffers.add(buf); + } + } + + for (ReadBuffer buf : completedBuffers) { + manualEviction(buf); + } + + getReadAheadQueue().clear(); + getInProgressList().clear(); + getCompletedReadList().clear(); + getFreeList().clear(); + for (int i = 0; i < maxBufferPoolSize; i++) { + bufferPool[i] = null; + } + bufferPool = null; + cpuMonitorThread.shutdownNow(); + memoryMonitorThread.shutdownNow(); + workerPool.shutdownNow(); + resetBufferManager(); + } } - /** - * {@inheritDoc} - */ @VisibleForTesting @Override - public void testResetReadBufferManager(final int readAheadBlockSize, - final int thresholdAgeMilliseconds) { - // TODO: To be implemented + public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) { + setReadAheadBlockSize(readAheadBlockSize); + setThresholdAgeMilliseconds(thresholdAgeMilliseconds); + testResetReadBufferManager(); } - /** - * {@inheritDoc} - */ - @Override - public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) { - // TODO: To be implemented + @VisibleForTesting + public void callTryEvict() { + tryEvict(); } - private final ThreadFactory namedThreadFactory = new ThreadFactory() { - private int count = 0; - @Override - public Thread newThread(Runnable r) { - return new Thread(r, \"ReadAheadV2-Thread-\" + count++); + @VisibleForTesting + public int getNumBuffers() { + LOCK.lock(); + try { + return numberOfActiveBuffers; + } finally { + LOCK.unlock(); } - }; + } @Override void resetBufferManager() { setBufferManager(null); // reset the singleton instance + setIsConfigured(false); } private static void setBufferManager(ReadBufferManagerV2 manager) { bufferManager = manager; } + + private static void setIsConfigured(boolean configured) { + isConfigured = configured; + } + + private final ThreadFactory workerThreadFactory = new ThreadFactory() { + private int count = 0; + @Override + public Thread newThread(Runnable r) { + Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++); + t.setDaemon(true); + return t; + } + }; + + private void printTraceLog(String message, Object... args) { + if (LOGGER.isTraceEnabled()) { + LOGGER.trace(message, args); + } + } + + private void printDebugLog(String message, Object... args) { + LOGGER.debug(message, args); + } + + @VisibleForTesting + double getMemoryLoad() { + MemoryMXBean osBean = ManagementFactory.getMemoryMXBean(); + MemoryUsage memoryUsage = osBean.getHeapMemoryUsage(); + return (double) memoryUsage.getUsed() / memoryUsage.getMax(); + } + + @VisibleForTesting + public double getCpuLoad() { + OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean( + OperatingSystemMXBean.class); + return osBean.getSystemCpuLoad(); Review Comment: can return negative values as well, should return 0 when nrgative", "created": "2025-10-24T10:17:08.678+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459687572 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { + int currentPoolSize = workerRefs.size(); + double cpuLoad = getCpuLoad(); + int requiredPoolSize = getRequiredThreadPoolSize(); + int newThreadPoolSize; + printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize); + if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) { + // Submit more background tasks. + newThreadPoolSize = Math.min(maxThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED)); + // Create new Worker Threads + for (int i = currentPoolSize; i < newThreadPoolSize; i++) { + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); + workerPool.submit(worker); + } + printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) { + newThreadPoolSize = Math.max(minThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED)); + // Signal the extra workers to stop + while (workerRefs.size() > newThreadPoolSize) { + ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1); + worker.stop(); + } + printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else { + printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize); + } + } + /** - * {@inheritDoc} + * Similar to System.currentTimeMillis, except implemented with System.nanoTime(). + * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization), + * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core. + * Note: it is not monotonic across Sockets, and even within a CPU, its only the + * more recent parts which share a clock across all cores. + * + * @return current time in milliseconds */ - @VisibleForTesting - @Override - public void callTryEvict() { - // TODO: To be implemented + private long currentTimeMillis() { + return System.nanoTime() / 1000 / 1000; + } + + private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) { + for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) { + ReadBuffer readBuffer = it.next(); + if (readBuffer.getStream() == stream) { + it.remove(); + // As failed ReadBuffers (bufferIndex = -1) are already pushed to free + // list in doneReading method, we will skip adding those here again. + if (readBuffer.getBufferindex() != -1) { + pushToFreeList(readBuffer.getBufferindex()); + } + } + } } /** - * {@inheritDoc} + * Test method that can clean up the current state of readAhead buffers and + * the lists. Will also trigger a fresh init. */ @VisibleForTesting @Override public void testResetReadBufferManager() { - // TODO: To be implemented + synchronized (this) { + ArrayList<ReadBuffer> completedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if (buf != null) { + completedBuffers.add(buf); + } + } + + for (ReadBuffer buf : completedBuffers) { + manualEviction(buf); + } + + getReadAheadQueue().clear(); + getInProgressList().clear(); + getCompletedReadList().clear(); + getFreeList().clear(); + for (int i = 0; i < maxBufferPoolSize; i++) { + bufferPool[i] = null; + } + bufferPool = null; + cpuMonitorThread.shutdownNow(); + memoryMonitorThread.shutdownNow(); + workerPool.shutdownNow(); + resetBufferManager(); + } } - /** - * {@inheritDoc} - */ @VisibleForTesting @Override - public void testResetReadBufferManager(final int readAheadBlockSize, - final int thresholdAgeMilliseconds) { - // TODO: To be implemented + public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) { + setReadAheadBlockSize(readAheadBlockSize); + setThresholdAgeMilliseconds(thresholdAgeMilliseconds); + testResetReadBufferManager(); } - /** - * {@inheritDoc} - */ - @Override - public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) { - // TODO: To be implemented + @VisibleForTesting + public void callTryEvict() { + tryEvict(); } - private final ThreadFactory namedThreadFactory = new ThreadFactory() { - private int count = 0; - @Override - public Thread newThread(Runnable r) { - return new Thread(r, \"ReadAheadV2-Thread-\" + count++); + @VisibleForTesting + public int getNumBuffers() { + LOCK.lock(); + try { + return numberOfActiveBuffers; + } finally { + LOCK.unlock(); } - }; + } @Override void resetBufferManager() { setBufferManager(null); // reset the singleton instance + setIsConfigured(false); } private static void setBufferManager(ReadBufferManagerV2 manager) { bufferManager = manager; } + + private static void setIsConfigured(boolean configured) { + isConfigured = configured; + } + + private final ThreadFactory workerThreadFactory = new ThreadFactory() { + private int count = 0; + @Override + public Thread newThread(Runnable r) { + Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++); + t.setDaemon(true); + return t; + } + }; + + private void printTraceLog(String message, Object... args) { + if (LOGGER.isTraceEnabled()) { + LOGGER.trace(message, args); + } + } + + private void printDebugLog(String message, Object... args) { + LOGGER.debug(message, args); + } + + @VisibleForTesting + double getMemoryLoad() { + MemoryMXBean osBean = ManagementFactory.getMemoryMXBean(); + MemoryUsage memoryUsage = osBean.getHeapMemoryUsage(); + return (double) memoryUsage.getUsed() / memoryUsage.getMax(); + } + + @VisibleForTesting + public double getCpuLoad() { + OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean( + OperatingSystemMXBean.class); + return osBean.getSystemCpuLoad(); + } + + @VisibleForTesting + public static ReadBufferManagerV2 getInstance() { + return bufferManager; + } + + @VisibleForTesting + public int getMinBufferPoolSize() { + return minBufferPoolSize; + } + + @VisibleForTesting + public int getMaxBufferPoolSize() { + return maxBufferPoolSize; + } + + @VisibleForTesting + public int getCurrentThreadPoolSize() { + return workerRefs.size(); + } + + @VisibleForTesting + public int getCpuMonitoringIntervalInMilliSec() { + return cpuMonitoringIntervalInMilliSec; + } + + @VisibleForTesting + public int getMemoryMonitoringIntervalInMilliSec() { + return memoryMonitoringIntervalInMilliSec; + } + + @VisibleForTesting + public ScheduledExecutorService getCpuMonitoringThread() { + return cpuMonitorThread; + } + + public int getRequiredThreadPoolSize() { + return (int) Math.ceil(THREAD_POOL_REQUIREMENT_BUFFER + * (getReadAheadQueue().size() + getInProgressList().size())); // 20% more for buffer + } + + private boolean isFreeListEmpty() { + LOCK.lock(); Review Comment: At some places we are using synchronized and at others making use of LOCK, can we use a single synchronization strategy across ?", "created": "2025-10-24T10:20:27.567+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459676934 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { + int currentPoolSize = workerRefs.size(); + double cpuLoad = getCpuLoad(); + int requiredPoolSize = getRequiredThreadPoolSize(); + int newThreadPoolSize; + printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize); + if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) { + // Submit more background tasks. + newThreadPoolSize = Math.min(maxThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED)); + // Create new Worker Threads + for (int i = currentPoolSize; i < newThreadPoolSize; i++) { + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); + workerPool.submit(worker); + } + printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) { + newThreadPoolSize = Math.max(minThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED)); + // Signal the extra workers to stop + while (workerRefs.size() > newThreadPoolSize) { + ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1); + worker.stop(); + } + printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else { + printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize); + } + } + /** - * {@inheritDoc} + * Similar to System.currentTimeMillis, except implemented with System.nanoTime(). + * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization), + * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core. + * Note: it is not monotonic across Sockets, and even within a CPU, its only the + * more recent parts which share a clock across all cores. + * + * @return current time in milliseconds */ - @VisibleForTesting - @Override - public void callTryEvict() { - // TODO: To be implemented + private long currentTimeMillis() { + return System.nanoTime() / 1000 / 1000; + } + + private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) { + for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) { + ReadBuffer readBuffer = it.next(); + if (readBuffer.getStream() == stream) { + it.remove(); + // As failed ReadBuffers (bufferIndex = -1) are already pushed to free + // list in doneReading method, we will skip adding those here again. + if (readBuffer.getBufferindex() != -1) { + pushToFreeList(readBuffer.getBufferindex()); + } + } + } } /** - * {@inheritDoc} + * Test method that can clean up the current state of readAhead buffers and + * the lists. Will also trigger a fresh init. */ @VisibleForTesting @Override public void testResetReadBufferManager() { - // TODO: To be implemented + synchronized (this) { + ArrayList<ReadBuffer> completedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if (buf != null) { + completedBuffers.add(buf); + } + } + + for (ReadBuffer buf : completedBuffers) { + manualEviction(buf); + } + + getReadAheadQueue().clear(); + getInProgressList().clear(); + getCompletedReadList().clear(); + getFreeList().clear(); + for (int i = 0; i < maxBufferPoolSize; i++) { + bufferPool[i] = null; + } + bufferPool = null; + cpuMonitorThread.shutdownNow(); + memoryMonitorThread.shutdownNow(); + workerPool.shutdownNow(); + resetBufferManager(); + } } - /** - * {@inheritDoc} - */ @VisibleForTesting @Override - public void testResetReadBufferManager(final int readAheadBlockSize, - final int thresholdAgeMilliseconds) { - // TODO: To be implemented + public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) { + setReadAheadBlockSize(readAheadBlockSize); + setThresholdAgeMilliseconds(thresholdAgeMilliseconds); + testResetReadBufferManager(); } - /** - * {@inheritDoc} - */ - @Override - public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) { - // TODO: To be implemented + @VisibleForTesting + public void callTryEvict() { + tryEvict(); } - private final ThreadFactory namedThreadFactory = new ThreadFactory() { - private int count = 0; - @Override - public Thread newThread(Runnable r) { - return new Thread(r, \"ReadAheadV2-Thread-\" + count++); + @VisibleForTesting + public int getNumBuffers() { + LOCK.lock(); + try { + return numberOfActiveBuffers; + } finally { + LOCK.unlock(); } - }; + } @Override void resetBufferManager() { setBufferManager(null); // reset the singleton instance + setIsConfigured(false); } private static void setBufferManager(ReadBufferManagerV2 manager) { bufferManager = manager; } + + private static void setIsConfigured(boolean configured) { + isConfigured = configured; + } + + private final ThreadFactory workerThreadFactory = new ThreadFactory() { + private int count = 0; + @Override + public Thread newThread(Runnable r) { + Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++); + t.setDaemon(true); + return t; + } + }; + + private void printTraceLog(String message, Object... args) { + if (LOGGER.isTraceEnabled()) { + LOGGER.trace(message, args); + } + } + + private void printDebugLog(String message, Object... args) { + LOGGER.debug(message, args); + } + + @VisibleForTesting + double getMemoryLoad() { + MemoryMXBean osBean = ManagementFactory.getMemoryMXBean(); + MemoryUsage memoryUsage = osBean.getHeapMemoryUsage(); + return (double) memoryUsage.getUsed() / memoryUsage.getMax(); + } + + @VisibleForTesting + public double getCpuLoad() { + OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean( + OperatingSystemMXBean.class); + return osBean.getSystemCpuLoad(); Review Comment: can return negative values as well, should return 0 when negative", "created": "2025-10-24T10:21:09.265+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459698158 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -17,67 +17,91 @@ */ package org.apache.hadoop.fs.azurebfs.services; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus; + +import com.sun.management.OperatingSystemMXBean; + import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.MemoryMXBean; +import java.lang.management.MemoryUsage; +import java.util.ArrayList; +import java.util.Collection; +import java.util.Iterator; +import java.util.LinkedList; +import java.util.List; +import java.util.Stack; +import java.util.concurrent.CountDownLatch; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.SynchronousQueue; import java.util.concurrent.ThreadFactory; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.ReentrantLock; -import org.apache.hadoop.classification.VisibleForTesting; -import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; -import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus; import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.classification.VisibleForTesting; -final class ReadBufferManagerV2 extends ReadBufferManager { +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED; + +/** + * The Improved Read Buffer Manager for Rest AbfsClient. + */ +public class ReadBufferManagerV2 extends ReadBufferManager { + // Internal constants + private static final ReentrantLock LOCK = new ReentrantLock(); // Thread Pool Configurations private static int minThreadPoolSize; private static int maxThreadPoolSize; + private static int cpuMonitoringIntervalInMilliSec; + private static double cpuThreshold; + private static int threadPoolUpscalePercentage; + private static int threadPoolDownscalePercentage; private static int executorServiceKeepAliveTimeInMilliSec; + private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size + private static boolean isDynamicScalingEnabled; + + private ScheduledExecutorService cpuMonitorThread; private ThreadPoolExecutor workerPool; + private final List<ReadBufferWorker> workerRefs = new ArrayList<>(); Review Comment: Shouldn't this be synchronized as there can be concurrent modifications to the list ?", "created": "2025-10-24T10:23:39.101+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459704141 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { + int currentPoolSize = workerRefs.size(); + double cpuLoad = getCpuLoad(); + int requiredPoolSize = getRequiredThreadPoolSize(); + int newThreadPoolSize; + printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize); + if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) { + // Submit more background tasks. + newThreadPoolSize = Math.min(maxThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED)); + // Create new Worker Threads + for (int i = currentPoolSize; i < newThreadPoolSize; i++) { + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); + workerPool.submit(worker); + } + printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) { + newThreadPoolSize = Math.max(minThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED)); + // Signal the extra workers to stop + while (workerRefs.size() > newThreadPoolSize) { + ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1); + worker.stop(); + } + printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else { + printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize); + } + } + /** - * {@inheritDoc} + * Similar to System.currentTimeMillis, except implemented with System.nanoTime(). + * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization), + * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core. + * Note: it is not monotonic across Sockets, and even within a CPU, its only the + * more recent parts which share a clock across all cores. + * + * @return current time in milliseconds */ - @VisibleForTesting - @Override - public void callTryEvict() { - // TODO: To be implemented + private long currentTimeMillis() { + return System.nanoTime() / 1000 / 1000; + } + + private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) { + for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) { + ReadBuffer readBuffer = it.next(); + if (readBuffer.getStream() == stream) { + it.remove(); + // As failed ReadBuffers (bufferIndex = -1) are already pushed to free + // list in doneReading method, we will skip adding those here again. + if (readBuffer.getBufferindex() != -1) { + pushToFreeList(readBuffer.getBufferindex()); + } + } + } } /** - * {@inheritDoc} + * Test method that can clean up the current state of readAhead buffers and + * the lists. Will also trigger a fresh init. */ @VisibleForTesting @Override public void testResetReadBufferManager() { - // TODO: To be implemented + synchronized (this) { + ArrayList<ReadBuffer> completedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if (buf != null) { + completedBuffers.add(buf); + } + } + + for (ReadBuffer buf : completedBuffers) { + manualEviction(buf); + } + + getReadAheadQueue().clear(); + getInProgressList().clear(); + getCompletedReadList().clear(); + getFreeList().clear(); + for (int i = 0; i < maxBufferPoolSize; i++) { + bufferPool[i] = null; + } + bufferPool = null; + cpuMonitorThread.shutdownNow(); Review Comment: will be null if dynamic scaling is not enabled and can lead to null pointer", "created": "2025-10-24T10:25:20.770+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459731120 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { + int currentPoolSize = workerRefs.size(); + double cpuLoad = getCpuLoad(); + int requiredPoolSize = getRequiredThreadPoolSize(); + int newThreadPoolSize; + printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize); + if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) { + // Submit more background tasks. + newThreadPoolSize = Math.min(maxThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED)); + // Create new Worker Threads + for (int i = currentPoolSize; i < newThreadPoolSize; i++) { + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); + workerPool.submit(worker); + } + printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) { + newThreadPoolSize = Math.max(minThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED)); + // Signal the extra workers to stop + while (workerRefs.size() > newThreadPoolSize) { + ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1); + worker.stop(); + } + printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else { + printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize); + } + } + /** - * {@inheritDoc} + * Similar to System.currentTimeMillis, except implemented with System.nanoTime(). + * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization), + * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core. + * Note: it is not monotonic across Sockets, and even within a CPU, its only the + * more recent parts which share a clock across all cores. + * + * @return current time in milliseconds */ - @VisibleForTesting - @Override - public void callTryEvict() { - // TODO: To be implemented + private long currentTimeMillis() { + return System.nanoTime() / 1000 / 1000; Review Comment: Use constants", "created": "2025-10-24T10:34:39.834+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459736675 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { Review Comment: add javadocs for the newly added functions", "created": "2025-10-24T10:36:34.982+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459731120 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { + int currentPoolSize = workerRefs.size(); + double cpuLoad = getCpuLoad(); + int requiredPoolSize = getRequiredThreadPoolSize(); + int newThreadPoolSize; + printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize); + if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) { + // Submit more background tasks. + newThreadPoolSize = Math.min(maxThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED)); + // Create new Worker Threads + for (int i = currentPoolSize; i < newThreadPoolSize; i++) { + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); + workerPool.submit(worker); + } + printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) { + newThreadPoolSize = Math.max(minThreadPoolSize, + (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED)); + // Signal the extra workers to stop + while (workerRefs.size() > newThreadPoolSize) { + ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1); + worker.stop(); + } + printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize); + } else { + printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize); + } + } + /** - * {@inheritDoc} + * Similar to System.currentTimeMillis, except implemented with System.nanoTime(). + * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization), + * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core. + * Note: it is not monotonic across Sockets, and even within a CPU, its only the + * more recent parts which share a clock across all cores. + * + * @return current time in milliseconds */ - @VisibleForTesting - @Override - public void callTryEvict() { - // TODO: To be implemented + private long currentTimeMillis() { + return System.nanoTime() / 1000 / 1000; Review Comment: Use constants or better way could be TimeUnit.NANOSECONDS.toMillis(System.nanoTime());", "created": "2025-10-24T10:38:40.167+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459753340 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## Review Comment: A lot of code is repeated between V1 and V2, would it be better to put the common code in super class ?", "created": "2025-10-24T10:42:45.440+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459755892 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { + synchronized (this) { + if (isFreeListEmpty()) { + printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\"); + return; // No free buffers available, so cannot downscale. + } + int freeIndex = popFromFreeList(); + bufferPool[freeIndex] = null; + removedBufferList.add(freeIndex); + decrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + } + } + } + + private boolean manualEviction(final ReadBuffer buf) { + printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode()); + return evict(buf); + } + + private void adjustThreadPool() { Review Comment: While adjusting thread pool size, should we not consider the memory threshold as well ?", "created": "2025-10-24T10:43:32.514+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459777164 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + return (getFromList(list, eTag, requestedOffset) != null); + } + + private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag, + final long requestedOffset) { + for (ReadBuffer buffer : list) { + if (eTag.equals(buffer.getETag())) { + if (buffer.getStatus() == ReadBufferStatus.AVAILABLE + && requestedOffset >= buffer.getOffset() + && requestedOffset < buffer.getOffset() + buffer.getLength()) { + return buffer; + } else if (requestedOffset >= buffer.getOffset() + && requestedOffset + < buffer.getOffset() + buffer.getRequestedLength()) { + return buffer; + } + } + } + return null; } /** - * {@inheritDoc} + * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list. + * The objective is to find just one buffer - there is no advantage to evicting more than one. + * @return whether the eviction succeeded - i.e., were we able to free up one buffer */ - @VisibleForTesting - @Override - public int getNumBuffers() { - return numberOfActiveBuffers; + private synchronized boolean tryEvict() { + ReadBuffer nodeToEvict = null; + if (getCompletedReadList().size() <= 0) { + return false; // there are no evict-able buffers + } + + long currentTimeInMs = currentTimeMillis(); + + // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isFullyConsumed()) { + nodeToEvict = buf; + break; + } + } + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see) + for (ReadBuffer buf : getCompletedReadList()) { + if (buf.isAnyByteConsumed()) { + nodeToEvict = buf; + break; + } + } + + if (nodeToEvict != null) { + return manualEviction(nodeToEvict); + } + + // next, try any old nodes that have not been consumed + // Failed read buffers (with buffer index=-1) that are older than + // thresholdAge should be cleaned up, but at the same time should not + // report successful eviction. + // Queue logic expects that a buffer is freed up for read ahead when + // eviction is successful, whereas a failed ReadBuffer would have released + // its buffer when its status was set to READ_FAILED. + long earliestBirthday = Long.MAX_VALUE; + ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>(); + for (ReadBuffer buf : getCompletedReadList()) { + if ((buf.getBufferindex() != -1) + && (buf.getTimeStamp() < earliestBirthday)) { + nodeToEvict = buf; + earliestBirthday = buf.getTimeStamp(); + } else if ((buf.getBufferindex() == -1) + && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) { + oldFailedBuffers.add(buf); + } + } + + for (ReadBuffer buf : oldFailedBuffers) { + manualEviction(buf); + } + + if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) { + return manualEviction(nodeToEvict); + } + + printTraceLog(\"No buffer eligible for eviction\"); + // nothing can be evicted + return false; + } + + private boolean evict(final ReadBuffer buf) { + if (buf.getRefCount() > 0) { + // If the buffer is still being read, then we cannot evict it. + printTraceLog( + \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset()); + return false; + } + // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(), + // avoid adding it to availableBufferList. + if (buf.getBufferindex() != -1) { + pushToFreeList(buf.getBufferindex()); + } + getCompletedReadList().remove(buf); + buf.setTracingContext(null); + printTraceLog( + \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), + buf.isFullyConsumed(), buf.isAnyByteConsumed()); + return true; + } + + private void waitForProcess(final String eTag, final long position, boolean isFirstRead) { + ReadBuffer readBuf; + synchronized (this) { + readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead); + if (readBuf == null) { + readBuf = getFromList(getInProgressList(), eTag, position); + } + } + if (readBuf != null) { // if in in-progress queue, then block for it + try { + printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\", + readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex()); + readBuf.getLatch().await(); // blocking wait on the caller stream's thread + // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread + // is done processing it (in doneReading). There, the latch is set after removing the buffer from + // getInProgressList(). So this latch is safe to be outside the synchronized block. + // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock + // while waiting, so no one will be able to change any state. If this becomes more complex in the future, + // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched. + } catch (InterruptedException ex) { + Thread.currentThread().interrupt(); + } + printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \" + + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(), + readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode()); + } + } + + private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) { + ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset); + /* + * If this prefetch was triggered by first read of this input stream, + * we should not remove it from queue and let it complete by backend threads. + */ + if (buffer != null && isFirstRead) { + return buffer; + } + if (buffer != null) { + getReadAheadQueue().remove(buffer); + notifyAll(); // lock is held in calling method + pushToFreeList(buffer.getBufferindex()); + } + return null; } + + private int getBlockFromCompletedQueue(final String eTag, final long position, + final int length, final byte[] buffer) throws IOException { + ReadBuffer buf = getBufferFromCompletedQueue(eTag, position); + + if (buf == null) { + return 0; + } + + buf.startReading(); // atomic increment of refCount. + + if (buf.getStatus() == ReadBufferStatus.READ_FAILED) { + // To prevent new read requests to fail due to old read-ahead attempts, + // return exception only from buffers that failed within last getThresholdAgeMilliseconds() + if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) { + throw buf.getErrException(); + } else { + return 0; + } + } + + if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) + || (position >= buf.getOffset() + buf.getLength())) { + return 0; + } + + int cursor = (int) (position - buf.getOffset()); + int availableLengthInBuffer = buf.getLength() - cursor; + int lengthToCopy = Math.min(length, availableLengthInBuffer); + System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy); + if (cursor == 0) { + buf.setFirstByteConsumed(true); + } + if (cursor + lengthToCopy == buf.getLength()) { + buf.setLastByteConsumed(true); + } + buf.setAnyByteConsumed(true); + + buf.endReading(); // atomic decrement of refCount + return lengthToCopy; + } + + private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) { + for (ReadBuffer buffer : getCompletedReadList()) { + // Buffer is returned if the requestedOffset is at or above buffer's + // offset but less than buffer's length or the actual requestedLength + if (eTag.equals(buffer.getETag()) + && (requestedOffset >= buffer.getOffset()) + && ((requestedOffset < buffer.getOffset() + buffer.getLength()) + || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) { + return buffer; + } + } + return null; + } + + private synchronized boolean tryMemoryUpscale() { + if (!isDynamicScalingEnabled) { + printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\"); + return false; // Dynamic scaling is disabled, so no upscaling. + } + double memoryLoad = getMemoryLoad(); + if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) { + // Create and Add more buffers in getFreeList(). + if (removedBufferList.isEmpty()) { + bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()]; + pushToFreeList(getNumBuffers()); + } else { + // Reuse a removed buffer index. + int freeIndex = removedBufferList.pop(); + if (freeIndex >= bufferPool.length) { + printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\", + freeIndex, bufferPool.length); + return false; + } + bufferPool[freeIndex] = new byte[getReadAheadBlockSize()]; + pushToFreeList(freeIndex); + } + incrementActiveBufferCount(); + printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers()); + return true; + } + printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\", + getNumBuffers(), memoryLoad); + return false; + } + + private void scheduledEviction() { + for (ReadBuffer buf : getCompletedReadList()) { + if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) { + // If the buffer is older than thresholdAge, evict it. + printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\", + buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode()); + evict(buf); + } + } + + double memoryLoad = getMemoryLoad(); + if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) { Review Comment: So we are evicting based on memory only and scaling and descaling based on CPU, IMO there should be a common function which considers both the params", "created": "2025-10-24T10:50:01.002+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7832: URL: https://github.com/apache/hadoop/pull/7832#discussion_r2459788986 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java: ########## @@ -106,123 +166,731 @@ void init() { executorServiceKeepAliveTimeInMilliSec, TimeUnit.MILLISECONDS, new SynchronousQueue<>(), - namedThreadFactory); + workerThreadFactory); workerPool.allowCoreThreadTimeOut(true); for (int i = 0; i < minThreadPoolSize; i++) { - ReadBufferWorker worker = new ReadBufferWorker(i, this); + ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager()); + workerRefs.add(worker); workerPool.submit(worker); } ReadBufferWorker.UNLEASH_WORKERS.countDown(); + + if (isDynamicScalingEnabled) { + cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> { + Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\"); + t.setDaemon(true); + return t; + }); + cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool, + getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(), + TimeUnit.MILLISECONDS); + } + + printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\", + numberOfActiveBuffers, workerRefs.size()); } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method to queueing read-ahead. + * @param stream which read-ahead is requested from. + * @param requestedOffset The offset in the file which should be read. + * @param requestedLength The length to read. */ @Override - public void queueReadAhead(final AbfsInputStream stream, - final long requestedOffset, - final int requestedLength, - final TracingContext tracingContext) { - // TODO: To be implemented + public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, + final int requestedLength, TracingContext tracingContext) { + printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode()); + ReadBuffer buffer; + synchronized (this) { + if (isAlreadyQueued(stream.getETag(), requestedOffset)) { + // Already queued for this offset, so skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) { + // No buffers are available and more buffers cannot be created. Skip queuing. + printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\", + stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode()); + return; + } + + // Create a new ReadBuffer to keep the prefetched data and queue. + buffer = new ReadBuffer(); + buffer.setStream(stream); // To map buffer with stream that requested it + buffer.setETag(stream.getETag()); // To map buffer with file it belongs to + buffer.setPath(stream.getPath()); + buffer.setOffset(requestedOffset); + buffer.setLength(0); + buffer.setRequestedLength(requestedLength); + buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE); + buffer.setLatch(new CountDownLatch(1)); + buffer.setTracingContext(tracingContext); + + if (isFreeListEmpty()) { + /* + * By now there should be at least one buffer available. + * This is to double sure that after upscaling or eviction, + * we still have free buffer available. If not, we skip queueing. + */ + return; + } + Integer bufferIndex = popFromFreeList(); + buffer.setBuffer(bufferPool[bufferIndex]); + buffer.setBufferindex(bufferIndex); + getReadAheadQueue().add(buffer); + notifyAll(); + printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\", + stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode()); + } } /** - * {@inheritDoc} + * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a + * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading + * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead + * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because + * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own + * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time). + * + * @param stream of the file to read bytes for + * @param position the offset in the file to do a read for + * @param length the length to read + * @param buffer the buffer to read data into. Note that the buffer will be written into from offset 0. + * @return the number of bytes read */ @Override - public int getBlock(final AbfsInputStream stream, - final long position, - final int length, - final byte[] buffer) throws IOException { - // TODO: To be implemented + public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) + throws IOException { + // not synchronized, so have to be careful with locking + printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\", + stream.getPath(), stream.getETag(), position, length, stream.hashCode()); + + String requestedETag = stream.getETag(); + boolean isFirstRead = stream.isFirstRead(); + + // Wait for any in-progress read to complete. + waitForProcess(requestedETag, position, isFirstRead); + + int bytesRead = 0; + synchronized (this) { + bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer); + } + if (bytesRead > 0) { + printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\", + requestedETag, position, bytesRead, stream.hashCode()); + return bytesRead; + } + + // otherwise, just say we got nothing - calling thread can do its own read return 0; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on. + * @return {@link ReadBuffer} + * @throws InterruptedException if thread is interrupted */ @Override public ReadBuffer getNextBlockToRead() throws InterruptedException { - // TODO: To be implemented - return null; + ReadBuffer buffer = null; + synchronized (this) { + // Blocking Call to wait for prefetch to be queued. + while (getReadAheadQueue().size() == 0) { + wait(); + } + + buffer = getReadAheadQueue().remove(); + notifyAll(); + if (buffer == null) { + return null; + } + buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS); + getInProgressList().add(buffer); + } + printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode()); + return buffer; } /** - * {@inheritDoc} + * {@link ReadBufferWorker} thread calls this method to post completion. * + * @param buffer the buffer whose read was completed + * @param result the {@link ReadBufferStatus} after the read operation in the worker thread + * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read */ @Override - public void doneReading(final ReadBuffer buffer, - final ReadBufferStatus result, + public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) { - // TODO: To be implemented + printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\", + buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead); + synchronized (this) { + // If this buffer has already been purged during + // close of InputStream then we don't update the lists. + if (getInProgressList().contains(buffer)) { + getInProgressList().remove(buffer); + if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) { + // Successful read, so update the buffer status and length + buffer.setStatus(ReadBufferStatus.AVAILABLE); + buffer.setLength(bytesActuallyRead); + } else { + // Failed read, reuse buffer for next read, this buffer will be + // evicted later based on eviction policy. + pushToFreeList(buffer.getBufferindex()); + } + // completed list also contains FAILED read buffers + // for sending exception message to clients. + buffer.setStatus(result); + buffer.setTimeStamp(currentTimeMillis()); + getCompletedReadList().add(buffer); + } + } + + //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results + buffer.getLatch().countDown(); // wake up waiting threads (if any) } /** - * {@inheritDoc} + * Purging the buffers associated with an {@link AbfsInputStream} + * from {@link ReadBufferManagerV2} when stream is closed. + * @param stream input stream. */ - @Override - public void purgeBuffersForStream(final AbfsInputStream stream) { - // TODO: To be implemented + public synchronized void purgeBuffersForStream(AbfsInputStream stream) { + printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream); + getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream); + purgeList(stream, getCompletedReadList()); + } + + private boolean isAlreadyQueued(final String eTag, final long requestedOffset) { + // returns true if any part of the buffer is already queued + return (isInList(getReadAheadQueue(), eTag, requestedOffset) + || isInList(getInProgressList(), eTag, requestedOffset) + || isInList(getCompletedReadList(), eTag, requestedOffset)); + } + + private boolean isInList(final Collection<ReadBuffer> list, final String eTag, Review Comment: javadocs missing for many functions", "created": "2025-10-24T10:53:01.203+0000"}], "derived_tasks": {"summary": "ABFS: [ReadAheadV2] Implement Read Buffer Manager V2 with improved aggressiveness", "classifications": ["feature", "improvement", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19605", "title": "Upgrade Protobuf 3.25.5 for docker images", "description": "HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-07-07T02:22:39.000+0000", "updated": "2025-10-20T06:10:11.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2196670293 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: Thanks for PR @pan3793. Please let me know once you're done with all the changes and I can verify it on Windows.", "created": "2025-07-10T06:04:11.695+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248461577 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at [CI_URL] in case of problems.", "created": "2025-09-03T09:31:17.885+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248556711 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at [CI_URL] in case of problems.", "created": "2025-09-03T09:59:42.980+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3249774993 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 3s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 21m 20s | | trunk passed | | -1 :x: | compile | 1m 59s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | mvnsite | 15m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 72m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 20m 41s | | the patch passed | | -1 :x: | compile | 2m 16s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | cc | 2m 16s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 2m 16s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 12m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 222m 8s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 31s | | The patch does not generate ASF License warnings. | | | | 370m 34s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac | | uname | Linux 8e5a65a25879 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / eb4b43e28804ae48b229ac03a65db9f13ae327b6 | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 4390 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | [CI_URL] | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-03T15:40:47.859+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3252905469 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at [CI_URL] in case of problems.", "created": "2025-09-04T10:02:01.860+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3256642378 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 25s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 54s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 22m 44s | | trunk passed | | -1 :x: | compile | 2m 22s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | mvnsite | 15m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 73m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 22m 1s | | the patch passed | | +1 :green_heart: | compile | 7m 53s | | the patch passed | | -1 :x: | cc | 7m 53s | [/results-compile-cc-root.txt]([CI_URL] | root generated 138 new + 14 unchanged - 32 fixed = 152 total (was 46) | | +1 :green_heart: | golang | 7m 53s | | the patch passed | | +1 :green_heart: | javac | 7m 53s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 14m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 752m 15s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 2m 23s | | The patch does not generate ASF License warnings. | | | | 901m 21s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.mapreduce.v2.app.webapp.TestAMWebApp | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.fs.compat.common.TestHdfsCompatShellCommand | | | hadoop.fs.compat.common.TestHdfsCompatDefaultSuites | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac golang | | uname | Linux 2a55a31923ae 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8434cddca56fd45806321201aefefd63f228d5dc | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 4507 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | [CI_URL] | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-05T01:02:27.406+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2324003089 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: @GauthamBanasandra Now it passes on all Linux containers. Due to my limited CPP experience, my change might be dirty ...", "created": "2025-09-05T03:18:48.946+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2324012372 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: @GauthamBanasandra do you remember why we should install `libprotobuf-dev` and `libprotoc-dev` through APT? seems we can remove it and always use the manually installed protobuf instead?", "created": "2025-09-05T03:30:09.095+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3284186207 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at [CI_URL] in case of problems.", "created": "2025-09-12T07:56:08.108+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3286927808 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 1s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 1s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 5s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 20m 5s | | trunk passed | | -1 :x: | compile | 2m 3s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | mvnsite | 14m 31s | | trunk passed | | +1 :green_heart: | shadedclient | 67m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 19m 8s | | the patch passed | | +1 :green_heart: | compile | 7m 46s | | the patch passed | | -1 :x: | cc | 7m 46s | [/results-compile-cc-root.txt]([CI_URL] | root generated 138 new + 14 unchanged - 32 fixed = 152 total (was 46) | | +1 :green_heart: | golang | 7m 46s | | the patch passed | | +1 :green_heart: | javac | 7m 46s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 12m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 34m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 679m 21s | [/patch-unit-root.txt]([CI_URL] | root in the patch passed. | | +1 :green_heart: | asflicense | 2m 25s | | The patch does not generate ASF License warnings. | | | | 825m 26s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.server.federation.security.token.TestZKDelegationTokenSecretManagerImpl | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.mapreduce.v2.app.webapp.TestAMWebApp | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac golang | | uname | Linux 3b8eaff37aac 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cf6a61b0a949f587d3234109b83c98aef9f445f6 | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 4094 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | [CI_URL] | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-12T21:40:47.467+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3388050172 @steveloughran protobuf upgrading is done here, I made it work at least on the Linux platform, but I'm not very familiar with cpp toolchains, the CMake files changes might need to be reviewed by a domain expert.", "created": "2025-10-10T02:17:52.535+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2441876283 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: These libraries are needed for C/C++ code that uses protobuf. Regarding installing these from `apt` v/s manual installation - I would prefer the former wherever possible as it keeps the environment clean. It's easy to remove or upgrade the libraries when installed through `apt`.", "created": "2025-10-18T09:10:27.805+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2443891194 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: @GauthamBanasandra, but it has already been installed `install-protobuf.sh`. I think the major advantage of manual installation is to make the protobuf version consistent with Java and across all Linux distributions. Anyway, duplicated installation is not a major issue. Could you please review this PR? I tested it in all current Dockerfiles, but my changes in CMake files might be dirty (sorry, I'm not familiar with cpp toolchains)", "created": "2025-10-20T06:10:11.240+0000"}], "derived_tasks": {"summary": "Upgrade Protobuf 3.25.5 for docker images - HADOOP-19289 upgraded protobuf-java 3", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-19599", "title": "Fix file permission errors as per the platform", "description": "The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly. However, an error message in the same scenario on Windows ends with \"(Access is denied)\" error. This results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*. Thus, we need to make the appropriate check in accordance with the platform.", "status": "Open", "priority": "Major", "reporter": "Gautham Banasandra", "assignee": "Gautham Banasandra", "created": "2025-06-28T17:39:22.000+0000", "updated": "2025-10-26T00:24:12.000+0000", "labels": ["pull-request-available"], "components": ["hadoop-common"], "comments": [{"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3081984805 @GauthamBanasandra Thank you for your contribution \u2014 I believe this PR is ready to be merged.", "created": "2025-07-17T00:48:11.650+0000"}, {"author": "ASF GitHub Bot", "body": "GauthamBanasandra commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3082098761 Yes @slfan1989 . It's ready. But since this PR changes a source file, I'm trying to get the Yetus to run locally on this one, so that we run all the tests. The Jenkins build agents for Windows are failing due to low disk space. Hence I'm working on getting Yetus to run locally. I should be able to merge this by next week.", "created": "2025-07-17T01:43:01.584+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3447890882 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-26T00:24:12.761+0000"}], "derived_tasks": {"summary": "Fix file permission errors as per the platform - The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-19486", "title": "Fully Support Java 23, 24 and 25", "description": "Hadoop trunk today mostly supports JDK17, but doesn't work at all on JDK23. (and conversely on JDK24 to be released in less than two weeks) While there are many smaller issues, the major breaking change is the SecurityManager removal (JEP411/486), and its many consequences. The obvious change is that Subjec.doAs() and Subject.current() no longer work by default, and the replacement APIs must be used. The more insidius change is that when SecurityManager is disabled then JDK22+ does not propapage the Subject to new Threads, which is something that Hadoop absolutely relies on. Note that Hadoop is always built with with JDK 17 (if the JDK is 17 or newer), unless the target version is specifically overriden. This is not a problem, JDK17 class files running on a JDK 24 JVM is the expected use case for binary distributions. We may want to run some tests where Hadoop is also compiled for the lastest JVM later. (taget Java 24 + JVM 24)", "status": "Open", "priority": "Major", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "created": "2025-03-07T05:55:11.000+0000", "updated": "2025-10-22T09:51:51.000+0000", "labels": [], "components": [], "comments": [{"author": "Shilun Fan", "body": "[~stoty] I originally hoped to support JDK 21 after supporting JDK 17, so it would be really awesome if it could support even newer JDK versions.", "created": "2025-03-07T06:03:21.494+0000"}, {"author": "Istvan Toth", "body": "Could you also check the rest of the subtasks and dependent tasks here [~slfan1989]\u00a0 ? These are fixing varius test issues on JDK11-23 , and are a requirement for adding the big JEP411 related fixes. I've split them up into small easy-to-review patches. As a side effect, with these fixes applied the test suite should also run cleanly with JDK17 and 21.", "created": "2025-03-17T16:41:30.674+0000"}, {"author": "Gary D. Gregory", "body": "Hi All, would you please set expectations as to when we will be able to run on Java 24/25-ea without getting failures like:", "created": "2025-03-27T15:12:45.232+0000"}, {"author": "Paul R\u00fctter", "body": "I'm also interested in the progress of this task, since i'm running into the same exception when trying to run Spark 4.x against Java 25. Any update on this one?", "created": "2025-07-23T13:21:44.270+0000"}, {"author": "Istvan Toth", "body": "Most of the small issued are fixed. There are two major issues left, one is SecurityManager removal + getSubject() removal, the other is the change in the principal inheritance for new threads. I have a WIP patch that fixes both, but they need some more polish, and I haven't had the time to work on them lately.", "created": "2025-07-28T12:16:31.586+0000"}, {"author": "Hugo Costa", "body": "Hello Istvan, have you had any more time to work on that patch? JDK25 went GA last week and this is blocking our adoption", "created": "2025-09-24T15:26:50.196+0000"}, {"author": "Istvan Toth", "body": "You can follow the discussion at https://github.com/apache/hadoop/pull/7892", "created": "2025-09-24T16:13:24.646+0000"}], "derived_tasks": {"summary": "Fully Support Java 23, 24 and 25 - Hadoop trunk today mostly supports JDK17, but doesn't work at all on JDK23", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-19472", "title": "ABFS: Enhance performance of ABFS driver for write-heavy workloads", "description": "The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.", "status": "Open", "priority": "Minor", "reporter": "Anmol Asrani", "assignee": "Anmol Asrani", "created": "2025-02-25T09:54:55.000+0000", "updated": "2025-10-23T11:23:10.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "anmolanmol1234 opened a new pull request, #7669: URL: https://github.com/apache/hadoop/pull/7669 Enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes. ![{05B55BCA-EF1F-496D-B1ED-17DCD394DDA1}](https://github.com/user-attachments/assets/5ebd5ad7-51db-4028-812f-ce9da9266984) The proposed design advocates for a centralized `WriteThreadPoolSizeManager` class to handle the collective thread allocation required for all write operations across the system, replacing the current CachedThreadPool in AzureBlobFileSystemStore. This centralized approach ensures that the initial thread pool size is set at `4 * number of available processors` and dynamically adjusts the pool size based on the system's current CPU utilization. This adaptive scaling and descaling mechanism optimizes resource usage and responsiveness. Moreover, this shared thread pool is accessible and utilized by all output streams, streamlining resource management and promoting efficient concurrency across write operations.", "created": "2025-05-03T08:22:10.199+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848521492 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 164 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 801, Failures: 0, Errors: 0, Skipped: 117 [ERROR] Tests run: 146, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 640, Failures: 0, Errors: 0, Skipped: 215 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 171 [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 643, Failures: 0, Errors: 0, Skipped: 144 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [ERROR] Tests run: 640, Failures: 0, Errors: 0, Skipped: 146 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [ERROR] Tests run: 638, Failures: 0, Errors: 0, Skipped: 164 [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4 [ERROR] Tests run: 672, Failures: 0, Errors: 0, Skipped: 167 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 215 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24", "created": "2025-05-03T08:35:17.220+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848547561 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 3s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 19s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 31s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 14 new + 2 unchanged - 0 fixed = 16 total (was 2) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 20m 18s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 19s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 77m 36s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths At WriteThreadPoolSizeManager.java:on all exception paths At WriteThreadPoolSizeManager.java:[line 268] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a1000f66baec 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1ba12f6247567e3f5e9087c00fb52e741b1eb98c | | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-05-03T09:52:51.621+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2857800447 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 18s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 30s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 40s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 22m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 20s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 78m 44s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths At WriteThreadPoolSizeManager.java:on all exception paths At WriteThreadPoolSizeManager.java:[line 263] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3b526ffc404b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8fe08428cb90f286b3d7408e60bd3c05bbde7ba6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 558 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-05-07T09:15:21.506+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2858500737 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 23m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 50s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 17s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 74m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3ee980188ead 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c56bdcbeec1025c521eae998c50aaf72d72458d7 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-05-07T12:59:31.510+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2911679089 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 23s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 25s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 23m 37s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 22s | | The patch does not generate ASF License warnings. | | | | 80m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 251c9a94f809 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ca7be8805b4a932ebcfbef19749c9b8b3e0497e1 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-05-27T08:41:23.368+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2912198409 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 22s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 53s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 5s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 17s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 76m 56s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 957e28bff80b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5e79b10358006820caa9e39d1d472571aba1f56c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-05-27T11:40:52.735+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3164446290 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 30m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 39s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 52s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 20s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 91m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b233612874b4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 73a8012c216fb9222f1be6bb639378f77091a305 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-07T14:31:26.632+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3195908154 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 16s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 36s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 15s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 22s | | The patch does not generate ASF License warnings. | | | | 86m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a58d8ee4661b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3164050e6fccde89f26d597bee73fa42a725dc3a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 729 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-18T09:35:38.124+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3195944600 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 49s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 38s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 26m 50s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 86m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 345d7c0ea8f7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / db7ae1fa85a844e69cbdfde57081ce77091c3799 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-18T09:46:18.409+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3197457197 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 30m 15s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 31s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 32 new + 2 unchanged - 0 fixed = 34 total (was 2) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 84m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 806146c83fa5 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 812ea4660ae805dd76ddb18bf25a92a80c70e0b0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-18T15:43:41.228+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2313716838 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -321,16 +330,13 @@ public void close() throws IOException { try { Futures.allAsList(futures).get(); // shutdown the threadPool and set it to null. - HadoopExecutors.shutdown(boundedThreadPool, LOG, - 30, TimeUnit.SECONDS); - boundedThreadPool = null; } catch (InterruptedException e) { LOG.error(\"Interrupted freeing leases\", e); Thread.currentThread().interrupt(); } catch (ExecutionException e) { LOG.error(\"Error freeing leases\", e); } finally { - IOUtils.cleanupWithLogger(LOG, getClient()); + IOUtils.cleanupWithLogger(LOG, poolSizeManager, getClient()); Review Comment: For the non-dynamic pool- how are we closing the boundedThreadPool? Would we need HadoopExecutors.shutdown(..) for it?", "created": "2025-09-01T11:38:52.355+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2313736451 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); Review Comment: we're naming the threads in non-dynamic pool and the manager pool for dynamic write pool. Should we also name the threads for dynamic case?", "created": "2025-09-01T11:48:19.191+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3242214939 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 38s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 78m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2e870a45b9cb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cafe1e5e618e22865f26a57b6891dc57a368e658 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-01T12:41:34.036+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2314852491 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); + + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getLowTierMemoryMultiplier(); + } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getMediumTierMemoryMultiplier(); + } else { + multiplier = abfsConfiguration.getHighTierMemoryMultiplier(); + } + return availableProcessors * multiplier; + } + + /** + * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem. + * + * @param filesystemName the name of the filesystem. + * @param abfsConfiguration the configuration for the ABFS. + * + * @return the singleton instance. + */ + public static synchronized WriteThreadPoolSizeManager getInstance( + String filesystemName, AbfsConfiguration abfsConfiguration) { + /* Check if an instance already exists in the map for the given filesystem */ + WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get( + filesystemName); + + /* If an existing instance is found, return it */ + if (existingInstance != null && existingInstance.boundedThreadPool != null + && !existingInstance.boundedThreadPool.isShutdown()) { + return existingInstance; + } + + /* Otherwise, create a new instance, put it in the map, and return it */ + LOG.debug( + \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\", + filesystemName); + WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager( + filesystemName, abfsConfiguration); + POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance); + return newInstance; + } + + /** + * Adjusts the thread pool size to the specified maximum pool size. + * + * @param newMaxPoolSize the new maximum pool size. + */ + private void adjustThreadPoolSize(int newMaxPoolSize) { + synchronized (this) { + ThreadPoolExecutor threadPoolExecutor + = ((ThreadPoolExecutor) boundedThreadPool); + int currentCorePoolSize = threadPoolExecutor.getCorePoolSize(); + + if (newMaxPoolSize >= currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); + LOG.debug(\"The pool size is: {} \", threadPoolExecutor.getPoolSize()); + LOG.debug(\"The active thread count is: {}\", threadPoolExecutor.getActiveCount()); + } + } + + /** + * Starts monitoring the CPU utilization and adjusts the thread pool size accordingly. + */ + synchronized void startCPUMonitoring() { + cpuMonitorExecutor.scheduleAtFixedRate(() -> { + double cpuUtilization = getCpuUtilization(); + LOG.debug(\"Current CPU Utilization is this: {}\", cpuUtilization); + try { + adjustThreadPoolSizeBasedOnCPU(cpuUtilization); + } catch (InterruptedException e) { + throw new RuntimeException(String.format( + \"Thread pool size adjustment interrupted for filesystem %s\", + filesystemName), e); + } + }, 0, getAbfsConfiguration().getWriteCpuMonitoringInterval(), TimeUnit.SECONDS); + } + + /** + * Gets the current CPU utilization. + * + * @return the CPU utilization as a percentage (0.0 to 1.0). + */ + private double getCpuUtilization() { + OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean(); + if (osBean instanceof com.sun.management.OperatingSystemMXBean) { + com.sun.management.OperatingSystemMXBean sunOsBean + = (com.sun.management.OperatingSystemMXBean) osBean; + double cpuLoad = sunOsBean.getSystemCpuLoad(); + if (cpuLoad >= 0) { Review Comment: if cpuLoad is -1.0, should we log it?", "created": "2025-09-02T03:55:19.950+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2314884087 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -438,6 +438,10 @@ public class AbfsConfiguration{ FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION) private boolean isChecksumValidationEnabled; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = Review Comment: Nit: we can remove it (part of prev PR)", "created": "2025-09-02T04:31:25.563+0000"}, {"author": "ASF GitHub Bot", "body": "manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2314884087 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -438,6 +438,10 @@ public class AbfsConfiguration{ FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION) private boolean isChecksumValidationEnabled; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = Review Comment: Nit: we can remove it and related ones below (part of prev PR)", "created": "2025-09-02T04:33:03.176+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3252145988 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 17s | | https://github.com/apache/hadoop/pull/7669 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-04T06:38:48.437+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3252822256 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 24s | | trunk passed | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 27m 36s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 27m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 27s | | The patch does not generate ASF License warnings. | | | | 93m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f783f8b2200a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6f5cd826e33f1d86cc5f4aa373268b6ef03fb4bc | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 563 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-04T09:37:37.727+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3257523521 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 16m 4s | | Docker failed to build run-specific yetus/hadoop:tp-31571}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-05T08:30:39.811+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3258488704 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 9m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 33m 34s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 0m 16s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 23s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | trunk passed | | -1 :x: | shadedclient | 22m 8s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 33s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 22s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 21s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 4m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 23s | | ASF License check generated no output? | | | | 75m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 88e24d886e71 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ed6c78ee45d94c69e87c25c3b95b55effeed6923 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-05T14:07:23.238+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3265728007 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 167 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 822, Failures: 0, Errors: 0, Skipped: 119 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 661, Failures: 0, Errors: 0, Skipped: 235 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 178 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 668, Failures: 0, Errors: 0, Skipped: 161 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 658, Failures: 0, Errors: 0, Skipped: 237 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 665, Failures: 0, Errors: 0, Skipped: 173 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 660, Failures: 0, Errors: 0, Skipped: 195 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 693, Failures: 0, Errors: 0, Skipped: 176 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 658, Failures: 0, Errors: 0, Skipped: 234 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24", "created": "2025-09-08T10:50:59.689+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3265990736 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 32s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 45s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 18s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 78m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 74a5f58b2863 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 443556cdb787d2c6be17e9317a9870812950506e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-08T12:08:02.077+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3317397225 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 24s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 22s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 49s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 35s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 24m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 22s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 98m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c3b676228a26 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6a0e5bcaabcdfcbbc3ba4dc1ab3cefc20362dfdf | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-09-22T07:45:21.363+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2450656923 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); Review Comment: ll the log lines can be combined into a single log here. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getLowTierMemoryMultiplier(); + } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getMediumTierMemoryMultiplier(); + } else { + multiplier = abfsConfiguration.getHighTierMemoryMultiplier(); + } + return availableProcessors * multiplier; + } + + /** + * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem. + * + * @param filesystemName the name of the filesystem. + * @param abfsConfiguration the configuration for the ABFS. + * + * @return the singleton instance. + */ + public static synchronized WriteThreadPoolSizeManager getInstance( + String filesystemName, AbfsConfiguration abfsConfiguration) { + /* Check if an instance already exists in the map for the given filesystem */ + WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get( + filesystemName); + + /* If an existing instance is found, return it */ + if (existingInstance != null && existingInstance.boundedThreadPool != null + && !existingInstance.boundedThreadPool.isShutdown()) { + return existingInstance; + } + + /* Otherwise, create a new instance, put it in the map, and return it */ + LOG.debug( + \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\", + filesystemName); + WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager( + filesystemName, abfsConfiguration); + POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance); + return newInstance; + } + + /** + * Adjusts the thread pool size to the specified maximum pool size. + * + * @param newMaxPoolSize the new maximum pool size. + */ + private void adjustThreadPoolSize(int newMaxPoolSize) { + synchronized (this) { + ThreadPoolExecutor threadPoolExecutor + = ((ThreadPoolExecutor) boundedThreadPool); + int currentCorePoolSize = threadPoolExecutor.getCorePoolSize(); + + if (newMaxPoolSize >= currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); Review Comment: Setting both core pool size and max pool size to same value will make it a fixed size thread pool. We should only set max pool size and executor will spawn new threads only when needed. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { Review Comment: Are we computing thread pool size based on available memory? Shouldn't it be available cpu? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( Review Comment: We are using a fixed Thread Pool executor service here. with initialThreadPoolSize Where are we setting the max threread pool size here? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getLowTierMemoryMultiplier(); + } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getMediumTierMemoryMultiplier(); + } else { + multiplier = abfsConfiguration.getHighTierMemoryMultiplier(); + } + return availableProcessors * multiplier; + } + + /** + * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem. + * + * @param filesystemName the name of the filesystem. + * @param abfsConfiguration the configuration for the ABFS. + * + * @return the singleton instance. + */ + public static synchronized WriteThreadPoolSizeManager getInstance( + String filesystemName, AbfsConfiguration abfsConfiguration) { + /* Check if an instance already exists in the map for the given filesystem */ + WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get( + filesystemName); + + /* If an existing instance is found, return it */ + if (existingInstance != null && existingInstance.boundedThreadPool != null + && !existingInstance.boundedThreadPool.isShutdown()) { + return existingInstance; + } + + /* Otherwise, create a new instance, put it in the map, and return it */ + LOG.debug( + \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\", + filesystemName); + WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager( + filesystemName, abfsConfiguration); + POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance); + return newInstance; + } + + /** + * Adjusts the thread pool size to the specified maximum pool size. + * + * @param newMaxPoolSize the new maximum pool size. + */ + private void adjustThreadPoolSize(int newMaxPoolSize) { + synchronized (this) { + ThreadPoolExecutor threadPoolExecutor + = ((ThreadPoolExecutor) boundedThreadPool); + int currentCorePoolSize = threadPoolExecutor.getCorePoolSize(); + + if (newMaxPoolSize >= currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); Review Comment: Have a single log line ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); Review Comment: This seems a bit misleading. Configuration says its the max count but t is used as a inital thread pool size, may be a better variable name like `configuredMaxPoolSize` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -478,6 +478,57 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES) private int maxApacheHttpClientIoExceptionsRetries; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT, + DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT) + private boolean dynamicWriteThreadPoolEnablement; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME, + DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME) + private int writeThreadPoolKeepAliveTime; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL, + MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL, + MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL, + DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL) + private int writeCpuMonitoringInterval; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE, Review Comment: Is this the minimum thread pool size? Should we name it likewise then?", "created": "2025-10-22T07:18:00.525+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454205775 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -321,16 +330,13 @@ public void close() throws IOException { try { Futures.allAsList(futures).get(); // shutdown the threadPool and set it to null. - HadoopExecutors.shutdown(boundedThreadPool, LOG, - 30, TimeUnit.SECONDS); - boundedThreadPool = null; } catch (InterruptedException e) { LOG.error(\"Interrupted freeing leases\", e); Thread.currentThread().interrupt(); } catch (ExecutionException e) { LOG.error(\"Error freeing leases\", e); } finally { - IOUtils.cleanupWithLogger(LOG, getClient()); + IOUtils.cleanupWithLogger(LOG, poolSizeManager, getClient()); Review Comment: That is taken care when the AzureBlobFileSystemStore is shutdown ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); Review Comment: Taken", "created": "2025-10-23T07:36:53.064+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454234411 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); + + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getLowTierMemoryMultiplier(); + } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getMediumTierMemoryMultiplier(); + } else { + multiplier = abfsConfiguration.getHighTierMemoryMultiplier(); + } + return availableProcessors * multiplier; + } + + /** + * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem. + * + * @param filesystemName the name of the filesystem. + * @param abfsConfiguration the configuration for the ABFS. + * + * @return the singleton instance. + */ + public static synchronized WriteThreadPoolSizeManager getInstance( + String filesystemName, AbfsConfiguration abfsConfiguration) { + /* Check if an instance already exists in the map for the given filesystem */ + WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get( + filesystemName); + + /* If an existing instance is found, return it */ + if (existingInstance != null && existingInstance.boundedThreadPool != null + && !existingInstance.boundedThreadPool.isShutdown()) { + return existingInstance; + } + + /* Otherwise, create a new instance, put it in the map, and return it */ + LOG.debug( + \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\", + filesystemName); + WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager( + filesystemName, abfsConfiguration); + POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance); + return newInstance; + } + + /** + * Adjusts the thread pool size to the specified maximum pool size. + * + * @param newMaxPoolSize the new maximum pool size. + */ + private void adjustThreadPoolSize(int newMaxPoolSize) { + synchronized (this) { + ThreadPoolExecutor threadPoolExecutor + = ((ThreadPoolExecutor) boundedThreadPool); + int currentCorePoolSize = threadPoolExecutor.getCorePoolSize(); + + if (newMaxPoolSize >= currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); + LOG.debug(\"The pool size is: {} \", threadPoolExecutor.getPoolSize()); + LOG.debug(\"The active thread count is: {}\", threadPoolExecutor.getActiveCount()); + } + } + + /** + * Starts monitoring the CPU utilization and adjusts the thread pool size accordingly. + */ + synchronized void startCPUMonitoring() { + cpuMonitorExecutor.scheduleAtFixedRate(() -> { + double cpuUtilization = getCpuUtilization(); + LOG.debug(\"Current CPU Utilization is this: {}\", cpuUtilization); + try { + adjustThreadPoolSizeBasedOnCPU(cpuUtilization); + } catch (InterruptedException e) { + throw new RuntimeException(String.format( + \"Thread pool size adjustment interrupted for filesystem %s\", + filesystemName), e); + } + }, 0, getAbfsConfiguration().getWriteCpuMonitoringInterval(), TimeUnit.SECONDS); + } + + /** + * Gets the current CPU utilization. + * + * @return the CPU utilization as a percentage (0.0 to 1.0). + */ + private double getCpuUtilization() { + OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean(); + if (osBean instanceof com.sun.management.OperatingSystemMXBean) { + com.sun.management.OperatingSystemMXBean sunOsBean + = (com.sun.management.OperatingSystemMXBean) osBean; + double cpuLoad = sunOsBean.getSystemCpuLoad(); + if (cpuLoad >= 0) { Review Comment: taken", "created": "2025-10-23T07:49:04.744+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454238973 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -438,6 +438,10 @@ public class AbfsConfiguration{ FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION) private boolean isChecksumValidationEnabled; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = Review Comment: taken care by trunk merge", "created": "2025-10-23T07:50:41.003+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454257729 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -478,6 +478,57 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES) private int maxApacheHttpClientIoExceptionsRetries; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT, + DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT) + private boolean dynamicWriteThreadPoolEnablement; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME, + DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME) + private int writeThreadPoolKeepAliveTime; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL, + MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL, + MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL, + DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL) + private int writeCpuMonitoringInterval; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE, Review Comment: This was used to determine the no. of threads to spawn for CPU monitoring and was kept default as 1 but it would be better to always have a single thread and hence removed this config,", "created": "2025-10-23T07:58:17.584+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454304102 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); Review Comment: Taken", "created": "2025-10-23T08:17:18.877+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454309973 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( Review Comment: So we are starting with the configured value of writeconcurrentrequestcount as the initial thread pool size which is also the max until it is scaled further, the logic for which is present in the adjustThreadPoolBasedOnCpu", "created": "2025-10-23T08:19:42.481+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3435800307 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 31m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 17m 44s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 17m 58s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 53 new + 1472 unchanged - 0 fixed = 1525 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 53 new + 1413 unchanged - 0 fixed = 1466 total (was 1413) | | -1 :x: | spotbugs | 0m 49s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 1 new + 178 unchanged - 0 fixed = 179 total (was 178) | | +1 :green_heart: | shadedclient | 18m 36s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 77m 36s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.getAbfsConfiguration() may expose internal representation by returning WriteThreadPoolSizeManager.abfsConfiguration At WriteThreadPoolSizeManager.java:by returning WriteThreadPoolSizeManager.abfsConfiguration At WriteThreadPoolSizeManager.java:[line 127] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e7d97cc8e101 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7708c26a556b7b84bd7edb6c70304f6f301eb0c3 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 642 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T08:49:58.762+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454540355 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); Review Comment: taken", "created": "2025-10-23T09:45:15.835+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454542830 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { Review Comment: This is during initialization when CPU doesn't play a role, CPU is used for scaling and descaling only in the current design", "created": "2025-10-23T09:46:16.037+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454547932 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getLowTierMemoryMultiplier(); + } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getMediumTierMemoryMultiplier(); + } else { + multiplier = abfsConfiguration.getHighTierMemoryMultiplier(); + } + return availableProcessors * multiplier; + } + + /** + * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem. + * + * @param filesystemName the name of the filesystem. + * @param abfsConfiguration the configuration for the ABFS. + * + * @return the singleton instance. + */ + public static synchronized WriteThreadPoolSizeManager getInstance( + String filesystemName, AbfsConfiguration abfsConfiguration) { + /* Check if an instance already exists in the map for the given filesystem */ + WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get( + filesystemName); + + /* If an existing instance is found, return it */ + if (existingInstance != null && existingInstance.boundedThreadPool != null + && !existingInstance.boundedThreadPool.isShutdown()) { + return existingInstance; + } + + /* Otherwise, create a new instance, put it in the map, and return it */ + LOG.debug( + \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\", + filesystemName); + WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager( + filesystemName, abfsConfiguration); + POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance); + return newInstance; + } + + /** + * Adjusts the thread pool size to the specified maximum pool size. + * + * @param newMaxPoolSize the new maximum pool size. + */ + private void adjustThreadPoolSize(int newMaxPoolSize) { + synchronized (this) { + ThreadPoolExecutor threadPoolExecutor + = ((ThreadPoolExecutor) boundedThreadPool); + int currentCorePoolSize = threadPoolExecutor.getCorePoolSize(); + + if (newMaxPoolSize >= currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); Review Comment: If we do on need basis, it doesn't serve our purpose of being aggressive. This was tried as well and even though max was set to higher number, available threads were not getting used.", "created": "2025-10-23T09:47:45.980+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454550766 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager> + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getLowTierMemoryMultiplier(); + } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) { + multiplier = abfsConfiguration.getMediumTierMemoryMultiplier(); + } else { + multiplier = abfsConfiguration.getHighTierMemoryMultiplier(); + } + return availableProcessors * multiplier; + } + + /** + * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem. + * + * @param filesystemName the name of the filesystem. + * @param abfsConfiguration the configuration for the ABFS. + * + * @return the singleton instance. + */ + public static synchronized WriteThreadPoolSizeManager getInstance( + String filesystemName, AbfsConfiguration abfsConfiguration) { + /* Check if an instance already exists in the map for the given filesystem */ + WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get( + filesystemName); + + /* If an existing instance is found, return it */ + if (existingInstance != null && existingInstance.boundedThreadPool != null + && !existingInstance.boundedThreadPool.isShutdown()) { + return existingInstance; + } + + /* Otherwise, create a new instance, put it in the map, and return it */ + LOG.debug( + \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\", + filesystemName); + WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager( + filesystemName, abfsConfiguration); + POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance); + return newInstance; + } + + /** + * Adjusts the thread pool size to the specified maximum pool size. + * + * @param newMaxPoolSize the new maximum pool size. + */ + private void adjustThreadPoolSize(int newMaxPoolSize) { + synchronized (this) { + ThreadPoolExecutor threadPoolExecutor + = ((ThreadPoolExecutor) boundedThreadPool); + int currentCorePoolSize = threadPoolExecutor.getCorePoolSize(); + + if (newMaxPoolSize >= currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); Review Comment: taken", "created": "2025-10-23T09:48:41.077+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3436426336 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 16m 11s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 16m 24s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 50 new + 1472 unchanged - 0 fixed = 1522 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 50 new + 1413 unchanged - 0 fixed = 1463 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 16m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 67m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5c77e6ac2aef 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fb58cf38644cae72dc0238fa354724d0af74d820 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 612 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T11:23:09.821+0000"}], "derived_tasks": {"summary": "ABFS: Enhance performance of ABFS driver for write-heavy workloads - The goal of this work item is to enhance the performance of ABFS Driver for wr...", "classifications": ["improvement", "sub-task", "performance"], "qa_pairs": []}}
{"id": "HADOOP-19402", "title": "[JDK11] JDiff Support JDK11", "description": "JDiff is a tool used to generate Java API documentation and display differences between Java classes, primarily for detecting changes in Java API across versions. However, JDiff does not support JDK 9 and above, and there are no plans for further updates. Maven version: JDiff 1.0.9 https://mvnrepository.com/artifact/jdiff/jdiff/1.0.9 Official website: JDiff https://jdiff.sourceforge.net/ During JDK 11 support, the changes in HADOOP-15304 allowed the project to compile successfully but skipped JDiff. However, generating the JDiff XML is still an essential step in the release process. To release Hadoop for JDK 11 and above, we need a tool similar to JDiff to handle API comparison and detect version changes.", "status": "In Progress", "priority": "Major", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "created": "2025-01-24T23:47:07.000+0000", "updated": "2025-10-19T09:44:38.000+0000", "labels": ["pull-request-available"], "components": ["build", "documentation"], "comments": [{"author": "Shilun Fan", "body": "[~stevel@apache.org] [~aajisaka] [~ayushtkn] My idea is to release a higher version of JDiff (that supports JDK 11 and above) ourselves, to support the Hadoop project. Do you have any suggestions?", "created": "2025-01-24T23:57:40.555+0000"}, {"author": "Steve Loughran", "body": "Simplest would be to make it your own GH repo...it's not redistributed so no license issues", "created": "2025-01-25T18:57:26.104+0000"}, {"author": "Shilun Fan", "body": "I'm working on compatibility optimization for Jdiff and hope to complete it successfully. However, the relevant code seems outdated and needs to be updated.", "created": "2025-07-12T02:23:58.307+0000"}, {"author": "H. Vetinari", "body": "{quote}However, JDiff does not support JDK 9 and above, and there are no plans for further updates. {quote} To me that sounds like an unhealthy approach. If upstream jdiff stopped maintenance, the right approach is to drop reliance on jdiff (even if your docs are less nice as a consequence), rather than taking over maintenance of a completely unrelated tool. Hadoop is struggling with staying up-to-date already (years behind LTS Java support), so it's really not a good idea IMO to add yet more things that cannibalise maintainer attention.", "created": "2025-08-25T23:07:15.661+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #8038: URL: https://github.com/apache/hadoop/pull/8038#issuecomment-3419416423 @steveloughran @pan3793 This PR has been in preparation for quite some time by Hualong, and it consists of two main parts: - Refactoring JDiff to make it compatible and functional under JDK 17. - Updating Hadoop\u2019s custom doc-related annotations to ensure they are compatible with JDK 17 as well. I\u2019d appreciate your help reviewing this PR. For reference, the JDiff-related changes are available in [zhtttylz/jdiff#4](https://github.com/zhtttylz/jdiff/pull/4)", "created": "2025-10-19T08:42:25.452+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8038: URL: https://github.com/apache/hadoop/pull/8038#issuecomment-3419528107 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 3s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 28m 31s | | trunk passed | | +1 :green_heart: | compile | 15m 14s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 35s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 2m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 8s | | trunk passed | | +1 :green_heart: | javadoc | 2m 8s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 2m 0s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 46s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 41s | | branch/hadoop-project-dist no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 25m 49s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 17s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 14s | [/patch-mvninstall-hadoop-common-project_hadoop-annotations.txt]([CI_URL] | hadoop-annotations in the patch failed. | | -1 :x: | compile | 0m 18s | [/patch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 18s | [/patch-compile-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 18s | [/patch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 18s | [/patch-compile-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 8 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 15s | [/buildtool-patch-checkstyle-root.txt]([CI_URL] | The patch fails to run checkstyle in root | | -1 :x: | mvnsite | 0m 14s | [/patch-mvnsite-hadoop-common-project_hadoop-annotations.txt]([CI_URL] | hadoop-annotations in the patch failed. | | -1 :x: | javadoc | 0m 14s | [/patch-javadoc-hadoop-common-project_hadoop-annotations-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-annotations in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 13s | [/patch-javadoc-hadoop-common-project_hadoop-annotations-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-annotations in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +0 :ok: | spotbugs | 0m 16s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 0m 14s | [/patch-spotbugs-hadoop-common-project_hadoop-annotations.txt]([CI_URL] | hadoop-annotations in the patch failed. | | +0 :ok: | spotbugs | 0m 17s | | hadoop-project-dist has no data from spotbugs | | -1 :x: | shadedclient | 1m 2s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 15s | | hadoop-project in the patch passed. | | -1 :x: | unit | 0m 14s | [/patch-unit-hadoop-common-project_hadoop-annotations.txt]([CI_URL] | hadoop-annotations in the patch failed. | | +1 :green_heart: | unit | 0m 14s | | hadoop-project-dist in the patch passed. | | +1 :green_heart: | asflicense | 0m 28s | | The patch does not generate ASF License warnings. | | | | 136m 24s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8038 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux a85451735ded 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8f9c11f991f293848a323de038a99b4b4a512adf | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 768 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-annotations hadoop-project-dist U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-19T09:44:38.140+0000"}], "derived_tasks": {"summary": "[JDK11] JDiff Support JDK11 - JDiff is a tool used to generate Java API documentation and display differences between Java classes, primarily for d...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19217", "title": "Introduce getTrashPolicy to FileSystem API", "description": "Hadoop FileSystem supports multiple FileSystem implementations awareness (e.g. client is aware of both hdfs:// and ofs:// protocols). However, it seems that currently Hadoop TrashPolicy remains the same regardless of the URI scheme. The TrashPolicy is governed by \"fs.trash.classname\" configuration and stays the same regardless of the FileSystem implementation. For example, HDFS defaults to TrashPolicyDefault and Ozone defaults to TrashPolicyOzone, but only one will be picked since the the configuration will be overwritten by the other. Therefore, I propose to tie the TrashPolicy implementation to each FileSystem implementation by introducing a new FileSystem#getTrashPolicy interface. TrashPolicy#getInstance can call FileSystem#getTrashPolicy to get the appropriate TrashPolicy.", "status": "Open", "priority": "Major", "reporter": "Ivan Andika", "assignee": null, "created": "2024-07-02T10:32:50.000+0000", "updated": "2025-10-19T02:56:35.000+0000", "labels": [], "components": ["fs"], "comments": [{"author": "Steve Loughran", "body": "ok, in HADOOP-18013 I tried to start on this with an fs.SCHEME.trash policy; having a full API would be even more agile * see the javadoc comments in the top of FileSystem.java about implementing any new filesystem api * needs matching coverage in filestystem.md specification and a contract test.", "created": "2024-07-02T10:45:30.060+0000"}], "derived_tasks": {"summary": "Introduce getTrashPolicy to FileSystem API - Hadoop FileSystem supports multiple FileSystem implementations awareness (e", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-19212", "title": "[JDK23] org.apache.hadoop.security.UserGroupInformation use of Subject needs to move to replacement APIs", "description": "`javax.security.auth.Subject.getSubject` and `Subject.doAs` were deprecated for removal in JDK 17. The replacement APIs are `Subject.current` and `callAs`. See [JEP 411]([https://openjdk.org/jeps/411]) for background. The `Subject.getSubject` API has been \"degraded\" in JDK 23 to throw `UnsupportedOperationException` if not running with the option to allow a SecurityManager. In a future JDK release, the `Subject.getSubject` API will be degraded further to throw`UnsupportedOperationException` unconditionally. [renaissance/issues/439]([https://github.com/renaissance-benchmarks/renaissance/issues/439]) is a failure with a Spark benchmark due to the code in `org.apache.hadoop.security.UserGroupInformation` using the deprecated `Subject.getSubject` method. The maintainers of this code need to migrate this code to the replacement APIs to ensure that this code will continue to work once the security manager feature is removed.", "status": "Open", "priority": "Major", "reporter": "Alan Bateman", "assignee": null, "created": "2024-06-28T08:02:28.000+0000", "updated": "2025-10-22T10:04:17.000+0000", "labels": ["pull-request-available"], "components": ["security"], "comments": [{"author": "Alan Bateman", "body": "The JDK Enhancement Proposal (JEP) for the removal of the Security Management (SM) implementation is currently being drafted. One piece of this is the `Subject.getSubject(AccessControlContext)` method that is proposed to be re-specified to throw `UnsupportedOperastionException` unconditionally. The workaround in JDK 23 to allow the method continue to work if a SM is allowed to be set goes away. As per the JDK release notes, code using `Subject.getSubject(AccessControlContext)` is strongly encouraged to move to the replacement APIs as soon as possible.", "created": "2024-08-30T14:29:05.771+0000"}, {"author": "Steve Loughran", "body": "restoration would be great. we don't use the whole security manager stuff but we do have different kerberos principals doing things and want to isolate their permissions. for example, file managers can be created by different accounts in the same process. maybe not at the same time, but after a job is done a long live process can deleteAllforUGI and have that principal's instances closed", "created": "2024-09-03T20:39:57.841+0000"}, {"author": "Alan Bateman", "body": "I'm not sure what you mean by \"restoration would be great\". \u00a0`Subject.getSubject(AccessControlContext)` is deprecated for removal, the next step will change this method to throw `UnsupportedOperationException` always. Code using `Subject.doAs` and `Subject.getSubject` should be changed to use the replacement APIs `Subject.callAs` and `Subject.current` as soon as possible.", "created": "2024-09-04T06:47:40.714+0000"}, {"author": "Steve Loughran", "body": "problem here is that Subject.doAs() is ubiquitous in a codebase which still (at least until 3.4.1 ships) builds against java8 and where some of the java17 issues are still surfacing. the only way we can let people run on java23 is going to have to be reflection games", "created": "2024-09-04T10:24:43.536+0000"}, {"author": "Alan Bateman", "body": "If you really need to span 10+ years of JDK releases then using multi-release JARs or using core reflection to invoke the Subject methods will work. \u00a0`Subject.getSubject(AccessControlContext)` has been deprecated for removal since JDK 17, that's the important method to find usages of so they can be changed to use `Subject.current()`. \u00a0The `Subject.doAs` methods are deprecated for removal too but will continue to work when the SM is removed (they will just invoke the action with the Subject), at leat until the methods are eventually removed.", "created": "2024-09-04T10:41:05.860+0000"}, {"author": "Steve Loughran", "body": "not 3.4.1. we may have to keep 3.4.x java8 and release a 3.5.0 with java17 and other incompatible changes", "created": "2024-10-02T15:00:34.891+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1802382646 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapter.java: ########## @@ -0,0 +1,60 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.security.auth.Subject; + +/** + * javax.security.auth.Subject.getSubject is deprecated for removal. + * The replacement API exists only in Java 18 and above. + * This class helps use the newer API if available, without raising the language level. + */ +public class SubjectAdapter { + private static Logger log = LoggerFactory.getLogger(SubjectAdapter.class); + private static final HiddenSubjectAdapter instance; + static { + int version = 0; + try { + version = Integer.parseInt(System.getProperty(\"java.specification.version\")); + } catch (Throwable ignored) {} + if (version >= 18) { + instance = new SubjectAdapterJava18AndAbove(); + } else { + instance = new ClassicSubjectAdapter(); + } + } + + private SubjectAdapter() {} + + public static Subject getSubject() { + return instance.getSubject(); + } + + /** + * This main method is included so that this is trivially tested using multiple JDKs outside the scope test sources + * @param args ignored + */ + public static void main(String[] args) { Review Comment: can we remove this test method?", "created": "2024-10-16T05:34:00.146+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1802383411 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java: ########## @@ -91,6 +89,7 @@ import org.apache.hadoop.util.StringUtils; import org.apache.hadoop.util.Time; +import org.apache.hadoop.util.subject.SubjectAdapter; Review Comment: this \"import\" belongs to the above category", "created": "2024-10-16T05:35:08.267+0000"}, {"author": "ASF GitHub Bot", "body": "AlanBateman commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-2415929746 [JEP 486: Permanently Disable the Security Manager](https://openjdk.org/jeps/486) is on the JDK's technical roadmap. As part of the changes (PR in review) will be for Subject.current to throw unconditionally, meaning the workaround to allow a SecurityManager will no longer work. So the timing of the PR here is good.", "created": "2024-10-16T07:18:16.493+0000"}, {"author": "ASF GitHub Bot", "body": "jbrinegar commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1803323144 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapter.java: ########## @@ -0,0 +1,60 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import javax.security.auth.Subject; + +/** + * javax.security.auth.Subject.getSubject is deprecated for removal. + * The replacement API exists only in Java 18 and above. + * This class helps use the newer API if available, without raising the language level. + */ +public class SubjectAdapter { + private static Logger log = LoggerFactory.getLogger(SubjectAdapter.class); + private static final HiddenSubjectAdapter instance; + static { + int version = 0; + try { + version = Integer.parseInt(System.getProperty(\"java.specification.version\")); + } catch (Throwable ignored) {} + if (version >= 18) { + instance = new SubjectAdapterJava18AndAbove(); + } else { + instance = new ClassicSubjectAdapter(); + } + } + + private SubjectAdapter() {} + + public static Subject getSubject() { + return instance.getSubject(); + } + + /** + * This main method is included so that this is trivially tested using multiple JDKs outside the scope test sources + * @param args ignored + */ + public static void main(String[] args) { Review Comment: removed in latest commit. it is significantly more difficult to prove classloading mechanics in a junit test than it is in a main.", "created": "2024-10-16T15:16:27.163+0000"}, {"author": "ASF GitHub Bot", "body": "jbrinegar commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1803323852 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java: ########## @@ -91,6 +89,7 @@ import org.apache.hadoop.util.StringUtils; import org.apache.hadoop.util.Time; +import org.apache.hadoop.util.subject.SubjectAdapter; Review Comment: moved up in the latest commit", "created": "2024-10-16T15:16:49.353+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-2417842205 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 38m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 22s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 5m 38s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 36m 52s | | trunk passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | -1 :x: | compile | 9m 6s | [/branch-compile-root-jdkPrivateBuild-1.8.0_422-8u422-b05-1~20.04-b05.txt]([CI_URL] | root in trunk failed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05. | | +1 :green_heart: | checkstyle | 1m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 45s | | trunk passed | | +1 :green_heart: | javadoc | 3m 7s | | trunk passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | javadoc | 2m 14s | | trunk passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | spotbugs | 4m 52s | | trunk passed | | +1 :green_heart: | shadedclient | 47m 24s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 47m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 20s | | the patch passed | | +1 :green_heart: | compile | 19m 36s | | the patch passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | javac | 19m 36s | | the patch passed | | +1 :green_heart: | compile | 17m 59s | | the patch passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | javac | 17m 59s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 26s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 46 new + 73 unchanged - 0 fixed = 119 total (was 73) | | +1 :green_heart: | mvnsite | 2m 26s | | the patch passed | | +1 :green_heart: | javadoc | 1m 56s | | the patch passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | javadoc | 1m 35s | | the patch passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | spotbugs | 3m 56s | | the patch passed | | +1 :green_heart: | shadedclient | 42m 48s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 29s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 19m 35s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 288m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.47 ServerAPI=1.47 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7081 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d3c644186516 5.15.0-118-generic #128-Ubuntu SMP Fri Jul 5 09:28:59 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4325f46eeb78b186d4cb29641b974212cd35ff25 | | Default Java | Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 3098 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-10-16T20:06:19.174+0000"}, {"author": "ASF GitHub Bot", "body": "jbrinegar commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-2417954717 I am not sure I have the permissions to retrigger the failing checks. Ultimate failure was on the remote side: ``` [WARNING] c++: fatal error: cannot execute \u00e2\u20ac\u02dcas\u00e2\u20ac\u2122: vfork: Resource temporarily unavailable [WARNING] compilation terminated. [WARNING] make[2]: *** [main/native/libhdfspp/tests/CMakeFiles/user_lock_test.dir/build.make:63: main/native/libhdfspp/tests/CMakeFiles/user_lock_test.dir/user_lock_test.cc.o] Error 1 ``` This java code compiles just fine: ``` [INFO] Reactor Summary for Apache Hadoop Common Project 3.5.0-SNAPSHOT: [INFO] [INFO] Apache Hadoop Annotations .......................... SUCCESS [ 0.710 s] [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 0.271 s] [INFO] Apache Hadoop Auth ................................. SUCCESS [ 0.769 s] [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 0.094 s] [INFO] Apache Hadoop Common ............................... SUCCESS [ 36.642 s] [INFO] Apache Hadoop NFS .................................. SUCCESS [ 7.181 s] [INFO] Apache Hadoop KMS .................................. SUCCESS [ 0.227 s] [INFO] Apache Hadoop Registry ............................. SUCCESS [ 0.439 s] [INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.018 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 46.854 s [INFO] Finished at: 2024-10-16T16:54:14-04:00 [INFO] ------------------------------------------------------------------------ ```", "created": "2024-10-16T21:00:30.692+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1806337086 ########## hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/util/subject/TestSubjectAdapter.java: ########## @@ -0,0 +1,34 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import org.junit.jupiter.api.Test; + +import static org.junit.jupiter.api.Assertions.assertDoesNotThrow; + +class TestSubjectAdapter { + + @Test + void getSubject() { + // how getSubject operates depends on the JVM calling it. + // asserting that it does not throw is a valid test, especially on Java 18 and above + // prior to Java 18, this method is just a simple wrapper + assertDoesNotThrow(() -> SubjectAdapter.getSubject()); Review Comment: Use our LambdaTestUtils.intercept() here as it fails better, can validates the type and message ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapterJava18AndAbove.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class SubjectAdapterJava18AndAbove implements HiddenSubjectAdapter { Review Comment: Please use our new DynMethods classes, (which we copied from parquet and which is also found in iceberg). It fails better and as it is so common it's good to get familiar with it", "created": "2024-10-18T11:24:59.012+0000"}, {"author": "ASF GitHub Bot", "body": "jbrinegar commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1819853459 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapterJava18AndAbove.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class SubjectAdapterJava18AndAbove implements HiddenSubjectAdapter { Review Comment: @steveloughran my comment from here still applies. https://github.com/apache/hadoop/pull/7081#discussion_r1785492078", "created": "2024-10-28T22:30:44.005+0000"}, {"author": "ASF GitHub Bot", "body": "jbrinegar commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1819853824 ########## hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/util/subject/TestSubjectAdapter.java: ########## @@ -0,0 +1,34 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import org.junit.jupiter.api.Test; + +import static org.junit.jupiter.api.Assertions.assertDoesNotThrow; + +class TestSubjectAdapter { + + @Test + void getSubject() { + // how getSubject operates depends on the JVM calling it. + // asserting that it does not throw is a valid test, especially on Java 18 and above + // prior to Java 18, this method is just a simple wrapper + assertDoesNotThrow(() -> SubjectAdapter.getSubject()); Review Comment: The comment in https://github.com/apache/hadoop/pull/7081#discussion_r1785492078 applies here as well.", "created": "2024-10-28T22:31:19.025+0000"}, {"author": "Istvan Toth", "body": "This not really related to JDK17. Shouldn't we create a JDK23 umbrella ticket, and move this there ?", "created": "2025-01-20T06:31:04.420+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921863972 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapter.java: ########## @@ -0,0 +1,52 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; + +/** + * javax.security.auth.Subject.getSubject is deprecated for removal. + * The replacement API exists only in Java 18 and above. + * This class helps use the newer API if available, without raising the language level. + */ +public class SubjectAdapter { + private static final HiddenGetSubject instance; + static { + int version = 0; + try { + version = Integer.parseInt(System.getProperty(\"java.specification.version\")); Review Comment: IMO testing trying to load the new classes and failing over to the old classes if they cannot be found would be more robust.", "created": "2025-01-20T06:38:02.916+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921866821 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/HiddenSubjectAdapter.java: ########## @@ -0,0 +1,25 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; + +public interface HiddenSubjectAdapter { Review Comment: I don't really like the name, maybe rename to JDKSpecificSubjectAdapter or similar ?", "created": "2025-01-20T06:41:34.876+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921870186 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/GetSubjectNg.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class GetSubjectNg implements HiddenGetSubject { + private final Method currentMethod; + + GetSubjectNg() { + try { + currentMethod = Subject.class.getMethod(\"current\"); Review Comment: @steveloughran , @jbrinegar 's point is valid. How do you propose solving the circular dependency issue ? Should the DynMethods code be copied to hadoop-auth ? Is there some suiteable small external dependency that provides DynMethods ?", "created": "2025-01-20T06:45:55.494+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921884028 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/GetSubjectNg.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class GetSubjectNg implements HiddenGetSubject { + private final Method currentMethod; + + GetSubjectNg() { + try { + currentMethod = Subject.class.getMethod(\"current\"); Review Comment: moving `Dyn*` to `hadoop-auth` module? then they can be consumed by every hadoop module.", "created": "2025-01-20T07:01:13.403+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921894133 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/GetSubjectNg.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class GetSubjectNg implements HiddenGetSubject { + private final Method currentMethod; + + GetSubjectNg() { + try { + currentMethod = Subject.class.getMethod(\"current\"); Review Comment: You are right. hadoop-common already depends on hadoop-auth, so that should be transparent to the rest of the code.", "created": "2025-01-20T07:10:43.568+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921894133 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/GetSubjectNg.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class GetSubjectNg implements HiddenGetSubject { + private final Method currentMethod; + + GetSubjectNg() { + try { + currentMethod = Subject.class.getMethod(\"current\"); Review Comment: I agree. I've just checjed that hadoop-common already depends on hadoop-auth, so the move should be transparent to all users.", "created": "2025-01-20T07:11:30.210+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1921894133 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/GetSubjectNg.java: ########## @@ -0,0 +1,48 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; +import java.lang.reflect.InvocationTargetException; +import java.lang.reflect.Method; + +/** + * Indirectly calls Subject.current(), which exists in Java 18 and above only + */ +class GetSubjectNg implements HiddenGetSubject { + private final Method currentMethod; + + GetSubjectNg() { + try { + currentMethod = Subject.class.getMethod(\"current\"); Review Comment: I agree. I've just confirmed that hadoop-common already depends on hadoop-auth, so the move should be transparent to all users.", "created": "2025-01-20T07:11:41.764+0000"}, {"author": "ASF GitHub Bot", "body": "cnauroth commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1929084760 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/ClassicSubjectAdapter.java: ########## @@ -0,0 +1,37 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; Review Comment: Can you please add a package-info.java in this new sub-directory and annotate it as private and unstable? Example: https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/package-info.java ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/HiddenSubjectAdapter.java: ########## @@ -0,0 +1,25 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; + +public interface HiddenSubjectAdapter { Review Comment: Can this be package-private instead of public?", "created": "2025-01-24T18:40:24.131+0000"}, {"author": "ASF GitHub Bot", "body": "ryantomlinson95 commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-2635156100 Thank you for working on this patch @jbrinegar! This will help eliminate a big blocker for us in trying to upgrade our stack to JDK 23.", "created": "2025-02-04T21:52:17.464+0000"}, {"author": "Dongjoon Hyun", "body": "Just a question. Do we have a timeline for Apache Hadoop 3.5.0? bq. not 3.4.1. we may have to keep 3.4.x java8 and release a 3.5.0 with java17 and other incompatible changes", "created": "2025-02-13T17:59:50.437+0000"}, {"author": "Chris Nauroth", "body": "Hello [~dongjoon] . We are actively working toward a 3.5.0 with Java 17 support. The latest discussion is here: [https://lists.apache.org/thread/4ct6f8lzs9sx56mw6c8w0sc38z76gm9g] Many of the most recent patches in trunk are related to Java 17 support. I wouldn't say we're at a point of having a firm timeline yet though. I saw you had filed SPARK-51168 for a potential upgrade to Hadoop 3.4.2 (also not in flight yet) in Spark 4.1.0. We can update that bug if we can get a 3.5.0 done for the Spark 4.1.0 release timeline.", "created": "2025-02-13T18:39:06.463+0000"}, {"author": "Dongjoon Hyun", "body": "Thank you for sharing the status, [~cnauroth].", "created": "2025-02-14T15:51:55.248+0000"}, {"author": "ASF GitHub Bot", "body": "stoty opened a new pull request, #7434: URL: https://github.com/apache/hadoop/pull/7434 \u2026use of Subject needs to move to replacement APIs ### Description of PR This is an alternative more ambitious PR for HADOOP-19212 , which aims to fully solve the JEP411 Subject changes ### How was this patch tested? Not tested yet, this is a draft ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-02-26T13:35:14.852+0000"}, {"author": "Istvan Toth", "body": "This is not that simple. on JDK22+ Subject is NOT propagated automatically (default settings).  package wtf; import java.util.concurrent.Callable; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import javax.security.auth.Subject; public class SubjectTester { public static Subject testSubject = new Subject(); public static void main(String[] args) { Subject.callAs(testSubject, new CheckSubject(\"same thread\", testSubject)); ExecutorService es = Executors.newSingleThreadExecutor(); Subject.callAs(testSubject, () -> es.submit(new CheckSubject(\"different thread\", testSubject))); } public static class CheckSubject implements Callable { String name; Subject expected; public CheckSubject(String name, Subject expected) { this.name = name; this.expected = expected; } @Override public Void call() { Subject found = Subject.current(); if (found != null && found == expected) { System.out.println(\"Subject inherited in \" + name); } else { System.out.println(\"!!! Subject not in thread in \" + name); } return null; } } }", "created": "2025-02-27T10:11:47.571+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2689156249 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 14 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 30s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 18m 39s | | trunk passed | | +1 :green_heart: | compile | 8m 16s | | trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 7m 18s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 1m 59s | | trunk passed | | +1 :green_heart: | mvnsite | 8m 28s | | trunk passed | | -1 :x: | javadoc | 0m 49s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-hdfs in trunk failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04. | | +1 :green_heart: | javadoc | 8m 4s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 12m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 22s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 4m 58s | | the patch passed | | +1 :green_heart: | compile | 7m 56s | | the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 7m 56s | | the patch passed | | +1 :green_heart: | compile | 7m 22s | | the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 7m 22s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 12 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 149 line(s) with tabs. | | -0 :warning: | checkstyle | 1m 58s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 149 new + 956 unchanged - 5 fixed = 1105 total (was 961) | | +1 :green_heart: | mvnsite | 8m 38s | | the patch passed | | -1 :x: | javadoc | 0m 27s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | -1 :x: | javadoc | 0m 42s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-common in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 48s | [/patch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-hdfs in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 28s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | spotbugs | 14m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 29s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 12m 37s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 1m 13s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 57s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 3m 28s | | hadoop-yarn-server-common in the patch passed. | | -1 :x: | unit | 93m 23s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt]([CI_URL] | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 23m 58s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 26m 15s | | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-mapreduce-client-common in the patch passed. | | -1 :x: | unit | 3m 33s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-hs.txt]([CI_URL] | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 112m 29s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 25m 23s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 44m 22s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 1m 11s | | hadoop-dynamometer-workload in the patch passed. | | -1 :x: | asflicense | 0m 42s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 542m 42s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens | | | hadoop.mapreduce.v2.hs.server.TestHSAdminServer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 52cfc1551f4f 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3efb9d26de8bda967281a2b419079d4743c21c21 | | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 3820 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-02-27T21:31:40.045+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2695975073 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 6m 57s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 48 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 6m 10s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 20m 3s | | trunk passed | | +1 :green_heart: | compile | 8m 33s | | trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 7m 24s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 2m 5s | | trunk passed | | +1 :green_heart: | mvnsite | 13m 8s | | trunk passed | | -1 :x: | javadoc | 0m 42s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-hdfs in trunk failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04. | | +1 :green_heart: | javadoc | 11m 51s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +0 :ok: | spotbugs | 0m 24s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 24s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 20m 26s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 9m 15s | | the patch passed | | +1 :green_heart: | compile | 8m 6s | | the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 8m 6s | | the patch passed | | +1 :green_heart: | compile | 7m 21s | | the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 7m 21s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 30 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 149 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 6s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 166 new + 2341 unchanged - 19 fixed = 2507 total (was 2360) | | +1 :green_heart: | mvnsite | 16m 5s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | -1 :x: | javadoc | 0m 44s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-common in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 50s | [/patch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-hdfs in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 27s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 18s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 27s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 19m 2s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 24s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 35s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 12m 43s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 29s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 2m 7s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 11s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 45s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 53s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 4m 58s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 3m 39s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 24s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | -1 :x: | unit | 92m 51s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt]([CI_URL] | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 23m 54s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 2m 54s | | hadoop-yarn-server-tests in the patch passed. | | +1 :green_heart: | unit | 26m 19s | | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 2s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-mapreduce-client-common in the patch passed. | | -1 :x: | unit | 7m 15s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt]([CI_URL] | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 36s | | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 112m 11s | | hadoop-mapreduce-client-jobclient in the patch passed. | | -1 :x: | unit | 0m 59s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch failed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in the patch failed. | | -1 :x: | unit | 0m 38s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-workload.txt]([CI_URL] | hadoop-dynamometer-workload in the patch failed. | | -1 :x: | unit | 0m 40s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch failed. | | -1 :x: | unit | 0m 39s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt]([CI_URL] | hadoop-gridmix in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | unit | 0m 38s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | unit | 0m 36s | [/patch-unit-hadoop-tools_hadoop-compat-bench.txt]([CI_URL] | hadoop-compat-bench in the patch failed. | | -1 :x: | asflicense | 0m 49s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 572m 59s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens | | | hadoop.mapreduce.v2.app.TestJobEndNotifier | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint | | uname | Linux d122f8a941ab 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ee4d6c5330ab01e82a49dd8fca3935c2379c9fa | | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-04T01:48:26.887+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2706369326 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 33m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 5s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 5s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 5s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 5s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 5s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 5s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 5s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 112 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 43s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 19m 5s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 4s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 53s | | trunk passed | | +1 :green_heart: | javadoc | 21m 49s | | trunk passed | | +0 :ok: | spotbugs | 0m 31s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 27s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 20m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 44s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 11s | | the patch passed | | -1 :x: | compile | 5m 55s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 55s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 61 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 19s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 5798 unchanged - 34 fixed = 6066 total (was 5832) | | +1 :green_heart: | mvnsite | 23m 12s | | the patch passed | | +1 :green_heart: | xmllint | 0m 1s | | No new issues. | | -1 :x: | javadoc | 0m 29s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 24s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 24s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 29s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 18m 36s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 26s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 33s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 44s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 31s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 14s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 11s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 5m 1s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 55s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 3s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 42s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 32s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 94m 58s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 12s | | hadoop-yarn-server-nodemanager in the patch passed. | | -1 :x: | unit | 2m 56s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt]([CI_URL] | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 34m 50s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 0s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 2s | | hadoop-mapreduce-client-common in the patch passed. | | -1 :x: | unit | 7m 14s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt]([CI_URL] | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 29s | | hadoop-mapreduce-client-hs in the patch passed. | | -1 :x: | unit | 144m 55s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt]([CI_URL] | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 25m 29s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 3m 51s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 42m 25s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 41s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 59s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 41s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 53s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 9s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 15s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 7s | | hadoop-mapreduce-examples in the patch passed. | | -1 :x: | unit | 7m 39s | [/patch-unit-hadoop-tools_hadoop-streaming.txt]([CI_URL] | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 17s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 38s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 1m 15s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 17m 15s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 2m 49s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 49s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 3s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 0m 58s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 4m 0s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 59s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 874m 10s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.server.TestMiniYarnCluster | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.TestRMFailover | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | | hadoop.mapreduce.v2.app.TestJobEndNotifier | | | hadoop.mapred.pipes.TestPipeApplication | | | hadoop.streaming.TestStreamingOutputOnlyKeys | | | hadoop.streaming.TestStreamReduceNone | | | hadoop.streaming.TestStreamingSeparator | | | hadoop.streaming.TestStreamAggregate | | | hadoop.tools.TestHadoopArchiveLogs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 760fd41a6336 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 922cfbc863b017859a813cb13c0e76396ffad3b2 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 2969 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-07T12:45:13.137+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2707212516 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 25s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 4s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 4s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 4s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 4s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 4s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 4s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 4s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 113 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 6m 19s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 20m 44s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 13s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 14s | | trunk passed | | +1 :green_heart: | javadoc | 21m 1s | | trunk passed | | +0 :ok: | spotbugs | 0m 31s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 25s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 21m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 26s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 12m 6s | | the patch passed | | -1 :x: | compile | 6m 25s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 6m 25s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 61 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 18s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 5805 unchanged - 34 fixed = 6073 total (was 5839) | | +1 :green_heart: | mvnsite | 22m 27s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 29s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 23s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 13s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 18s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 21m 20s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 26s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 35s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 36s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 22s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 0m 57s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 14s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 9s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 33s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 58s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 0s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 32s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 28s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 94m 52s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 30s | | hadoop-yarn-server-nodemanager in the patch passed. | | -1 :x: | unit | 3m 10s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt]([CI_URL] | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 35m 17s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 5s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 7s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 7m 33s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 58s | | hadoop-mapreduce-client-hs in the patch passed. | | -1 :x: | unit | 118m 21s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt]([CI_URL] | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 30s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 35s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 42m 57s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 42s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 57s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 39s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 8s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 36s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 18s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 15s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 37s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 16s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 47s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 1m 9s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 15m 2s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 3m 0s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 47s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 15s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 1m 1s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 3m 56s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 58s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 813m 58s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.server.TestMiniYarnCluster | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.TestRMFailover | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | | hadoop.mapred.pipes.TestPipeApplication | | | hadoop.tools.TestHadoopArchiveLogs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux ca7ad3048e9b 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 90084e4480aa313ed2ec6a6810054ec750073f6e | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-07T19:09:43.117+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2707775436 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 34m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 7s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 7s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 7s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 7s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 7s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 7s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 7s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 113 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 6m 10s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 33m 1s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 12m 20s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 5m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 34m 34s | | trunk passed | | +1 :green_heart: | javadoc | 32m 35s | | trunk passed | | +0 :ok: | spotbugs | 0m 44s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 43s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 35m 29s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 20m 21s | | the patch passed | | -1 :x: | compile | 11m 4s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 11m 4s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 61 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 4m 42s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 5800 unchanged - 34 fixed = 6068 total (was 5834) | | +1 :green_heart: | mvnsite | 33m 56s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 39s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 39s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 2m 23s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 43s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 35m 14s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 35s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 45s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 15m 10s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 44s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 28s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 51s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 53s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 6m 47s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 3m 41s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 6m 21s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 20m 2s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 4m 34s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 112m 46s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 58m 5s | | hadoop-yarn-server-nodemanager in the patch passed. | | -1 :x: | unit | 3m 48s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt]([CI_URL] | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 29m 24s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 9m 31s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 34s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 9m 39s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 5m 15s | | hadoop-mapreduce-client-hs in the patch passed. | | -1 :x: | unit | 137m 40s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt]([CI_URL] | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 58s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 3m 14s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 43m 44s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 1m 0s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 1m 19s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 23m 7s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 28s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 21m 27s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 46s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 3m 14s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 27s | | hadoop-mapreduce-examples in the patch passed. | | -1 :x: | unit | 1m 25s | [/patch-unit-hadoop-tools_hadoop-streaming.txt]([CI_URL] | hadoop-streaming in the patch failed. | | -1 :x: | unit | 1m 0s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-workload.txt]([CI_URL] | hadoop-dynamometer-workload in the patch failed. | | -1 :x: | unit | 1m 0s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt]([CI_URL] | hadoop-dynamometer-infra in the patch failed. | | -1 :x: | unit | 1m 2s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch failed. | | -1 :x: | unit | 1m 2s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt]([CI_URL] | hadoop-gridmix in the patch failed. | | -1 :x: | unit | 1m 10s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | unit | 1m 2s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | unit | 13m 0s | | hadoop-sls in the patch passed. | | -1 :x: | unit | 0m 55s | [/patch-unit-hadoop-tools_hadoop-resourceestimator.txt]([CI_URL] | hadoop-resourceestimator in the patch failed. | | -1 :x: | unit | 0m 55s | [/patch-unit-hadoop-tools_hadoop-compat-bench.txt]([CI_URL] | hadoop-compat-bench in the patch failed. | | -1 :x: | asflicense | 1m 24s | [/results-asflicense.txt]([CI_URL] | The patch generated 3 ASF License warnings. | | | | 1040m 53s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.util.TestNativeCodeLoader | | | hadoop.crypto.TestCryptoCodec | | | hadoop.yarn.server.TestMiniYarnCluster | | | hadoop.yarn.client.TestRMFailover | | | hadoop.mapred.pipes.TestPipeApplication | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 45c6ad59e75f 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 55b3cef7f23820d759046004b38bfb9bc1c8f244 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3001 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-08T00:24:19.080+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2708798511 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 4s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 4s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 4s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 4s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 4s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 4s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 4s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 115 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 6m 10s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 18m 59s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 5m 58s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 38s | | trunk passed | | +1 :green_heart: | javadoc | 21m 27s | | trunk passed | | +0 :ok: | spotbugs | 0m 31s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 19m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 16s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 18s | | the patch passed | | -1 :x: | compile | 5m 49s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 49s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 61 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 19s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 5813 unchanged - 34 fixed = 6081 total (was 5847) | | +1 :green_heart: | mvnsite | 23m 6s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 29s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 25s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 21s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 30s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 19m 5s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 26s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 34s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 41s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 27s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 10s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 8s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 57s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 57s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 5s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 43s | | hadoop-yarn-server-common in the patch passed. | | -1 :x: | unit | 2m 42s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-applicationhistoryservice.txt]([CI_URL] | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | -1 :x: | unit | 75m 45s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt]([CI_URL] | hadoop-yarn-server-resourcemanager in the patch passed. | | -1 :x: | unit | 54m 4s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt]([CI_URL] | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 6s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 26m 47s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 4s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 1s | | hadoop-mapreduce-client-common in the patch passed. | | -1 :x: | unit | 7m 4s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt]([CI_URL] | hadoop-mapreduce-client-app in the patch passed. | | -1 :x: | unit | 3m 23s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-hs.txt]([CI_URL] | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 104m 30s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 13s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 10s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 41m 58s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 28s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 42s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 12s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 2s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 19m 55s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 3s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 18s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 0m 50s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 15s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 1s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 30s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 0m 51s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 14m 7s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 2m 48s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 29s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 11m 51s | | hadoop-sls in the patch passed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-tools_hadoop-resourceestimator.txt]([CI_URL] | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 3m 28s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 45s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 763m 34s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices | | | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesConfigurationMutation | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerConfigMutation | | | hadoop.yarn.server.resourcemanager.webapp.fairscheduler.TestRMWebServicesFairScheduler | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentage | | | hadoop.yarn.server.resourcemanager.webapp.fairscheduler.TestRMWebServicesFairSchedulerCustomResourceTypes | | | hadoop.yarn.webapp.TestRMWithCSRFFilter | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppCustomResourceTypes | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDefaultLabel | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageAndWeightVector | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigAbsoluteMode | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsCustomResourceTypes | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModePercentageAndWeightVector | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedLegacyQueueCreationAbsoluteMode | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModePercentageAndWeight | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesSchedulerActivities | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppAttempts | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesContainers | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndWeight | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedMode | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageVector | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodeLabels | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigWeightMode | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesSchedulerActivitiesWithMultiNodesEnabled | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageAndWeight | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedLegacyQueueCreation | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServiceAppsNodelabel | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfig | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndWeightVector | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesForCSWithPartitions | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesReservation | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigWeightModeDQC | | | hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps | | | hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers | | | hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesAuxServices | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | | hadoop.yarn.server.nodemanager.webapp.TestNMWebServices | | | hadoop.yarn.client.cli.TestSchedConfCLI | | | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesTasks | | | hadoop.mapreduce.v2.app.webapp.TestAMWebServices | | | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobConf | | | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobs | | | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesAttempts | | | hadoop.mapreduce.v2.app.webapp.TestAMWebServicesAttempt | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobs | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobConf | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesLogsExtend | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServices | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesLogs | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesTasks | | | hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesAttempts | | | hadoop.tools.TestHadoopArchiveLogs | | | hadoop.resourceestimator.service.TestResourceEstimatorService | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 3f69dbee13b7 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 094e3501525efe0d7b42ae4629c019bbfdc866e1 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3032 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-09T10:59:34.335+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2709097783 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 5s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 5s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 5s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 5s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 5s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 5s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 5s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 116 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 45s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 23m 5s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 7m 13s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 38s | | trunk passed | | +1 :green_heart: | mvnsite | 22m 47s | | trunk passed | | +1 :green_heart: | javadoc | 21m 9s | | trunk passed | | +0 :ok: | spotbugs | 0m 31s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 22m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 18s | | the patch passed | | -1 :x: | compile | 5m 48s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 48s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 61 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 20s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 5896 unchanged - 34 fixed = 6164 total (was 5930) | | +1 :green_heart: | mvnsite | 23m 13s | | the patch passed | | +1 :green_heart: | xmllint | 0m 1s | | No new issues. | | -1 :x: | javadoc | 0m 28s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 24s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 23s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 28s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 19m 3s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 24s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 35s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 46s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 28s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 6s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 9s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 13s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 58s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 52s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 3s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 41s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 30s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 95m 15s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 43s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 10s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 35m 43s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 4s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 9s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 7m 31s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 55s | | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 118m 59s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 9s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 33s | | hadoop-federation-balance in the patch passed. | | -1 :x: | unit | 38m 28s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 43s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 57s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 37s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 2s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 18s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 15s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 15s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 2s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 30s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 18s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 44s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 1m 14s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 14m 57s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 2m 59s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 46s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 14s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 1m 1s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 4m 1s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 59s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 813m 52s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | | hadoop.hdfs.server.federation.router.TestRouterNamenodeHeartbeat | | | hadoop.tools.TestHadoopArchiveLogs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 37ececc7d8f2 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5eb253574f388632a24222b41d97cef40fb6273e | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3260 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-09T22:15:21.352+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2711747453 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 4s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 4s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 4s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 4s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 4s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 4s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 4s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 116 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 31s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 18m 42s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 9s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 46s | | trunk passed | | +1 :green_heart: | javadoc | 21m 45s | | trunk passed | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 20m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 7s | | the patch passed | | -1 :x: | compile | 5m 50s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 50s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 61 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 12s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 5896 unchanged - 34 fixed = 6164 total (was 5930) | | +1 :green_heart: | mvnsite | 22m 54s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 30s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 25s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 28s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 29s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 19m 5s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 24s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 34s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 44s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 29s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 9s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 18s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 29s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 36s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 4m 53s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 41s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 29s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 95m 2s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 39s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 10s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 35m 25s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 7s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 7s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 7m 35s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 57s | | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 104m 10s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 4s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 38s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 42m 21s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 43s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 54s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 32s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 46s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 13s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 14s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 34s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 16s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 46s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 1m 14s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 15m 4s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 3m 1s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 46s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 17s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 1m 2s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 3m 57s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 59s | [/results-asflicense.txt]([CI_URL] | The patch generated 2 ASF License warnings. | | | | 797m 41s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | | hadoop.tools.TestHadoopArchiveLogs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 841b5de025e9 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 799e7e753d0128d9450ed42b575c41eddeabf586 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-10T20:28:11.431+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2720161953 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 15s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-13T06:55:36.744+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-2720718431 @ryantomlinson95 @cnauroth @steveloughran @jbrinegar @jojochuang @pan3793 I'd like to bring to your attention HADOOP-19486, and the corresponding thread I have opened on dev@hadoop.apache.org , where I track my full JDK23 support. Please check my email, and join the discussion.", "created": "2025-03-13T10:18:35.160+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2722829367 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 5s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 5s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 5s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 5s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 5s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 5s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 5s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 118 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 33s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 18m 49s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 10s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 24m 19s | | trunk passed | | +1 :green_heart: | javadoc | 21m 59s | | trunk passed | | +0 :ok: | spotbugs | 0m 30s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 29s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 20m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 23s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 36s | | the patch passed | | -1 :x: | compile | 5m 43s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 43s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 62 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 15s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 270 new + 5893 unchanged - 34 fixed = 6163 total (was 5927) | | +1 :green_heart: | mvnsite | 23m 27s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 27s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 24s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 28s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 27s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 18m 57s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 23s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 35s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 45s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 25s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 13s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 12s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 5m 3s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 57s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 1s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 41s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 30s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 96m 7s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 36s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 8s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 30m 12s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 3s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 8s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-mapreduce-client-shuffle in the patch passed. | | +1 :green_heart: | unit | 7m 30s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 52s | | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 105m 9s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 20s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 16s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 38m 37s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 44s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 57s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 32s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 8s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 36s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 16s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 14s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 1s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 31s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 17s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 46s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 1m 13s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 15m 14s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 3m 2s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 49s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 16s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 1m 2s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 3m 55s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 59s | [/results-asflicense.txt]([CI_URL] | The patch generated 2 ASF License warnings. | | | | 795m 17s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | | hadoop.tools.TestHadoopArchiveLogs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 389e261c99f7 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2492f970d8fe10a10d17c810860e1a0be51c4cac | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3295 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-13T22:34:24.985+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2730666146 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 32m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 8s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 8s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 8s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 8s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 8s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 8s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 8s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 121 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 50s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 32m 16s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 11m 12s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 4m 52s | | trunk passed | | +1 :green_heart: | mvnsite | 37m 46s | | trunk passed | | +1 :green_heart: | javadoc | 33m 21s | | trunk passed | | +0 :ok: | spotbugs | 0m 44s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 45s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 34m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 38s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 21m 1s | | the patch passed | | -1 :x: | compile | 10m 50s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 10m 50s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 62 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 1s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 4m 50s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 268 new + 6021 unchanged - 36 fixed = 6289 total (was 6057) | | +1 :green_heart: | mvnsite | 33m 50s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 40s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 37s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 2m 22s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 45s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 32m 53s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 39s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 50s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 14m 58s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 45s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 30s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 53s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 52s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 6m 44s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 3m 38s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 6m 27s | | hadoop-yarn-common in the patch passed. | | -1 :x: | unit | 20m 40s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-common.txt]([CI_URL] | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 4m 35s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | -1 :x: | unit | 64m 50s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt]([CI_URL] | hadoop-yarn-server-resourcemanager in the patch passed. | | -1 :x: | unit | 0m 47s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt]([CI_URL] | hadoop-yarn-server-nodemanager in the patch failed. | | -1 :x: | unit | 0m 29s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests.txt]([CI_URL] | hadoop-yarn-server-tests in the patch failed. | | -1 :x: | unit | 0m 43s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch failed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt]([CI_URL] | hadoop-mapreduce-client-core in the patch failed. | | -1 :x: | unit | 0m 34s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-common.txt]([CI_URL] | hadoop-mapreduce-client-common in the patch failed. | | -1 :x: | unit | 0m 47s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt]([CI_URL] | hadoop-mapreduce-client-app in the patch failed. | | -1 :x: | unit | 0m 58s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-hs.txt]([CI_URL] | hadoop-mapreduce-client-hs in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt]([CI_URL] | hadoop-mapreduce-client-jobclient in the patch failed. | | +1 :green_heart: | unit | 10m 47s | | hadoop-distcp in the patch passed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-tools_hadoop-federation-balance.txt]([CI_URL] | hadoop-federation-balance in the patch failed. | | -1 :x: | unit | 0m 43s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in the patch failed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt]([CI_URL] | hadoop-yarn-server-router in the patch failed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-documentstore.txt]([CI_URL] | hadoop-yarn-server-timelineservice-documentstore in the patch failed. | | -1 :x: | unit | 0m 56s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt]([CI_URL] | hadoop-yarn-applications-distributedshell in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-unmanaged-am-launcher.txt]([CI_URL] | hadoop-yarn-applications-unmanaged-am-launcher in the patch failed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt]([CI_URL] | hadoop-yarn-services-core in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-api.txt]([CI_URL] | hadoop-yarn-services-api in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-nativetask.txt]([CI_URL] | hadoop-mapreduce-client-nativetask in the patch failed. | | -1 :x: | unit | 0m 48s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-examples.txt]([CI_URL] | hadoop-mapreduce-examples in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-tools_hadoop-streaming.txt]([CI_URL] | hadoop-streaming in the patch failed. | | -1 :x: | unit | 0m 45s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-workload.txt]([CI_URL] | hadoop-dynamometer-workload in the patch failed. | | -1 :x: | unit | 0m 43s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt]([CI_URL] | hadoop-dynamometer-infra in the patch failed. | | -1 :x: | unit | 0m 42s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt]([CI_URL] | hadoop-gridmix in the patch failed. | | -1 :x: | unit | 0m 31s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | unit | 0m 42s | [/patch-unit-hadoop-tools_hadoop-sls.txt]([CI_URL] | hadoop-sls in the patch failed. | | -1 :x: | unit | 0m 41s | [/patch-unit-hadoop-tools_hadoop-resourceestimator.txt]([CI_URL] | hadoop-resourceestimator in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-tools_hadoop-compat-bench.txt]([CI_URL] | hadoop-compat-bench in the patch failed. | | +0 :ok: | asflicense | 0m 45s | | ASF License check generated no output? | | | | 616m 57s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.util.TestNativeCodeLoader | | | hadoop.crypto.TestCryptoCodec | | | hadoop.yarn.server.federation.policies.amrmproxy.TestLocalityMulticastAMRMProxyPolicy | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServiceAppsNodelabel | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageVector | | | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesReservation | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 065d89e45112 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / da49510746b1d2a9563dc0200dbd17a840ee6d26 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 1377 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-17T19:48:48.551+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2731260133 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 17m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 5s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 5s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 5s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 5s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 5s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 5s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 5s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 121 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 6m 18s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 18m 40s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 3s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 23s | | trunk passed | | +1 :green_heart: | javadoc | 21m 40s | | trunk passed | | +0 :ok: | spotbugs | 0m 31s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 29s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 20m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 10s | | the patch passed | | -1 :x: | compile | 5m 47s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 47s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 62 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 18s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 262 new + 6027 unchanged - 35 fixed = 6289 total (was 6062) | | +1 :green_heart: | mvnsite | 21m 59s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 22s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 19s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 1m 17s | [/new-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +0 :ok: | spotbugs | 0m 25s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | -1 :x: | shadedclient | 22m 41s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 18s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 26s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 31s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 22s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 0m 55s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 8s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 5m 8s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 4m 35s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 35s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 26s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 93m 39s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 20s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 7s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 30m 25s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 19s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 10s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 7m 36s | | hadoop-mapreduce-client-app in the patch passed. | | -1 :x: | unit | 0m 52s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-hs.txt]([CI_URL] | hadoop-mapreduce-client-hs in the patch failed. | | -1 :x: | unit | 0m 42s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt]([CI_URL] | hadoop-mapreduce-client-jobclient in the patch failed. | | -1 :x: | unit | 0m 35s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch failed. | | -1 :x: | unit | 0m 37s | [/patch-unit-hadoop-tools_hadoop-federation-balance.txt]([CI_URL] | hadoop-federation-balance in the patch failed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt]([CI_URL] | hadoop-hdfs-rbf in the patch failed. | | -1 :x: | unit | 0m 30s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt]([CI_URL] | hadoop-yarn-server-router in the patch failed. | | -1 :x: | unit | 0m 30s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-timelineservice-documentstore.txt]([CI_URL] | hadoop-yarn-server-timelineservice-documentstore in the patch failed. | | -1 :x: | unit | 0m 31s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-distributedshell.txt]([CI_URL] | hadoop-yarn-applications-distributedshell in the patch failed. | | -1 :x: | unit | 0m 32s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-unmanaged-am-launcher.txt]([CI_URL] | hadoop-yarn-applications-unmanaged-am-launcher in the patch failed. | | -1 :x: | unit | 0m 34s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core.txt]([CI_URL] | hadoop-yarn-services-core in the patch failed. | | -1 :x: | unit | 0m 36s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-api.txt]([CI_URL] | hadoop-yarn-services-api in the patch failed. | | -1 :x: | unit | 0m 29s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-nativetask.txt]([CI_URL] | hadoop-mapreduce-client-nativetask in the patch failed. | | -1 :x: | unit | 0m 33s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-examples.txt]([CI_URL] | hadoop-mapreduce-examples in the patch failed. | | -1 :x: | unit | 0m 33s | [/patch-unit-hadoop-tools_hadoop-streaming.txt]([CI_URL] | hadoop-streaming in the patch failed. | | -1 :x: | unit | 0m 32s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-workload.txt]([CI_URL] | hadoop-dynamometer-workload in the patch failed. | | -1 :x: | unit | 0m 35s | [/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt]([CI_URL] | hadoop-dynamometer-infra in the patch failed. | | -1 :x: | unit | 0m 36s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch failed. | | -1 :x: | unit | 0m 36s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt]([CI_URL] | hadoop-gridmix in the patch failed. | | -1 :x: | unit | 0m 38s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | unit | 0m 33s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | unit | 12m 5s | | hadoop-sls in the patch passed. | | -1 :x: | unit | 0m 32s | [/patch-unit-hadoop-tools_hadoop-resourceestimator.txt]([CI_URL] | hadoop-resourceestimator in the patch failed. | | -1 :x: | unit | 0m 31s | [/patch-unit-hadoop-tools_hadoop-compat-bench.txt]([CI_URL] | hadoop-compat-bench in the patch failed. | | -1 :x: | asflicense | 0m 46s | [/results-asflicense.txt]([CI_URL] | The patch generated 2 ASF License warnings. | | | | 549m 34s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager | | | Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.fs; locked 50% of time Unsynchronized access at AllocationFileLoaderService.java:50% of time Unsynchronized access at AllocationFileLoaderService.java:[line 121] | | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 731909c4beb5 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1ebb257c5c0ae63a93aab209f28333e645f734b1 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 2498 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-18T00:26:46.007+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2732005059 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 4s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 4s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 4s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 5s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 5s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 5s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 5s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 121 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 6m 8s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 23m 5s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 7m 2s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 19m 13s | | trunk passed | | +1 :green_heart: | javadoc | 18m 23s | | trunk passed | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 23s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 21m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 27s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 48s | | the patch passed | | -1 :x: | compile | 6m 25s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 6m 25s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 63 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 24s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 187 new + 6027 unchanged - 35 fixed = 6214 total (was 6062) | | +1 :green_heart: | mvnsite | 23m 21s | | the patch passed | | +1 :green_heart: | xmllint | 0m 1s | | No new issues. | | -1 :x: | javadoc | 0m 30s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 25s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 30s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | +1 :green_heart: | shadedclient | 22m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 23s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 34s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 37s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 29s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 12s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 13s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 4m 56s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 54s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 6s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 43s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 29s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 95m 25s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 53s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 14s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 30m 8s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 5s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 12s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 7m 35s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 52s | | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 118m 34s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 26m 22s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 12s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 42m 41s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 43s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 56s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 21m 40s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 5s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 47s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 12s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 16s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 35s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 20s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 46s | | hadoop-dynamometer-infra in the patch passed. | | -1 :x: | unit | 1m 15s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt]([CI_URL] | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 15m 27s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 3m 2s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 45s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 15s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 1m 1s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 3m 57s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 59s | [/results-asflicense.txt]([CI_URL] | The patch generated 2 ASF License warnings. | | | | 813m 19s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | | hadoop.tools.TestHadoopArchiveLogs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux c8f2a0a51f72 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 62d3437ca429567b0263868be36453fb028ded31 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3255 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-18T07:55:44.623+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #7081: URL: https://github.com/apache/hadoop/pull/7081#discussion_r1942770365 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapter.java: ########## @@ -0,0 +1,52 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; + +/** + * javax.security.auth.Subject.getSubject is deprecated for removal. + * The replacement API exists only in Java 18 and above. + * This class helps use the newer API if available, without raising the language level. + */ +public class SubjectAdapter { + private static final HiddenGetSubject instance; + static { + int version = 0; + try { + version = Integer.parseInt(System.getProperty(\"java.specification.version\")); Review Comment: > IMO testing trying to load the new classes and failing over to the old classes if they cannot be found would be more robust. interesting thought. how about * we avoid Subject * pull the string on L33 to a constant ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/subject/SubjectAdapter.java: ########## @@ -0,0 +1,52 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util.subject; + +import javax.security.auth.Subject; + +/** + * javax.security.auth.Subject.getSubject is deprecated for removal. + * The replacement API exists only in Java 18 and above. + * This class helps use the newer API if available, without raising the language level. + */ +public class SubjectAdapter { + private static final HiddenGetSubject instance; + static { + int version = 0; + try { + version = Integer.parseInt(System.getProperty(\"java.specification.version\")); Review Comment: @stoty interesting thought about attempting and fallbacks We'd try the java18+ first & fall back to java <= 17 otherwise? or do it in the opposite direction, at least for now?", "created": "2025-03-18T11:06:06.096+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7434: URL: https://github.com/apache/hadoop/pull/7434#discussion_r2001305312 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,250 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience; + +@InterfaceAudience.Private() +public class SubjectUtil { Review Comment: This is my alternate compatibility shim, @steveloughran ( based on Avatica's which is based on Jetty's) @steveloughran .", "created": "2025-03-18T15:17:28.760+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-2733666333 My alternate patch tries the new API first, and falls back to the old one, @steveloughran . I have pinged you on that one, so that you can check it out, and maybe discuss it there. Conversion either way has an overhead, as the Callable / PrivilegedExceptionAction conversion and exception re-wrapping has to happen, but IMO anything we do inside those doAs() blocks is heavyweight enough that the overhad is relatively negligible.", "created": "2025-03-18T15:25:36.870+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7434: URL: https://github.com/apache/hadoop/pull/7434#discussion_r2001305312 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,250 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience; + +@InterfaceAudience.Private() +public class SubjectUtil { Review Comment: This is my alternate compatibility shim, @steveloughran ( based on Avatica's which is based on Jetty's)", "created": "2025-03-18T15:26:39.693+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2734845424 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 5s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 5s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 5s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 5s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 5s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 5s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 5s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 120 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 5m 32s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 19m 53s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 6m 6s | [/branch-compile-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | checkstyle | 2m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 48s | | trunk passed | | +1 :green_heart: | javadoc | 21m 33s | | trunk passed | | +0 :ok: | spotbugs | 0m 31s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +0 :ok: | spotbugs | 0m 29s | | branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 20m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 23s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 13m 11s | | the patch passed | | -1 :x: | compile | 5m 43s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 5m 43s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 62 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt]([CI_URL] | The patch 152 line(s) with tabs. | | -0 :warning: | checkstyle | 2m 22s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 188 new + 6037 unchanged - 35 fixed = 6225 total (was 6072) | | +1 :green_heart: | mvnsite | 23m 7s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 28s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-common-project_hadoop-auth generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +0 :ok: | spotbugs | 0m 25s | | hadoop-project has no data from spotbugs | | +0 :ok: | spotbugs | 0m 28s | | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests has no data from spotbugs | | +1 :green_heart: | shadedclient | 21m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 23s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 0m 36s | | hadoop-auth in the patch passed. | | -1 :x: | unit | 12m 46s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 0m 29s | | hadoop-kms in the patch passed. | | +1 :green_heart: | unit | 1m 4s | | hadoop-registry in the patch passed. | | +1 :green_heart: | unit | 2m 9s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 1m 16s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 5m 1s | | hadoop-hdfs-httpfs in the patch passed. | | +1 :green_heart: | unit | 2m 57s | | hadoop-hdfs-nfs in the patch passed. | | +1 :green_heart: | unit | 5m 5s | | hadoop-yarn-common in the patch passed. | | +1 :green_heart: | unit | 18m 43s | | hadoop-yarn-server-common in the patch passed. | | +1 :green_heart: | unit | 3m 28s | | hadoop-yarn-server-applicationhistoryservice in the patch passed. | | +1 :green_heart: | unit | 96m 11s | | hadoop-yarn-server-resourcemanager in the patch passed. | | +1 :green_heart: | unit | 54m 38s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 3m 4s | | hadoop-yarn-server-tests in the patch passed. | | -1 :x: | unit | 30m 13s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-client.txt]([CI_URL] | hadoop-yarn-client in the patch passed. | | +1 :green_heart: | unit | 7m 7s | | hadoop-mapreduce-client-core in the patch passed. | | +1 :green_heart: | unit | 1m 10s | | hadoop-mapreduce-client-common in the patch passed. | | +1 :green_heart: | unit | 7m 32s | | hadoop-mapreduce-client-app in the patch passed. | | +1 :green_heart: | unit | 3m 53s | | hadoop-mapreduce-client-hs in the patch passed. | | +1 :green_heart: | unit | 106m 25s | | hadoop-mapreduce-client-jobclient in the patch passed. | | +1 :green_heart: | unit | 25m 55s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | unit | 2m 38s | | hadoop-federation-balance in the patch passed. | | +1 :green_heart: | unit | 42m 40s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 0m 42s | | hadoop-yarn-server-router in the patch passed. | | +1 :green_heart: | unit | 0m 58s | | hadoop-yarn-server-timelineservice-documentstore in the patch passed. | | +1 :green_heart: | unit | 22m 40s | | hadoop-yarn-applications-distributedshell in the patch passed. | | +1 :green_heart: | unit | 1m 8s | | hadoop-yarn-applications-unmanaged-am-launcher in the patch passed. | | +1 :green_heart: | unit | 20m 32s | | hadoop-yarn-services-core in the patch passed. | | +1 :green_heart: | unit | 2m 17s | | hadoop-yarn-services-api in the patch passed. | | +1 :green_heart: | unit | 2m 15s | | hadoop-mapreduce-client-nativetask in the patch passed. | | +1 :green_heart: | unit | 1m 3s | | hadoop-mapreduce-examples in the patch passed. | | +1 :green_heart: | unit | 6m 32s | | hadoop-streaming in the patch passed. | | +1 :green_heart: | unit | 1m 17s | | hadoop-dynamometer-workload in the patch passed. | | +1 :green_heart: | unit | 0m 45s | | hadoop-dynamometer-infra in the patch passed. | | +1 :green_heart: | unit | 1m 12s | | hadoop-archive-logs in the patch passed. | | +1 :green_heart: | unit | 15m 22s | | hadoop-gridmix in the patch passed. | | +1 :green_heart: | unit | 3m 2s | | hadoop-aws in the patch passed. | | +1 :green_heart: | unit | 2m 47s | | hadoop-azure in the patch passed. | | +1 :green_heart: | unit | 12m 18s | | hadoop-sls in the patch passed. | | +1 :green_heart: | unit | 0m 56s | | hadoop-resourceestimator in the patch passed. | | +1 :green_heart: | unit | 3m 46s | | hadoop-compat-bench in the patch passed. | | -1 :x: | asflicense | 0m 59s | [/results-asflicense.txt]([CI_URL] | The patch generated 2 ASF License warnings. | | | | 801m 7s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.crypto.TestCryptoCodec | | | hadoop.util.TestNativeCodeLoader | | | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E | | | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints | | | hadoop.yarn.client.api.impl.TestNMClient | | | hadoop.yarn.client.api.impl.TestAMRMClient | | | hadoop.yarn.client.api.impl.TestAMRMProxy | | | hadoop.yarn.client.api.impl.TestYarnClient | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.48 ServerAPI=1.48 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7434 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle xmllint | | uname | Linux 95dbf6de1978 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b60116842d71ff736ff46aaba807999d6deb8ff6 | | Default Java | Red Hat, Inc.-1.8.0_412-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3307 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-common-project/hadoop-kms hadoop-common-project/hadoop-registry hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-hdfs-project/hadoop-hdfs-nfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-federation-balance hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask hadoop-mapreduce-project/hadoop-mapreduce-examples hadoop-tools/hadoop-streaming hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra hadoop-tools/hadoop-archive-logs hadoop-tools/hadoop-gridmix hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-sls hadoop-tools/hadoop-resourceestimator hadoop-tools/hadoop-compat-bench U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 spotbugs=4.2.2 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-18T22:05:53.340+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2752232341 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 19s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-25T18:52:33.310+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2754257369 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 20s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-26T12:36:30.217+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2754594798 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 19s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-26T14:17:44.784+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2754601591 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 24s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-03-26T14:19:42.747+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2776584925 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 19s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-03T18:17:21.148+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2777560120 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 18s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-04T05:00:51.496+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2779597416 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 18s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-04T19:40:20.796+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2810306323 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 17s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-16T17:57:08.141+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2811779045 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 21s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-17T05:26:57.102+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2811927773 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 25s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-17T06:34:45.121+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2813175155 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 19s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-17T14:41:35.314+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2841729274 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 22s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-04-30T11:51:17.154+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2879170720 > I don't think we need to remove the old methods, we can keep them for a very long time ... Given the mature status and version adoptions of the Hadoop project, I think the removal will likely never happen. It's fair enough to deprecate those methods, though I prefer not to add `@Deprecated` to those methods personally.", "created": "2025-05-16T14:38:02.595+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-2879194783 Thanks @pan3793 . At the moment this is my personal development branch, there are still some issues to fix and clean up on trunk before I can make a focused patch for the Subject and Thread changes. All method names, etc can be considered placeholders for now, we can discuss those details once I am able to clean up this patch.", "created": "2025-05-16T14:38:17.544+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3191323897 Hi @stoty, do you have a plan to continue this work? I briefly went through the change, if I understand correctly, this PR mainly consists of 2 parts: 1. route UGI doAs to `Subject.callAs` for newer JDKs, and fallback to original API for older JDKs 2. migrate Thread / ThreadPool invocations to HadoopThread / HadoopThreadPool, to restore the capability of Subject propagation The part 1 change is relatively light, Trino's forked Hadoop project only changes this part. Can we change this part first to allow the downstream project that uses the Hadoop client to work with the new JDK?", "created": "2025-08-15T11:43:13.113+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3191598426 > Hi @stoty, do you have a plan to continue this work? Yes, but I don't have the time right now. I hope to be able to return to this in about a month. > > I briefly went through the change, if I understand correctly, this PR mainly consists of 2 parts: > > 1. route UGI doAs to `Subject.callAs` for newer JDKs, and fallback to original API for older JDKs > 2. migrate Thread / ThreadPool invocations to HadoopThread / HadoopThreadPool, to restore the capability of Subject propagation Yes, that's correct. I think that HadoopThreadPool is not really needed, as the executors don't really preserve subjects either, and we probably already have code to set the subject where needed. > > The part 1 change is relatively light, Trino's forked Hadoop project only changes this part. Can we change this part first to allow the downstream project that uses the Hadoop client to work with the new JDK? The Thread changes are required for Java 22+, while the doAs() stuff is only fully removed in 23+ (it can be turned back on by enabling SecurityManager before that). I don't know how much of the MR / HDFS client would work on JDK22+ without the Thread changes though, there are a lot of changes, I don't remember how much of them are used on the client side without digging back in the code. The Subject changes should be enough to support JDK21 without securityManager. The two changes are orthogonal, so the order doesn't really matter.", "created": "2025-08-15T14:11:08.157+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3191602640 Feel free to split the doAs() patches out from here. My plan is to do it in two phases: - Make the changes in Subject/SubjectUtil mostly (which is small patch) - Deprecate and replace the old calls in the rest of the code (which is bigger one)", "created": "2025-08-15T14:13:11.340+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3191760091 @stoty Thanks for your detailed reply! Now, I mainly care use case of Spark, I have successfully run some internal Spark workloads with Hadoop 3.4.1 (of course, trunk works too) client on JDK 21 and 22, but Spark fails to start on JDK 23+ with error ``` 25/08/15 23:11:12 ERROR SparkContext: Error initializing SparkContext.", "created": "2025-08-15T15:18:00.799+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3195164669 > @stoty Thanks for your detailed reply! > > Now, I mainly care use case of Spark, I have successfully run some internal Spark workloads with Hadoop 3.4.1 (of course, trunk works too) client on JDK 21 and 22, but Spark fails to start on JDK 23+ with error It's surprising that the client works on JDK22. Is that on a kerberized cluster ?", "created": "2025-08-18T05:39:00.347+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3195175522 @stoty it works with JDK 22 on both simple and kerberized. With some tunes based on this PR, I also managed to run Spark with Java 25 on a simple cluster. <img width=\"1168\" height=\"298\" alt=\"5057b21c461c17bb8679d11df73e5378\" src=\"https://github.com/user-attachments/assets/f5986aff-8479-4327-83e1-3859b48a4b7e\" />", "created": "2025-08-18T05:43:32.078+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3195229412 I support anything that unlocks at least client JVM support (even if partial) ASAP.", "created": "2025-08-18T05:57:17.573+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3198839110 @stoty @pan3793 Thank you all for the discussion. I can help with the code review.", "created": "2025-08-19T00:49:27.311+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7434: URL: https://github.com/apache/hadoop/pull/7434#issuecomment-3200354502 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 19s | | https://github.com/apache/hadoop/pull/7434 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | Console output | [CI_URL] | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-19T11:26:03.109+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 opened a new pull request, #7886: URL: https://github.com/apache/hadoop/pull/7886 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR https://docs.oracle.com/en/java/javase/24/security/migrating-deprecated-removal-subject-getsubject-and-subject-doas-methods-subject-current-and-subje.html > In JDK 24, the Security Manager has been permanently disabled. See [JEP 486](https://openjdk.org/jeps/486) for more information. This PR is extracted from @stoty's PR https://github.com/apache/hadoop/pull/7434, with some tweaks. The main goal is to make minimal changes to make the Hadoop client compatible with Java 25, which unlocks downstream projects that rely on the Hadoop client, e.g. Spark, to support Java 25. ### How was this patch tested? Integrated with Spark 4.1.0-SNAPSHOT (master branch) with Java 25. Note: Spark uses the Hadoop Shaded client. Currently, the Hadoop trunk branch's hadoop-`client-minicluster` is broken, it requires some fixes (will send dedicated PRs to fix) ahead. Before ```", "created": "2025-08-19T11:51:39.996+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3200497776 I think it would be a good idea to initiate a discussion on this implementation on the hadoop-common dev list, or at least ping everyone here who commented on the orignal PRs so that we can get a consensus on this. Of course this looks good to me. Also could you add a \"co-authored-by\" line so that I show up in the github statistics ?", "created": "2025-08-19T12:10:16.363+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3200577468 @stoty thanks for your suggestion. I updated the PR description with the \"co-authored-by\" sections. Since the `hadoop-client-minicluster` is broken now, I'd like to fix it first before pinging reviewers, so that they can easily to verify this patch.", "created": "2025-08-19T12:34:12.388+0000"}, {"author": "ASF GitHub Bot", "body": "cxzl25 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3200722730 Thanks @pan3793 , I have done some verification in the production cluster to confirm that this PR works.", "created": "2025-08-19T13:13:40.334+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3201278651 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 49s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 33m 7s | | trunk passed | | +1 :green_heart: | compile | 15m 52s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 26s | | trunk passed | | +1 :green_heart: | javadoc | 2m 4s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 33s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 26s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 17s | | the patch passed | | +1 :green_heart: | compile | 15m 8s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 8s | | the patch passed | | +1 :green_heart: | compile | 14m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 23s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 4 new + 73 unchanged - 0 fixed = 77 total (was 73) | | +1 :green_heart: | mvnsite | 2m 22s | | the patch passed | | -1 :x: | javadoc | 0m 38s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | -1 :x: | javadoc | 0m 36s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | spotbugs | 3m 59s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 42s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 48s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 230m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6c1cacd17c9d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0b077ad5d9c9f19c9238e90df994d292f1c46eea | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2508 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-19T15:43:28.804+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3201319668 Spotted an issue during self-review, the thrown exception seems to have an additional wrapper on JDK 25, related test cases ``` Error: Failures: Error: TestClientRMTokens.testShortCircuitRenewCancelDifferentHostDifferentPort:355->checkShortCircuitRenewCancel:431 expected: <getProxy> but was: <java.lang.RuntimeException: getProxy> Error: TestClientRMTokens.testShortCircuitRenewCancelDifferentHostSamePort:344->checkShortCircuitRenewCancel:431 expected: <getProxy> but was: <java.lang.RuntimeException: getProxy> Error: TestClientRMTokens.testShortCircuitRenewCancelSameHostDifferentPort:333->checkShortCircuitRenewCancel:431 expected: <getProxy> but was: <java.lang.RuntimeException: getProxy> ```", "created": "2025-08-19T15:56:07.617+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3201395893 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 46s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 38m 26s | | trunk passed | | +1 :green_heart: | compile | 18m 11s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 18s | | trunk passed | | +1 :green_heart: | javadoc | 2m 1s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 34s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 28s | | trunk passed | | +1 :green_heart: | shadedclient | 44m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 19s | | the patch passed | | +1 :green_heart: | compile | 17m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 6s | | the patch passed | | +1 :green_heart: | compile | 15m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 24s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 4 new + 73 unchanged - 0 fixed = 77 total (was 73) | | +1 :green_heart: | mvnsite | 2m 16s | | the patch passed | | -1 :x: | javadoc | 0m 43s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | -1 :x: | javadoc | 0m 40s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | spotbugs | 3m 49s | | the patch passed | | +1 :green_heart: | shadedclient | 43m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 44s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 39s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 255m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 512b641ece1f 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e5d544b6b0faf24d063a948bb7d75d78a9aa6bf6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1249 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-19T16:31:08.100+0000"}, {"author": "ASF GitHub Bot", "body": "Copilot commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2286639042 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,214 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + return callAs(subject, privilegedActionToCallable(action)); + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + */ + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) (ce.getCause()); + throw new PrivilegedActionException(cause); + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } + + /** + * Maps to Subject.currect() is available, otherwise maps to Review Comment: The word 'currect' should be 'current' in the comment. ```suggestion * Maps to Subject.current() if available, otherwise maps to ```", "created": "2025-08-19T23:49:23.414+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3204310618 This PR is ready for review. cc @steveloughran @stoty @cnauroth @slfan1989 @jbrinegar Also cc @dongjoon-hyun @LuciferYang from the Spark community, this unblocks Spark to support JDK 25", "created": "2025-08-20T06:03:59.800+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3204323175 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 38s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 44s | | trunk passed | | +1 :green_heart: | compile | 8m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 30s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 44s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 30s | | trunk passed | | +1 :green_heart: | javadoc | 1m 15s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 0s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 21s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 45s | | the patch passed | | +1 :green_heart: | compile | 8m 5s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 5s | | the patch passed | | +1 :green_heart: | compile | 7m 33s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 44s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 5 new + 73 unchanged - 0 fixed = 78 total (was 73) | | +1 :green_heart: | mvnsite | 1m 31s | | the patch passed | | -1 :x: | javadoc | 0m 29s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | -1 :x: | javadoc | 0m 29s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | spotbugs | 2m 22s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 8s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 18m 23s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 142m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6347885a56c9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 69a2ccf2e93a8301eeb642c8555d8f487ccc9e21 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-20T06:09:12.616+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3204655042 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 9s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 19s | | trunk passed | | +1 :green_heart: | compile | 15m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 30s | | trunk passed | | +1 :green_heart: | javadoc | 2m 5s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 31s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 18s | | the patch passed | | +1 :green_heart: | compile | 14m 59s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 59s | | the patch passed | | +1 :green_heart: | compile | 13m 30s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 20s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 4 new + 73 unchanged - 0 fixed = 77 total (was 73) | | +1 :green_heart: | mvnsite | 2m 27s | | the patch passed | | -1 :x: | javadoc | 0m 45s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | -1 :x: | javadoc | 0m 43s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-common-project_hadoop-auth-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0) | | +1 :green_heart: | spotbugs | 3m 52s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 44s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 42s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 7s | | The patch does not generate ASF License warnings. | | | | 230m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 36424f2be61f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9ea6317d26eab726d06714f3293330358267b4a8 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-20T07:41:40.036+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3204858520 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 29s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 51s | | trunk passed | | +1 :green_heart: | compile | 8m 26s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 30s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 1s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 5s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 22s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 47s | | the patch passed | | +1 :green_heart: | compile | 7m 54s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 7m 54s | | the patch passed | | +1 :green_heart: | compile | 7m 25s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 42s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 5 new + 73 unchanged - 0 fixed = 78 total (was 73) | | +1 :green_heart: | mvnsite | 1m 27s | | the patch passed | | +1 :green_heart: | javadoc | 1m 12s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 3s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 17s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 9s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 18m 21s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 143m 5s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0e8c0b671fe2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5d407b3118312afbe3d847d63b55d6c1dd926e75 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3153 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-20T08:41:20.141+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3205376828 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 27s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 42s | | trunk passed | | +1 :green_heart: | compile | 17m 49s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 11s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 24s | | trunk passed | | +1 :green_heart: | javadoc | 1m 57s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 30s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 15s | | the patch passed | | +1 :green_heart: | compile | 16m 49s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 16m 49s | | the patch passed | | +1 :green_heart: | compile | 15m 10s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 10s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 22s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 4 new + 73 unchanged - 0 fixed = 77 total (was 73) | | +1 :green_heart: | mvnsite | 2m 21s | | the patch passed | | +1 :green_heart: | javadoc | 1m 57s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 44m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 43s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 33s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 251m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 16801a02db0d 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a2f75253f54471477fed330402685add6c201a46 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2136 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-20T10:14:06.020+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3206270962 trivial checkstyles need fixing ``` ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java:35:@Private:1: Utility classes should not have a public or default constructor. [HideUtilityClassConstructor] ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java:37: private static MethodHandle CALL_AS;:31: Name 'CALL_AS' must match pattern '^[a-z][a-zA-Z0-9]*$'. [StaticVariableName] ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java:38: private static MethodHandle CURRENT;:31: Name 'CURRENT' must match pattern '^[a-z][a-zA-Z0-9]*$'. [StaticVariableName] ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java:196: /**: First sentence should end with a period. [JavadocStyle] ```", "created": "2025-08-20T13:08:20.641+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3206283726 @steveloughran just fixed 1 and 4, but 2 and 3 look weird, why the static var name should start with a lower-case letter? ``` ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java:37: private static MethodHandle CALL_AS;:31: Name 'CALL_AS' must match pattern '^[a-z][a-zA-Z0-9]*$'. [StaticVariableName] ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java:38: private static MethodHandle CURRENT;:31: Name 'CURRENT' must match pattern '^[a-z][a-zA-Z0-9]*$'. [StaticVariableName] ```", "created": "2025-08-20T13:11:11.170+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2288121282 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; Review Comment: thes should be final, as the static sections are trying to set them up as such. This may need some tuning of that code so that the compiler is happy there's only one attempt ever made to set it. ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { Review Comment: add test, verify that * callable throwing a PrivilegedActionException is mapped to completion exception * callable raising any RTE is handled correctly ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, as CompletionException should always wrap an exception + throw ce; + } + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an exception. + * The cause of the {@code PrivilegedActionException} is set to the exception + * thrown by {@code action.run()}. + */ + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + throw new PrivilegedActionException(cause); + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } + + /** + * Maps to Subject.current() if available, otherwise maps to + * Subject.getSubject() + * + * @return the current subject + */ + public static Subject current() { + try { + return (Subject) CURRENT.invoke(); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + @SuppressWarnings(\"unused\") + private static <T> PrivilegedExceptionAction<T> callableToPrivilegedExceptionAction( + Callable<T> callable) { + return callable::call; + } + + private static <T> Callable<T> privilegedExceptionActionToCallable( + PrivilegedExceptionAction<T> action) { + return action::run; + } + + private static <T> Callable<T> privilegedActionToCallable( + PrivilegedAction<T> action) { + return action::run; + } + + @SuppressWarnings(\"unchecked\") + private static <E extends Throwable> RuntimeException sneakyThrow(Throwable e) throws E { Review Comment: add javadoc -it is a use of templates I've not seen before...having the type of raised exception inferred from the context around the invocation ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, as CompletionException should always wrap an exception Review Comment: unless the invoked callable raises it ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: tests to verify handling. what happens if the callable raises a CompletionException without an inner cause? that \"should never happen\" path is reached -so verify ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { Review Comment: add javadocs. I think there should also be some boolean indicating whether the new or old methods were picked up. This can be accompanied by a unit test which verifies that on java > 17 the new ones are found.", "created": "2025-08-20T13:24:20.882+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3206361417 @steveloughran thanks for your feedback, will address ASAP. BTW, I see that Hadoop 3.4.2 RC2 failed, if this PR gets merged before 3.4.2 is released, would you mind including it in 3.4.2? (I don't mean block the release, just nice to have)", "created": "2025-08-20T13:32:26.377+0000"}, {"author": "ASF GitHub Bot", "body": "LuciferYang commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2288345234 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,232 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, as CompletionException should always wrap an exception + throw ce; + } + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an exception. + * The cause of the {@code PrivilegedActionException} is set to the exception + * thrown by {@code action.run()}. + */ + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + throw new PrivilegedActionException(cause); + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } + + /** + * Maps to Subject.current() if available, otherwise maps to Subject.getSubject(). + * + * @return the current subject + */ + public static Subject current() { + try { + return (Subject) CURRENT.invoke(); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + @SuppressWarnings(\"unused\") + private static <T> PrivilegedExceptionAction<T> callableToPrivilegedExceptionAction( + Callable<T> callable) { + return callable::call; + } + + private static <T> Callable<T> privilegedExceptionActionToCallable( + PrivilegedExceptionAction<T> action) { + return action::run; + } + + private static <T> Callable<T> privilegedActionToCallable( + PrivilegedAction<T> action) { + return action::run; + } + + @SuppressWarnings(\"unchecked\") + private static <E extends Throwable> RuntimeException sneakyThrow(Throwable e) throws E { + throw (E) e; Review Comment: Type checking should be performed here. For `RuntimeException` and `Error`, they should be thrown directly, while other exceptions should be wrapped into a `RuntimeException` and then thrown. Additionally, does this method need to specify a return value type? ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,232 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); Review Comment: I suggest throwing `ExceptionInInitializerError` rather than `AssertionError` in the static block.", "created": "2025-08-20T14:22:48.324+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2288469830 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, as CompletionException should always wrap an exception + throw ce; + } + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an exception. + * The cause of the {@code PrivilegedActionException} is set to the exception + * thrown by {@code action.run()}. + */ + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + throw new PrivilegedActionException(cause); + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } + + /** + * Maps to Subject.current() if available, otherwise maps to + * Subject.getSubject() + * + * @return the current subject + */ + public static Subject current() { + try { + return (Subject) CURRENT.invoke(); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + @SuppressWarnings(\"unused\") + private static <T> PrivilegedExceptionAction<T> callableToPrivilegedExceptionAction( + Callable<T> callable) { + return callable::call; + } + + private static <T> Callable<T> privilegedExceptionActionToCallable( + PrivilegedExceptionAction<T> action) { + return action::run; + } + + private static <T> Callable<T> privilegedActionToCallable( + PrivilegedAction<T> action) { + return action::run; + } + + @SuppressWarnings(\"unchecked\") + private static <E extends Throwable> RuntimeException sneakyThrow(Throwable e) throws E { Review Comment: this article explains how it works https://www.baeldung.com/java-sneaky-throws", "created": "2025-08-20T15:01:40.385+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2288478008 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,232 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, as CompletionException should always wrap an exception + throw ce; + } + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an exception. + * The cause of the {@code PrivilegedActionException} is set to the exception + * thrown by {@code action.run()}. + */ + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + throw new PrivilegedActionException(cause); + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } + + /** + * Maps to Subject.current() if available, otherwise maps to Subject.getSubject(). + * + * @return the current subject + */ + public static Subject current() { + try { + return (Subject) CURRENT.invoke(); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + @SuppressWarnings(\"unused\") + private static <T> PrivilegedExceptionAction<T> callableToPrivilegedExceptionAction( + Callable<T> callable) { + return callable::call; + } + + private static <T> Callable<T> privilegedExceptionActionToCallable( + PrivilegedExceptionAction<T> action) { + return action::run; + } + + private static <T> Callable<T> privilegedActionToCallable( + PrivilegedAction<T> action) { + return action::run; + } + + @SuppressWarnings(\"unchecked\") + private static <E extends Throwable> RuntimeException sneakyThrow(Throwable e) throws E { + throw (E) e; Review Comment: the intention here is to allow the caller method to throw the checked exception without explicitly exception list declaration https://www.baeldung.com/java-sneaky-throws", "created": "2025-08-20T15:04:50.583+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2288540918 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: I see, it's possible in the legacy `doAs` case", "created": "2025-08-20T15:28:41.666+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3206914304 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 51s | | trunk passed | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 32s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 28s | | trunk passed | | +1 :green_heart: | javadoc | 1m 13s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 5s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 5s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 47s | | the patch passed | | +1 :green_heart: | compile | 8m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 18s | | the patch passed | | +1 :green_heart: | compile | 8m 23s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 8m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 47s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 4 new + 73 unchanged - 0 fixed = 77 total (was 73) | | +1 :green_heart: | mvnsite | 1m 24s | | the patch passed | | +1 :green_heart: | javadoc | 0m 59s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 9s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 18m 18s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 145m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e10307dfd1c1 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0a79322c61d8022717b0f821d69ebb65f19ff3b9 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1267 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-20T15:36:08.025+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289004862 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: during constructing the test cases, I realized the current approach seems impossible to keep original exception propagation behavior ... for example, in JDK 17, `SubjectUtil.callAs` binds to `Subject.doAs`, it's unable to distinguish 1) `action` throws a `PrivilegedActionException`, from 2) the bound `doAs` wrap the original exception thrown by `action` with a `PrivilegedActionException` @stoty I think I need to go back to https://github.com/apache/hadoop/pull/7081 direction ...", "created": "2025-08-20T18:49:14.443+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3208983015 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 52s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 43s | | trunk passed | | +1 :green_heart: | compile | 8m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 28s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 5s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 5s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 20s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 23s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 13s | [/patch-mvninstall-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-auth in the patch failed. | | -1 :x: | mvninstall | 0m 21s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | -1 :x: | compile | 0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 28s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 28s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 28s | [/results-checkstyle-hadoop-common-project.txt]([CI_URL] | hadoop-common-project: The patch generated 1 new + 73 unchanged - 0 fixed = 74 total (was 73) | | -1 :x: | mvnsite | 0m 14s | [/patch-mvnsite-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-auth in the patch failed. | | -1 :x: | mvnsite | 0m 26s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | -1 :x: | javadoc | 0m 19s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-common in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 20s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | -1 :x: | spotbugs | 0m 13s | [/patch-spotbugs-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-auth in the patch failed. | | -1 :x: | spotbugs | 0m 23s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | -1 :x: | shadedclient | 5m 9s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 13s | [/patch-unit-hadoop-common-project_hadoop-auth.txt]([CI_URL] | hadoop-auth in the patch failed. | | -1 :x: | unit | 0m 22s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 82m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0ec628cbf49c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 74b45de59c0fc8b4cd9974bfd53d1ff2ba7ee814 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-21T04:47:22.403+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289902982 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: the exception propagation mechanism is changed in Java 12 ... I refactored the code, and added `TestSubjectUtil` to cover all cases I can imagine. Now, the UT passed under JDK 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25 ``` mvn -Dsurefire.failIfNoSpecifiedTests=false install -pl :hadoop-auth -am -Dtest=TestSubjectUtil ```", "created": "2025-08-21T05:35:30.344+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289910860 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { Review Comment: I don't get this. Why limit this to ClassCastException ? Do we expect the type to be changed between instanceOf and the case ? Both the legacy and new JDK implementation (and the docs) explicitly wrap any Exception into a CompletionException. At this point we're either not dealing with an Exception (i.e. it's an Error or raw Throwable), or the JDK implementation is broken. We should just catch any Exception here", "created": "2025-08-21T05:41:55.605+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289911704 ########## hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/TestSubjectUtil.java: ########## @@ -0,0 +1,322 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import static org.junit.jupiter.api.Assertions.*; + +import org.junit.jupiter.api.Test; + +import java.io.IOException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +public class TestSubjectUtil { + + // \"1.8\"->8, \"9\"->9, \"10\"->10 + private static final int JAVA_SPEC_VER = Math.max(8, Integer.parseInt( + System.getProperty(\"java.specification.version\").split(\"\\\\.\")[0])); Review Comment: `Shell` is inaccessible from `hadoop-auth` module, so duplicate it.", "created": "2025-08-21T05:42:35.632+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289919196 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } else { + try { + return (T) DO_AS_THROW_EXCEPTION.invoke(subject, action); Review Comment: This is a nice optimization for older JDKs which avoids the wrapping shenenigans.", "created": "2025-08-21T05:48:16.306+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289925400 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { Review Comment: @stoty this code snippet comes from your original patch ... but I think it's reasonable. `Subject.callAs` catches `Exception` and wraps it in a `CompletionException`, but `CompletionException` accepts `Throwable` in ctor, and `CompletionException.getCause` also returns a `Throwable` instead of `Exception`. The catch of `ClassCastException` is for line 227, which shouldn't happen logically, though. ``` Exception cause = (Exception) ce.getCause(); ``` ``` public static <T> T callAs(final Subject subject, final Callable<T> action) throws CompletionException { Objects.requireNonNull(action); try { return ScopedValue.where(SCOPED_SUBJECT, subject).call(action::call); } catch (Exception e) { throw new CompletionException(e); } } ``` ``` public CompletionException(Throwable cause) { super(cause); } ``` As the comment said, it should never happen, I'm fine to either keep or remove it.", "created": "2025-08-21T05:52:49.148+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289939472 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { Review Comment: After second thought, we can simplify it to ``` Throwable cause = ce.getCause(); if (cause instanceof RuntimeException) { throw (RuntimeException) cause; } else if (cause instanceof Exception) { throw new PrivilegedActionException((Exception) cause); } else { throw sneakyThrow(cause); } ```", "created": "2025-08-21T06:02:14.513+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289946280 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,232 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); Review Comment: thanks for suggestion, adopted", "created": "2025-08-21T06:06:44.599+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289948065 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: https://github.com/openjdk/jdk21/blob/890adb6410dab4606a4f26a942aed02fb2f55387/src/java.base/share/classes/javax/security/auth/Subject.java#L375 https://github.com/openjdk/jdk24u/blob/b42f146edb8324bbb164bb706e8ad39d60aaf25b/src/java.base/share/classes/javax/security/auth/Subject.java#L327 So doAs() throws NPE and Security exceptions directly, and wraps Exceptions from the Action in a PrivilegedExceptionAction , while callAs() wraps every Exception in a CompletionException. So yes, we do lose some information when using callAs(), as we can't (easily) distinguish between NPEs and SecurityExceptions thrown in doAs itself or in the callable. However, UGI.doAs() unwraps PrivilegedActionExceptions anyway (and adds the IOException constraint) so I can't really see any case when this would actually matter. So by shortcutting doAs() for older JDKs the only difference is that we're throwing NPEs and SecurityExceptions from the doAs() method itself directly, while in the wrapped case we're wrapping them in an UndeclaredThrowableException, and even this is only matters in JDK 17, and JDK18+ wraps all exceptions in a CompletionException anyway (just like SubjectUtil does) I support adding the shortcut, but more as optimization than a correctness fix.", "created": "2025-08-21T06:07:57.752+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289979092 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: I see, I added `Objects.requireNonNull(action)` to each method first line", "created": "2025-08-21T06:26:54.728+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2289979092 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/SubjectUtil.java: ########## @@ -0,0 +1,230 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +@Private +public class SubjectUtil { + private static MethodHandle CALL_AS; + private static MethodHandle CURRENT; + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.doAs() is deprecated for removal and replaced by Subject.callAs(). + // Lookup first the new API, since for Java versions where both exist, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CALL_AS = lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + try { + // Lookup the old API. + MethodType oldSignature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + MethodHandle doAs = lookup.findStatic(Subject.class, \"doAs\", oldSignature); + // Convert the Callable used in the new API to the PrivilegedAction used + // in the old API. + MethodType convertSignature = MethodType.methodType( + PrivilegedExceptionAction.class, Callable.class); + MethodHandle converter = lookup.findStatic( + SubjectUtil.class, \"callableToPrivilegedExceptionAction\", convertSignature); + CALL_AS = MethodHandles.filterArguments(doAs, 1, converter); + } catch (NoSuchMethodException e) { + throw new AssertionError(e); + } + } + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + static { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + CURRENT = lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + CURRENT = MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new AssertionError(e); + } + } + + /** + * Maps to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * It also wraps the Callable so that the Subject is propagated to the new thread + * in all Java versions. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (PrivilegedActionException e) { + throw new CompletionException(e.getCause()); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + + /** + * Maps action to a Callable, and delegates to callAs(). On older JVMs, the + * action may be double wrapped (into Callable, and then back into + * PrivilegedAction). + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { Review Comment: I see, I added `Objects.requireNonNull(action)` at the begin of each method", "created": "2025-08-21T06:41:37.361+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290020425 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { Review Comment: nit: The first if clause is redundant, sneakyThrow will already handle that case. This is mostly equivalent to what the JDK compatibility method does, so this is as good as it gets.", "created": "2025-08-21T06:49:13.893+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290023163 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = Review Comment: nit: call this DO_AS_LEGACY ?", "created": "2025-08-21T06:50:23.887+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290027991 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws NullPointerException if action is null + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + Objects.requireNonNull(action); + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws NullPointerException if action is null + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + Objects.requireNonNull(action); + if (HAS_CALL_AS) { Review Comment: nit: this if can be avoided by adding another layer of reflection and setting it at initialization time.", "created": "2025-08-21T06:52:13.920+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290036185 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { Review Comment: `cause instanceof RuntimeException` is necessary, otherwise subclass of RuntimeException goes branch `cause instanceof Exception` ...", "created": "2025-08-21T06:54:48.763+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290088825 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = Review Comment: Do you mean `DO_AS_LEGACY` and `DO_AS_THROW_EXCEPTION_LEGACY`? I feel the DO_AS is clear enough, with an additional suffix might cause the name to be too long.", "created": "2025-08-21T07:06:12.894+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290110076 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws NullPointerException if action is null + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + Objects.requireNonNull(action); + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws NullPointerException if action is null + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + Objects.requireNonNull(action); + if (HAS_CALL_AS) { Review Comment: was thought about while tuning the code, but I think it might make the logic more complicated ... I suppose it won't introduce perf overhead, personally, I tend to use straightforward if-else control flow for cases that don't need to be extensible.", "created": "2025-08-21T07:15:25.136+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3209318082 @steveloughran @LuciferYang it's ready for the next round review, could you please take another look? thank you in advance.", "created": "2025-08-21T07:18:19.145+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3209406330 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 59s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 41s | | trunk passed | | +1 :green_heart: | compile | 8m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 31s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 32s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 4s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 25s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 41s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 45s | | the patch passed | | +1 :green_heart: | compile | 7m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 7m 58s | | the patch passed | | +1 :green_heart: | compile | 7m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 40s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 28s | | the patch passed | | +1 :green_heart: | javadoc | 1m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 3s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 24s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 8s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 18m 45s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 143m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 4f4afbbd2280 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5264f428fb628914a5bf14a465711e08ce3a71d6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1267 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-21T07:50:17.947+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2290110076 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws NullPointerException if action is null + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + Objects.requireNonNull(action); + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws NullPointerException if action is null + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + Objects.requireNonNull(action); + if (HAS_CALL_AS) { Review Comment: was thought about while tuning the code, but I think it might make the logic more complicated ... I suppose the current approach won't introduce perf overhead, personally, I tend to use straightforward if-else control flow for cases that don't need to be extensible.", "created": "2025-08-21T08:19:40.448+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3209740920 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 43s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 28s | | trunk passed | | +1 :green_heart: | compile | 15m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 27s | | trunk passed | | +1 :green_heart: | javadoc | 2m 5s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 34s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 11s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 37m 38s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 18s | | the patch passed | | +1 :green_heart: | compile | 15m 8s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 8s | | the patch passed | | +1 :green_heart: | compile | 13m 47s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 47s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 26s | | the patch passed | | +1 :green_heart: | javadoc | 1m 55s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 40s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 51s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 46s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 53s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 229m 21s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b918c5d43cd9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 824800f376be1cbb562c8d5ef4b23ff16328a773 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2161 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-21T09:30:21.697+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3209909330 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 57s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 44s | | trunk passed | | +1 :green_heart: | compile | 18m 0s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 12s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 17s | | trunk passed | | +1 :green_heart: | javadoc | 2m 0s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 32s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 30s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 42m 58s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 17s | | the patch passed | | +1 :green_heart: | compile | 17m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 17s | | the patch passed | | +1 :green_heart: | compile | 15m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 20s | | the patch passed | | +1 :green_heart: | javadoc | 1m 48s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 43m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 44s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 35s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 251m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b41fa05aeec4 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e92529b95c47f3c6ffad6725ea02a1b5f05e6155 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3135 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-21T10:16:24.009+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3209953623 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 1s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 36m 6s | | trunk passed | | +1 :green_heart: | compile | 17m 4s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 31s | | trunk passed | | +1 :green_heart: | javadoc | 2m 3s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 19s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 39m 45s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 18s | | the patch passed | | +1 :green_heart: | compile | 15m 4s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 4s | | the patch passed | | +1 :green_heart: | compile | 13m 36s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 25s | | the patch passed | | +1 :green_heart: | javadoc | 1m 58s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 43s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 22m 52s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 7s | | The patch does not generate ASF License warnings. | | | | 236m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6f28875c50ff 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9512cb2539173f641ca41b2f6ec97abbb1a76f06 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-21T10:23:46.468+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3210011576 @pan3793 Thank you for your contribution! From my perspective, I would give this a +0.5, mainly because I\u2019m not very familiar with this part. However, I hope this PR can receive consensus and full approval from the other reviewers. cc: @stoty @steveloughran @cnauroth @cxzl25 @LuciferYang @dongjoon-hyun", "created": "2025-08-21T10:44:57.161+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2293075102 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = Review Comment: Sorry, I misread the code, it's fine.", "created": "2025-08-22T08:28:34.095+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3216585269 @stoty Do you have any further suggestions? If we confirm that this PR is fine, we will proceed to merge it. cc: @pan3793", "created": "2025-08-23T08:56:26.514+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3216586652 @ahmarsuhail Pan asked me offline whether this feature could be released in 3.4.2. Do you think that\u2019s possible? I see that the vote for 3.4.2 is still ongoing \u2014 can we include this PR in RC3? cc: @pan3793 @steveloughran", "created": "2025-08-23T08:59:14.577+0000"}, {"author": "ASF GitHub Bot", "body": "LuciferYang commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3218679283 It would be very delightful if this pr could be included in 3.4.2.", "created": "2025-08-25T03:02:39.588+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3218732651 > @stoty Do you have any further suggestions? If we confirm that this PR is fine, we will proceed to merge it. cc: @pan3793 This is derived from the same code that Jetty has, save the shortcut fallback for JDK17. I think that this is fine.", "created": "2025-08-25T03:45:19.117+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2297057369 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ Review Comment: You may want to note that this is derived from Avatica which is derived from the Jetty implementation.", "created": "2025-08-25T03:48:49.092+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2297061960 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,302 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.lang.reflect.UndeclaredThrowableException; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ +@Private +public final class SubjectUtil { + private static final MethodHandle CALL_AS = lookupCallAs(); + static final boolean HAS_CALL_AS = CALL_AS != null; + private static final MethodHandle DO_AS = HAS_CALL_AS ? null : lookupDoAs(); + private static final MethodHandle DO_AS_THROW_EXCEPTION = + HAS_CALL_AS ? null : lookupDoAsThrowException(); + private static final MethodHandle CURRENT = lookupCurrent(); + + private static MethodHandle lookupCallAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + try { + // Subject.callAs() is available since Java 18. + return lookup.findStatic(Subject.class, \"callAs\", + MethodType.methodType(Object.class, Subject.class, Callable.class)); + } catch (NoSuchMethodException x) { + return null; + } + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAs() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupDoAsThrowException() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + MethodType signature = MethodType.methodType( + Object.class, Subject.class, PrivilegedExceptionAction.class); + return lookup.findStatic(Subject.class, \"doAs\", signature); + } catch (IllegalAccessException | NoSuchMethodException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupCurrent() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + // Subject.getSubject(AccessControlContext) is deprecated for removal and + // replaced by Subject.current(). + // Lookup first the new API, since for Java versions where both exists, the + // new API delegates to the old API (e.g. Java 18, 19 and 20). + // Otherwise (e.g. Java 17), lookup the old API. + return lookup.findStatic( + Subject.class, \"current\", MethodType.methodType(Subject.class)); + } catch (NoSuchMethodException e) { + MethodHandle getContext = lookupGetContext(); + MethodHandle getSubject = lookupGetSubject(); + return MethodHandles.filterReturnValue(getContext, getSubject); + } catch (IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetSubject() { + MethodHandles.Lookup lookup = MethodHandles.lookup(); + try { + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + return lookup.findStatic(Subject.class, + \"getSubject\", MethodType.methodType(Subject.class, contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + private static MethodHandle lookupGetContext() { + try { + // Use reflection to work with Java versions that have and don't have + // AccessController. + Class<?> controllerKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessController\"); + Class<?> contextKlass = ClassLoader.getSystemClassLoader() + .loadClass(\"java.security.AccessControlContext\"); + + MethodHandles.Lookup lookup = MethodHandles.lookup(); + return lookup.findStatic( + controllerKlass, \"getContext\", MethodType.methodType(contextKlass)); + } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException e) { + throw new ExceptionInInitializerError(e); + } + } + + /** + * Map to Subject.callAs() if available, otherwise maps to Subject.doAs(). + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws CompletionException if {@code action.call()} throws an exception. + * The cause of the {@code CompletionException} is set to the exception + * thrown by {@code action.call()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T callAs(Subject subject, Callable<T> action) throws CompletionException { + if (HAS_CALL_AS) { + try { + return (T) CALL_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } else { + try { + return doAs(subject, callableToPrivilegedAction(action)); + } catch (Exception e) { + throw new CompletionException(e); + } + } + } + + /** + * Map action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * <p> + * Note: Exception propagation behavior is different since Java 12, it always + * throw the original exception thrown by action; for lower Java versions, + * throw a PrivilegedActionException that wraps the original exception when + * action throw a checked exception. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs(Subject subject, PrivilegedAction<T> action) { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedActionToCallable(action)); + } catch (CompletionException ce) { + Throwable cause = ce.getCause(); + if (cause != null) { + throw sneakyThrow(cause); + } else { + // This should never happen, CompletionException thrown by Subject.callAs + // should always wrap an exception + throw ce; + } + } + } else { + try { + return (T) DO_AS.invoke(subject, action); + } catch (Throwable t) { + throw sneakyThrow(t); + } + } + } + + /** + * Maps action to a Callable on Java 18 onwards, and delegates to callAs(). + * Call Subject.doAs directly on older JVM. + * + * @param subject the subject this action runs as + * @param action the action to run + * @return the result of the action + * @param <T> the type of the result + * @throws PrivilegedActionException if {@code action.run()} throws an checked exception. + * The cause of the {@code PrivilegedActionException} is set to the exception thrown + * by {@code action.run()}. + */ + @SuppressWarnings(\"unchecked\") + public static <T> T doAs( + Subject subject, PrivilegedExceptionAction<T> action) throws PrivilegedActionException { + if (HAS_CALL_AS) { + try { + return callAs(subject, privilegedExceptionActionToCallable(action)); + } catch (CompletionException ce) { + try { + Exception cause = (Exception) ce.getCause(); + if (cause instanceof RuntimeException) { + throw (RuntimeException) cause; + } else { + throw new PrivilegedActionException(cause); + } + } catch (ClassCastException castException) { + // This should never happen, as PrivilegedExceptionAction should not wrap + // non-checked exceptions + throw new PrivilegedActionException(new UndeclaredThrowableException(ce.getCause())); + } + } + } else { + try { + return (T) DO_AS_THROW_EXCEPTION.invoke(subject, action); Review Comment: COme to think of it's for JDK 17 only, but it's still nice.", "created": "2025-08-25T03:54:26.258+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3218920468 > +1 LGTM > > nit: May want to add notes about the linegae of this implementation. @stoty Thank you very much for reviewing the code!", "created": "2025-08-25T05:50:52.024+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #7886: URL: https://github.com/apache/hadoop/pull/7886#discussion_r2297213226 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SubjectUtil.java: ########## @@ -0,0 +1,305 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.security.authentication.util; + +import java.lang.invoke.MethodHandle; +import java.lang.invoke.MethodHandles; +import java.lang.invoke.MethodType; +import java.security.PrivilegedAction; +import java.security.PrivilegedActionException; +import java.security.PrivilegedExceptionAction; +import java.util.Objects; +import java.util.concurrent.Callable; +import java.util.concurrent.CompletionException; + +import javax.security.auth.Subject; + +import org.apache.hadoop.classification.InterfaceAudience.Private; + +/** + * An utility class that adapt the Security Manager and APIs related to it for + * JDK 8 and above. + * <p> + * In JDK 17, the Security Manager and APIs related to it have been deprecated + * and are subject to removal in a future release. There is no replacement for + * the Security Manager. See <a href=\"https://openjdk.org/jeps/411\">JEP 411</a> + * for discussion and alternatives. + * <p> + * In JDK 24, the Security Manager has been permanently disabled. See + * <a href=\"https://openjdk.org/jeps/486\">JEP 486</a> for more information. + */ Review Comment: thank you for the information, let me update it.", "created": "2025-08-25T06:17:14.570+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3219393461 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 57s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 37s | | trunk passed | | +1 :green_heart: | compile | 8m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 28s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 29s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 3s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 7s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 27s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 42s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 46s | | the patch passed | | +1 :green_heart: | compile | 8m 6s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 6s | | the patch passed | | +1 :green_heart: | compile | 7m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 40s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 26s | | the patch passed | | +1 :green_heart: | javadoc | 1m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 4s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 17s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 6s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 18m 26s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 150m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7886 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 8093bb24782a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 917baf07f148f4ba27eda4b5317965808d2a0a4c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3153 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-25T08:52:09.034+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3219550498 @stoty @cxzl25 @LuciferYang Thank you all for your help and support! @pan3793 Thanks for the contribution! I will add @steveloughran @stoty @cxzl25 @LuciferYang to the review list. I will add @stoty as a co-author.", "created": "2025-08-25T09:39:06.453+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #7886: URL: https://github.com/apache/hadoop/pull/7886", "created": "2025-08-26T02:24:09.094+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3222343535 @pan3793 @stoty Thanks for the contribution! @steveloughran @cxzl25 @LuciferYang @cnauroth Thanks for the review!", "created": "2025-08-26T02:25:12.433+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 opened a new pull request, #7897: URL: https://github.com/apache/hadoop/pull/7897 Port https://github.com/apache/hadoop/pull/7886 to branch-3.4", "created": "2025-08-26T03:14:45.070+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3222707890 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 7m 12s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 2m 33s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 40s | | branch-3.4 passed | | +1 :green_heart: | compile | 8m 59s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 52s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 45s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 1m 34s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 1m 3s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 51s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 25s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 22m 27s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 0m 41s | | the patch passed | | +1 :green_heart: | compile | 9m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 14s | | the patch passed | | +1 :green_heart: | compile | 10m 11s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 10m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 49s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 25s | | the patch passed | | +1 :green_heart: | javadoc | 0m 56s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 48s | | hadoop-auth in the patch passed. | | +1 :green_heart: | unit | 16m 31s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 151m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7897 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ecc8f405f067 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / 4e66b56ce33b4febba4d013bacf2f50eedfbfaba | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1271 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common U: hadoop-common-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-08-26T05:46:58.585+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3223281829 @steveloughran We want to backport this PR to branch-3.4. Do you think that\u2019s okay?", "created": "2025-08-26T09:04:35.997+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3223326801 @slfan1989 @pan3793 Voting for RC3 is currently on-going, started last last thursday. We have 5 binding votes already so I think we should be able to successfully release 3.4.2 soon. If for any reason that doesn't happen, we can get this into 3.4.2", "created": "2025-08-26T09:17:57.531+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3223418522 @ahmarsuhail thank you for the information", "created": "2025-08-26T09:38:12.575+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 merged PR #7897: URL: https://github.com/apache/hadoop/pull/7897", "created": "2025-08-27T02:06:53.221+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226503661 @pan3793 Thanks for the contribution! From my personal perspective, this is a relatively important feature, and I suggest backporting it to branch-3.4. If anyone has different opinions, please feel free to discuss. cc: @stoty", "created": "2025-08-27T02:08:52.733+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226681187 It's already merged :) I'm fine with this backport. The question whether we want to fully support Java 18+ in 3.4.3. I was working on the assumption that support for Java 18+ was only targeted for 3.5.0. This is just one (admittedly critical) part of support for newer Javas, I alone have more than a dozen Java 25 patches in trunk which were never backported to 3.4, and the large patch for Subject propagation in threads is still WIP. IMO None of those patches are particularly controversial, though backporting them would take some work. The options are: - Hold back 3.4.2 for this patch - Consensus seems to be not do that (and I kind of agree) - Merge just this patch, and release in 3.4.3 - This seems to be enough for at least the Spark tests to pass - Backport full Java 25 support and release in 3.4.3 - This is my preference, as this is the fastest way to unlock the full Hadoop ecosystem for Java 18+", "created": "2025-08-27T04:15:59.576+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226696282 @stoty > The question whether we want to fully support Java 18+ in 3.4.3. My current goal is to make Hadoop \"client\" support Java 18+ in the next 3.4.x. version, fully Java 18+ support should target 3.5. > the large patch for Subject propagation ... I roughly understand what you are trying to fix, but I haven't seen real failures caused by this, do you have a concrete example that is affected by the lack of change? and is it must be fixed on Hadoop itself, and can not be workarounded by downstream projects? > - Merge just this patch, and release in 3.4.3 - This seems to be enough for at least the Spark tests to pass I won't block 3.4.2 release if it's already in good shape, I prefer this option.", "created": "2025-08-27T04:27:27.767+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226824363 > @stoty > > > The question whether we want to fully support Java 18+ in 3.4.3. > > My current goal is to make Hadoop \"client\" support Java 18+ in the next 3.4.x. version, fully Java 18+ support should target 3.5. My problem with that is that I am not at all convinced that this patch alone fixes the client fully. The only reliable data point we have is that the Spark tests are running with it. Which parts does Spark even test ? HDFS ? MR/Yarn ? Do Spark tests use secure mode / kerberos ? Do they use doAs/proxyUser ? > > > the large patch for Subject propagation ... > > I roughly understand what you are trying to fix, but I haven't seen real failures caused by this, do you have a concrete example that is affected by the lack of change? and is it must be fixed on Hadoop itself, and can not be workarounded by downstream projects? Just try and run the tests on JDK24/25 without the Thread fixes (i.e current trunk). Many tests will fail without them. Unfortunately I don't have notes on the specific tests, and it was a long enough time ago that I can't remember. > > > * Merge just this patch, and release in 3.4.3 - This seems to be enough for at least the Spark tests to pass > > I won't block 3.4.2 release if it's already in good shape, I prefer this option. I agree with not delaying 3.4.2 further, but have reservations about only adding this patch in 3.4.3, as noted above. Going for full JDK25 support in 3.4.3 doesn't block 3.4.2 either.", "created": "2025-08-27T05:53:42.382+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226831506 Basically, I am wary of shipping a 3.4.3 that looks like it works with Java 18+ at first glance, but can break down in interesting ways in some use cases.", "created": "2025-08-27T05:58:27.371+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226850766 > Which parts does Spark even test ? HDFS ? MR/Yarn ? Do Spark tests use secure mode / kerberos ? Do they use doAs/proxyUser ? Spark UT does not cover HDFS/MR tests, it has YARN integration tests by leveraging `MiniYARNCluster`, the Kerberos test coverage is low, with almost all tests run under SIMPLE mode. > I am wary of shipping a 3.4.3 that looks like it works with Java 18+ at first glance This is better than nothing, we can continue to fix issues discovered by downstream projects and release new patched versions.", "created": "2025-08-27T06:06:02.193+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3226918448 > > Which parts does Spark even test ? HDFS ? MR/Yarn ? Do Spark tests use secure mode / kerberos ? Do they use doAs/proxyUser ? > > Spark UT does not cover HDFS/MR tests, it has YARN integration tests by leveraging `MiniYARNCluster`, the Kerberos test coverage is low, with almost all tests run under SIMPLE mode. Thanks > > > I am wary of shipping a 3.4.3 that looks like it works with Java 18+ at first glance > > This is better than nothing, we can continue to fix issues discovered by downstream projects and release new patched versions. Or we can backport all JDK 18+ patches, and have better confidence that it will work on the first try. Really there are only two major patches, one is this one, and the other one is the thread patch. The rest are quite small/simple or only affect tests.", "created": "2025-08-27T06:26:23.373+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3227325729 @stoty WDYT have Hadoop 3.4.3 RC first and hold for a period (at least 3 weeks), to allow downstream projects to integrate and report issues?", "created": "2025-08-27T08:44:38.225+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3227344398 I'm OK with that as long as we communicate clearly that the JDK24-25 support is not a properly tested. Ideally, we'd have the test suite passing with JDK25 before we declare it properly supported. I understand and respect that you want something for Spark, I just don't want ppl to get the impression that 3.4.3 is properly tested and production ready on JDK18+.", "created": "2025-08-27T08:50:07.058+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7897: URL: https://github.com/apache/hadoop/pull/7897#issuecomment-3227367655 > I'm OK with that as long as we communicate clearly that the JDK24-25 support is not a properly tested. Absolutely!", "created": "2025-08-27T08:57:10.370+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3231666377 > @slfan1989 @pan3793 Voting for RC3 is currently on-going, started last last thursday. We have 5 binding votes already so I think we should be able to successfully release 3.4.2 soon. If for any reason that doesn't happen, we can get this into 3.4.2 @ahmarsuhail Sorry for the late reply. I\u2019ve discussed with @pan3793 offline, and we plan to release this PR in the next version, as additional PRs may be needed to ensure greater stability. Thank you all for your contributions to the 3.4.2 release\uff01", "created": "2025-08-28T03:21:04.863+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #7081: URL: https://github.com/apache/hadoop/pull/7081#issuecomment-3314278513 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-09-20T00:20:52.764+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #7081: HADOOP-19212. Avoid Subject.getSubject method on newer JVMs URL: https://github.com/apache/hadoop/pull/7081", "created": "2025-09-21T00:24:00.647+0000"}, {"author": "ASF GitHub Bot", "body": "celtric commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3337986136 > I\u2019ve discussed with @pan3793 offline, and we plan to release this PR in the next version, as additional PRs may be needed to ensure greater stability. Thank you all for your contributions to the 3.4.2 release\uff01 @slfan1989 do you have a rough idea of when will this next version be released, that contains this PR's fix?", "created": "2025-09-26T10:26:48.522+0000"}, {"author": "Steve Loughran", "body": "there's a commit with this ID in both trunk and branch-3.4. Is that enough for us to mark as fixed for those versions? That's ignoring the big issue of subject propagation", "created": "2025-10-07T19:00:18.067+0000"}, {"author": "Istvan Toth", "body": "IMO yes. The ticket is explicitly about handling the Subject changes, which is done. It is also good enough to handle Java up to 21.", "created": "2025-10-08T05:48:33.671+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3380152434 > > I\u2019ve discussed with @pan3793 offline, and we plan to release this PR in the next version, as additional PRs may be needed to ensure greater stability. Thank you all for your contributions to the 3.4.2 release\uff01 > > @slfan1989 do you have a rough idea of when will this next version be released, that contains this PR's fix? @celtric Sorry for the late reply. Hadoop 3.4.2 has been successfully released. We are still progressing with some follow-up work, so the timeline for the next release has not yet been finalized. I personally expect to complete the releases of Hadoop 3.4.3 and Hadoop 3.5.0 in Q4 of this year or Q1 of next year.", "created": "2025-10-08T07:41:34.407+0000"}, {"author": "ASF GitHub Bot", "body": "celtric commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3388664966 > @celtric Sorry for the late reply. Hadoop 3.4.2 has been successfully released. We are still progressing with some follow-up work, so the timeline for the next release has not yet been finalized. I personally expect to complete the releases of Hadoop 3.4.3 and Hadoop 3.5.0 in Q4 of this year or Q1 of next year. Thanks for the info!", "created": "2025-10-10T07:36:59.853+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3401215792 @celtric you can help with these releases by regression testing the betas, that's always a great contribution. Everyone's setup is slightly different, and the more coverage we get of different deployments the better", "created": "2025-10-14T10:50:46.709+0000"}, {"author": "ASF GitHub Bot", "body": "celtric commented on PR #7886: URL: https://github.com/apache/hadoop/pull/7886#issuecomment-3401341315 > @celtric you can help with these releases by regression testing the betas, that's always a great contribution. Everyone's setup is slightly different, and the more coverage we get of different deployments the better Where can I find the betas? I've been unable to locate them.", "created": "2025-10-14T11:29:39.131+0000"}, {"author": "H. Vetinari", "body": "This should be a subtask of HADOOP-19486, not of HADOOP-17177", "created": "2025-10-22T10:04:00.355+0000"}], "derived_tasks": {"summary": "[JDK23] org.apache.hadoop.security.UserGroupInformation use of Subject needs to move to replacement APIs - `javax", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "HADOOP-19197", "title": "S3A: Support AWS KMS Encryption Context", "description": "S3A properties allow users to choose the AWS KMS key ({_}fs.s3a.encryption.key{_}) and S3 encryption algorithm to be used (f{_}s.s3a.encryption.algorithm{_}). In addition to the AWS KMS Key, an encryption context can be used as non-secret data that adds additional integrity and authenticity to check the encrypted data. However, there is no option to specify the [AWS KMS Encryption Context|https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context] in S3A. In AWS SDK v2 the encryption context in S3 requests is set by the parameter [ssekmsEncryptionContext.|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/CreateMultipartUploadRequest.Builder.html#ssekmsEncryptionContext(java.lang.String)] It receives a base64-encoded UTF-8 string holding JSON with the encryption context key-value pairs. The value of this parameter could be set by the user in a new property {_}*fs.s3a.encryption.context*{_}, and be stored in the [EncryptionSecrets|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecrets.java] to later be used when setting the encryption parameters in [RequestFactoryImpl|https://github.com/apache/hadoop/blob/f92a8ab8ae54f11946412904973eb60404dee7ff/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java].", "status": "Reopened", "priority": "Blocker", "reporter": "Raphael Azzolini", "assignee": "Raphael Azzolini", "created": "2024-06-06T23:33:09.000+0000", "updated": "2025-10-23T15:09:30.000+0000", "labels": ["pull-request-available"], "components": ["fs/s3"], "comments": [{"author": "Viraj Jasani", "body": "We need to use it at 3 places:\u00a0CopyObjectRequest,\u00a0PutObjectRequest and\u00a0CreateMultipartUploadRequest.", "created": "2024-06-07T05:12:55.641+0000"}, {"author": "Steve Loughran", "body": "makes sense. as well as the request factory you will need to * add support to org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets & EncryptionSecretOperations (test in TestMarshalledCredentials) * extend existing KMS tests to include this in some of the operations, especially ITestS3AHugeFilesEncryption which validates the multipart and copy stuff * update {hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/encryption.md}", "created": "2024-06-07T11:06:29.879+0000"}, {"author": "Viraj Jasani", "body": "How about we allow user to configure _fs.s3a.encryption.context_ similar to how we allow for {_}fs.s3a.aws.credentials.provider.mapping{_}? i.e. key-value pair of String values, let S3A take care of converting the key-value pairs to Base64 encoded JSON of String key-value pairs. Given that the context is anyways sent in plain text (it's just Base64 encoded JSON String, not a secret key), we can allow user to configure plain text key-value pairs separate by \"=\" with {_}fs.s3a.encryption.context{_}. Sample validation error when we pass anything other than Base64 encoded Json:", "created": "2024-06-07T23:38:41.650+0000"}, {"author": "Raphael Azzolini", "body": "Like this?  <property> <name>fs.s3a.encryption.context</name> <value> key1=value1, key2=value2, key3=value3, key4=value4, key5=value5 </value> </property>  Yes, that should be OK to do.", "created": "2024-06-07T23:58:15.302+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini opened a new pull request, #6874: URL: https://github.com/apache/hadoop/pull/6874 Add the property fs.s3a.encryption.context that allow users to specify the AWS KMS Encryption Context to be used in S3A. The value of the encryption context is a key/value string that will be Base64 encoded and set in the parameter ssekmsEncryptionContext from the S3 client. Contributed by Raphael Azzolini ### Description of PR This code change adds a new property to S3A: fs.s3a.encryption.context\\ The property's value accepts a set of key/value attributes to be set on S3's encryption context. The value of the property will be base64 encoded and set in the parameter ssekmsEncryptionContext from the S3 client. ### How was this patch tested? S3's head-object response doesn't contain the object encryption key. Therefore, I enabled CloudTrails data logs in my bucket to verify that the tests were passing the encryption context to the request. I added this property to `auth-keys.xml` ``` <property> <name>fs.s3a.encryption.context</name> <value> project=hadoop, jira=HADOOP-19197, component=fs/s3 </value> </property> ``` Then I executed the following tests: ``` mvn clean verify -Dit.test=ITestS3AEncryption* -Dtest=none [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.s3a.ITestS3AEncryptionSSEKMSDefaultKeyWithEncryptionContext [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.10 s", "created": "2024-06-08T05:37:15.244+0000"}, {"author": "Raphael Azzolini", "body": "[~vjasani], I created a PR based on your suggestion, let me know if it is OK and if you have any feedback on the code change. I tested the code with the integration tests [~stevel@apache.org] pointed out, and I verified in CloudTrail logs that the {{x-amz-server-side-encryption-context}} was set as expected. S3's head-object doesn't return the object's encryption context, so the only way to verify the change was to enable CloudTrail data logs and check the requests made by the integration tests.", "created": "2024-06-08T05:42:15.299+0000"}, {"author": "Viraj Jasani", "body": "Amazing, will take a look. Thanks for working on this!", "created": "2024-06-08T05:52:41.962+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2155925382 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 9 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 36s | | trunk passed | | +1 :green_heart: | compile | 17m 22s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | compile | 16m 6s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | checkstyle | 4m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 44s | | trunk passed | | +1 :green_heart: | javadoc | 1m 56s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 45s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | spotbugs | 3m 54s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 26s | | the patch passed | | +1 :green_heart: | compile | 16m 44s | | the patch passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javac | 16m 44s | | the patch passed | | +1 :green_heart: | compile | 15m 50s | | the patch passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | javac | 15m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 39s | | the patch passed | | +1 :green_heart: | javadoc | 1m 50s | | the patch passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 44s | | the patch passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | spotbugs | 4m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 34m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 20m 50s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 5s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 257m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6874 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint | | uname | Linux 60270e4a493d 5.15.0-106-generic #116-Ubuntu SMP Wed Apr 17 09:17:56 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 71ffcd5849f4f4e99f7117b5c84046e86e23aad4 | | Default Java | Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3151 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-06-08T09:55:39.292+0000"}, {"author": "Junping Du", "body": "The fix passed the CI and make sense from a quick look. Can someone take a closer look?", "created": "2024-06-19T06:05:24.109+0000"}, {"author": "ASF GitHub Bot", "body": "ahmarsuhail commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1675660341 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1402,6 +1406,79 @@ public static String getS3EncryptionKey( } } + /** + * Get any SSE context, without propagating exceptions from + * JCEKs files. + * @param bucket bucket to query for + * @param conf configuration to examine + * @return the encryption context value or \"\" + * @throws IllegalArgumentException bad arguments. + */ + public static String getS3EncryptionContext( Review Comment: looks like this method is only used in tests? ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecretOperations.java: ########## @@ -61,4 +61,21 @@ public static Optional<String> getSSEAwsKMSKey(final EncryptionSecrets secrets) return Optional.empty(); } } + + /** + * Gets the SSE-KMS context if present, else don't set it in the S3 request. + * + * @param secrets source of the encryption secrets. + * @return an optional AWS KMS encryption context to attach to a request. + */ + public static Optional<String> getSSEAwsKMSEncryptionContext(final EncryptionSecrets secrets) { + if ((secrets.getEncryptionMethod() == S3AEncryptionMethods.SSE_KMS + || secrets.getEncryptionMethod() == S3AEncryptionMethods.DSSE_KMS) + && secrets.hasEncryptionKey() Review Comment: can't you have encryption context without defining a secret key, if you're just using an AWS KMS key? so then you shouldn't have this condition? ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionDSSEKMSUserDefinedKeyWithEncryptionContext.java: ########## @@ -0,0 +1,70 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; Review Comment: your ITests are not very useful as they don't assert on anything. I also don't think you need 3 new ITests that extend AbstractTestS3AEncryption. I recommend that you add a single ITest, that writes an object with encryption context. parameterize it for SSE-KMS default key, SS-KMS customer key, DSSE-KMS. Then do a GET on the object and assert on the encryption context. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1493,7 +1570,10 @@ public static EncryptionSecrets buildEncryptionSecrets(String bucket, LOG.debug(\"Data is unencrypted\"); break; } - return new EncryptionSecrets(encryptionMethod, encryptionKey); + + String encryptionContext = getS3EncryptionContextBase64Encoded(bucket, conf, Review Comment: nit: move this statement above the switch ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestSSEConfiguration.java: ########## @@ -147,39 +151,42 @@ void setProviderOption(final Configuration conf, } /** - * Assert that the exception text from {@link #getAlgorithm(String, String)} + * Assert that the exception text from {@link #getAlgorithm(String, String, String)} * is as expected. * @param expected expected substring in error * @param alg algorithm to ask for * @param key optional key value + * @param context optional encryption context value * @throws Exception anything else which gets raised */ public void assertGetAlgorithmFails(String expected, - final String alg, final String key) throws Exception { + final String alg, final String key, final String context) throws Exception { intercept(IOException.class, expected, - () -> getAlgorithm(alg, key)); + () -> getAlgorithm(alg, key, context)); } private S3AEncryptionMethods getAlgorithm(S3AEncryptionMethods algorithm, - String key) + String key, + String encryptionContext) throws IOException { - return getAlgorithm(algorithm.getMethod(), key); + return getAlgorithm(algorithm.getMethod(), key, encryptionContext); } - private S3AEncryptionMethods getAlgorithm(String algorithm, String key) + private S3AEncryptionMethods getAlgorithm(String algorithm, String key, String encryptionContext) throws IOException { - return getEncryptionAlgorithm(BUCKET, buildConf(algorithm, key)); + return getEncryptionAlgorithm(BUCKET, buildConf(algorithm, key, encryptionContext)); } /** * Build a new configuration with the given S3-SSE algorithm * and key. * @param algorithm algorithm to use, may be null * @param key key, may be null + * @param context encryption context, may be null * @return the new config. */ @SuppressWarnings(\"deprecation\") - private Configuration buildConf(String algorithm, String key) { + private Configuration buildConf(String algorithm, String key, String context) { Review Comment: nit: rename to encryptionContext", "created": "2024-07-15T09:41:36.100+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1677840394 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/TestS3ADelegationTokenSupport.java: ########## @@ -76,7 +76,7 @@ public void testSessionTokenDecode() throws Throwable { renewer, new URI(\"s3a://anything/\"), new MarshalledCredentials(\"a\", \"b\", \"\"), - new EncryptionSecrets(S3AEncryptionMethods.SSE_S3, \"\"), + new EncryptionSecrets(S3AEncryptionMethods.SSE_S3, \"\", \"\"), Review Comment: extend test to verify the secrets are round tripped correctly ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1402,6 +1406,79 @@ public static String getS3EncryptionKey( } } + /** + * Get any SSE context, without propagating exceptions from + * JCEKs files. + * @param bucket bucket to query for + * @param conf configuration to examine + * @return the encryption context value or \"\" + * @throws IllegalArgumentException bad arguments. + */ + public static String getS3EncryptionContext( Review Comment: 1. `FunctionalIO.uncheckIOExceptions()` can do the wrapping now. Though If it is for tests, why bother? 2. If new methods in production code are needed, lets start a new S3AEncryption class in fs.s3a.impl for this stuff, S3AUtils is too big and while leaving it alone helps backports, there's no need to make things worse", "created": "2024-07-15T13:38:47.477+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1680195423 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecretOperations.java: ########## @@ -61,4 +61,21 @@ public static Optional<String> getSSEAwsKMSKey(final EncryptionSecrets secrets) return Optional.empty(); } } + + /** + * Gets the SSE-KMS context if present, else don't set it in the S3 request. + * + * @param secrets source of the encryption secrets. + * @return an optional AWS KMS encryption context to attach to a request. + */ + public static Optional<String> getSSEAwsKMSEncryptionContext(final EncryptionSecrets secrets) { + if ((secrets.getEncryptionMethod() == S3AEncryptionMethods.SSE_KMS + || secrets.getEncryptionMethod() == S3AEncryptionMethods.DSSE_KMS) + && secrets.hasEncryptionKey() Review Comment: Right! If we don't specify the key, then the aws/s3 managed key will be used. I will remove `&& secrets.hasEncryptionKey()` from the condition.", "created": "2024-07-17T00:12:04.546+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1680279024 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/TestS3ADelegationTokenSupport.java: ########## @@ -76,7 +76,7 @@ public void testSessionTokenDecode() throws Throwable { renewer, new URI(\"s3a://anything/\"), new MarshalledCredentials(\"a\", \"b\", \"\"), - new EncryptionSecrets(S3AEncryptionMethods.SSE_S3, \"\"), + new EncryptionSecrets(S3AEncryptionMethods.SSE_S3, \"\", \"\"), Review Comment: Not sure if this is was you meant, but in the next revision I will add values to the key and encryption context instead of empty string. Then I will assert that they match the valyes in the decoded identifier. I will do the same in `testSessionTokenIdentifierRoundTrip()`", "created": "2024-07-17T01:39:52.173+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1680310920 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionDSSEKMSUserDefinedKeyWithEncryptionContext.java: ########## @@ -0,0 +1,70 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; Review Comment: I will create just a single ITest class. Regarding the assertion, I couldn't find any S3 API that allow me to get the object's encryption context, `HeadObject`, for example, returns only `ServerSideEncryption` and `SSEKMSKeyId`. The way that I found to test it is add a condition to the IAM Role policy or KMS Key policy requiring the encryption context to be set, like: ``` \"Condition\": { \"StringEquals\": { \"kms:EncryptionContext:project\": \"hadoop\", \"kms:EncryptionContext:jira\": \"HADOOP-19197\" } } ``` Let me know if you know a way to get the encryption context from S3 or if you have a better idea on how to test it.", "created": "2024-07-17T02:07:13.332+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2234545215 @steveloughran, regarding #6884, I didn't find any risks of conflict with this PR, but I think that the new code in `S3AFileSystem.java` [that you added a comment](https://github.com/apache/hadoop/pull/6884/files#r1668630755) could be moved to the new class `S3AEncryption.java` you suggested, and that I am adding to the next commit. I also added [a comment](https://github.com/apache/hadoop/pull/6884/files#r1681852975) to that PR asking if the new properties need to be added to `core-default.xml` and `index.md`.", "created": "2024-07-17T22:56:08.202+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1681885682 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionDSSEKMSUserDefinedKeyWithEncryptionContext.java: ########## @@ -0,0 +1,70 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; Review Comment: I executed `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify` in us-east-1, and I updated the PR description adding this information and explained how I tested that the encryption context is set in the S3 objects.", "created": "2024-07-17T23:25:38.009+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2235245529 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 39s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 36s | | trunk passed | | +1 :green_heart: | compile | 17m 50s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | compile | 16m 18s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | checkstyle | 4m 52s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 47s | | trunk passed | | +1 :green_heart: | javadoc | 1m 59s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 46s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 34m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 28s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | compile | 16m 17s | [/patch-compile-root-jdkUbuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2. | | -1 :x: | javac | 16m 17s | [/patch-compile-root-jdkUbuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2. | | -1 :x: | compile | 15m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_412-8u412-ga-1~20.04.1-b08.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08. | | -1 :x: | javac | 15m 42s | [/patch-compile-root-jdkPrivateBuild-1.8.0_412-8u412-ga-1~20.04.1-b08.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 4m 17s | | the patch passed | | -1 :x: | mvnsite | 0m 52s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | javadoc | 1m 52s | | the patch passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | -1 :x: | javadoc | 0m 50s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_412-8u412-ga-1~20.04.1-b08.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_412-8u412-ga-1~20.04.1-b08 with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | -1 :x: | spotbugs | 0m 50s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | shadedclient | 35m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 42s | | hadoop-common in the patch passed. | | -1 :x: | unit | 0m 51s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | asflicense | 1m 4s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 252m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.46 ServerAPI=1.46 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6874 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint | | uname | Linux a54a2d23b372 5.15.0-106-generic #116-Ubuntu SMP Wed Apr 17 09:17:56 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c2596c7df724a528bb5ce9b5a5c01935dc99556e | | Default Java | Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-07-18T03:31:40.389+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2237422122 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 30s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 2s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 45s | | trunk passed | | +1 :green_heart: | compile | 17m 25s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | compile | 16m 4s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | checkstyle | 4m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 41s | | trunk passed | | +1 :green_heart: | javadoc | 1m 57s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 43s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | spotbugs | 4m 1s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 13s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 27s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | compile | 16m 4s | [/patch-compile-root-jdkUbuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2. | | -1 :x: | javac | 16m 4s | [/patch-compile-root-jdkUbuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2. | | -1 :x: | compile | 15m 20s | [/patch-compile-root-jdkPrivateBuild-1.8.0_412-8u412-ga-1~20.04.1-b08.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08. | | -1 :x: | javac | 15m 20s | [/patch-compile-root-jdkPrivateBuild-1.8.0_412-8u412-ga-1~20.04.1-b08.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 13s | | the patch passed | | -1 :x: | mvnsite | 0m 53s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | javadoc | 1m 51s | | the patch passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 43s | | the patch passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | -1 :x: | spotbugs | 0m 50s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | shadedclient | 36m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 44s | | hadoop-common in the patch passed. | | -1 :x: | unit | 0m 48s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 239m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.46 ServerAPI=1.46 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6874 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint | | uname | Linux fda291b182e1 5.15.0-106-generic #116-Ubuntu SMP Wed Apr 17 09:17:56 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4de0f65dc4efa985359155afd8bc4777751a9bf6 | | Default Java | Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 1251 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-07-18T19:47:08.690+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2237455839 The checks are failing because they are trying to compile three classes that I removed in the second commit. I don't know how to make it not use them. ``` [ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-6874/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionDSSEKMSUserDefinedKeyWithEncryptionContext.java:[48,40] cannot find symbol [ERROR] symbol: method getS3EncryptionContext(java.lang.String,org.apache.hadoop.conf.Configuration) [ERROR] location: class org.apache.hadoop.fs.s3a.S3AUtils [ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-6874/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEKMSUserDefinedKeyWithEncryptionContext.java:[43,40] cannot find symbol [ERROR] symbol: method getS3EncryptionContext(java.lang.String,org.apache.hadoop.conf.Configuration) [ERROR] location: class org.apache.hadoop.fs.s3a.S3AUtils [ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-6874/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEKMSDefaultKeyWithEncryptionContext.java:[43,40] cannot find symbol [ERROR] symbol: method getS3EncryptionContext(java.lang.String,org.apache.hadoop.conf.Configuration) [ERROR] location: class org.apache.hadoop.fs.s3a.S3AUtils ```", "created": "2024-07-18T19:55:23.458+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2243532260 > The checks are failing because they are trying to compile three classes that I removed in the second commit. I don't know how to make it not use them. needs a squashed commit. let me review the changes first", "created": "2024-07-22T18:09:55.204+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6874: URL: https://github.com/apache/hadoop/pull/6874#discussion_r1686963532 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEKMSWithEncryptionContext.java: ########## @@ -0,0 +1,100 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * <p> + * http://www.apache.org/licenses/LICENSE-2.0 + * <p> + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.io.UncheckedIOException; +import java.util.Set; + +import org.apache.hadoop.thirdparty.com.google.common.collect.ImmutableSet; + +import org.apache.commons.lang.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.s3a.impl.S3AEncryption; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.skip; +import static org.apache.hadoop.fs.s3a.Constants.S3_ENCRYPTION_CONTEXT; +import static org.apache.hadoop.fs.s3a.Constants.S3_ENCRYPTION_KEY; +import static org.apache.hadoop.fs.s3a.S3AEncryptionMethods.DSSE_KMS; +import static org.apache.hadoop.fs.s3a.S3AEncryptionMethods.SSE_KMS; +import static org.apache.hadoop.fs.s3a.S3ATestUtils.assume; +import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestBucketName; + +/** + * Concrete class that extends {@link AbstractTestS3AEncryption} + * and tests KMS encryption with encryption context. + * S3's HeadObject doesn't return the object's encryption context. + * Therefore, we don't have a way to assert its value in code. + * In order to properly test if the encryption context is being set, + * the KMS key or the IAM User need to have a deny statements like the one below in the policy: + * <pre> + * { + * \"Effect\": \"Deny\", + * \"Principal\": { + * \"AWS\": \"*\" + * }, + * \"Action\": \"kms:Decrypt\", + * \"Resource\": \"*\", + * \"Condition\": { + * \"StringNotEquals\": { + * \"kms:EncryptionContext:project\": \"hadoop\" + * } + * } + * } + * </pre> + * With the statement above, S3A will fail to read the object from S3 if it was encrypted + * without the key-pair <code>\"project\": \"hadoop\"</code> in the encryption context. + */ +public class ITestS3AEncryptionSSEKMSWithEncryptionContext + extends AbstractTestS3AEncryption { + + private static final Set<S3AEncryptionMethods> KMS_ENCRYPTION_ALGORITHMS = ImmutableSet.of( + SSE_KMS, DSSE_KMS); + + private S3AEncryptionMethods encryptionAlgorithm; + + @Override + protected Configuration createConfiguration() { + try { + // get the KMS key and context for this test. + Configuration c = new Configuration(); + final String bucketName = getTestBucketName(c); + String kmsKey = S3AUtils.getS3EncryptionKey(bucketName, c); + String encryptionContext = S3AEncryption.getS3EncryptionContext(bucketName, c); + encryptionAlgorithm = S3AUtils.getEncryptionAlgorithm(bucketName, c); + assume(\"Expected a KMS encryption algorithm\", + KMS_ENCRYPTION_ALGORITHMS.contains(encryptionAlgorithm)); + if (StringUtils.isBlank(encryptionContext)) { + skip(S3_ENCRYPTION_CONTEXT + \" is not set.\"); + } + Configuration conf = super.createConfiguration(); + conf.set(S3_ENCRYPTION_KEY, kmsKey); Review Comment: use `removeBaseAndBucketOverrides(conf, String...)` and list all options you intend to set..stops overrides getting in the way. doesn't work for jceks overrides, which always take priority", "created": "2024-07-22T18:17:44.879+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-2244139487 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 56s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 8 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 18s | | trunk passed | | +1 :green_heart: | compile | 17m 36s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | compile | 16m 11s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | checkstyle | 4m 59s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 43s | | trunk passed | | +1 :green_heart: | javadoc | 1m 56s | | trunk passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 45s | | trunk passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | spotbugs | 3m 51s | | trunk passed | | +1 :green_heart: | shadedclient | 34m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 29s | | the patch passed | | +1 :green_heart: | compile | 16m 51s | | the patch passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javac | 16m 51s | | the patch passed | | +1 :green_heart: | compile | 16m 0s | | the patch passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | javac | 16m 0s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 23s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 40s | | the patch passed | | +1 :green_heart: | javadoc | 1m 53s | | the patch passed with JDK Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 | | +1 :green_heart: | javadoc | 1m 45s | | the patch passed with JDK Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | +1 :green_heart: | spotbugs | 4m 15s | | the patch passed | | +1 :green_heart: | shadedclient | 34m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 49s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 1s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 255m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.46 ServerAPI=1.46 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6874 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint | | uname | Linux 983bc8e41580 5.15.0-106-generic #116-Ubuntu SMP Wed Apr 17 09:17:56 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 898b883bbead9b619014260e9ee4c476bc18e9f2 | | Default Java | Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.23+9-post-Ubuntu-1ubuntu120.04.2 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_412-8u412-ga-1~20.04.1-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-07-23T02:37:57.247+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #6874: URL: https://github.com/apache/hadoop/pull/6874", "created": "2024-07-23T16:09:07.672+0000"}, {"author": "Raphael Azzolini", "body": "[~stevel@apache.org] should we mark this Jira as done since my pull request is merged? By the way, in which version will it be released?", "created": "2024-11-27T19:22:23.238+0000"}, {"author": "Steve Loughran", "body": "it's in trunk, so I will mark as done if you a cherrypick pr to branch-3.2 and rerun the integration tests then I will merge there too, so it will come out with hadoop 3.4.2", "created": "2024-11-27T21:07:58.678+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini opened a new pull request, #7193: URL: https://github.com/apache/hadoop/pull/7193 Add the property fs.s3a.encryption.context that allow users to specify the AWS KMS Encryption Context to be used in S3A. The value of the encryption context is a key/value string that will be Base64 encoded and set in the parameter ssekmsEncryptionContext from the S3 client. Contributed by Raphael Azzolini ### Description of PR This code change adds a new property to S3A: fs.s3a.encryption.context\\ The property's value accepts a set of key/value attributes to be set on S3's encryption context. The value of the property will be base64 encoded and set in the parameter ssekmsEncryptionContext from the S3 client. This change was merged to trunk by the pull request https://github.com/apache/hadoop/pull/6874. This request is to merge the code change to branch-3.4. ### How was this patch tested? Tested in us-east-1 with `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify` I added a new test `ITestS3AEncryptionSSEKMSWithEncryptionContext`. However, S3's head-object response doesn't contain the object encryption key. Therefore, I enabled CloudTrails data logs in my bucket to verify that the tests were passing the encryption context to the request. I added this property to `auth-keys.xml` ``` <property> <name>fs.s3a.encryption.context</name> <value> project=hadoop, jira=HADOOP-19197, component=fs/s3 </value> </property> ``` Then I executed the following tests: ``` mvn clean verify -Dit.test=ITestS3AEncryption* -Dtest=none [INFO] ------------------------------------------------------- [INFO] T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.s3a.ITestS3AEncryptionSSEKMSDefaultKeyWithEncryptionContext [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.10 s", "created": "2024-11-27T22:39:58.106+0000"}, {"author": "Raphael Azzolini", "body": "Pull request for branch-3.4: https://github.com/apache/hadoop/pull/7193", "created": "2024-11-27T22:40:25.092+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7193: URL: https://github.com/apache/hadoop/pull/7193#issuecomment-2505168787 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 1s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 8 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 4m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 39m 37s | | branch-3.4 passed | | +1 :green_heart: | compile | 17m 45s | | branch-3.4 passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 16m 52s | | branch-3.4 passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga | | +1 :green_heart: | checkstyle | 5m 4s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 2m 45s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 1m 57s | | branch-3.4 passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 45s | | branch-3.4 passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga | | +1 :green_heart: | spotbugs | 3m 59s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 34m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 29s | | the patch passed | | +1 :green_heart: | compile | 17m 31s | | the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 17m 31s | | the patch passed | | +1 :green_heart: | compile | 16m 57s | | the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga | | +1 :green_heart: | javac | 16m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 18s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 39s | | the patch passed | | +1 :green_heart: | javadoc | 1m 49s | | the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 44s | | the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga | | +1 :green_heart: | spotbugs | 4m 18s | | the patch passed | | +1 :green_heart: | shadedclient | 34m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 43s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 3m 4s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 254m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.47 ServerAPI=1.47 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7193 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint | | uname | Linux 2bab37f2f2c0 5.15.0-124-generic #134-Ubuntu SMP Fri Sep 27 20:20:17 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / a89d77b4422f8efbcf013945936c39f2ff423bf5 | | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga | | Test Results | [CI_URL] | | Max. process+thread count | 1251 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-11-28T02:55:56.918+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on PR #7193: URL: https://github.com/apache/hadoop/pull/7193#issuecomment-2506410976 @steveloughran, I just finished running the integration tests and updated the description. The PR should be good to merge, and then we can close the JIRA. Thank you!", "created": "2024-11-28T15:51:43.996+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7193: URL: https://github.com/apache/hadoop/pull/7193#issuecomment-2516927725 did you have to make any changes for the backport? if so, what? That's all I need to know before approval.", "created": "2024-12-04T10:44:56.019+0000"}, {"author": "Raphael Azzolini", "body": "I didn't need to make any changes. I just cherry-picked the commit from trunk, there were no conflicts.", "created": "2024-12-04T17:50:08.081+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #7193: URL: https://github.com/apache/hadoop/pull/7193", "created": "2024-12-06T15:11:50.452+0000"}, {"author": "Steve Loughran", "body": "excellent! merged as is, and gave you the credit for the work. expect to see a 3.4.2 out early next year", "created": "2024-12-06T15:13:15.290+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-3118073395 This is incompatible at the wire serialization level, as EncryptionSecrets says ``` \" * <i>Important.</i> * If the wire format is ever changed incompatibly, * update the serial version UID to ensure that older clients get safely * rejected. \" ``` We've added a new string to the payload, so it needs a new serialization ID generated and set in the java source. @raphaelazzolini would you be able to change that serialization number ASAP? I've updated the docs to indicate wire incompatibility.", "created": "2025-07-25T14:28:57.806+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran opened a new pull request, #7830: URL: https://github.com/apache/hadoop/pull/7830 Followup the main HADOOP-19197 patch to address serialization and compilation issues * Recreate serialization ID * Restore two arg constructor * Define DEFAULT_S3_ENCRYPTION_CONTEXT to specify what the default value is (just \"\", but being explicit) * Tests ### How was this patch tested? Unit tests. ITests failures are meaningless until #7814 is in. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-07-25T15:19:04.221+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-3118583528 I've created #7830 to address this. Still no wire compatibility but the serialization ID is changed, we've got tests to verify context is marshalled. And the binary API is restored. We need this in to 3.4.2", "created": "2025-07-25T15:23:32.137+0000"}, {"author": "Steve Loughran", "body": "tagging as blocker as either the followup is in or we revert, and as the followup in straightforward then, unless we absolutely want wire compatibility of delegation tokens, it'd be good to ship this feature", "created": "2025-07-25T15:25:11.079+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-3118654326 @steveloughran sorry, I missed this part. Looks like it is handled by your last commit. Do you need that I change anything else on this?", "created": "2025-07-25T15:29:46.515+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3119619943 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 47s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 26s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 32s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 126m 17s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7830 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux eb51a9d6f805 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7b1b8a66505a9b7084404a29b6fbef329cbd633e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 701 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-07-25T17:26:58.795+0000"}, {"author": "Steve Loughran", "body": "I'm actually going to roll this back from 3.4.x; it will break deployments where the cluster is on a different version. We have broader scope for incompatible changes on 3.5.0, which could include the unmarshalling code (which would run on the server) recognising the old version and only trying to unmarshall the first two fields. a 3.5.x client talking to a 3.4.x yarn service would still break, but that's never a supported delployment.", "created": "2025-07-28T12:48:48.992+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3128487083 taking attention away from this as I've just reverted the patch from 3.4.x This still needs to go in though", "created": "2025-07-28T18:36:45.027+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6874: URL: https://github.com/apache/hadoop/pull/6874#issuecomment-3128517079 @raphaelazzolini reviews of that PR because even though it's not a blocker for 3.4.2, if it doesn't get in soon I'll forget about it", "created": "2025-07-28T18:48:15.061+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3129877851 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 14m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 29s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 141m 21s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7830 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0653ff8799b8 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5d66063f1254acc4a7d6007d6a1a30e1325c06be | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 549 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-07-28T21:00:53.381+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on code in PR #7830: URL: https://github.com/apache/hadoop/pull/7830#discussion_r2238003629 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecrets.java: ########## @@ -70,7 +75,7 @@ public class EncryptionSecrets implements Writable, Serializable { /** * Encryption context: base64-encoded UTF-8 string. */ - private String encryptionContext = \"\"; + private String encryptionContext = DEFAULT_S3_ENCRYPTION_CONTEXT; Review Comment: encryptionAlgorithm and encryptionKey in the lines above are assigned to `\"\"` instead of a constant. I think we can create a default constant for them too in a future commit.", "created": "2025-07-28T23:07:26.712+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on code in PR #7830: URL: https://github.com/apache/hadoop/pull/7830#discussion_r2239519985 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecrets.java: ########## @@ -54,7 +56,10 @@ public class EncryptionSecrets implements Writable, Serializable { public static final int MAX_SECRET_LENGTH = 2048; - private static final long serialVersionUID = 1208329045511296375L; + /** + * Change this after any change to the payload: {@value}. + */ + private static final long serialVersionUID = 8834417969966697162L; Review Comment: How did you generate this id? Is there a way to introduce a unit test to validate if new fields were added and the id should be updated?", "created": "2025-07-29T11:40:26.473+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #7830: URL: https://github.com/apache/hadoop/pull/7830#discussion_r2243211963 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecrets.java: ########## @@ -54,7 +56,10 @@ public class EncryptionSecrets implements Writable, Serializable { public static final int MAX_SECRET_LENGTH = 2048; - private static final long serialVersionUID = 1208329045511296375L; + /** + * Change this after any change to the payload: {@value}. + */ + private static final long serialVersionUID = 8834417969966697162L; Review Comment: IntelliJ has a helper * java serialization: you MUST update the value if the payload the set of serialized fields (everything non file not tagged as `transient`) changes. I think consensus is you can generate any sufficiently random number and all is good. Key is: change it. * hadoop writable (which is how this stuff is actually marshalled in delegation tokens): you implement the read/write. This means we can be adaptive here in reading old versions too. which is what I'll do. I don't worry about the java serialization so much as it'll only surface if people are trying to save delegation tokens in odd ways", "created": "2025-07-30T16:18:20.789+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3137107212 Latest commit will read old versions. What it doesn't do is track which version it received, so if it ever has to save that DT again it'll return a new one. I don't know if it that is an issue in the use case of hadoop 3.4.x app launched into cluster with 3.5.x servers; the DT list will now be safely parsed by the yarn RM, but if the list is saved again (and we do that for passing RM to container Credentials, don't we?) then the new version is saved. So if a container is now launched with a 3.4.x hadoop-aws module, it wouldn't be able to unmarshall the data. fix'd be to remember and use when saving, -but I need to be sure it is worth the effort first", "created": "2025-07-30T16:44:32.127+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3137450335 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 46s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 3 new + 1 unchanged - 0 fixed = 4 total (was 1) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 29s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 30s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 126m 39s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7830 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 48a3fb0fa640 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e8277f2be25df61e7a955f2805762ad73a74b131 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 701 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-07-30T18:40:29.448+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3353458530 @raphaelazzolini I'm really neglecting this. Do you want to take this to completion. Add your suggested changes plus tests that all is good", "created": "2025-09-30T19:09:00.566+0000"}, {"author": "ASF GitHub Bot", "body": "raphaelazzolini commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3366425576 > @raphaelazzolini I'm really neglecting this. Do you want to take this to completion. Add your suggested changes plus tests that all is good and we can target 3.4.3 @steveloughran yes, I can take it to completion, I just want to confirm what we are missing here. At the current state, old versions are safely read but the new version, but we can't parse new version in the old version of the class. Is the new version -> old version scenario that you are asking me to complete?", "created": "2025-10-03T16:48:34.095+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7830: URL: https://github.com/apache/hadoop/pull/7830#issuecomment-3437539569 yes. if the old version gets a new record, there's nothing that can be done.", "created": "2025-10-23T15:09:29.983+0000"}], "derived_tasks": {"summary": "S3A: Support AWS KMS Encryption Context - S3A properties allow users to choose the AWS KMS key ({_}fs", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "HADOOP-19165", "title": "Explore dropping protobuf 2.5.0 from the distro", "description": "explore if protobuf-2.5.0 can be dropped from distro, it is a transitive dependency from HBase, but HBase doesn't use it in the code. Check if it is the only one pulling it into the distro & will something break if we exclude that, if none lets get rid of it", "status": "Resolved", "priority": "Major", "reporter": "Ayush Saxena", "assignee": "Ayush Saxena", "created": "2024-05-07T17:32:54.000+0000", "updated": "2025-10-22T12:05:14.000+0000", "labels": ["pull-request-available"], "components": ["build", "yarn"], "comments": [{"author": "Steve Loughran", "body": "relates to HADOOP-18487, where I tried to do most of this, but still couidn't stop it cropping up in yarn.", "created": "2024-05-13T17:16:53.196+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn opened a new pull request, #7051: URL: https://github.com/apache/hadoop/pull/7051 ### Description of PR Drop protobuf 2.5.0 from the distribution, Hadoop uses the one from hadoop-thirdparty, we ideally don't need it. ### How was this patch tested? Manually **1. The contents of the distro:** **Before:** ``` ayushsaxena@ayushsaxena share % ls -l -R . | grep protobuf -rw-r--r--@ 1 ayushsaxena staff 1952718 Sep 18 10:54 hadoop-shaded-protobuf_3_25-1.3.0.jar -rw-r--r--@ 1 ayushsaxena staff 1952718 Sep 18 10:56 hadoop-shaded-protobuf_3_25-1.3.0.jar -rw-r--r--@ 1 ayushsaxena staff 5105 Sep 18 10:16 grpc-protobuf-1.53.0.jar -rw-r--r--@ 1 ayushsaxena staff 7555 Sep 18 10:16 grpc-protobuf-lite-1.53.0.jar -rw-r--r--@ 1 ayushsaxena staff 1875927 Sep 18 10:16 hbase-shaded-protobuf-4.1.5.jar -rw-r--r--@ 1 ayushsaxena staff 533455 Sep 18 10:16 protobuf-java-2.5.0.jar ``` **Now:** ``` ayushsaxena@ayushsaxena share % ls -l -R . | grep protobuf -rw-r--r--@ 1 ayushsaxena staff 1952718 Sep 18 12:43 hadoop-shaded-protobuf_3_25-1.3.0.jar -rw-r--r--@ 1 ayushsaxena staff 1952718 Sep 18 12:44 hadoop-shaded-protobuf_3_25-1.3.0.jar -rw-r--r--@ 1 ayushsaxena staff 5105 Sep 18 12:48 grpc-protobuf-1.53.0.jar -rw-r--r--@ 1 ayushsaxena staff 7555 Sep 18 12:48 grpc-protobuf-lite-1.53.0.jar -rw-r--r--@ 1 ayushsaxena staff 1875927 Sep 18 12:48 hbase-shaded-protobuf-4.1.5.jar ``` No protbuf-java-2.5.0 now in the distro **2. Maven dependency:tree** **Before:** ``` ayushsaxena@ayushsaxena hadoop % mvn dependency:tree | grep protobuf-java:jar:2.5.0:compile [INFO] | +- com.google.protobuf:protobuf-java:jar:2.5.0:compile [INFO] | +- com.google.protobuf:protobuf-java:jar:2.5.0:compile [INFO] | +- com.google.protobuf:protobuf-java:jar:2.5.0:compile [INFO] | | +- com.google.protobuf:protobuf-java:jar:2.5.0:compile ayushsaxena@ayushsaxena hadoop % ``` **Now:** ``` ayushsaxena@ayushsaxena hadoop % mvn dependency:tree | grep protobuf-java:jar:2.5.0:compile ayushsaxena@ayushsaxena hadoop % ``` Ran some UT selectively, Waiting for the CI to report any errors, if any ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2024-09-18T07:38:54.131+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #7051: URL: https://github.com/apache/hadoop/pull/7051#issuecomment-2357889146 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 42s | | trunk passed | | +1 :green_heart: | compile | 0m 17s | | trunk passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | compile | 0m 16s | | trunk passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | mvnsite | 0m 19s | | trunk passed | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | javadoc | 0m 17s | | trunk passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | shadedclient | 53m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 10s | | the patch passed | | +1 :green_heart: | compile | 0m 10s | | the patch passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | javac | 0m 10s | | the patch passed | | +1 :green_heart: | compile | 0m 10s | | the patch passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | javac | 0m 10s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 11s | | the patch passed | | +1 :green_heart: | javadoc | 0m 10s | | the patch passed with JDK Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 | | +1 :green_heart: | javadoc | 0m 10s | | the patch passed with JDK Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | +1 :green_heart: | shadedclient | 20m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 13s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 76m 55s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.47 ServerAPI=1.47 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/7051 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 400320336ae4 5.15.0-117-generic #127-Ubuntu SMP Fri Jul 5 20:13:28 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 59c5539a26df0b7c3fb57b4c9abccee04b029142 | | Default Java | Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.24+8-post-Ubuntu-1ubuntu320.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_422-8u422-b05-1~20.04-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-09-18T08:56:53.015+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #7051: URL: https://github.com/apache/hadoop/pull/7051#issuecomment-2364352795 Hasn't #6629 done this.", "created": "2024-09-20T19:03:23.013+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on PR #7051: URL: https://github.com/apache/hadoop/pull/7051#issuecomment-2364363790 Hbase-2 still declares protobuf-2.5.0, the original ticket has details https://issues.apache.org/jira/browse/HADOOP-19107?focusedCommentId=17843349&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17843349 I couldn't figure out why HBase still defines it, but since you were mentioning to drop protobuf-2.5, so I thought of giving it a try if it doesn't break anything...", "created": "2024-09-20T19:10:29.346+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on PR #7051: URL: https://github.com/apache/hadoop/pull/7051#issuecomment-2370199206 According to HBASE-27436 and HBASE-27436, protobuf 2.5 can be purged if Hadoop does not use the HBase co-processor feature.", "created": "2024-09-24T05:23:44.116+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn merged PR #7051: URL: https://github.com/apache/hadoop/pull/7051", "created": "2024-09-24T15:28:44.604+0000"}, {"author": "Ayush Saxena", "body": "Committed to trunk. Thanx everyone for the reviews!!!", "created": "2024-09-24T15:29:07.404+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran opened a new pull request, #8044: URL: https://github.com/apache/hadoop/pull/8044 ### Description of PR #7051 for branch-3.4 ### How was this patch tested? yetus's problem ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2025-10-21T18:24:55.247+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #8044: URL: https://github.com/apache/hadoop/pull/8044#issuecomment-3428654224 if this cherrypick takes I'll commit it", "created": "2025-10-21T18:25:14.102+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #8044: URL: https://github.com/apache/hadoop/pull/8044#issuecomment-3429525038 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 19m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 45s | | branch-3.4 passed | | +1 :green_heart: | compile | 0m 21s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 27s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 0m 37s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 85m 59s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 12s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 12s | | the patch passed | | +1 :green_heart: | compile | 0m 12s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 12s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 41m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 19s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 150m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/8044 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 7ca8cfced20c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / a70e901a65683e20763c20353cd5f51e91227772 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 527 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-21T20:57:00.712+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #8044: URL: https://github.com/apache/hadoop/pull/8044#issuecomment-3431774918 tests were happy, it looks like publishing was playing up ``` ERROR: Failed to write github status. Token expired or missing repo:status write? ```", "created": "2025-10-22T10:54:39.210+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #8044: URL: https://github.com/apache/hadoop/pull/8044", "created": "2025-10-22T10:55:11.038+0000"}], "derived_tasks": {"summary": "Explore dropping protobuf 2.5.0 from the distro - explore if protobuf-2", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-19058", "title": "[JDK-17] Fix UT Failures in hadoop common, hdfs, yarn", "description": "Most of the UT's failed with below exception:", "status": "Open", "priority": "Major", "reporter": "Bilwa S T", "assignee": "Bilwa S T", "created": "2024-01-31T05:58:01.000+0000", "updated": "2025-10-19T09:02:15.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "BilwaST opened a new pull request, #6531: URL: https://github.com/apache/hadoop/pull/6531 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2024-02-06T06:08:35.671+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-1928973004 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 54s | | trunk passed | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 75m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | compile | 0m 13s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 13s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 16s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 33m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 17s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 115m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6531 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux b47656071f4b 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fcb51d70221c64831ab78e20abb3d48f59bbf506 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 672 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-02-06T08:05:18.664+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on code in PR #6531: URL: https://github.com/apache/hadoop/pull/6531#discussion_r1480648680 ########## hadoop-project/pom.xml: ########## @@ -168,7 +168,18 @@ <enforced.maven.version>[3.3.0,)</enforced.maven.version> <!", "created": "2024-02-06T23:15:19.662+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6531: URL: https://github.com/apache/hadoop/pull/6531#discussion_r1481358429 ########## hadoop-project/pom.xml: ########## @@ -168,7 +168,18 @@ <enforced.maven.version>[3.3.0,)</enforced.maven.version> <!", "created": "2024-02-07T11:53:58.881+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-1931896429 commented. I looked at surefire and it is at 3.2.5; probably time to upgrade. I propose it is done before doing this one, splitting them in two. and we need java8 and java17 profiles so we can add other options in other places", "created": "2024-02-07T11:59:23.053+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-1933275327 > commented. I looked at surefire and it is at 3.2.5; probably time to upgrade. I propose it is done before doing this one, splitting them in two. > > and we need java8 and java17 profiles so we can add other options in other places I agree with your points, but there are some additional details to be added: - I agree to establish profiles for Java 8 and Java 17, and I think we should also include Java 11 and Java 21. - There are still some pending tasks related to Java 11. We need to continue working on HADOOP-15984, specifically updating Jersey from version 1.19 to 2.x. Ideally, we should update it to Jersey 3.x. \uff08I will start following this JIRA in the next 1-2 days\uff09 - We need to migrate from JUnit 4 to JUnit 5. Some progress has already been made in Yarn. - We should consider upgrading the version of the Maven Surefire Plugin. Currently, we are using `3.0.0-M1`, but I suggest upgrading it to `3.2.2`. I have noticed that some projects are using this version, and I will try it out. I will create a JIRA ticket for Java 17 compile support. I am aware that there is a lot of work to be done, and I hope that we can address the JDK support issue this year.", "created": "2024-02-08T02:40:10.493+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-2077803597 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 00s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 01s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 01s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 01s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 00s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 00s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 92m 58s | | trunk passed | | +1 :green_heart: | compile | 4m 29s | | trunk passed | | +1 :green_heart: | mvnsite | 4m 34s | | trunk passed | | +1 :green_heart: | javadoc | 4m 33s | | trunk passed | | +1 :green_heart: | shadedclient | 246m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 2m 01s | | the patch passed | | +1 :green_heart: | compile | 1m 55s | | the patch passed | | +1 :green_heart: | javac | 1m 55s | | the patch passed | | +1 :green_heart: | blanks | 0m 00s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 1m 59s | | the patch passed | | +1 :green_heart: | javadoc | 1m 57s | | the patch passed | | +1 :green_heart: | shadedclient | 152m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 5m 29s | | The patch does not generate ASF License warnings. | | | | 418m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/6531 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | MINGW64_NT-10.0-17763 fef11fae28ac 3.4.10-87d57229.x86_64 2024-02-14 20:17 UTC x86_64 Msys | | Build tool | maven | | Personality | /c/hadoop/dev-support/bin/hadoop.sh | | git revision | trunk / fcb51d70221c64831ab78e20abb3d48f59bbf506 | | Default Java | Azul Systems, Inc.-1.8.0_332-b09 | | Test Results | [CI_URL] | | modules | C: hadoop-project U: hadoop-project | | Console output | [CI_URL] | | versions | git=2.44.0.windows.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-25T17:30:28.063+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6531: URL: https://github.com/apache/hadoop/pull/6531#discussion_r1481361766 ########## hadoop-project/pom.xml: ########## @@ -168,7 +168,18 @@ <enforced.maven.version>[3.3.0,)</enforced.maven.version> <!", "created": "2024-04-26T13:24:48.654+0000"}, {"author": "ASF GitHub Bot", "body": "pan3793 commented on code in PR #6531: URL: https://github.com/apache/hadoop/pull/6531#discussion_r1627386093 ########## hadoop-project/pom.xml: ########## @@ -168,7 +168,18 @@ <enforced.maven.version>[3.3.0,)</enforced.maven.version> <!", "created": "2024-06-05T09:45:51.108+0000"}, {"author": "ASF GitHub Bot", "body": "stoty commented on code in PR #6531: URL: https://github.com/apache/hadoop/pull/6531#discussion_r1910158374 ########## hadoop-project/pom.xml: ########## @@ -168,7 +168,18 @@ <enforced.maven.version>[3.3.0,)</enforced.maven.version> <!", "created": "2025-01-10T10:23:47.199+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-3353257455 Do we still need this?", "created": "2025-09-30T18:01:00.908+0000"}, {"author": "ASF GitHub Bot", "body": "szetszwo commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-3407431152 @slfan1989 , do you know if the unit tests described here are still failing? If not, let's resolve the JIRA.", "created": "2025-10-15T16:59:54.347+0000"}, {"author": "ASF GitHub Bot", "body": "slfan1989 commented on PR #6531: URL: https://github.com/apache/hadoop/pull/6531#issuecomment-3419461498 > @slfan1989 , do you know if the unit tests described here are still failing? If not, let's resolve the JIRA. @szetszwo Sorry for the late reply. The compilation-related changes in this PR have already been completed in #7114. We currently know that there are still five unit tests with issues, which are as follows: ``` hadoop.hdfs.tools.TestDFSAdmin hadoop.yarn.sls.appmaster.TestAMSimulator hadoop.yarn.server.router.webapp.TestFederationWebApp hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler hadoop.yarn.server.router.webapp.TestRouterWebServicesREST ``` I will continue to follow up on these unit test issues.", "created": "2025-10-19T09:02:15.657+0000"}], "derived_tasks": {"summary": "[JDK-17] Fix UT Failures in hadoop common, hdfs, yarn - Most of the UT's failed with below exception:", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-18993", "title": "S3A: Add option fs.s3a.classloader.isolation (#6301)", "description": "In HADOOP-17372 the S3AFileSystem forces the configuration classloader to be the same as the one that loaded S3AFileSystem. This leads to the impossibility in Spark applications to load third party credentials providers as user jars. The option fs.s3a.classloader.isolation (default: true) can be set to false to disable s3a classloader isolation; This can assist in using custom credential providers and other extension points.", "status": "Resolved", "priority": "Minor", "reporter": "Antonio Murgia", "assignee": "Antonio Murgia", "created": "2023-11-27T21:56:10.000+0000", "updated": "2025-10-23T16:31:41.000+0000", "labels": ["pull-request-available"], "components": ["fs/s3"], "comments": [{"author": "ASF GitHub Bot", "body": "tmnd1991 opened a new pull request, #6301: URL: https://github.com/apache/hadoop/pull/6301 ### Description of PR In [HADOOP-17372](https://issues.apache.org/jira/browse/HADOOP-17372) the S3AFileSystem forces the configuration classloader to be the same as the one that loaded S3AFileSystem. This leads to the impossibility in Spark applications to load third party credentials providers as user jars. I propose to add a configuration key fs.s3a.extensions.isolated.classloader with a default value of true that if set to false will not perform the classloader set. ### How was this patch tested? Unit tests at `org.apache.hadoop.fs.s3a.TestS3AFileSystemIsolatedClassloader`. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-11-27T22:10:38.368+0000"}, {"author": "Antonio Murgia", "body": "[~stevel@apache.org]", "created": "2023-11-27T22:59:14.561+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1828852213 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 59s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 5 new + 3 unchanged - 0 fixed = 8 total (was 3) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 4s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | -1 :x: | asflicense | 0m 32s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 125m 9s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.TestS3AFileSystemIsolatedClassloader | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 367a1fc828e3 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 73a3f9cbcca19b4a918299c67e6747b0adbaafe0 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 558 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T00:17:02.792+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1828878097 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 7m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 24s | | trunk passed | | +1 :green_heart: | javadoc | 0m 16s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 6 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 9s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | -1 :x: | shadedclient | 26m 10s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 19s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +0 :ok: | asflicense | 0m 23s | | ASF License check generated no output? | | | | 103m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 4efc12bab4a4 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / af3b88cb27ac8402e4dc5c9c7f9959848d430665 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T00:42:48.510+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1828886667 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 30s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 3s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 9s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 41s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 23s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 5 new + 3 unchanged - 0 fixed = 8 total (was 3) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 47s | | hadoop-aws in the patch passed. | | -1 :x: | asflicense | 0m 35s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 125m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 4b8b3a25cd6e 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7c47e46a852d4f7cba69b261729bda7fd34755ed | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T00:51:47.977+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1829533760 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 4s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 26s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 49s | | hadoop-aws in the patch passed. | | -1 :x: | asflicense | 0m 34s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 123m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 5e3babe9b077 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a6943805aafec2261a0605c0157d0b4d05051aca | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T10:29:56.689+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1829549574 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 24s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 50s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 125m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 306aeafd9e8a 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7999b0f79009ac0c48ba8a99efa8baf1a6e125ab | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 675 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T10:38:57.455+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1407730449 ########## hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md: ########## @@ -561,6 +561,30 @@ obtain the credentials needed to access AWS services in the role the EC2 VM was deployed as. This AWS credential provider is enabled in S3A by default. +## Custom AWS Credential Providers and Apache Spark + +Apache Spark employs two class loaders, one that loads \"distribution\" (Spark + Hadoop) classes and one that +loads custom user classes. If the user wants to load custom implementations of AWS Credential Providers Review Comment: and signers", "created": "2023-11-28T13:07:16.557+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1407730449 ########## hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md: ########## @@ -561,6 +561,30 @@ obtain the credentials needed to access AWS services in the role the EC2 VM was deployed as. This AWS credential provider is enabled in S3A by default. +## Custom AWS Credential Providers and Apache Spark + +Apache Spark employs two class loaders, one that loads \"distribution\" (Spark + Hadoop) classes and one that +loads custom user classes. If the user wants to load custom implementations of AWS Credential Providers Review Comment: say, and custom signers, delegation token providers or any other dynamically loaded extension class (note trunk is AWS v2 sdk. you credential provider will work, but only if you restore the v1 sdk to the classpath)", "created": "2023-11-28T13:17:32.906+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1407732777 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { Review Comment: This instantiates an FS, so MUST be an integration test. * name starts with I * extends AbstractS3ATestBase ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -718,6 +718,18 @@ public void initialize(URI name, Configuration originalConf) } } + void isolateClassloader(Configuration conf, ClassLoader classLoader) { Review Comment: * add @VisibleForTesting, though if the test can go through `initialize()` then it can be kept private, which is better * javadoc ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); + conf.setBoolean(Constants.AWS_S3_EXTENSIONS_ISOLATED_CLASSLOADER, true); + fs.isolateClassloader(conf, classLoader); + Assert.assertTrue(conf.getClassLoader() instanceof CustomClassLoader); Review Comment: should it actually be equality, rather than instanceof ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1360,4 +1360,22 @@ private Constants() { * Value: {@value}. */ public static final boolean OPTIMIZED_COPY_FROM_LOCAL_DEFAULT = true; + + + /** + * To use the same classloader that loaded S3AFileSystem to al Review Comment: review this text as `al` seems a typo ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1360,4 +1360,22 @@ private Constants() { * Value: {@value}. */ public static final boolean OPTIMIZED_COPY_FROM_LOCAL_DEFAULT = true; + + + /** + * To use the same classloader that loaded S3AFileSystem to al + * load the user extensions, such as {{fs.s3a.aws.credentials.provider}}. + * It is useful to turn this off for Apache Spark applications that + * might load S3AFileSystem from the Spark distribution (Launcher classloader) + * while users might want to provide custom extensions (loaded by Spark MutableClassloader). + * Default value: true. + */ + public static final String AWS_S3_EXTENSIONS_ISOLATED_CLASSLOADER = Review Comment: I don't know if this is the right name, it's not really an extension (we use fs.s3a.ext for \"any plugin can make up a name and we won't do any in the fs) maybe `fs.s3a.classloader.isolation` ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); + conf.setBoolean(Constants.AWS_S3_EXTENSIONS_ISOLATED_CLASSLOADER, true); + fs.isolateClassloader(conf, classLoader); + Assert.assertTrue(conf.getClassLoader() instanceof CustomClassLoader); + } + } + + @Test + public void isolatedClasspathDefault() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); Review Comment: call fs.initialize(conf) then fs.getConfiguration() you can verify that the classloader isolation happens in the setup process ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); + conf.setBoolean(Constants.AWS_S3_EXTENSIONS_ISOLATED_CLASSLOADER, true); + fs.isolateClassloader(conf, classLoader); + Assert.assertTrue(conf.getClassLoader() instanceof CustomClassLoader); + } + } + + @Test + public void isolatedClasspathDefault() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); + fs.isolateClassloader(conf, classLoader); + Assert.assertTrue(conf.getClassLoader() instanceof CustomClassLoader); Review Comment: use AssertJ assert .isInstanceOf() ########## hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md: ########## @@ -561,6 +561,30 @@ obtain the credentials needed to access AWS services in the role the EC2 VM was deployed as. This AWS credential provider is enabled in S3A by default. +## Custom AWS Credential Providers and Apache Spark + +Apache Spark employs two class loaders, one that loads \"distribution\" (Spark + Hadoop) classes and one that +loads custom user classes. If the user wants to load custom implementations of AWS Credential Providers +through user provided jars will need to set the following configuration: + +```xml +<property> + <name>fs.s3a.extensions.isolated.classloader</name> + <value>false</value> +</property> +<property> + <name>fs.s3a.aws.credentials.provider</name> + <value>CustomCredentialsProvider</value> +</property> + +``` + +If the following property is not set or set to true, the following exception will be thrown: Review Comment: nit: put `true` in backticks ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); Review Comment: use `getConfiguration()` of and you'll get the set up config from the superclass", "created": "2023-11-28T13:20:48.738+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1829829581 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 30s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 5s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 18s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 52s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 122m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux f026a2dd436a 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 91d82f1dfe9c00eae0716b4004d0ba60008f1f37 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 700 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T13:21:12.826+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1830050277 Thanks for the review @steveloughran ! I addressed your concerns as part of latest commit f0ce436, hope I haven't missed anything", "created": "2023-11-28T15:15:07.421+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1830172322 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 49s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 7s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 12s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 25s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | compile | 0m 30s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 30s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 25s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 25s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | -1 :x: | mvnsite | 0m 27s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | -1 :x: | spotbugs | 0m 25s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | shadedclient | 34m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 29s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 121m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 4be8310e269f 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f0ce436fcb13845f78fa136bb62ab2617c73e49a | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 565 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T16:11:07.841+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1830653575 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 28s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 7s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 25s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | compile | 0m 31s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 31s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 25s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 25s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | -1 :x: | mvnsite | 0m 26s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | -1 :x: | spotbugs | 0m 24s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | shadedclient | 34m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 29s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 121m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux d72481fc7c1b 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 63028b58a8c3519bded25f766b6d272cdb570be0 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T20:16:27.459+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1830670304 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 26s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | -1 :x: | compile | 0m 32s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 32s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 25s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 25s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-aws in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | -1 :x: | mvnsite | 0m 27s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | -1 :x: | spotbugs | 0m 25s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | shadedclient | 34m 36s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 30s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch failed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 121m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 8ad73b8de70f 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5e4adecd87f2c38dfa4899e13356e0abe70eac98 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 745 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-28T20:27:13.536+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1830970923 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 7s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3) | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 51s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 122m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux adcac5283aab 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cc4684a103339edbbb75b2d1282b5ac0effafdcb | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 639 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-29T00:02:47.319+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1831433712 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 33m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 48s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 124m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 29257bed143d 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f8376928af7e7d06b457020550db9f9b5ed646cd | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 621 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-29T08:28:29.516+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1835627559 Hi @steveloughran , can you have a look at the PR? P.S. I don't know how CI/CD of Hadoop works, but I see a checkstyle failed for a file that is not (anymore) part of the changes. Is this normal? How can I fix it?", "created": "2023-12-01T07:46:30.654+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1412838038 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,85 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.lang.reflect.Constructor; +import java.lang.reflect.InvocationTargetException; + +import org.apache.hadoop.conf.Configuration; +import static org.assertj.core.api.Assertions.*; +import org.junit.Test; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader customClassLoader = new CustomClassLoader(); + + private <T> T createClass(ClassLoader clsLoader, Class<T> toLoad) { + try { + Class<?> cls = clsLoader.loadClass(toLoad.getCanonicalName()); + Constructor<?> constructor = cls.getConstructor(); + return (T) constructor.newInstance(); + } catch(ClassNotFoundException | InstantiationException | IllegalAccessException | + IllegalArgumentException | InvocationTargetException | NoSuchMethodException | + SecurityException e) { + throw new RuntimeException(e); + } + } + + @Test + public void isolatedClassloader() throws IOException { + try (S3AFileSystem fs = getFileSystem()) { Review Comment: this isn't going to work right. the fs is already initialized at that point; double initing is trouble (maybe we should reject it?). better to create a new S3AFileSystem() then initialize it ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1360,4 +1360,22 @@ private Constants() { * Value: {@value}. */ public static final boolean OPTIMIZED_COPY_FROM_LOCAL_DEFAULT = true; + + + /** + * To use the same classloader that loaded S3AFileSystem to load + * user extensions, such as {{fs.s3a.aws.credentials.provider}}. + * It is useful to turn this off for Apache Spark applications that + * might load S3AFileSystem from the Spark distribution (Launcher classloader) + * while users might want to provide custom extensions (loaded by Spark MutableClassloader). + * Default value: true. Review Comment: add a line stating the value, as IDEs show these ``` * Value: {@value} ``` ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1360,4 +1360,22 @@ private Constants() { * Value: {@value}. */ public static final boolean OPTIMIZED_COPY_FROM_LOCAL_DEFAULT = true; + + + /** + * To use the same classloader that loaded S3AFileSystem to load + * user extensions, such as {{fs.s3a.aws.credentials.provider}}. Review Comment: use {@code fs.s3a.aws.credentials.provider} for the formatting ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,85 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.lang.reflect.Constructor; +import java.lang.reflect.InvocationTargetException; + +import org.apache.hadoop.conf.Configuration; +import static org.assertj.core.api.Assertions.*; Review Comment: check the hadoop ordering rules here ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,85 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.lang.reflect.Constructor; +import java.lang.reflect.InvocationTargetException; + +import org.apache.hadoop.conf.Configuration; +import static org.assertj.core.api.Assertions.*; +import org.junit.Test; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader customClassLoader = new CustomClassLoader(); + + private <T> T createClass(ClassLoader clsLoader, Class<T> toLoad) { + try { + Class<?> cls = clsLoader.loadClass(toLoad.getCanonicalName()); + Constructor<?> constructor = cls.getConstructor(); + return (T) constructor.newInstance(); + } catch(ClassNotFoundException | InstantiationException | IllegalAccessException | + IllegalArgumentException | InvocationTargetException | NoSuchMethodException | + SecurityException e) { + throw new RuntimeException(e); + } + } + + @Test + public void isolatedClassloader() throws IOException { + try (S3AFileSystem fs = getFileSystem()) { + Configuration conf = createClass(customClassLoader, Configuration.class); + assertEquals(fs.getConf().getClassLoader(), customClassLoader); Review Comment: use AssertJ asserts. They are more detailed and avoid the bug your asserts here have, in that the expected vs actual parameters are in the wrong order -so the error text will be misleading. rather than fix up these asserts, just embrace AssertJ, which we are adopting across all new hadoop-aws test case. thanks", "created": "2023-12-02T16:54:18.311+0000"}, {"author": "Shilun Fan", "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.", "created": "2024-01-04T12:51:53.717+0000"}, {"author": "Shilun Fan", "body": "We don't need to set a fix version. We can only confirm the fix version once the PR is merged.", "created": "2024-01-16T08:13:27.321+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1898563769 any update on this?", "created": "2024-01-18T14:17:04.517+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1899944588 Hi @steveloughran, I found some issues on running integration tests against my s3 bucket, I needed to tweak some confs and still getting an unknown host exception. I will spend another couple of hours on this today probably and let you know :)", "created": "2024-01-19T08:09:25.414+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1458672773 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); + conf.setBoolean(Constants.AWS_S3_EXTENSIONS_ISOLATED_CLASSLOADER, true); + fs.isolateClassloader(conf, classLoader); + Assert.assertTrue(conf.getClassLoader() instanceof CustomClassLoader); + } + } + + @Test + public void isolatedClasspathDefault() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); Review Comment: I actually could not use `getConfiguration()` otherwise `Configuration` object would've been initialised in a way that made my tests fail, I hope the way I did it is still good enough", "created": "2024-01-19T09:42:28.224+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1458691419 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); Review Comment: I actually could not use getConfiguration() otherwise Configuration object would've been initialised in a way that made my tests fail, I hope the way I did it is still good enough ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,61 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import org.apache.hadoop.conf.Configuration; +import org.junit.Assert; +import org.junit.Test; + +public class TestS3AFileSystemIsolatedClassloader { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader classLoader = new CustomClassLoader(); + + @Test + public void isolatedClasspath() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); + conf.setBoolean(Constants.AWS_S3_EXTENSIONS_ISOLATED_CLASSLOADER, true); + fs.isolateClassloader(conf, classLoader); + Assert.assertTrue(conf.getClassLoader() instanceof CustomClassLoader); + } + } + + @Test + public void isolatedClasspathDefault() throws IOException { + try (S3AFileSystem fs = new S3AFileSystem()) { + Configuration conf = new Configuration(); Review Comment: I actually could not use `getConfiguration()` otherwise `Configuration` object would've been initialised in a way that made my tests fail, I hope the way I did it is still good enough", "created": "2024-01-19T09:54:25.401+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1900208017 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 21s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 23s | | trunk passed | | +1 :green_heart: | javadoc | 0m 16s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 41s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 3 unchanged - 0 fixed = 7 total (was 3) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 9s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 39s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 20s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 91m 55s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux ae2e5fb4359b 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fb821c11be1e7c0e2447c3f115c0a065ddd2590d | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 712 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-19T11:09:03.969+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1900264586 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 33m 3s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 24s | | trunk passed | | +1 :green_heart: | javadoc | 0m 16s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 16s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 3 unchanged - 0 fixed = 7 total (was 3) | | +1 :green_heart: | mvnsite | 0m 17s | | the patch passed | | +1 :green_heart: | javadoc | 0m 9s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 39s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 91m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 69d6807ab663 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / dd8d780f2723f3e96583656feedd470b5fb5957c | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 625 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-19T11:44:50.648+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1900429593 So finally I go the final \"push\" on this. I was able to verify my tests pass through running locally: `mvn -pl hadoop-tools/hadoop-aws verify -Dit.test=ITestS3AFileSystemIsolatedClassloader -Dtest=none` while running `mvn -pl hadoop-tools/hadoop-aws clean verify` lead to 40ish errors/failures that to my eyes look completely unrelated to this patch. I will wait for yetus results in order to see if I've left anything behind :) Maybe the last thing that I might do to improve this is to make it a unit test instead of an integration one, but given [your comment](https://github.com/apache/hadoop/pull/6301#discussion_r1407732777) I guess given the fact that instantiates and fs it must be an IT test and I can't think a way of testing this without instantiating the fs.", "created": "2024-01-19T13:29:50.524+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1900515636 bq. 40ish errors/failures that to my eyes look completely unrelated to this patch. that's a bit high...I'm seeing two continually reproducable, some mored in big parallel runs with an s3 store on a different continent and a VPN in the way: #6467 were you testing with a normal s3 store and the usual access/secret auth?", "created": "2024-01-19T14:26:30.199+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1900631924 These are the mvn results: ``` Tests run: 1315, Failures: 5, Errors: 85, Skipped: 295 ``` So only 5 failures: ``` [ERROR] Failures: [ERROR] ITestS3AClosedFS.testClosedInstrumentation:111 [S3AInstrumentation.hasMetricSystem()] expected:<[fals]e> but was:<[tru]e> [ERROR] ITestS3AConfiguration.testRequestTimeout:444 Configured fs.s3a.connection.request.timeout is different than what AWS sdk configuration uses internally expected:<120000> but was:<15000> [ERROR] ITestS3AConfiguration.testS3SpecificSignerOverride:574 Expected a java.io.IOException to be thrown, but got the result: : HeadBucketResponse(BucketRegion=eu-west-1, AccessPointAlias=false) [ERROR] ITestS3ACommitterFactory.testEverything:115->testInvalidFileBinding:165 Expected a org.apache.hadoop.fs.s3a.commit.PathCommitException to be thrown, but got the result: : FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202401190481_0001}; taskId=attempt_202401190481_0001_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@35401dbc}; outputPath=s3a://agile-hadoop-s3-test/test/testEverything, workPath=s3a://agile-hadoop-s3-test/test/testEverything/_temporary/1/_temporary/attempt_202401190481_0001_m_000000_0, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false} [ERROR] ITestS3AFileSystemStatistic.testBytesReadWithStream:72->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in number of FS bytes read by InputStreams expected:<2048> but was:<19619524> ``` yes, I'm testing from my laptop providing this `auth-keys.xml`: ![image](https://github.com/apache/hadoop/assets/7031242/cb5f610d-21f7-46df-a6eb-f93f454526a3) All the following error out during setup because of `java.lang.IllegalArgumentException: An endpoint cannot set when fs.s3a.endpoint.fips is true : https://s3.eu-west-1.amazonaws.com`: - ITestS3AAWSCredentialsProvider - ITestS3AFailureHandling - ITestS3APrefetchingCacheFiles - ITestDelegatedMRJob - ITestS3GuardTool - ITestS3Select - ITestS3SelectCLI - ITestS3SelectLandsat - ITestS3SelectMRJob While these error out for different reasons: - ITestS3ARequesterPays \u00bb lambda$testRequesterPaysDisabledFails$0:112 \u00bb AWSRedirect Received... - ITestMarkerTool \u00bb AWSRedirect - ITestS3ACannedACLs \u00bb AWSBadRequest - ITestSessionDelegationInFilesystem \u00bb AccessDenied - ITestAWSStatisticCollection \u00bb AccessDenied s3a://land... - ITestDelegatedMRJob \u00bb AccessDenied s3a://osm-pds/pla...", "created": "2024-01-19T15:30:07.847+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1900834397 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 58s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 54s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 59s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 123m 25s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 78930d1898ad 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2a77456121e06ed1c9709e0f48a8c93a68bb96b0 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 628 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-19T17:45:01.327+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1901473578 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 37s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | trunk passed | | +1 :green_heart: | shadedclient | 31m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 59s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 22s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 57s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 124m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 84cd21ad6b68 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7daaeef31790edbab348e703cd9ae17d42c7c846 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 696 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-20T00:50:28.063+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1914283535 Hi @steveloughran will you have any time to review this soon?", "created": "2024-01-29T09:23:33.962+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1919155443 @tmnd1991 if you saw my backlog of reviews and my own todo list, you'd know the answer to that.", "created": "2024-01-31T13:58:45.257+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1472864071 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,122 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.util.HashMap; +import java.util.Map; +import java.util.function.Consumer; + +import org.assertj.core.api.Assertions; +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader customClassLoader = new CustomClassLoader(); + + private S3AFileSystem createNewTestFs(Configuration conf) { + try { + S3AFileSystem fs = new S3AFileSystem(); + fs.initialize(getFileSystem().getUri(), conf); + return fs; + } catch (IOException e) { Review Comment: just have it throw the exception all the way up; no need to wrap ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,122 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.util.HashMap; +import java.util.Map; +import java.util.function.Consumer; + +import org.assertj.core.api.Assertions; +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { Review Comment: nit: javadocs to explian what it does ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,122 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.util.HashMap; +import java.util.Map; +import java.util.function.Consumer; + +import org.assertj.core.api.Assertions; +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + + public static class CustomClassLoader extends ClassLoader { Review Comment: should be private ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -764,6 +764,26 @@ public void initialize(URI name, Configuration originalConf) } } + /** + * If classloader isolation is {@code true} + * (through {@link Constants#AWS_S3_CLASSLOADER_ISOLATION}) or not + * explicitly set, then the classLoader of the input configuration object + * will be set to the input classloader, otherwise nothing will happen. + * @param conf configuration object. + * @param classLoader isolated classLoader. + */ + private void isolateClassloader(Configuration conf, ClassLoader classLoader) { Review Comment: move into S3AUtils, make static. I have a dream of S3AFs getting smaller -or at least no larger ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,122 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.util.HashMap; +import java.util.Map; +import java.util.function.Consumer; + +import org.assertj.core.api.Assertions; +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader customClassLoader = new CustomClassLoader(); + + private S3AFileSystem createNewTestFs(Configuration conf) { + try { + S3AFileSystem fs = new S3AFileSystem(); + fs.initialize(getFileSystem().getUri(), conf); + return fs; + } catch (IOException e) { + throw new RuntimeException(e); + } + } + + private void test(Map<String, String> confToSet, Consumer<FileSystem> asserts) Review Comment: * can you add a javadoc to explain what is happening * can you give it a slightly more explicit name, e.g assertInNewFilesystem() ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java: ########## @@ -0,0 +1,122 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import java.io.IOException; +import java.util.HashMap; +import java.util.Map; +import java.util.function.Consumer; + +import org.assertj.core.api.Assertions; +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; + +public class ITestS3AFileSystemIsolatedClassloader extends AbstractS3ATestBase { + + public static class CustomClassLoader extends ClassLoader { + } + + private final ClassLoader customClassLoader = new CustomClassLoader(); + + private S3AFileSystem createNewTestFs(Configuration conf) { + try { + S3AFileSystem fs = new S3AFileSystem(); + fs.initialize(getFileSystem().getUri(), conf); + return fs; + } catch (IOException e) { + throw new RuntimeException(e); + } + } + + private void test(Map<String, String> confToSet, Consumer<FileSystem> asserts) + throws IOException { + ClassLoader previousClassloader = Thread.currentThread().getContextClassLoader(); + try { + Thread.currentThread().setContextClassLoader(customClassLoader); + Configuration conf = new Configuration(); + Assertions.assertThat(conf.getClassLoader()).isEqualTo(customClassLoader); + S3ATestUtils.prepareTestConfiguration(conf); + for (Map.Entry<String, String> e : confToSet.entrySet()) { + conf.set(e.getKey(), e.getValue()); + } + try (S3AFileSystem fs = createNewTestFs(conf)) { + asserts.accept(fs); + } + } finally { + Thread.currentThread().setContextClassLoader(previousClassloader); + } + } + + private Map<String, String> mapOf() { + return new HashMap<>(); + } + + private Map<String, String> mapOf(String key, String value) { Review Comment: nice", "created": "2024-01-31T14:07:41.141+0000"}, {"author": "ASF GitHub Bot", "body": "agilelab-tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1919349632 thanks for the review @steveloughran. I addressed your concerns and rebased. Now running `mvn -pl hadoop-tools/hadoop-aws clean verify`. I'll let you know the result soon.", "created": "2024-01-31T15:33:17.139+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1919496989 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 1s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 31m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 18s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 19m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 11s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 36s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 18s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 81m 36s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 8e87691c21cb 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 285ede2ffed48449e13bf92c1195bba748629662 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 551 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-31T16:46:11.965+0000"}, {"author": "ASF GitHub Bot", "body": "agilelab-tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1919541155 This the result of verify, looks good to me: ``` [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestS3AClosedFS.testClosedInstrumentation:111 [S3AInstrumentation.hasMetricSystem()] expected:<[fals]e> but was:<[tru]e> [ERROR] ITestS3AConfiguration.testS3SpecificSignerOverride:582 Expected a java.io.IOException to be thrown, but got the result: : HeadBucketResponse(BucketRegion=eu-west-1, AccessPointAlias=false) [ERROR] ITestS3ACommitterFactory.testEverything:115->testInvalidFileBinding:165 Expected a org.apache.hadoop.fs.s3a.commit.PathCommitException to be thrown, but got the result: : FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202401310688_0001}; taskId=attempt_202401310688_0001_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2534ada2}; outputPath=s3a://agile-hadoop-s3-test/test/testEverything, workPath=s3a://agile-hadoop-s3-test/test/testEverything/_temporary/1/_temporary/attempt_202401310688_0001_m_000000_0, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false} [ERROR] ITestS3AFileSystemStatistic.testBytesReadWithStream:72->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in number of FS bytes read by InputStreams expected:<2048> but was:<19636738> [ERROR] Errors: [ERROR] ITestS3AAWSCredentialsProvider.testAnonymousProvider:184 \u00bb AWSRedirect Receive... [ERROR] ITestS3ACannedACLs>AbstractS3ATestBase.setup:111->AbstractFSContractTestBase.setup:205->AbstractFSContractTestBase.mkdirs:363 \u00bb AWSBadRequest [ERROR] ITestS3AFailureHandling.testMultiObjectDeleteNoPermissions:186->lambda$testMultiObjectDeleteNoPermissions$1:188 \u00bb S3 [ERROR] ITestS3AFailureHandling.testSingleObjectDeleteNoPermissionsTranslated:212->lambda$testSingleObjectDeleteNoPermissionsTranslated$2:213 \u00bb AWSRedirect [ERROR] ITestS3APrefetchingCacheFiles.testCacheFileExistence:111 \u00bb AWSRedirect Receive... [ERROR] ITestS3ARequesterPays.testRequesterPaysDisabledFails:108->lambda$testRequesterPaysDisabledFails$0:112 \u00bb AWSRedirect [ERROR] ITestS3ARequesterPays.testRequesterPaysOptionSuccess:72 \u00bb AWSRedirect Received... [ERROR] ITestDelegatedMRJob.testCommonCrawlLookup:234 \u00bb AccessDenied s3a://osm-pds/pla... [ERROR] ITestDelegatedMRJob.testCommonCrawlLookup:234 \u00bb AccessDenied s3a://osm-pds/pla... [ERROR] ITestDelegatedMRJob.testJobSubmissionCollectsTokens:281 \u00bb AccessDenied s3a://o... [ERROR] ITestDelegatedMRJob.testJobSubmissionCollectsTokens:281 \u00bb AccessDenied s3a://o... [ERROR] ITestSessionDelegationInFilesystem.testDelegatedFileSystem:347->readLandsatMetadata:614 \u00bb AccessDenied [ERROR] ITestS3GuardTool.testLandsatBucketRequireEncrypted:85->AbstractS3GuardToolTestBase.runToFailure:128->AbstractS3GuardToolTestBase.lambda$runToFailure$0:129 \u00bb AWSRedirect [ERROR] ITestS3GuardTool.testLandsatBucketRequireGuarded:68->AbstractS3GuardToolTestBase.runToFailure:128->AbstractS3GuardToolTestBase.lambda$runToFailure$0:129 \u00bb AWSRedirect [ERROR] ITestS3GuardTool.testLandsatBucketRequireUnencrypted:78->AbstractS3GuardToolTestBase.run:114 \u00bb AWSRedirect [ERROR] ITestS3GuardTool.testLandsatBucketUnguarded:61->AbstractS3GuardToolTestBase.run:114 \u00bb AWSRedirect [ERROR] ITestAWSStatisticCollection.testCommonCrawlStatistics:74 \u00bb AccessDenied s3a://... [ERROR] ITestAWSStatisticCollection.testLandsatStatistics:56 \u00bb AccessDenied s3a://land... [ERROR] ITestMarkerTool.testRunAuditManyObjectsInBucket:318->AbstractMarkerToolTest.runToFailure:274 \u00bb AWSRedirect [INFO] [ERROR] Tests run: 1277, Failures: 4, Errors: 19, Skipped: 295 ```", "created": "2024-01-31T17:09:10.003+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1919575320 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 5s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 38m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 57s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 140m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 16e53b757e43 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f9b61a96ad47eea921a777d4a544ccf6d0544803 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 676 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-31T17:29:17.531+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1919919797 ITestS3AClosedFS looks new. I'd have expected the others today (fix in progress) does it work on a test run on hadoop trunk without your pr", "created": "2024-01-31T20:40:34.323+0000"}, {"author": "ASF GitHub Bot", "body": "agilelab-tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1920123368 `ITestS3AClosedFS` runs well \"alone\" (i.e. `mvn -pl hadoop-tools/hadoop-aws verify -Dtest=none -Dit.test=ITestS3AClosedFS`) I will retry all together now and cross fingers. \ud83e\udd1e", "created": "2024-01-31T22:51:49.396+0000"}, {"author": "ASF GitHub Bot", "body": "agilelab-tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1921599332 So, I got time to check them. First: when I run it alone, the test passes, both on trunk (2f1718c36345736b93493e4e79fae766ea6d3233) and on my branch. Second: when I run all the tests it fails both on trunk and on my branch. Therefore I think I did not introduce the regression \ud83d\ude04", "created": "2024-02-01T15:31:43.620+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6301: URL: https://github.com/apache/hadoop/pull/6301#discussion_r1477078046 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1670,4 +1670,25 @@ public static String formatRange(long rangeStart, long rangeEnd) { return String.format(\"bytes=%d-%d\", rangeStart, rangeEnd); } + + /** + * If classloader isolation is {@code true} + * (through {@link Constants#AWS_S3_CLASSLOADER_ISOLATION}) or not + * explicitly set, then the classLoader of the input configuration object + * will be set to the input classloader, otherwise nothing will happen. + * @param conf configuration object. + * @param classLoader isolated classLoader. + */ + static void isolateClassloader(Configuration conf, ClassLoader classLoader) { Review Comment: let's rename this \"maybeIsolateClassloader\" to make clear it is conditional.", "created": "2024-02-03T15:00:50.956+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1925408806 Done \ud83d\udc4d", "created": "2024-02-03T17:36:55.277+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1925440736 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 18s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 57s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 7s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 55s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 134m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6301 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux c3b7978097ed 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f17aed06cca701deae8bf6103ca5f2813ea86d5f | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 551 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-02-03T19:52:43.266+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1927622965 I've looked at the closed fs test. It is verifying that threads are all gone so its potentially a sign of some thread leakage. Can you post the stack trace/output you are seeing? I don't see this as the cause, so will merge to trunk/branch-3.4; we can create a new JIRA with the new failure as it may need to be made more resilient as well as maybe improving reporting", "created": "2024-02-05T17:56:40.186+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #6301: URL: https://github.com/apache/hadoop/pull/6301", "created": "2024-02-05T17:59:38.816+0000"}, {"author": "Steve Loughran", "body": "[~tmnd91] : merged to trunk. Can you create a PR against branch-3.4 and rerun the tests, and then we can merge there and so target 3.4.1 as the release with this. thanks!", "created": "2024-02-05T18:04:31.083+0000"}, {"author": "ASF GitHub Bot", "body": "tmnd1991 commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1929251297 > I've looked at the closed fs test. It is verifying that threads are all gone so its potentially a sign of some thread leakage. Can you post the stack trace/output you are seeing? > > I don't see this as the cause, so will merge to trunk/branch-3.4; we can create a new JIRA with the new failure as it may need to be made more resilient as well as maybe improving reporting @steveloughran created the jira issue: https://issues.apache.org/jira/browse/HADOOP-19068", "created": "2024-02-06T10:48:45.387+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6301: URL: https://github.com/apache/hadoop/pull/6301#issuecomment-1932165399 thanks. low priority issue as it'll just be fs leakage somewhere. simplest fix would be to use closeAllForUGI() in setup to destroy any old ones around", "created": "2024-02-07T14:28:42.852+0000"}, {"author": "Brandon", "body": "Tried this with Hadoop 3.4.2. It appears the `fs.s3a.classloader.isolation` property does not have the desired effect, because the credentials providers are loaded using the classloader of S3AUtils class: [CredentialProviderListFactory#createAWSV2CredentialProvider|https://github.com/apache/hadoop/blob/4f9efcb3bc10d11d6c81457bc51e59dfe0791041/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/CredentialProviderListFactory.java#L299-L305] calls [S3AUtils#getInstanceFromReflection|https://github.com/apache/hadoop/blob/4f9efcb3bc10d11d6c81457bc51e59dfe0791041/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java#L642], which uses the classloader that loaded the S3AUtils class, not the classloader set on the Configuration", "created": "2025-10-22T06:00:22.377+0000"}, {"author": "Brandon", "body": "Created a follow-up ticket at HADOOP-19733", "created": "2025-10-22T19:57:17.292+0000"}], "derived_tasks": {"summary": "S3A: Add option fs.s3a.classloader.isolation (#6301) - In HADOOP-17372 the S3AFileSystem forces the configuration classloader to be the same as the...", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "HADOOP-18816", "title": "Rebuild Exceptions on Client side to get genuine exceptions", "description": "In current's RPC design, if Server sends an exception back, Client can only rebuild the exception according to the original exception's error message, if the exceptions has some fields containing important information, they will be discarded since we can not rebuild them based on message string easily. This ticket is to introduce a new interface for Exceptions which supports reconstructing. If Clients want to rebuild the exception, they can just implement the methods and the reconstruction will be done automatically. The interface uses String[] as parameter for simplicity.\u00a0 I thought of using Protobuf to store all the exceptions or fields, but the generacity can not be perfectlly met. So we need Client to support it by accepting \"String[]\" and transform the String to it's original type.", "status": "Open", "priority": "Major", "reporter": "Janus Chow", "assignee": "Janus Chow", "created": "2023-07-20T09:03:07.000+0000", "updated": "2025-10-19T00:24:49.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "symious opened a new pull request, #5863: URL: https://github.com/apache/hadoop/pull/5863 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR In current's RPC design, if Server sends an exception back, Client can only rebuild the exception according to the original exception's error message, if the exceptions has some fields containing important information, they will be discarded since we can not rebuild them based on message string easily. This ticket is to introduce a new interface for Exceptions which supports reconstructing. If Clients want to rebuild the exception, they can just implement the methods and the reconstruction will be done automatically. The interface uses String[] as parameter for simplicity. I thought of using Protobuf to store all the exceptions or fields, but the generacity can not be perfectlly met. So we need Client to support it by accepting \"String[]\" and transform the String to it's original type. JIRA: https://issues.apache.org/jira/browse/HADOOP-18816 ### How was this patch tested? unit test. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-07-20T09:16:24.413+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5863: URL: https://github.com/apache/hadoop/pull/5863#discussion_r1269222686 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RemoteException.java: ########## @@ -47,12 +50,23 @@ public RemoteException(String className, String msg) { * @param erCode may be null */ public RemoteException(String className, String msg, RpcErrorCodeProto erCode) { + this(className, msg, erCode, null); + } + + /** + * @param className wrapped exception, may be null Review Comment: nit: add a . here and in other javadocs, to keep the compilers happy ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestProtoBufRpc.java: ########## @@ -329,7 +331,30 @@ public void testProtoBufRandomException() throws Exception { .isEqualTo(RpcErrorCodeProto.ERROR_APPLICATION); } } - + + @Test (timeout=5000) + public void testProtoBufReconstructableException() throws Exception { + //No test with legacy + assumeFalse(testWithLegacy); + TestRpcService client = getClient(addr, conf); + + try { + client.error3(null, newEmptyRequest()); + } catch (ServiceException se) { + assertThat(se.getCause()).isInstanceOf(RemoteException.class); Review Comment: if you use the AssertJ assertThat() (which a lot of new code does) you can explictily use an .isInstanceOf() assert and get more detail on the error ########## hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto: ########## @@ -165,6 +165,7 @@ message RpcResponseHeaderProto { // The client should not interpret these bytes, but only // forward them to the router using RpcRequestHeaderProto.routerFederatedState. optional bytes routerFederatedState = 10; + optional ExceptionReconstructParamsProto exceptionReconstructParams = 11; Review Comment: add a comment above to ensure no confusion with the comment at L165 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/protocolPB/PBHelper.java: ########## @@ -134,4 +136,14 @@ public static FileStatusProto convert(FileStatus stat) throws IOException { return bld.build(); } + public static ExceptionReconstructParamsProto getReconstructParams(Throwable t) { + if (t instanceof ReconstructableException) { Review Comment: and instance of IOException? ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestReconstructableException.java: ########## @@ -0,0 +1,59 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.ipc; + +import java.io.IOException; + +/** + * Test class of ReconstructableException. + */ +public class TestReconstructableException extends IOException Review Comment: as all unit tests begin with Test, I'd prefer a different classname to aviod confusion ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestReconstructableException.java: ########## @@ -0,0 +1,59 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.ipc; + +import java.io.IOException; + +/** + * Test class of ReconstructableException. + */ +public class TestReconstructableException extends IOException + implements ReconstructableException<TestReconstructableException> { + + private String field1; + private String field2; Review Comment: make one final, to see what happens ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RemoteException.java: ########## @@ -47,12 +50,23 @@ public RemoteException(String className, String msg) { * @param erCode may be null */ public RemoteException(String className, String msg, RpcErrorCodeProto erCode) { + this(className, msg, erCode, null); + } + + /** + * @param className wrapped exception, may be null + * @param msg may be null + * @param erCode may be null + */ + public RemoteException(String className, String msg, RpcErrorCodeProto erCode, + ExceptionReconstructParamsProto paramsProto) { super(msg); this.className = className; if (erCode != null) Review Comment: good time to add {} here, or move to ? : instead. I know its old code, but it is adjacent to your work", "created": "2023-07-20T09:51:45.465+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on code in PR #5863: URL: https://github.com/apache/hadoop/pull/5863#discussion_r1269244864 ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestProtoBufRpc.java: ########## @@ -329,7 +331,30 @@ public void testProtoBufRandomException() throws Exception { .isEqualTo(RpcErrorCodeProto.ERROR_APPLICATION); } } - + + @Test (timeout=5000) + public void testProtoBufReconstructableException() throws Exception { + //No test with legacy + assumeFalse(testWithLegacy); + TestRpcService client = getClient(addr, conf); + + try { + client.error3(null, newEmptyRequest()); + } catch (ServiceException se) { + assertThat(se.getCause()).isInstanceOf(RemoteException.class); Review Comment: Updated the following assert, please have a look.", "created": "2023-07-20T10:09:12.298+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on code in PR #5863: URL: https://github.com/apache/hadoop/pull/5863#discussion_r1269245031 ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestReconstructableException.java: ########## @@ -0,0 +1,59 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.ipc; + +import java.io.IOException; + +/** + * Test class of ReconstructableException. + */ +public class TestReconstructableException extends IOException Review Comment: Updated the class name, please have a look.", "created": "2023-07-20T10:09:26.977+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on code in PR #5863: URL: https://github.com/apache/hadoop/pull/5863#discussion_r1269247088 ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestReconstructableException.java: ########## @@ -0,0 +1,59 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.ipc; + +import java.io.IOException; + +/** + * Test class of ReconstructableException. + */ +public class TestReconstructableException extends IOException + implements ReconstructableException<TestReconstructableException> { + + private String field1; + private String field2; Review Comment: I tried to make it final, but the seems not working, since a single string is needed to construct IOException, then the final field can not be initialized.", "created": "2023-07-20T10:11:12.178+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on code in PR #5863: URL: https://github.com/apache/hadoop/pull/5863#discussion_r1269247301 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/protocolPB/PBHelper.java: ########## @@ -134,4 +136,14 @@ public static FileStatusProto convert(FileStatus stat) throws IOException { return bld.build(); } + public static ExceptionReconstructParamsProto getReconstructParams(Throwable t) { + if (t instanceof ReconstructableException) { Review Comment: Updated, please have a look. ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RemoteException.java: ########## @@ -47,12 +50,23 @@ public RemoteException(String className, String msg) { * @param erCode may be null */ public RemoteException(String className, String msg, RpcErrorCodeProto erCode) { + this(className, msg, erCode, null); + } + + /** + * @param className wrapped exception, may be null Review Comment: Updated. ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RemoteException.java: ########## @@ -47,12 +50,23 @@ public RemoteException(String className, String msg) { * @param erCode may be null */ public RemoteException(String className, String msg, RpcErrorCodeProto erCode) { + this(className, msg, erCode, null); + } + + /** + * @param className wrapped exception, may be null + * @param msg may be null + * @param erCode may be null + */ + public RemoteException(String className, String msg, RpcErrorCodeProto erCode, + ExceptionReconstructParamsProto paramsProto) { super(msg); this.className = className; if (erCode != null) Review Comment: Updated.", "created": "2023-07-20T10:11:27.235+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on code in PR #5863: URL: https://github.com/apache/hadoop/pull/5863#discussion_r1269247655 ########## hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto: ########## @@ -165,6 +165,7 @@ message RpcResponseHeaderProto { // The client should not interpret these bytes, but only // forward them to the router using RpcRequestHeaderProto.routerFederatedState. optional bytes routerFederatedState = 10; + optional ExceptionReconstructParamsProto exceptionReconstructParams = 11; Review Comment: Added comment.", "created": "2023-07-20T10:11:47.281+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1643649586 @steveloughran Thank you for the review. Updated most of the comment, please have a look.", "created": "2023-07-20T10:11:55.559+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1643972707 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 49m 32s | | trunk passed | | +1 :green_heart: | compile | 18m 27s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 4s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 13s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 9s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 40m 35s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 17m 47s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 17m 47s | | the patch passed | | +1 :green_heart: | javac | 17m 47s | | the patch passed | | +1 :green_heart: | compile | 16m 53s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 16m 53s | | the patch passed | | +1 :green_heart: | javac | 16m 53s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 13s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 5 new + 240 unchanged - 2 fixed = 245 total (was 242) | | +1 :green_heart: | mvnsite | 1m 38s | | the patch passed | | -1 :x: | javadoc | 1m 8s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 2m 48s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | -1 :x: | shadedclient | 10m 15s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 33m 42s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 57s | | The patch does not generate ASF License warnings. | | | | 224m 19s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Unchecked/unconfirmed cast from java.io.IOException to org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructParams(Throwable) At PBHelper.java:org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructParams(Throwable) At PBHelper.java:[line 142] | | Failed junit tests | hadoop.ha.TestHealthMonitor | | | hadoop.ipc.TestMultipleProtocolServer | | | hadoop.ipc.TestRpcServerHandoff | | | hadoop.ipc.TestSaslRPC | | | hadoop.ha.TestHealthMonitorWithDedicatedHealthAddress | | | hadoop.ipc.TestProtoBufRPCCompatibility | | | hadoop.ipc.TestProtoBufRpc | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 457dde6f4dd2 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b4802bd24e6f3da9166f6e46dc8df6c559b6f3c3 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-20T13:54:33.468+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1643977249 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 5s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 52s | | trunk passed | | +1 :green_heart: | compile | 19m 31s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 45s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 40s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 5s | | trunk passed | | +1 :green_heart: | javadoc | 2m 22s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 50s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 34s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 31s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 55s | | the patch passed | | +1 :green_heart: | compile | 18m 16s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 18m 17s | | the patch passed | | +1 :green_heart: | javac | 18m 16s | | the patch passed | | +1 :green_heart: | compile | 18m 18s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 18m 18s | | the patch passed | | +1 :green_heart: | javac | 18m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 4m 23s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 4 new + 242 unchanged - 0 fixed = 246 total (was 242) | | +1 :green_heart: | mvnsite | 3m 6s | | the patch passed | | -1 :x: | javadoc | 1m 19s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | javadoc | 1m 55s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 59s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 36m 34s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 47s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 279m 17s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.ipc.TestProtoBufRpc | | | hadoop.ha.TestHealthMonitor | | | hadoop.ha.TestHealthMonitorWithDedicatedHealthAddress | | | hadoop.ipc.TestRpcServerHandoff | | | hadoop.ipc.TestProtoBufRPCCompatibility | | | hadoop.ipc.TestSaslRPC | | | hadoop.ipc.TestMultipleProtocolServer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux d39d904a7035 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 42569b32bcee1d1c50027214ee95e354476b31b1 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1307 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-20T13:57:14.594+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1643978793 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 34s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 49s | | trunk passed | | +1 :green_heart: | compile | 19m 28s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 46s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 8s | | trunk passed | | +1 :green_heart: | javadoc | 2m 30s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 52s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 36s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 53s | | the patch passed | | +1 :green_heart: | compile | 18m 33s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 18m 33s | | the patch passed | | +1 :green_heart: | javac | 18m 33s | | the patch passed | | +1 :green_heart: | compile | 18m 5s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 18m 5s | | the patch passed | | +1 :green_heart: | javac | 18m 5s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 4m 18s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 4 new + 242 unchanged - 0 fixed = 246 total (was 242) | | +1 :green_heart: | mvnsite | 2m 56s | | the patch passed | | -1 :x: | javadoc | 1m 14s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | javadoc | 1m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 5m 57s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 36m 33s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 47s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 277m 42s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.ipc.TestProtoBufRpc | | | hadoop.ha.TestHealthMonitor | | | hadoop.ha.TestHealthMonitorWithDedicatedHealthAddress | | | hadoop.ipc.TestRpcServerHandoff | | | hadoop.ipc.TestProtoBufRPCCompatibility | | | hadoop.ipc.TestSaslRPC | | | hadoop.ipc.TestMultipleProtocolServer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 316093f06a96 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 42569b32bcee1d1c50027214ee95e354476b31b1 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3150 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-20T13:58:07.530+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1644500065 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 27s | | trunk passed | | +1 :green_heart: | compile | 17m 22s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 16s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 47s | | trunk passed | | +1 :green_heart: | javadoc | 1m 23s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 57s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 21s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 35m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 59s | | the patch passed | | +1 :green_heart: | compile | 16m 30s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 16m 30s | | the patch passed | | +1 :green_heart: | javac | 16m 30s | | the patch passed | | +1 :green_heart: | compile | 16m 18s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 16m 18s | | the patch passed | | +1 :green_heart: | javac | 16m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 22s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 1 new + 236 unchanged - 6 fixed = 237 total (was 242) | | +1 :green_heart: | mvnsite | 1m 49s | | the patch passed | | +1 :green_heart: | javadoc | 1m 18s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 0s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 2m 48s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 36m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 29s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 11s | | The patch does not generate ASF License warnings. | | | | 226m 43s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Unchecked/unconfirmed cast from java.io.IOException to org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructParams(Throwable) At PBHelper.java:org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructParams(Throwable) At PBHelper.java:[line 143] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux dec971ccdf69 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8f83da4a1b1d55c7a10edc715e3ed09d44952248 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-20T19:42:57.757+0000"}, {"author": "ASF GitHub Bot", "body": "ferhui commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1644946546 @ZanderXu could you please take a look at this PR when you are available? Thanks", "created": "2023-07-21T03:53:24.166+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1645108620 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 5s | | trunk passed | | +1 :green_heart: | compile | 17m 26s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 18s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 46s | | trunk passed | | +1 :green_heart: | javadoc | 1m 25s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 59s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 20s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 35m 48s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 57s | | the patch passed | | +1 :green_heart: | compile | 16m 43s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 16m 43s | | the patch passed | | +1 :green_heart: | javac | 16m 43s | | the patch passed | | +1 :green_heart: | compile | 16m 6s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 16m 6s | | the patch passed | | +1 :green_heart: | javac | 16m 6s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 22s | | hadoop-common-project/hadoop-common: The patch generated 0 new + 236 unchanged - 6 fixed = 236 total (was 242) | | +1 :green_heart: | mvnsite | 1m 47s | | the patch passed | | +1 :green_heart: | javadoc | 1m 17s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 3m 13s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 40m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 19s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 11s | | The patch does not generate ASF License warnings. | | | | 228m 55s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Unchecked/unconfirmed cast from java.io.IOException to org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructParams(Throwable) At PBHelper.java:org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructParams(Throwable) At PBHelper.java:[line 144] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 6ca14feb20f8 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7d9bc8aa0438b55facef45feddb879d0211a37d8 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-21T07:24:11.569+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1645124570 @jojochuang Could you also help to review this PR when you are available?", "created": "2023-07-21T07:38:54.073+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1645435531 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 38s | | trunk passed | | +1 :green_heart: | compile | 19m 35s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 51s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 48s | | trunk passed | | +1 :green_heart: | javadoc | 1m 22s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 53s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 49s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 43s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 36m 9s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 18m 21s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 18m 21s | | the patch passed | | +1 :green_heart: | javac | 18m 22s | | the patch passed | | +1 :green_heart: | compile | 17m 46s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 17m 46s | | the patch passed | | +1 :green_heart: | javac | 17m 46s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 23s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 1 new + 236 unchanged - 6 fixed = 237 total (was 242) | | +1 :green_heart: | mvnsite | 1m 46s | | the patch passed | | +1 :green_heart: | javadoc | 1m 11s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 57s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 3m 4s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 36m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 33s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 234m 7s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Unchecked/unconfirmed cast from java.io.IOException to org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructProto(Throwable) At PBHelper.java:org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructProto(Throwable) At PBHelper.java:[line 148] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 6e5899ab2c02 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 365e43a75c8b2ee943d6675a712899cd37f9a57a | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-21T11:27:11.243+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1645438402 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 35s | | trunk passed | | +1 :green_heart: | compile | 19m 7s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 18m 7s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 47s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 58s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 37m 20s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 37m 48s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 18m 22s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 18m 22s | | the patch passed | | +1 :green_heart: | javac | 18m 22s | | the patch passed | | +1 :green_heart: | compile | 17m 39s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 17m 39s | | the patch passed | | +1 :green_heart: | javac | 17m 39s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 18s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 1 new + 236 unchanged - 6 fixed = 237 total (was 242) | | +1 :green_heart: | mvnsite | 1m 50s | | the patch passed | | +1 :green_heart: | javadoc | 1m 18s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 53s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 2m 57s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 36m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 34s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 12s | | The patch does not generate ASF License warnings. | | | | 234m 49s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Unchecked/unconfirmed cast from java.io.IOException to org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructProto(Throwable) At PBHelper.java:org.apache.hadoop.ipc.ReconstructableException in org.apache.hadoop.fs.protocolPB.PBHelper.getReconstructProto(Throwable) At PBHelper.java:[line 148] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 96a3fc6e21a3 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 365e43a75c8b2ee943d6675a712899cd37f9a57a | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2887 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-21T11:30:02.856+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1647593960 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 1s | | trunk passed | | +1 :green_heart: | compile | 17m 14s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 53s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 56s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 5s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 39m 33s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 59s | | the patch passed | | +1 :green_heart: | compile | 16m 41s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 16m 41s | | the patch passed | | +1 :green_heart: | javac | 16m 41s | | the patch passed | | +1 :green_heart: | compile | 16m 12s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 16m 12s | | the patch passed | | +1 :green_heart: | javac | 16m 12s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 21s | | hadoop-common-project/hadoop-common: The patch generated 0 new + 249 unchanged - 6 fixed = 249 total (was 255) | | +1 :green_heart: | mvnsite | 1m 45s | | the patch passed | | +1 :green_heart: | javadoc | 1m 16s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 0s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 16s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 9s | | The patch does not generate ASF License warnings. | | | | 231m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 2bfb52ed7eaf 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 79b91bab0065cae2460f2ed43d3dc3f1cb98edc2 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-24T09:56:52.306+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1647980381 This feature can lead to many use cases. For example, when returning a standbyException, the current active Namenode can be included in the error, eliminating the need for the client-side to traverse configuration namenodes.", "created": "2023-07-24T14:01:49.037+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1661528034 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +0 :ok: | buf | 0m 1s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 49s | | trunk passed | | +1 :green_heart: | compile | 17m 24s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 20s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 48s | | trunk passed | | +1 :green_heart: | javadoc | 1m 25s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 0s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 41s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 4s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 36m 32s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 57s | | the patch passed | | +1 :green_heart: | compile | 16m 32s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | cc | 16m 32s | | the patch passed | | +1 :green_heart: | javac | 16m 32s | | the patch passed | | +1 :green_heart: | compile | 16m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | cc | 16m 21s | | the patch passed | | +1 :green_heart: | javac | 16m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 21s | | hadoop-common-project/hadoop-common: The patch generated 0 new + 249 unchanged - 6 fixed = 249 total (was 255) | | +1 :green_heart: | mvnsite | 1m 49s | | the patch passed | | +1 :green_heart: | javadoc | 1m 20s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 0s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 49s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 15s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 14s | | The patch does not generate ASF License warnings. | | | | 224m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 5421522b5377 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b6b0b7f75f07c82c3985b47d3b91b60a485a67d9 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-02T05:42:32.988+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1665039781 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +0 :ok: | buf | 0m 0s | | buf was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 24s | | trunk passed | | +1 :green_heart: | compile | 17m 37s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 16m 0s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 1m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 51s | | trunk passed | | +1 :green_heart: | javadoc | 1m 23s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 1s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 2m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 36m 6s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 36m 34s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 56s | | the patch passed | | +1 :green_heart: | compile | 16m 37s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | cc | 16m 37s | | the patch passed | | +1 :green_heart: | javac | 16m 37s | | the patch passed | | +1 :green_heart: | compile | 16m 6s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | cc | 16m 6s | | the patch passed | | +1 :green_heart: | javac | 16m 6s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 23s | | hadoop-common-project/hadoop-common: The patch generated 0 new + 249 unchanged - 6 fixed = 249 total (was 255) | | +1 :green_heart: | mvnsite | 1m 47s | | the patch passed | | +1 :green_heart: | javadoc | 1m 18s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 1m 1s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 2m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 35m 56s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 18s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 12s | | The patch does not generate ASF License warnings. | | | | 238m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5863 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat | | uname | Linux 0145141209ef 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 64814889c8427099de9f3e73ba04fc9d5817b27d | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-04T05:38:10.278+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-1665122588 @ZanderXu PR updated, PTAL.", "created": "2023-08-04T07:14:11.738+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5863: URL: https://github.com/apache/hadoop/pull/5863#issuecomment-3413330781 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-17T00:22:39.657+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5863: HADOOP-18816. Rebuild exceptions on Client side URL: https://github.com/apache/hadoop/pull/5863", "created": "2025-10-19T00:24:49.552+0000"}], "derived_tasks": {"summary": "Rebuild Exceptions on Client side to get genuine exceptions - In current's RPC design, if Server sends an exception back, Client can only rebuild t...", "classifications": ["task"], "qa_pairs": []}}
{"id": "HADOOP-18788", "title": "Support LZ4 compressionLevel", "description": "The Hadoop's LZ4 compression codec now depends on lz4-java. There two type of compressor in lz4, `fastCompressor` and `highCompressor`. The default lz4 compressor in hadoop is fastCompressor, we also can use highCompressor by using config `IO_COMPRESSION_CODEC_LZ4_USELZ4HC_DEFAULT` When we want to use highCompressor in hadoop, we only can use the default compression level, which is level 9. while highCompressor in lz4-java supports compression level from 1 to 17. I think we can set a configuration to let users to choose different compresssionLevel for lz4 highCompressor.", "status": "Open", "priority": "Minor", "reporter": "wenweijian", "assignee": null, "created": "2023-06-29T17:35:40.000+0000", "updated": "2025-10-20T00:24:36.000+0000", "labels": ["pull-request-available"], "components": ["common"], "comments": [{"author": "ASF GitHub Bot", "body": "wenwj0 opened a new pull request, #5799: URL: https://github.com/apache/hadoop/pull/5799 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR The Hadoop's LZ4 compression codec now depends on lz4-java. There two type of compressor in lz4, `fastCompressor` and `highCompressor`. The default lz4 compressor in hadoop is fastCompressor, we also can use highCompressor by using config `IO_COMPRESSION_CODEC_LZ4_USELZ4HC_DEFAULT` When we want to use highCompressor in hadoop, we only can use the default compression level, which is level 9. while highCompressor in lz4-java supports compression level from 1 to 17. This PR add a configuration to let users to choose different compresssionLevel for lz4 highCompressor. ### How was this patch tested? UT ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-06-30T09:47:34.629+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5799: URL: https://github.com/apache/hadoop/pull/5799#issuecomment-1614683985 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 54s | | trunk passed | | +1 :green_heart: | compile | 18m 52s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 12s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 14s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 26s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 57s | | the patch passed | | +1 :green_heart: | compile | 17m 38s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 17m 38s | | the patch passed | | +1 :green_heart: | compile | 17m 2s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 17m 2s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 12s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 74 unchanged - 0 fixed = 76 total (was 74) | | +1 :green_heart: | mvnsite | 1m 38s | | the patch passed | | -1 :x: | javadoc | 1m 7s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | javadoc | 0m 50s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 46s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 0s | | The patch does not generate ASF License warnings. | | | | 241m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5799 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ed97422a48ac 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 33328235f0ac4f086ae359aa45805ce19edc6686 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-30T13:50:56.264+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5799: URL: https://github.com/apache/hadoop/pull/5799#issuecomment-3417558710 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-18T00:21:04.284+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5799: HADOOP-18788: Support lz4 highCompressor level. URL: https://github.com/apache/hadoop/pull/5799", "created": "2025-10-20T00:24:36.742+0000"}], "derived_tasks": {"summary": "Support LZ4 compressionLevel - The Hadoop's LZ4 compression codec now depends on lz4-java", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "HADOOP-18776", "title": "Add OptimizedS3AMagicCommitter For Zero Rename Commits to S3 Endpoints", "description": "The goal is to add a new S3A committer named *OptimizedS3AMagicCommitter* which is an another type of S3 Magic committer but with a better performance by taking in few tradeoffs. The following are the differences in MagicCommitter vs OptimizedMagicCommitter ||Operation||Magic Committer||*OptimizedS3AMagicCommitter*|| |commitTask\u00a0\u00a0\u00a0\u00a0|1. Lists all {{.pending}} files in its attempt directory. 2. The contents are loaded into a list of single pending uploads. 3. Saved to a {{.pendingset}} file in the job attempt directory.|1. Lists all {{.pending}} files in its attempt directory 2. The contents are loaded into a list of single pending uploads. 3. For each pending upload, commit operation is called (complete multiPartUpload)| |commitJob|1. Loads all {{.pendingset}} files in its job attempt directory 2. Then every pending commit in the job will be committed. 3. \"SUCCESS\" marker is created (if config is enabled) 4. \"__magic\" directory is cleaned up.|1. \"SUCCESS\" marker is created (if config is enabled) 2.\u00a0 \"__magic\" directory is cleaned up.| *Performance Benefits :-* # The primary performance boost due to distributed complete multiPartUpload call being made in the taskAttempts(Task containers/Executors) rather than a single job driver. In case of MagicCommitter it is O(files/threads). # It also saves a couple of S3 calls needed to PUT the \"{{{}.pendingset{}}}\" files and READ call to read them in the Job Driver. *TradeOffs :-* The tradeoffs are similar to the one in FileOutputCommitter V2 version. Users migrating from FileOutputCommitter V2 to OptimizedS3AMagicCommitter will no see behavioral change as such # During execution, intermediate data becomes visible after commitTask operation # On a failure, all output must be deleted and the job needs to be restarted. *Performance Benchmark :-* Cluster : c4.8x large (ec2-instance) Instance : 1 (primary) + 5 (core) Data Size : 3TB Partitioned(TPC-DS store_sales data) Engine\u00a0\u00a0\u00a0\u00a0 : Apache Spark 3.3.1 / Hadoop 3.3.3 Query: The following query inserts around 3000+ files into the table directory (ran for 3 iterations)  insert into <table> select ss_quantity from store_sales;  ||Committer||Iteration 1||Iteration 2||Iteration 3|| |Magic|126|127|122| |OptimizedMagic|50|51|58| So on an average, OptimizedMagicCommitter was *~2.3x* faster as compared to MagicCommitter. _*Note: Unlike MagicCommitter , OptimizedMagicCommitter is not suitable for all the cases where in user requires the guarantees of file not being visible in failure scenarios. Given the performance benefit, user can may choose to use this if they don't require any guarantees or have some mechanism to clean up the data before retrying.*_", "status": "Resolved", "priority": "Major", "reporter": "Syed Shameerur Rahman", "assignee": null, "created": "2023-06-19T08:45:38.000+0000", "updated": "2025-10-20T00:24:47.000+0000", "labels": ["pull-request-available"], "components": ["fs/s3"], "comments": [{"author": "ASF GitHub Bot", "body": "shameersss1 opened a new pull request, #5758: URL: https://github.com/apache/hadoop/pull/5758 This is a [WIP] PR. Raised the same to get validation from the community before making additional changes like (unit tests and integration tests). The goal is to add a new S3A committer named OptimizedS3AMagicCommitter which is an another type of S3 Magic committer but with a better performance by taking in few tradeoffs. Refer [HADOOP-18776](https://issues.apache.org/jira/browse/HADOOP-18776) for more information.", "created": "2023-06-19T08:58:09.167+0000"}, {"author": "Syed Shameerur Rahman", "body": "[~stevel@apache.org] , It would be great if you review the above PR. Note: It is a WIP PR (need to add unit tests and integration tests). I would like to get the communities though on this before taking this forward. Thanks", "created": "2023-06-19T09:01:44.138+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5758: URL: https://github.com/apache/hadoop/pull/5758#issuecomment-1596958653 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 36m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 39s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 34s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 44s | | The patch does not generate ASF License warnings. | | | | 110m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5758 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 9fc4d7719454 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f0e7d04b3dab4733f9994f91226797867a95063c | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-19T10:50:08.153+0000"}, {"author": "Steve Loughran", "body": "I understand what you've done, like the patch, but am going to have to block it. This is because, as you note, ,this has the same commit semantics as the v2 committer, and I consider the v2 committer *to be broken* see: https://github.com/steveloughran/zero-rename-committer/releases/tag/tag_release_2021-05-17 it lacks the ability to recover from task failure; people using v2 don't usually know/understand that, they just say \"oh look, faster, lets use\", switch to it, and then sometimes it goes wrong. rarely, but it does happen. I don't want to risk the same thing reoccuring on s3a. (note, v1 commit needs atomic dir rename so is broken on gcs; so there v2 committer use was allowed as \"they are both broken, this is faster\". Now we have the manifest committer in, gcs has an atomic commit too. Given you want to speed up s3a committer; are there things which could be done to the s3a committer to help it while still only manifesting in job commit? better parallism? there was a patch to improve the threadpool that I think has gone in, if not it should be reviewed and mergedd. Finally, I'd love to know size of jobs where you hit problems, use etc. If there's anything you can say publicly, that'd be great", "created": "2023-06-21T09:56:14.256+0000"}, {"author": "Syed Shameerur Rahman", "body": "[~stevel@apache.org] - Thanks a lot for taking a took at this. I fully understand your concerns. I am also aware of the same. > \"it lacks the ability to recover from task failure\" Yes this is true. When a task fails or the task JVM crashes in commitTask operation. Some files gets committed(visible) in the final path and some may not. If task re-attempts are enabled, A new task will come up and will write the files leading to duplicate(some) data in the final path. This issue can be solved by using this type of committer only for the use case where there is no task attempts and if any of the taskAttempts fails the job will also fail. This can still have files written by the failed taskAttempts in the final path but then since the job had failed, The user can clear off the data manually and re-run the same job. I guess the same issue is still possible with MagicS3ACommitter as well, Since commitJob is not atomic and if the Job Driver JVM crashes in commitJob operation it can also lead to some files being visible in the final path. > Finally, I'd love to know size of jobs where you hit problems, use etc. If there's anything you can say publicly, that'd be great My use case was, I had to write large number of files in a single query and since commitJob is single process(multi-threaded as opposed to distributed in the proposed use-case) which needs to call complete MPU for all these files it can become a bottleneck and hence explored other options\u00a0 ({*}~2.3x{*} faster as compared to MagicCommitter.) So my understanding is that when there is max 1 taskAttempt, This committer tend to behave similar (with same grantees) as MagicCommitter and hence can be used on specific use-cases.", "created": "2023-06-22T05:30:47.236+0000"}, {"author": "Steve Loughran", "body": "i see; and its not so much from the multi process that you get speedup as the fact that tasks commit as the job goes along; only the final task is on the critical path. there's also HADOOP-18757, which implies that we aren't threading enough in the magic committer. even there, with thousands of files you'd suffer. bq. This issue can be solved by using this type of committer only for the use case where there is no task attempts and if any of the taskAttempts fails the job will also fail that was the intent of the v2 committer, but too many people discovered a \"faster\" committer and switched to it without realising the flaw; at some point it even became the default. I don't want to enable people to make the same mistake again, sorry. so afraid i don't want this in. sorry. But if you do it externally we can add a link to it in the docs with the \"conditions needed for use\" section well covered. now, if spark was modified to recognise that timeout/failure meant job failure, then we could think about it as the failure would then be observable to all.", "created": "2023-07-06T16:34:21.010+0000"}, {"author": "Syed Shameerur Rahman", "body": "Thanks steve for your pointers. Sure, I will that will help. I am closing this Jira as \"won't fix\".", "created": "2023-07-13T05:37:00.276+0000"}, {"author": "Steve Loughran", "body": "thanks -sorry for being so strict here. will take an update/link in the docs, and even an appendix in the .tex file for the little paper which looks at all of this", "created": "2023-07-13T12:15:37.591+0000"}, {"author": "Steve Loughran", "body": "you know, one way this could be made to work would be if spark/mr could be configurable to fail fast on a timeout/failure in task attempt commit, rather than retry a different TA. now, we would need a way for a committer so say \"i am not recoverable\", and for that information to get into the commit process -this committer and the classic v2 one would both declare themselves as unrecoverable [~dongjoon] might have some suggestions there.", "created": "2023-07-13T16:52:23.702+0000"}, {"author": "Syed Shameerur Rahman", "body": "[~stevel@apache.org] - If i understood your comment, You are proposing something like even if this committer(which does complete mpu in commitTask) is enabled when task attempt retry is 1 then we are okay, If not there should be some mechanism to fail the job when we use this committer and \u00a0task attempt retry > 1 and the task which failed had called commitTask operation Am i correct?", "created": "2023-07-14T13:24:08.775+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5758: URL: https://github.com/apache/hadoop/pull/5758#issuecomment-3419090851 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-19T00:25:01.859+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5758: [WIP] HADOOP-18776: Add OptimizedS3AMagicCommitter For Zero Rename Commits to S3 Endpoints URL: https://github.com/apache/hadoop/pull/5758", "created": "2025-10-20T00:24:47.409+0000"}], "derived_tasks": {"summary": "Add OptimizedS3AMagicCommitter For Zero Rename Commits to S3 Endpoints - The goal is to add a new S3A committer named *OptimizedS3AMagicCommitter* ...", "classifications": ["feature", "new feature"], "qa_pairs": []}}
{"id": "HADOOP-18760", "title": "3.3.6 Release NOTICE and LICENSE file update", "description": "As far as I can tell looking at hadoop-project/pom.xml the only difference between 3.3.5 and 3.3.6 from a dependency point of view is mysql connector (HADOOP-18535) derby (HADOOP-18535, HADOOP-18693). Json-smart, snakeyaml and jetty, jettison are updated in LICENSE-binary already. grizzly was used in test scope only so its removal doesn't matter.", "status": "Open", "priority": "Blocker", "reporter": "Wei-Chiu Chuang", "assignee": "Wei-Chiu Chuang", "created": "2023-06-09T00:34:38.000+0000", "updated": "2025-10-20T00:24:49.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "Wei-Chiu Chuang", "body": "We can't use mysql connector as it's GPL. Either get rid of it or revert HADOOP-18535.", "created": "2023-06-09T00:59:12.860+0000"}, {"author": "Steve Loughran", "body": "we need to get an aws sdk update in too, because of a netty cve. I think that means that netty itself is going to need upgrading....", "created": "2023-06-09T10:53:22.316+0000"}, {"author": "Wei-Chiu Chuang", "body": "branch-3.3 is on netty 4.1.77 (trunk is on 4.1.89). To update we can cherrypick HADOOP-18646 (I'll take this up)", "created": "2023-06-09T17:46:51.647+0000"}, {"author": "ASF GitHub Bot", "body": "jojochuang opened a new pull request, #5740: URL: https://github.com/apache/hadoop/pull/5740 ### Description of PR Update Netty version in LICENSE-binary. AWS sdk, if we end up updating it, will include it in the next iteration. Mysql connector not included; this dependency will be removed by [HADOOP-18761](https://issues.apache.org/jira/browse/HADOOP-18761) ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-06-12T20:34:14.005+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5740: URL: https://github.com/apache/hadoop/pull/5740#issuecomment-1588144960 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | |||| _ branch-3.3 Compile Tests _ | | +1 :green_heart: | shadedclient | 35m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | shadedclient | 24m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | asflicense | 0m 45s | | The patch does not generate ASF License warnings. | | | | 65m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5740 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs | | uname | Linux 238c186c0144 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.3 / 94eb3913aa9b067af256ee2035c1d7c9b7d25351 | | Max. process+thread count | 687 (vs. ulimit of 5500) | | modules | C: . U: . | | Console output | [CI_URL] | | versions | git=2.17.1 maven=3.6.0 shellcheck=0.4.6 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-12T21:40:40.479+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5740: URL: https://github.com/apache/hadoop/pull/5740#issuecomment-3419090869 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-19T00:25:02.761+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5740: HADOOP-18760. 3.3.6 Release NOTICE and LICENSE file update. URL: https://github.com/apache/hadoop/pull/5740", "created": "2025-10-20T00:24:49.370+0000"}], "derived_tasks": {"summary": "3.3.6 Release NOTICE and LICENSE file update - As far as I can tell looking at hadoop-project/pom", "classifications": ["improvement", "task"], "qa_pairs": []}}
{"id": "HADOOP-18750", "title": "Spark History Server 3.3.1 fails to starts with Hadoop 3.3.x", "description": "When Spark History Server tries to start with Hadoop 3.3.4 (Happens only in Kerberos scenarios), it fails to do so with the following exception:  23/05/23 03:14:15 ERROR HistoryServer [main]: Failed to bind HistoryServer", "status": "Open", "priority": "Major", "reporter": "Aman Raj", "assignee": "Kamal Sharma", "created": "2023-05-24T04:20:55.000+0000", "updated": "2025-10-21T00:22:45.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "Aman Raj", "body": "Can anyone in the community please give me the contributor access to assign issue to myself.", "created": "2023-05-24T10:08:13.408+0000"}, {"author": "Rakesh Radhakrishnan", "body": "Done! Please raise PR if you have a proposal.", "created": "2023-05-24T10:11:51.504+0000"}, {"author": "ASF GitHub Bot", "body": "kamalsharma2 opened a new pull request, #5695: URL: https://github.com/apache/hadoop/pull/5695 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR This PR aims to resolve [HADOOP-18750](https://issues.apache.org/jira/browse/HADOOP-18750) ### How was this patch tested? Tested locally.", "created": "2023-05-29T09:25:38.641+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1566965939 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 33m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 24s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 56m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 2m 48s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 16s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 23m 35s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 17s | | hadoop-client-api in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 88m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5695 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux f7692c0f1ea3 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f2567a8946defd72fe4b822ec7c1f1ca6c0f93d2 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-api U: hadoop-client-modules/hadoop-client-api | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-29T10:55:44.647+0000"}, {"author": "ASF GitHub Bot", "body": "jojochuang commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1579770598 The changes breaks Hadoop client shading because the classpath prefix is not allowed in the shaded client jar. @sunchao any ideas why this is an issue with Hadoop 3.3.4...?", "created": "2023-06-07T02:26:12.199+0000"}, {"author": "ASF GitHub Bot", "body": "sunchao commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1581196632 @jojochuang @ayushtkn I think this has been an issue for a while (see https://issues.apache.org/jira/browse/SPARK-40964 for the same bug report) too, but just hasn't got a lot of attention. Long time back we also encountered a similar issue related to `javax.servlet` (see the discussions [here](https://issues.apache.org/jira/browse/SPARK-33212?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=17289618#comment-17289618)). In the end we found a workaround in Spark itself. > +2021-02-24 08:36:54,391 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.", "created": "2023-06-07T16:54:22.888+0000"}, {"author": "ASF GitHub Bot", "body": "amanraj2520 commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1581211764 +1 I think we can remove javax.servlet from relocation.", "created": "2023-06-07T17:04:43.812+0000"}, {"author": "ASF GitHub Bot", "body": "jojochuang commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1581228355 > In summary I think it'd be helpful to exclude javax.servlet from relocation. Ok. But still it breaks the build. I suspect we need to relax the allowed classpath prefix rule. @kamalsharma2 can you check if that's the case and update PR? It would be this script that need to update: https://github.com/apache/hadoop/blob/trunk/hadoop-client-modules/hadoop-client-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh (use HADOOP-17891 as reference: https://github.com/apache/hadoop/commit/b8f7c7527a7c33c204315a6ea615b4d3fd237744)", "created": "2023-06-07T17:17:59.385+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1583029528 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 20m 56s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 21m 56s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 50s | | trunk passed | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 43s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 23m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 3m 43s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 22m 16s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 17s | | hadoop-client-api in the patch passed. | | +1 :green_heart: | unit | 0m 16s | | hadoop-client-check-invariants in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 101m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5695 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 1c67920e37cf 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ec5e60d5426c2953dd48138af8879b46326f47db | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 560 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-api hadoop-client-modules/hadoop-client-check-invariants U: hadoop-client-modules | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-08T16:57:16.154+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1583191500 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 21m 25s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 21m 1s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 50s | | trunk passed | | +1 :green_heart: | javadoc | 0m 49s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 21m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 3m 1s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 20m 43s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 19s | | hadoop-client-api in the patch passed. | | +1 :green_heart: | unit | 0m 18s | | hadoop-client-check-invariants in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 96m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5695 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 53cf23effe2f 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ef9defadf710531709f2a7a060d7373b3f792b5f | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 728 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-api hadoop-client-modules/hadoop-client-check-invariants U: hadoop-client-modules | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-08T19:11:51.085+0000"}, {"author": "ASF GitHub Bot", "body": "jojochuang commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1583472070 I checked out the branch and ran the exact same command in precommit test. `mvn verify -fae --batch-mode -am -pl hadoop-client-modules/hadoop-client-check-invariants -pl hadoop-client-modules/hadoop-client-check-test-invariants -pl hadoop-client-modules/hadoop-client-integration-tests -Dtest=NoUnitTests -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true` The error is it cannot find the classpath: > [INFO] Running org.apache.hadoop.example.ITUseMiniCluster > [ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 19.938 s <<< FAILURE! - in org.apache.hadoop.example.ITUseMiniCluster > [ERROR] useWebHDFS(org.apache.hadoop.example.ITUseMiniCluster) Time elapsed: 19.632 s <<< ERROR! > java.lang.NoClassDefFoundError: javax/servlet/Servlet > at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:493) > at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:150) > at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:1064) > at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:875) > at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1130) > at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1105) > at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1879) > at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1397) > at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1166) > at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1039) > at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:971) > at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:591) > at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:530) > at org.apache.hadoop.example.ITUseMiniCluster.clusterUp(ITUseMiniCluster.java:77) > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) > at java.lang.reflect.Method.invoke(Method.java:498) > at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) > at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) > at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) > at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) > at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) > at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) > at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) > at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) > at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) > at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) > at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) > at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) > at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) > at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) > at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) > at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) > at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) > at org.junit.runners.ParentRunner.run(ParentRunner.java:413) > at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) > at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) > at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) > at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) > at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) > at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) > at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) > at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) > Caused by: java.lang.ClassNotFoundException: javax.servlet.Servlet > at java.net.URLClassLoader.findClass(URLClassLoader.java:387) > at java.lang.ClassLoader.loadClass(ClassLoader.java:418) > at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) > at java.lang.ClassLoader.loadClass(ClassLoader.java:351) > ... 44 more", "created": "2023-06-08T22:29:00.806+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1608885790 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 41s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 17m 33s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 21m 43s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 48s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 24m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 31s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 11m 54s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 1m 8s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 1m 2s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 4s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 24m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 19s | | hadoop-client-api in the patch passed. | | +1 :green_heart: | unit | 0m 19s | | hadoop-client-runtime in the patch passed. | | +1 :green_heart: | unit | 0m 18s | | hadoop-client-check-invariants in the patch passed. | | +1 :green_heart: | unit | 0m 19s | | hadoop-client-minicluster in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 111m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5695 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux ab1325972b65 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / df3b25828e6d0f0d11172a4bb0ccebcbf68f40c9 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 733 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-api hadoop-client-modules/hadoop-client-runtime hadoop-client-modules/hadoop-client-check-invariants hadoop-client-modules/hadoop-client-minicluster U: hadoop-client-modules | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-27T06:40:06.804+0000"}, {"author": "ASF GitHub Bot", "body": "kamalsharma2 commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1608904258 @jojochuang thanks for pointing out the issue. I've removed the shading of javax.servlet from other hadoop client modules too, can you please review it?", "created": "2023-06-27T06:57:07.088+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-1613632813 I am not sure about it, In my personal experience: shading without relocation often leads to strange issues if there are different versions of the same dependency is the classpath and this jar should be widely used. Does the solution pointed by @sunchao helps? or at worst if there is only few classes which creates issues, can we exclude just those classes from relocation? In any case, I am not sure about this. I will pass to other more experienced folks to take a call on this.", "created": "2023-06-29T18:49:59.310+0000"}, {"author": "Michael Smith", "body": "This is a rough problem: * Jetty [asserts|https://github.com/jetty/jetty.project/blob/jetty-9.4.58.v20250814/jetty-servlet/src/main/java/org/eclipse/jetty/servlet/FilterHolder.java#L99] that filters are assignable from javax.servlet.Filter. [Newer versions|https://github.com/jetty/jetty.project/blob/jetty-12.1.2/jetty-ee11/jetty-ee11-servlet/src/main/java/org/eclipse/jetty/ee11/servlet/FilterHolder.java] keep the assertion, but switch to jakarta.servlet.Filter. * Leaving javax.util.servlet unshaded causes problems for any project trying to use a different version or move to jakarta.servlet. That basically makes hadoop-runtime-client unusable in those cases.", "created": "2025-10-06T15:37:42.388+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5695: URL: https://github.com/apache/hadoop/pull/5695#issuecomment-3420104197 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-20T00:24:56.233+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5695: HADOOP-18750. Remove javax/servlet shading in hadoop-client-api URL: https://github.com/apache/hadoop/pull/5695", "created": "2025-10-21T00:22:45.192+0000"}], "derived_tasks": {"summary": "Spark History Server 3.3.1 fails to starts with Hadoop 3.3.x - When Spark History Server tries to start with Hadoop 3", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-18748", "title": "Configuration.get is slow", "description": "`Configuration.get` is slow mainly because of the overhead of `handleDeprecation` and eager creation of `overlay` even when null.", "status": "Open", "priority": "Major", "reporter": "Alkis Evlogimenos", "assignee": null, "created": "2023-05-22T13:11:08.000+0000", "updated": "2025-10-22T00:23:13.000+0000", "labels": ["pull-request-available"], "components": ["conf"], "comments": [{"author": "ASF GitHub Bot", "body": "alkis opened a new pull request, #5685: URL: https://github.com/apache/hadoop/pull/5685 ### Description of PR Optimize `Configuration.handleDeprecation` by adding a fastpath for when a property is not deprecated. While at it factor out property propagation so that it can be reused by both main and overlay properties. ### How was this patch tested? Existing tests. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-05-22T13:12:02.100+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1557524541 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 31s | | trunk passed | | +1 :green_heart: | compile | 16m 21s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 15m 8s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 28s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 1s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 52s | | the patch passed | | +1 :green_heart: | compile | 14m 57s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 14m 57s | | the patch passed | | +1 :green_heart: | compile | 14m 57s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 7s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 33s | | the patch passed | | +1 :green_heart: | javadoc | 1m 5s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 2m 40s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 23m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 1s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 185m 1s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Inconsistent synchronization of org.apache.hadoop.conf.Configuration.overlay; locked 60% of time Unsynchronized access at Configuration.java:60% of time Unsynchronized access at Configuration.java:[line 744] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 7b9924b03a27 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 980a042dd3ed176891c6ca12cba3b7d781570b53 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2997 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-22T16:18:24.733+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1557525853 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 2s | | trunk passed | | +1 :green_heart: | compile | 16m 38s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 15m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 10s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 40s | | trunk passed | | +1 :green_heart: | javadoc | 1m 9s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 15s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 52s | | the patch passed | | +1 :green_heart: | compile | 16m 12s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 16m 12s | | the patch passed | | +1 :green_heart: | compile | 15m 33s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 2s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 3 new + 108 unchanged - 0 fixed = 111 total (was 108) | | +1 :green_heart: | mvnsite | 1m 40s | | the patch passed | | +1 :green_heart: | javadoc | 1m 11s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 2m 45s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 22m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 1s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 0s | | The patch does not generate ASF License warnings. | | | | 181m 19s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Inconsistent synchronization of org.apache.hadoop.conf.Configuration.overlay; locked 75% of time Unsynchronized access at Configuration.java:75% of time Unsynchronized access at Configuration.java:[line 745] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 33fe50a049b7 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6db71fe43684c0e22c3de07a06aec35429a2fb6b | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-22T16:19:25.153+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1557526809 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 45s | | trunk passed | | +1 :green_heart: | compile | 16m 43s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 15m 23s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 14s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 37s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 36s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 51s | | the patch passed | | +1 :green_heart: | compile | 16m 9s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 16m 9s | | the patch passed | | +1 :green_heart: | compile | 15m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 9s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 3 new + 108 unchanged - 0 fixed = 111 total (was 108) | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 1m 12s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 2m 39s | [/new-spotbugs-hadoop-common-project_hadoop-common.html]([CI_URL] | hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 22m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 49s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 0s | | The patch does not generate ASF License warnings. | | | | 180m 38s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-common-project/hadoop-common | | | Inconsistent synchronization of org.apache.hadoop.conf.Configuration.overlay; locked 75% of time Unsynchronized access at Configuration.java:75% of time Unsynchronized access at Configuration.java:[line 745] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5d28f5b45641 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6db71fe43684c0e22c3de07a06aec35429a2fb6b | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-22T16:20:09.738+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1558808197 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 52s | | trunk passed | | +1 :green_heart: | compile | 17m 28s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 15m 46s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 6s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 33s | | trunk passed | | +1 :green_heart: | javadoc | 1m 10s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 50s | | the patch passed | | +1 :green_heart: | compile | 14m 55s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 14m 55s | | the patch passed | | +1 :green_heart: | compile | 14m 34s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 34s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 7s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 3 new + 108 unchanged - 0 fixed = 111 total (was 108) | | +1 :green_heart: | mvnsite | 1m 34s | | the patch passed | | +1 :green_heart: | javadoc | 1m 9s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 50s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 24s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 177m 5s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux bfed62771448 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f8a8bba7a0879a638d95109857c826f3edd71e50 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-23T08:35:45.622+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1558833168 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 19s | | trunk passed | | +1 :green_heart: | compile | 16m 23s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 19s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 57s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 40s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 18m 17s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 18m 17s | | the patch passed | | +1 :green_heart: | compile | 15m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 5s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 3 new + 108 unchanged - 0 fixed = 111 total (was 108) | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 1m 10s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 47s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 58s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 191m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f3d9a995e805 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f8a8bba7a0879a638d95109857c826f3edd71e50 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1500 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-23T08:49:08.125+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1558907685 you know, if we could speed up the basic .get() it can only be good. at the same time, this is such a broadly used piece of code, we are scared of doing anything risky to it. what can we do in terms of testing that all this is good? I'm thinking of concurrency as well as everything else?", "created": "2023-05-23T09:32:44.671+0000"}, {"author": "ASF GitHub Bot", "body": "alkis commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1558976068 > you know, if we could speed up the basic .get() it can only be good. at the same time, this is such a broadly used piece of code, we are scared of doing anything risky to it. > > what can we do in terms of testing that all this is good? I'm thinking of concurrency as well as everything else? That's a good point. I have specifically avoided a larger rewrite to reduce the risks in breaking the implementation. `Configuration` is used pervasively in most tests and there are specific tests for its basic and deprecated keys usages. In terms of concurrency I also avoided changing it to avoid risks - the concurrency is the same as before. Can you advice on how to apply this change in other branches as well such that older versions can receive the benefits?", "created": "2023-05-23T10:11:15.250+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1568232035 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 17s | | trunk passed | | +1 :green_heart: | compile | 18m 10s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 16s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 16s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 50s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 55s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 55s | | the patch passed | | +1 :green_heart: | compile | 17m 51s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 17m 51s | | the patch passed | | +1 :green_heart: | compile | 16m 37s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 10s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 3 new + 108 unchanged - 0 fixed = 111 total (was 108) | | +1 :green_heart: | mvnsite | 1m 28s | | the patch passed | | +1 :green_heart: | javadoc | 1m 5s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 36s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 31s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 192m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 336fa8421b14 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4ebb9dea3e0fb87a728d4825972cccf8d971bddf | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1782 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-30T11:02:59.117+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1571919004 regarding older versions, once in trunk it can be cherry picked into branch-3.3 to ship in 3.3.9; you'd do that by submitting a new PR against the branch with the patch cherrypicked from trunk to it...that lets yetus do its reviewing again. We wouldn't expect to do any code review at this point, just validate the backport worked", "created": "2023-06-01T11:59:42.013+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5685: URL: https://github.com/apache/hadoop/pull/5685#discussion_r1213046499 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -724,54 +724,41 @@ private String[] handleDeprecation(DeprecationContext deprecations, if (null != name) { name = name.trim(); } - // Initialize the return value with requested name - String[] names = new String[]{name}; - // Deprecated keys are logged once and an updated names are returned - DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); - if (keyInfo != null) { + + final DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); + final String[] names; + if (keyInfo == null) { + names = new String[]{name}; + // Handling deprecations is rare so bail out early for the common case. + if (deprecations.getReverseDeprecatedKeyMap().get(name) == null) { + return names; + } + } else { + names = keyInfo.newKeys; if (!keyInfo.getAndSetAccessed()) { logDeprecation(keyInfo.getWarningMessage(name)); } - // Override return value for deprecated keys - names = keyInfo.newKeys; } - // Update properties with deprecated key if already loaded and new - // deprecation has been added - updatePropertiesWithDeprecatedKeys(deprecations, names); - - // If there are no overlay values we can return early - Properties overlayProperties = getOverlay(); - if (overlayProperties.isEmpty()) { - return names; - } - // Update properties and overlays with reverse lookup values - for (String n : names) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(n); - if (deprecatedKey != null && !overlayProperties.containsKey(n)) { - String deprecatedValue = overlayProperties.getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(n, deprecatedValue); - overlayProperties.setProperty(n, deprecatedValue); - } - } + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), getProps()); + Properties overlay = getOverlay(); + if (!overlay.isEmpty()) { + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), overlay); } + return names; } - private void updatePropertiesWithDeprecatedKeys( - DeprecationContext deprecations, String[] newNames) { - for (String newName : newNames) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(newName); - if (deprecatedKey != null && !getProps().containsKey(newName)) { - String deprecatedValue = getProps().getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(newName, deprecatedValue); - } - } + private static void propagateProps(String[] keys, Map<String, String> map, Properties props) { Review Comment: needs javadocs and a name like \"propagateDeprecatedProperties\" would be more informative ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -724,54 +724,41 @@ private String[] handleDeprecation(DeprecationContext deprecations, if (null != name) { name = name.trim(); } - // Initialize the return value with requested name - String[] names = new String[]{name}; - // Deprecated keys are logged once and an updated names are returned - DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); - if (keyInfo != null) { + + final DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); + final String[] names; + if (keyInfo == null) { + names = new String[]{name}; + // Handling deprecations is rare so bail out early for the common case. + if (deprecations.getReverseDeprecatedKeyMap().get(name) == null) { + return names; + } + } else { + names = keyInfo.newKeys; if (!keyInfo.getAndSetAccessed()) { logDeprecation(keyInfo.getWarningMessage(name)); } - // Override return value for deprecated keys - names = keyInfo.newKeys; } - // Update properties with deprecated key if already loaded and new - // deprecation has been added - updatePropertiesWithDeprecatedKeys(deprecations, names); - - // If there are no overlay values we can return early - Properties overlayProperties = getOverlay(); - if (overlayProperties.isEmpty()) { - return names; - } - // Update properties and overlays with reverse lookup values - for (String n : names) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(n); - if (deprecatedKey != null && !overlayProperties.containsKey(n)) { - String deprecatedValue = overlayProperties.getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(n, deprecatedValue); - overlayProperties.setProperty(n, deprecatedValue); - } - } + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), getProps()); + Properties overlay = getOverlay(); + if (!overlay.isEmpty()) { + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), overlay); } + return names; } - private void updatePropertiesWithDeprecatedKeys( - DeprecationContext deprecations, String[] newNames) { - for (String newName : newNames) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(newName); - if (deprecatedKey != null && !getProps().containsKey(newName)) { - String deprecatedValue = getProps().getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(newName, deprecatedValue); - } - } + private static void propagateProps(String[] keys, Map<String, String> map, Properties props) { + for (String k : keys) { + String mk = map.get(k); + if (mk == null) continue; Review Comment: add braces", "created": "2023-06-01T12:06:12.979+0000"}, {"author": "ASF GitHub Bot", "body": "alkis commented on code in PR #5685: URL: https://github.com/apache/hadoop/pull/5685#discussion_r1213414808 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -724,54 +724,41 @@ private String[] handleDeprecation(DeprecationContext deprecations, if (null != name) { name = name.trim(); } - // Initialize the return value with requested name - String[] names = new String[]{name}; - // Deprecated keys are logged once and an updated names are returned - DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); - if (keyInfo != null) { + + final DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); + final String[] names; + if (keyInfo == null) { + names = new String[]{name}; + // Handling deprecations is rare so bail out early for the common case. + if (deprecations.getReverseDeprecatedKeyMap().get(name) == null) { + return names; + } + } else { + names = keyInfo.newKeys; if (!keyInfo.getAndSetAccessed()) { logDeprecation(keyInfo.getWarningMessage(name)); } - // Override return value for deprecated keys - names = keyInfo.newKeys; } - // Update properties with deprecated key if already loaded and new - // deprecation has been added - updatePropertiesWithDeprecatedKeys(deprecations, names); - - // If there are no overlay values we can return early - Properties overlayProperties = getOverlay(); - if (overlayProperties.isEmpty()) { - return names; - } - // Update properties and overlays with reverse lookup values - for (String n : names) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(n); - if (deprecatedKey != null && !overlayProperties.containsKey(n)) { - String deprecatedValue = overlayProperties.getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(n, deprecatedValue); - overlayProperties.setProperty(n, deprecatedValue); - } - } + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), getProps()); + Properties overlay = getOverlay(); + if (!overlay.isEmpty()) { + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), overlay); } + return names; } - private void updatePropertiesWithDeprecatedKeys( - DeprecationContext deprecations, String[] newNames) { - for (String newName : newNames) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(newName); - if (deprecatedKey != null && !getProps().containsKey(newName)) { - String deprecatedValue = getProps().getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(newName, deprecatedValue); - } - } + private static void propagateProps(String[] keys, Map<String, String> map, Properties props) { Review Comment: Added. I left the name as is because the function is not related to deprecated properties or not. ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -724,54 +724,41 @@ private String[] handleDeprecation(DeprecationContext deprecations, if (null != name) { name = name.trim(); } - // Initialize the return value with requested name - String[] names = new String[]{name}; - // Deprecated keys are logged once and an updated names are returned - DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); - if (keyInfo != null) { + + final DeprecatedKeyInfo keyInfo = deprecations.getDeprecatedKeyMap().get(name); + final String[] names; + if (keyInfo == null) { + names = new String[]{name}; + // Handling deprecations is rare so bail out early for the common case. + if (deprecations.getReverseDeprecatedKeyMap().get(name) == null) { + return names; + } + } else { + names = keyInfo.newKeys; if (!keyInfo.getAndSetAccessed()) { logDeprecation(keyInfo.getWarningMessage(name)); } - // Override return value for deprecated keys - names = keyInfo.newKeys; } - // Update properties with deprecated key if already loaded and new - // deprecation has been added - updatePropertiesWithDeprecatedKeys(deprecations, names); - - // If there are no overlay values we can return early - Properties overlayProperties = getOverlay(); - if (overlayProperties.isEmpty()) { - return names; - } - // Update properties and overlays with reverse lookup values - for (String n : names) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(n); - if (deprecatedKey != null && !overlayProperties.containsKey(n)) { - String deprecatedValue = overlayProperties.getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(n, deprecatedValue); - overlayProperties.setProperty(n, deprecatedValue); - } - } + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), getProps()); + Properties overlay = getOverlay(); + if (!overlay.isEmpty()) { + propagateProps(names, deprecations.getReverseDeprecatedKeyMap(), overlay); } + return names; } - private void updatePropertiesWithDeprecatedKeys( - DeprecationContext deprecations, String[] newNames) { - for (String newName : newNames) { - String deprecatedKey = deprecations.getReverseDeprecatedKeyMap().get(newName); - if (deprecatedKey != null && !getProps().containsKey(newName)) { - String deprecatedValue = getProps().getProperty(deprecatedKey); - if (deprecatedValue != null) { - getProps().setProperty(newName, deprecatedValue); - } - } + private static void propagateProps(String[] keys, Map<String, String> map, Properties props) { + for (String k : keys) { + String mk = map.get(k); + if (mk == null) continue; Review Comment: done", "created": "2023-06-01T16:34:57.162+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1572667584 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 24s | | trunk passed | | +1 :green_heart: | compile | 16m 9s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 14m 43s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 34s | | trunk passed | | +1 :green_heart: | javadoc | 1m 15s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 46s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 38s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 12s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 50s | | the patch passed | | +1 :green_heart: | compile | 16m 13s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 16m 13s | | the patch passed | | +1 :green_heart: | compile | 15m 26s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 6s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 1m 8s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 53s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 41s | | the patch passed | | -1 :x: | shadedclient | 22m 45s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 18m 58s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 182m 19s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.shell.TestCopyFromLocal | | | hadoop.io.file.tfile.TestVLong | | | hadoop.fs.impl.prefetch.TestBlockCache | | | hadoop.fs.permission.TestFsPermission | | | hadoop.fs.contract.rawlocal.TestRawlocalContractSetTimes | | | hadoop.fs.TestAvroFSInput | | | hadoop.conf.TestDeprecatedKeys | | | hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus | | | hadoop.security.alias.TestCredentialProviderFactory | | | hadoop.fs.shell.TestCpCommand | | | hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem | | | hadoop.fs.contract.rawlocal.TestRawlocalContractSeek | | | hadoop.fs.TestStat | | | hadoop.io.TestSequenceFileSync | | | hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays | | | hadoop.fs.contract.localfs.TestLocalFSContractOpen | | | hadoop.fs.shell.TestLs | | | hadoop.fs.TestGetFileBlockLocations | | | hadoop.fs.contract.sftp.TestSFTPContractSeek | | | hadoop.fs.contract.rawlocal.TestRawLocalContractVectoredRead | | | hadoop.fs.TestFsShellCopy | | | hadoop.fs.viewfs.TestFcPermissionsLocalFs | | | hadoop.fs.TestLocalDirAllocator | | | hadoop.fs.TestPath | | | hadoop.conf.TestConfiguration | | | hadoop.fs.TestLocalFSFileContextMainOperations | | | hadoop.fs.viewfs.TestChRootedFs | | | hadoop.io.file.tfile.TestTFileByteArrays | | | hadoop.conf.TestReconfiguration | | | hadoop.io.file.tfile.TestTFileComparators | | | hadoop.fs.TestFileSystemCanonicalization | | | hadoop.io.file.tfile.TestTFile | | | hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics | | | hadoop.fs.TestFcLocalFsPermission | | | hadoop.fs.contract.localfs.TestLocalFSContractMkdir | | | hadoop.fs.TestLocalFSCopyFromLocal | | | hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem | | | hadoop.fs.TestLocalFSFileContextCreateMkdir | | | hadoop.fs.shell.find.TestFind | | | hadoop.fs.contract.rawlocal.TestRawlocalContractRename | | | hadoop.io.TestSequenceFileSerialization | | | hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig | | | hadoop.fs.TestChecksumFileSystem | | | hadoop.fs.contract.rawlocal.TestRawlocalContractCreate | | | hadoop.conf.TestConfigurationDeprecation | | | hadoop.util.TestGenericOptionsParser | | | hadoop.fs.viewfs.TestChRootedFileSystem | | | hadoop.fs.TestHarFileSystemBasics | | | hadoop.fs.TestLocalFsFCStatistics | | | hadoop.fs.TestFileSystemCaching | | | hadoop.fs.TestFileContextResolveAfs | | | hadoop.security.TestUserGroupInformation | | | hadoop.fs.shell.TestCopy | | | hadoop.io.TestSequenceFile | | | hadoop.fs.viewfs.TestFcMainOperationsLocalFs | | | hadoop.fs.store.TestDataBlocks | | | hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays | | | hadoop.net.TestSwitchMapping | | | hadoop.io.file.tfile.TestTFileSeek | | | hadoop.security.alias.TestCredShell | | | hadoop.fs.ftp.TestFTPFileSystem | | | hadoop.fs.TestListFiles | | | hadoop.fs.TestRawLocalFileSystemContract | | | hadoop.net.TestScriptBasedMapping | | | hadoop.io.file.tfile.TestTFileComparator2 | | | hadoop.fs.contract.localfs.TestLocalFSContractContentSummary | | | hadoop.util.TestJsonSerialization | | | hadoop.io.TestSetFile | | | hadoop.fs.TestLocalFileSystemPermission | | | hadoop.io.TestSequenceFileAppend | | | hadoop.fs.TestSymlinkLocalFSFileSystem | | | hadoop.fs.TestFileUtil | | | hadoop.security.TestLdapGroupsMapping | | | hadoop.fs.shell.find.TestPrint0 | | | hadoop.io.nativeio.TestNativeIO | | | hadoop.fs.TestFsShellReturnCode | | | hadoop.fs.contract.localfs.TestLocalFSContractCreate | | | hadoop.fs.TestTruncatedInputBug | | | hadoop.fs.TestFsShellList | | | hadoop.fs.TestSymlinkLocalFSFileContext | | | hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir | | | hadoop.fs.TestTrash | | | hadoop.fs.shell.TestAclCommands | | | hadoop.security.TestLdapGroupsMappingWithBindUserSwitch | | | hadoop.io.file.tfile.TestTFileSplit | | | hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem | | | hadoop.security.token.TestDtUtilShell | | | hadoop.fs.shell.find.TestPrint | | | hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays | | | hadoop.fs.TestDefaultUri | | | hadoop.fs.contract.localfs.TestLocalFSContractVectoredRead | | | hadoop.fs.contract.localfs.TestLocalFSContractLoaded | | | hadoop.fs.TestFsShellTouch | | | hadoop.fs.shell.TestCopyPreserveFlag | | | hadoop.fs.viewfs.TestViewFsOverloadSchemeListStatus | | | hadoop.fs.contract.localfs.TestLocalFSContractSeek | | | hadoop.fs.TestFileContext | | | hadoop.fs.contract.rawlocal.TestRawlocalContractDelete | | | hadoop.io.TestMapFile | | | hadoop.fs.shell.TestPathData | | | hadoop.fs.shell.TestFsShellConcat | | | hadoop.net.TestScriptBasedMappingWithDependency | | | hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus | | | hadoop.fs.contract.localfs.TestLocalFSContractDelete | | | hadoop.crypto.key.TestKeyShell | | | hadoop.fs.contract.rawlocal.TestRawlocalContractOpen | | | hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs | | | hadoop.net.TestStaticMapping | | | hadoop.fs.TestFileContextDeleteOnExit | | | hadoop.fs.sftp.TestSFTPFileSystem | | | hadoop.fs.shell.TestCopyToLocal | | | hadoop.fs.shell.TestTextCommand | | | hadoop.io.TestBloomMapFile | | | hadoop.io.compress.TestBZip2Codec | | | hadoop.fs.contract.rawlocal.TestRawlocalContractPathHandle | | | hadoop.fs.shell.find.TestName | | | hadoop.fs.viewfs.TestViewFsLocalFs | | | hadoop.fs.contract.localfs.TestLocalFSContractSetTimes | | | hadoop.fs.TestFcLocalFsUtil | | | hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem | | | hadoop.io.compress.lz4.TestLz4CompressorDecompressor | | | hadoop.io.file.tfile.TestTFileUnsortedByteArrays | | | hadoop.fs.viewfs.TestFcCreateMkdirLocalFs | | | hadoop.fs.viewfs.TestHCFSMountTableConfigLoader | | | hadoop.io.file.tfile.TestTFileSeqFileComparison | | | hadoop.fs.TestFSMainOperationsLocalFileSystem | | | hadoop.fs.contract.rawlocal.TestRawlocalContractAppend | | | hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal | | | hadoop.io.compress.TestCodec | | | hadoop.fs.contract.localfs.TestLocalFSContractRename | | | hadoop.fs.viewfs.TestViewFsTrash | | | hadoop.io.file.tfile.TestTFileNoneCodecsStreams | | | hadoop.fs.shell.find.TestIname | | | hadoop.io.TestArrayFile | | | hadoop.io.file.tfile.TestTFileStreams | | | hadoop.fs.contract.localfs.TestLocalFSContractAppend | | | hadoop.fs.TestLocalFileSystem | | | hadoop.security.ssl.TestSSLFactory | | | hadoop.fs.TestChecksumFs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0037820e4654 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 601c265a372bf92f3d3ea3d1f5aed862b31151b6 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1981 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-01T19:39:27.433+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1580458734 1. everything broke. As I warned, Configuration is sensitive piece of code. 2. can you not rebase and force push unless you absolutely have to, as it complicates reviewing. thanks.", "created": "2023-06-07T10:29:44.280+0000"}, {"author": "ASF GitHub Bot", "body": "alkis commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1580679069 > 1. everything broke. As I warned, Configuration is sensitive piece of code. I am not sure the breakages are related. The error message is: ``` Unable to parse configuration fs.permissions.umask-mode with value dfs.umaskmode as octal or symbolic umask. ``` and this change does not have any effect on parsing. Do the tests work at HEAD? How can I trigger them? > 2. can you not rebase and force push unless you absolutely have to, as it complicates reviewing. thanks. Sure. I will merge instead if that's more convenient.", "created": "2023-06-07T12:10:20.296+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581049184 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 23s | | trunk passed | | +1 :green_heart: | compile | 16m 24s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 15m 8s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 32s | | trunk passed | | +1 :green_heart: | javadoc | 1m 22s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 56s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 52s | | the patch passed | | +1 :green_heart: | compile | 15m 42s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 15m 42s | | the patch passed | | +1 :green_heart: | compile | 14m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 52s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 6s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 32s | | the patch passed | | +1 :green_heart: | javadoc | 1m 8s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | -1 :x: | shadedclient | 22m 22s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 18m 44s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 6s | | The patch does not generate ASF License warnings. | | | | 188m 18s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.shell.TestCopyFromLocal | | | hadoop.io.file.tfile.TestVLong | | | hadoop.fs.impl.prefetch.TestBlockCache | | | hadoop.fs.permission.TestFsPermission | | | hadoop.fs.contract.rawlocal.TestRawlocalContractSetTimes | | | hadoop.fs.TestAvroFSInput | | | hadoop.conf.TestDeprecatedKeys | | | hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus | | | hadoop.security.alias.TestCredentialProviderFactory | | | hadoop.fs.shell.TestCpCommand | | | hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem | | | hadoop.fs.contract.rawlocal.TestRawlocalContractSeek | | | hadoop.fs.TestStat | | | hadoop.io.TestSequenceFileSync | | | hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays | | | hadoop.fs.contract.localfs.TestLocalFSContractOpen | | | hadoop.fs.shell.TestLs | | | hadoop.fs.TestGetFileBlockLocations | | | hadoop.fs.contract.sftp.TestSFTPContractSeek | | | hadoop.fs.contract.rawlocal.TestRawLocalContractVectoredRead | | | hadoop.fs.TestFsShellCopy | | | hadoop.fs.viewfs.TestFcPermissionsLocalFs | | | hadoop.fs.TestLocalDirAllocator | | | hadoop.fs.TestPath | | | hadoop.conf.TestConfiguration | | | hadoop.fs.TestLocalFSFileContextMainOperations | | | hadoop.fs.viewfs.TestChRootedFs | | | hadoop.io.file.tfile.TestTFileByteArrays | | | hadoop.conf.TestReconfiguration | | | hadoop.io.file.tfile.TestTFileComparators | | | hadoop.fs.TestFileSystemCanonicalization | | | hadoop.io.file.tfile.TestTFile | | | hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics | | | hadoop.fs.TestFcLocalFsPermission | | | hadoop.fs.contract.localfs.TestLocalFSContractMkdir | | | hadoop.fs.TestLocalFSCopyFromLocal | | | hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem | | | hadoop.fs.TestLocalFSFileContextCreateMkdir | | | hadoop.fs.shell.find.TestFind | | | hadoop.fs.contract.rawlocal.TestRawlocalContractRename | | | hadoop.io.TestSequenceFileSerialization | | | hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig | | | hadoop.fs.TestChecksumFileSystem | | | hadoop.fs.contract.rawlocal.TestRawlocalContractCreate | | | hadoop.conf.TestConfigurationDeprecation | | | hadoop.util.TestGenericOptionsParser | | | hadoop.fs.viewfs.TestChRootedFileSystem | | | hadoop.fs.TestHarFileSystemBasics | | | hadoop.fs.TestLocalFsFCStatistics | | | hadoop.fs.TestFileSystemCaching | | | hadoop.fs.TestFileContextResolveAfs | | | hadoop.security.TestUserGroupInformation | | | hadoop.fs.shell.TestCopy | | | hadoop.io.TestSequenceFile | | | hadoop.fs.viewfs.TestFcMainOperationsLocalFs | | | hadoop.fs.store.TestDataBlocks | | | hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays | | | hadoop.net.TestSwitchMapping | | | hadoop.io.file.tfile.TestTFileSeek | | | hadoop.security.alias.TestCredShell | | | hadoop.fs.ftp.TestFTPFileSystem | | | hadoop.fs.TestListFiles | | | hadoop.fs.TestRawLocalFileSystemContract | | | hadoop.net.TestScriptBasedMapping | | | hadoop.io.file.tfile.TestTFileComparator2 | | | hadoop.fs.contract.localfs.TestLocalFSContractContentSummary | | | hadoop.util.TestJsonSerialization | | | hadoop.io.TestSetFile | | | hadoop.fs.TestLocalFileSystemPermission | | | hadoop.io.TestSequenceFileAppend | | | hadoop.fs.TestSymlinkLocalFSFileSystem | | | hadoop.fs.TestFileUtil | | | hadoop.security.TestLdapGroupsMapping | | | hadoop.fs.shell.find.TestPrint0 | | | hadoop.io.nativeio.TestNativeIO | | | hadoop.fs.TestFsShellReturnCode | | | hadoop.fs.contract.localfs.TestLocalFSContractCreate | | | hadoop.fs.TestTruncatedInputBug | | | hadoop.fs.TestFsShellList | | | hadoop.fs.TestSymlinkLocalFSFileContext | | | hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir | | | hadoop.fs.TestTrash | | | hadoop.fs.shell.TestAclCommands | | | hadoop.security.TestLdapGroupsMappingWithBindUserSwitch | | | hadoop.io.file.tfile.TestTFileSplit | | | hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem | | | hadoop.security.token.TestDtUtilShell | | | hadoop.fs.shell.find.TestPrint | | | hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays | | | hadoop.fs.TestDefaultUri | | | hadoop.fs.contract.localfs.TestLocalFSContractVectoredRead | | | hadoop.fs.contract.localfs.TestLocalFSContractLoaded | | | hadoop.fs.TestFsShellTouch | | | hadoop.fs.shell.TestCopyPreserveFlag | | | hadoop.fs.viewfs.TestViewFsOverloadSchemeListStatus | | | hadoop.fs.contract.localfs.TestLocalFSContractSeek | | | hadoop.fs.TestFileContext | | | hadoop.fs.contract.rawlocal.TestRawlocalContractDelete | | | hadoop.io.TestMapFile | | | hadoop.fs.shell.TestPathData | | | hadoop.fs.shell.TestFsShellConcat | | | hadoop.net.TestScriptBasedMappingWithDependency | | | hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus | | | hadoop.fs.contract.localfs.TestLocalFSContractDelete | | | hadoop.crypto.key.TestKeyShell | | | hadoop.fs.contract.rawlocal.TestRawlocalContractOpen | | | hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs | | | hadoop.net.TestStaticMapping | | | hadoop.fs.TestFileContextDeleteOnExit | | | hadoop.fs.sftp.TestSFTPFileSystem | | | hadoop.fs.shell.TestCopyToLocal | | | hadoop.fs.shell.TestTextCommand | | | hadoop.io.TestBloomMapFile | | | hadoop.io.compress.TestBZip2Codec | | | hadoop.fs.contract.rawlocal.TestRawlocalContractPathHandle | | | hadoop.fs.shell.find.TestName | | | hadoop.fs.viewfs.TestViewFsLocalFs | | | hadoop.fs.contract.localfs.TestLocalFSContractSetTimes | | | hadoop.fs.TestFcLocalFsUtil | | | hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem | | | hadoop.io.compress.lz4.TestLz4CompressorDecompressor | | | hadoop.io.file.tfile.TestTFileUnsortedByteArrays | | | hadoop.fs.viewfs.TestFcCreateMkdirLocalFs | | | hadoop.fs.viewfs.TestHCFSMountTableConfigLoader | | | hadoop.io.file.tfile.TestTFileSeqFileComparison | | | hadoop.fs.TestFSMainOperationsLocalFileSystem | | | hadoop.fs.contract.rawlocal.TestRawlocalContractAppend | | | hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal | | | hadoop.io.compress.TestCodec | | | hadoop.fs.contract.localfs.TestLocalFSContractRename | | | hadoop.fs.viewfs.TestViewFsTrash | | | hadoop.io.file.tfile.TestTFileNoneCodecsStreams | | | hadoop.fs.shell.find.TestIname | | | hadoop.io.TestArrayFile | | | hadoop.io.file.tfile.TestTFileStreams | | | hadoop.fs.contract.localfs.TestLocalFSContractAppend | | | hadoop.fs.TestLocalFileSystem | | | hadoop.security.ssl.TestSSLFactory | | | hadoop.fs.TestChecksumFs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 15575c617a8b 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cd6b6c5d9b7403e8804d58f7ca656c8559dc2260 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1488 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-07T15:21:27.199+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581194417 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 43s | | trunk passed | | +1 :green_heart: | compile | 17m 26s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 14m 31s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 24s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 54s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 51s | | the patch passed | | +1 :green_heart: | compile | 14m 57s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 14m 57s | | the patch passed | | +1 :green_heart: | compile | 14m 12s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 12s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 8s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 31s | | the patch passed | | +1 :green_heart: | javadoc | 1m 7s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 32s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 32s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 7s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 187m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6dfc96c2a75e 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2f1a2ce7acbe45469ced141f68ba15e41d56db50 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3159 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-07T16:52:30.743+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581199504 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 29s | | trunk passed | | +1 :green_heart: | compile | 17m 9s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 5s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 44s | | trunk passed | | +1 :green_heart: | javadoc | 3m 22s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 9s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 50s | | the patch passed | | +1 :green_heart: | compile | 14m 59s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 14m 59s | | the patch passed | | +1 :green_heart: | compile | 14m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 8s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 1m 7s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 36s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 30s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 1m 6s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 195m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1ab18fc6a992 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2f1a2ce7acbe45469ced141f68ba15e41d56db50 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3149 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-07T16:56:40.968+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581218255 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 23s | | trunk passed | | +1 :green_heart: | compile | 17m 31s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 15m 46s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 14s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 32s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 50s | | the patch passed | | +1 :green_heart: | compile | 16m 39s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 16m 39s | | the patch passed | | +1 :green_heart: | compile | 15m 48s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 48s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 0s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 28s | | the patch passed | | +1 :green_heart: | javadoc | 1m 0s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 42s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 8s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 0m 56s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 198m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux bd9b5ce8bead 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 593c3c9d041c1f24a46e533f4d42f6d51a7e312e | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2060 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-07T17:08:43.746+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581220312 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 18s | | trunk passed | | +1 :green_heart: | compile | 22m 25s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 17m 18s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 32s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 46s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 1s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 45s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 51s | | the patch passed | | +1 :green_heart: | compile | 16m 47s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 16m 47s | | the patch passed | | +1 :green_heart: | compile | 15m 47s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 47s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 1s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 59s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 42s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 35s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 32s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 0m 57s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 206m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d79f61d6f082 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2f1a2ce7acbe45469ced141f68ba15e41d56db50 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1235 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-07T17:10:38.861+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581230125 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 48m 30s | | trunk passed | | +1 :green_heart: | compile | 22m 53s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 19m 7s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 37s | | trunk passed | | +1 :green_heart: | javadoc | 1m 18s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 34s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 54s | | the patch passed | | +1 :green_heart: | compile | 17m 41s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 17m 41s | | the patch passed | | +1 :green_heart: | compile | 16m 15s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 0s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common-project/hadoop-common: The patch generated 2 new + 108 unchanged - 0 fixed = 110 total (was 108) | | +1 :green_heart: | mvnsite | 1m 32s | | the patch passed | | +1 :green_heart: | javadoc | 1m 1s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 43s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 26m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 33s | | hadoop-common in the patch passed. | | -1 :x: | asflicense | 0m 58s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 215m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a15235b641e3 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2f1a2ce7acbe45469ced141f68ba15e41d56db50 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1253 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-07T17:19:48.910+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1581245823 ok, style failures ``` ./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java:754: Properties overlay = getOverlay();:16: 'overlay' hides a field. [HiddenField] ./hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/ConfigurationBenchmark.java:1:package org.apache.hadoop.conf;public class ConfigurationBenchmark {:31: ';' is not followed by whitespace. [WhitespaceAfter] ``` the overlay one needs new variable name. the ConfigurationBenchmark one looks like one of the PRs added a file, which somehow is still around for the style checking. Probably the strategy there is actually do a squash commit and forced write, so we are down to a single patch. I know, it's not \"elegant\" but it ensures that there's no memory of a transient file", "created": "2023-06-07T17:34:53.029+0000"}, {"author": "ASF GitHub Bot", "body": "alkis commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1582106658 > ok, style failures > > ``` > ./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java:754: Properties overlay = getOverlay();:16: 'overlay' hides a field. [HiddenField] > ./hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/ConfigurationBenchmark.java:1:package org.apache.hadoop.conf;public class ConfigurationBenchmark {:31: ';' is not followed by whitespace. [WhitespaceAfter] > ``` > > the overlay one needs new variable name. > > the ConfigurationBenchmark one looks like one of the PRs added a file, which somehow is still around for the style checking. > > Probably the strategy there is actually do a squash commit and forced write, so we are down to a single patch. I know, it's not \"elegant\" but it ensures that there's no memory of a transient file Fixed the style failures and rewrote the history to sidestep the accidental addition of the file.", "created": "2023-06-08T08:11:14.140+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1582390168 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 8s | | trunk passed | | +1 :green_heart: | compile | 15m 40s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 14m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 14s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 37s | | trunk passed | | +1 :green_heart: | javadoc | 1m 14s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 36s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 51s | | the patch passed | | +1 :green_heart: | compile | 14m 55s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 14m 55s | | the patch passed | | +1 :green_heart: | compile | 14m 33s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 14m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 7s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 36s | | the patch passed | | +1 :green_heart: | javadoc | 1m 4s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 33s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 33s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 180m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5685 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0048ebbdc570 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c44a6a5e793ab665c80da3b9c1f8fb82379dd486 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1263 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-08T11:12:36.131+0000"}, {"author": "ASF GitHub Bot", "body": "alkis commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-1594190310 @steveloughran any more thoughts on this?", "created": "2023-06-16T06:47:26.810+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5685: URL: https://github.com/apache/hadoop/pull/5685#issuecomment-3420104309 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-20T00:25:01.615+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5685: HADOOP-18748 Optimize Configuration.handleDeprecation URL: https://github.com/apache/hadoop/pull/5685", "created": "2025-10-22T00:23:13.170+0000"}], "derived_tasks": {"summary": "Configuration.get is slow - `Configuration", "classifications": ["improvement", "performance"], "qa_pairs": []}}
{"id": "HADOOP-18739", "title": "Parallelize concatenation of distcp chunks of separate files in CopyCommitter", "description": "While copying a folder containing large files consisting of multiple distcp chunks, copy committer synchronously picks chunks of each file and concatenates them. This part can be improved by parallelizing the concatenation of distcp chunks of separate files. We are able to save 2-3 minutes while copying a folder of 100 GB containing 20 files of 5GB size with this improvement. Contributing a patch for this.", "status": "Open", "priority": "Major", "reporter": "Abhay Yadav", "assignee": null, "created": "2023-05-10T20:30:02.000+0000", "updated": "2025-10-22T00:23:18.000+0000", "labels": ["pull-request-available"], "components": ["tools/distcp"], "comments": [{"author": "ASF GitHub Bot", "body": "developersarm opened a new pull request, #5640: URL: https://github.com/apache/hadoop/pull/5640 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR While copying a folder containing large files consisting of multiple distcp chunks, copy committer synchronously picks chunks of each file and concatenates them. This is very slow. As part of this PR, we are parallelising the concatenation of distcp chunks of separate files with a default thread pool of size 10. ### How was this patch tested? * Existing unit tests are passing which are testing both failure and success scenarios. * Tested the patch on distributed cloud setup by running a distcp job for copying a folder of size 100GB having 20 files of size 5GB from one cluster to another. Noticed an improvement of 2 mins in latency. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-05-10T21:36:41.372+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5640: URL: https://github.com/apache/hadoop/pull/5640#issuecomment-1542925543 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 33m 5s | | trunk passed | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 34s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 34s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 16s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-tools/hadoop-distcp: The patch generated 16 new + 12 unchanged - 0 fixed = 28 total (was 12) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | -1 :x: | spotbugs | 0m 50s | [/new-spotbugs-hadoop-tools_hadoop-distcp.html]([CI_URL] | hadoop-tools/hadoop-distcp generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 20m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 35s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 99m 39s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-distcp | | | Null passed for non-null parameter of java.util.concurrent.CompletionService.submit(Runnable, Object) in org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(Configuration) At CopyCommitter.java:of java.util.concurrent.CompletionService.submit(Runnable, Object) in org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(Configuration) At CopyCommitter.java:[line 284] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f4aa0024a051 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 88ecf3b78b133b54ed558bbb2c02e9333a9d670c | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 706 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-10T23:17:42.702+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5640: URL: https://github.com/apache/hadoop/pull/5640#issuecomment-1543494792 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 53s | | trunk passed | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 34s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 21s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-tools/hadoop-distcp: The patch generated 16 new + 12 unchanged - 0 fixed = 28 total (was 12) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 54s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 15m 7s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 99m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a17d7dd747a9 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cc15540cdaadd307cf9e8ea8c12e5706e5b0ed72 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 699 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-11T07:42:31.067+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5640: URL: https://github.com/apache/hadoop/pull/5640#issuecomment-1545674155 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 43s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 8s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 15s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-tools/hadoop-distcp: The patch generated 16 new + 16 unchanged - 0 fixed = 32 total (was 16) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 15m 13s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 99m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux efdc110e1164 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 16559fc875c0b13ada3e64583b06f3439b34d588 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 560 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-12T12:33:46.067+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5640: URL: https://github.com/apache/hadoop/pull/5640#issuecomment-1545689229 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 46s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 2s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 2s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 33s | | trunk passed | | +1 :green_heart: | compile | 0m 29s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 47s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 106m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a9606d1a53b1 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5204e688c8ed3000cbbbc3d087a546e49d9b6f97 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 540 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-12T12:45:17.979+0000"}, {"author": "Abhay Yadav", "body": "Hi [~stevel@apache.org], could you please review the PR?", "created": "2023-06-15T10:31:56.469+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5640: URL: https://github.com/apache/hadoop/pull/5640#issuecomment-3424219425 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-21T00:22:53.743+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5640: HADOOP-18739: Parallelize concatenation of distcp chunks of separate files URL: https://github.com/apache/hadoop/pull/5640", "created": "2025-10-22T00:23:18.725+0000"}], "derived_tasks": {"summary": "Parallelize concatenation of distcp chunks of separate files in CopyCommitter - While copying a folder containing large files consisting of multipl...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-18736", "title": "Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error", "description": "Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error before fix: !image-2023-05-10-14-33-49-601.png! after fix: !image-2023-05-10-14-34-02-922.png|width=583,height=32!", "status": "Open", "priority": "Major", "reporter": "fuchaohong", "assignee": null, "created": "2023-05-09T09:27:55.000+0000", "updated": "2025-10-22T00:23:20.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "fuchaohong opened a new pull request, #5634: URL: https://github.com/apache/hadoop/pull/5634 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-05-09T09:28:52.928+0000"}, {"author": "ASF GitHub Bot", "body": "fuchaohong commented on PR #5634: URL: https://github.com/apache/hadoop/pull/5634#issuecomment-1539789703 before fix\uff1a <img width=\"291\" alt=\"\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_16836199769214\" src=\"https://user-images.githubusercontent.com/80458235/237056450-ea38c684-e72c-4c26-95a1-7e74cbc6b108.png\"> after fix\uff1a ![image](https://user-images.githubusercontent.com/80458235/237056596-34d3ba98-0f5b-43a5-ae7d-bd2b55ada786.png)", "created": "2023-05-09T09:35:34.265+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5634: URL: https://github.com/apache/hadoop/pull/5634#issuecomment-1540014755 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 35m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 45s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 21m 26s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 14m 40s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 18s | | trunk passed | | +1 :green_heart: | javadoc | 1m 43s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 12s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 34s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 7s | | the patch passed | | -1 :x: | compile | 0m 53s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | cc | 0m 53s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | golang | 0m 53s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 0m 53s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 1m 28s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 19s | | hadoop-project-dist in the patch passed. | | -1 :x: | unit | 0m 44s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 146m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5634 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint cc golang | | uname | Linux 9053b5118bcc 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 12f4ffda0daed522c57780ac19e39601fb2bdf07 | | Default Java | Red Hat, Inc.-1.8.0_362-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 679 (vs. ulimit of 5500) | | modules | C: hadoop-project-dist hadoop-common-project/hadoop-common U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-09T11:57:20.330+0000"}, {"author": "ASF GitHub Bot", "body": "fuchaohong closed pull request #5634: HADOOP-18736\uff1aFix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error URL: https://github.com/apache/hadoop/pull/5634", "created": "2023-05-10T06:20:56.886+0000"}, {"author": "ASF GitHub Bot", "body": "fuchaohong opened a new pull request, #5635: URL: https://github.com/apache/hadoop/pull/5635 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-05-10T06:28:43.197+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5634: URL: https://github.com/apache/hadoop/pull/5634#issuecomment-1541580617 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 47m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 35s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 22m 1s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 16m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 5s | | trunk passed | | +1 :green_heart: | javadoc | 1m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 48s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 24m 8s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 25s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 4s | | the patch passed | | -1 :x: | compile | 0m 51s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | cc | 0m 51s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | golang | 0m 51s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | -1 :x: | javac | 0m 51s | [/patch-compile-root.txt]([CI_URL] | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 1m 28s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 0m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 15s | | hadoop-project-dist in the patch passed. | | -1 :x: | unit | 0m 41s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch failed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 160m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5634 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint cc golang | | uname | Linux 1939646b343a 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cd26a8da26573c93a1317ce929cfec99b1f9a8ad | | Default Java | Red Hat, Inc.-1.8.0_362-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 529 (vs. ulimit of 5500) | | modules | C: hadoop-project-dist hadoop-common-project/hadoop-common U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-10T08:35:10.978+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5635: URL: https://github.com/apache/hadoop/pull/5635#issuecomment-1541733861 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 35m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 58s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 20m 40s | | trunk passed | | +1 :green_heart: | compile | 14m 55s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 15s | | trunk passed | | +1 :green_heart: | javadoc | 1m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 7s | | the patch passed | | +1 :green_heart: | compile | 13m 50s | | the patch passed | | -1 :x: | cc | 13m 50s | [/results-compile-cc-root.txt]([CI_URL] | root generated 44 new + 144 unchanged - 0 fixed = 188 total (was 144) | | +1 :green_heart: | golang | 13m 50s | | the patch passed | | +1 :green_heart: | javac | 13m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 2m 18s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 1m 34s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 38s | | hadoop-project-dist in the patch passed. | | -1 :x: | unit | 18m 37s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 4s | | The patch does not generate ASF License warnings. | | | | 174m 31s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.service.launcher.TestServiceInterruptHandling | | | hadoop.crypto.TestCryptoCodec | | | hadoop.io.nativeio.TestNativeIO | | | hadoop.crypto.TestCryptoStreamsWithOpensslSm4CtrCryptoCodec | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5635 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint cc golang | | uname | Linux 306e7e622d16 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fb29170bd91436f14d674673f8097970a1bd8952 | | Default Java | Red Hat, Inc.-1.8.0_362-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 1251 (vs. ulimit of 5500) | | modules | C: hadoop-project-dist hadoop-common-project/hadoop-common U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-10T09:24:58.723+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5635: URL: https://github.com/apache/hadoop/pull/5635#issuecomment-1543302047 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 10s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 20m 14s | | trunk passed | | +1 :green_heart: | compile | 14m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 19s | | trunk passed | | +1 :green_heart: | javadoc | 1m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 49s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 6s | | the patch passed | | +1 :green_heart: | compile | 13m 44s | | the patch passed | | +1 :green_heart: | cc | 13m 44s | | the patch passed | | +1 :green_heart: | golang | 13m 44s | | the patch passed | | +1 :green_heart: | javac | 13m 44s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 2m 15s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 1m 29s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 48s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 38s | | hadoop-project-dist in the patch passed. | | -1 :x: | unit | 18m 40s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 138m 13s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.service.launcher.TestServiceInterruptHandling | | | hadoop.crypto.TestCryptoCodec | | | hadoop.crypto.TestCryptoStreamsWithOpensslSm4CtrCryptoCodec | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5635 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint cc golang | | uname | Linux 082673a7cd2d 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c3b14d767f6a029abb094298102edd6610c6560c | | Default Java | Red Hat, Inc.-1.8.0_362-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 2902 (vs. ulimit of 5500) | | modules | C: hadoop-project-dist hadoop-common-project/hadoop-common U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-11T04:05:52.886+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5635: URL: https://github.com/apache/hadoop/pull/5635#issuecomment-1543753278 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 11s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 18m 14s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 23m 3s | | trunk passed | | +1 :green_heart: | compile | 15m 48s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 2s | | trunk passed | | +1 :green_heart: | javadoc | 1m 20s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 5s | | the patch passed | | +1 :green_heart: | compile | 15m 3s | | the patch passed | | +1 :green_heart: | cc | 15m 3s | | the patch passed | | +1 :green_heart: | golang | 15m 3s | | the patch passed | | +1 :green_heart: | javac | 15m 3s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 2m 1s | | the patch passed | | +1 :green_heart: | xmllint | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 1m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 29s | | hadoop-project-dist in the patch passed. | | -1 :x: | unit | 18m 30s | [/patch-unit-hadoop-common-project_hadoop-common.txt]([CI_URL] | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 54s | | The patch does not generate ASF License warnings. | | | | 151m 1s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.crypto.TestCryptoStreamsWithOpensslSm4CtrCryptoCodec | | | hadoop.crypto.TestCryptoCodec | | | hadoop.service.launcher.TestServiceInterruptHandling | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5635 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint cc golang | | uname | Linux d7882882d152 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7aff4bcc80a0cbce0f6b55cc59357b1ea4311059 | | Default Java | Red Hat, Inc.-1.8.0_362-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 3134 (vs. ulimit of 5500) | | modules | C: hadoop-project-dist hadoop-common-project/hadoop-common U: . | | Console output | [CI_URL] | | versions | git=2.9.5 maven=3.6.3 xmllint=20901 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-11T10:37:20.301+0000"}, {"author": "ASF GitHub Bot", "body": "fuchaohong opened a new pull request, #5811: URL: https://github.com/apache/hadoop/pull/5811 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-07-05T10:29:19.625+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5811: URL: https://github.com/apache/hadoop/pull/5811#issuecomment-1621496895 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 6m 49s | | Docker failed to build run-specific yetus/hadoop:tp-7298}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5811 | | Console output | [CI_URL] | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-05T10:40:35.201+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5811: URL: https://github.com/apache/hadoop/pull/5811#issuecomment-1622776914 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 0m 8s | | Docker failed to build run-specific yetus/hadoop:tp-23209}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5811 | | Console output | [CI_URL] | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-07-06T01:23:24.788+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5811: URL: https://github.com/apache/hadoop/pull/5811#issuecomment-3417558679 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-18T00:21:02.283+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5811: HADOOP-18736: Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error. URL: https://github.com/apache/hadoop/pull/5811", "created": "2025-10-20T00:24:33.026+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5635: URL: https://github.com/apache/hadoop/pull/5635#issuecomment-3424219509 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-21T00:22:56.766+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5635: HADOOP-18736: Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error URL: https://github.com/apache/hadoop/pull/5635", "created": "2025-10-22T00:23:20.451+0000"}], "derived_tasks": {"summary": "Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error - Fix '-Dbundle", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-18723", "title": "Add detail logs if distcp checksum mismatch", "description": "We encountered some errors of mismatch checksum during Distcp jobs. It took us some time to figure out that checksum type is different. Adding error logs shall help us to figure out such problems faster.", "status": "Open", "priority": "Major", "reporter": "Janus Chow", "assignee": "Janus Chow", "created": "2023-04-27T11:10:23.000+0000", "updated": "2025-10-22T00:23:24.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "symious opened a new pull request, #5603: URL: https://github.com/apache/hadoop/pull/5603 ### Description of PR We encountered some errors of mismatch checksum during Distcp jobs. It took us some time to figure out that checksum type is different. Adding error logs shall help us to figure out such problems faster. ### How was this patch tested? Add unit test. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-04-27T11:15:51.069+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1525672157 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 31s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 14s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-tools/hadoop-distcp: The patch generated 2 new + 20 unchanged - 0 fixed = 22 total (was 20) | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 50s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 113m 56s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5603 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2addaf850a03 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b1060a84e3874aa8cee2d5db6e6fa493187c9753 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 632 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-27T13:11:08.256+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1526686991 @ayushtkn Thank you for the review. Updated the PR, PTAL.", "created": "2023-04-27T22:56:18.762+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1526826490 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 46s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 19s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 31s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 16s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 34s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 114m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5603 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux be8441455bf1 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a832a9b6341c3764f7f8f5e47ab3a67628dd8357 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 619 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-28T00:51:11.769+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1181071239 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -592,10 +592,14 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( // comparison that took place and return not compatible. // else if matched, return compatible with the matched result. if (sourceChecksum == null || targetChecksum == null) { + LOG.error(\"Checksum incompatible. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); return CopyMapper.ChecksumComparison.INCOMPATIBLE; } else if (sourceChecksum.equals(targetChecksum)) { return CopyMapper.ChecksumComparison.TRUE; } + LOG.error(\"Checksum not equal. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); Review Comment: Error seems to much for this case, info should be more than enough for these cases, and I don't think we need to put the checksum in the log, rather put the ``sourcePath`` and ``targetPath``", "created": "2023-04-29T12:17:18.792+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1181071239 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -592,10 +592,14 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( // comparison that took place and return not compatible. // else if matched, return compatible with the matched result. if (sourceChecksum == null || targetChecksum == null) { + LOG.error(\"Checksum incompatible. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); return CopyMapper.ChecksumComparison.INCOMPATIBLE; } else if (sourceChecksum.equals(targetChecksum)) { return CopyMapper.ChecksumComparison.TRUE; } + LOG.error(\"Checksum not equal. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); Review Comment: Error seems to much for this case, info should be more than enough for these cases, and I don't think we need to put the checksum in the log, rather put the ``sourcePath`` and ``targetPath``, or at worst both path & checksum", "created": "2023-04-30T15:06:02.561+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1181517673 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -592,10 +592,14 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( // comparison that took place and return not compatible. // else if matched, return compatible with the matched result. if (sourceChecksum == null || targetChecksum == null) { + LOG.error(\"Checksum incompatible. Source checksum: {}, target checksum: {}\", Review Comment: this is not unusual against object stores, as all stores which don't have hdfs-+compatible checksums disable them so that distcp to cloud stores don't blow up. If you logged at error then there'd be an entry for every single copy. propose: use a LogExactlyOnce at info to day source or target fs doesn't support checksums and that they should use -skipCrc ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java: ########## @@ -569,6 +570,7 @@ private void testCommitWithChecksumMismatch(boolean skipCrc) fs, new Path(sourceBase + srcFilename), null, fs, new Path(targetBase + srcFilename), sourceCurrStatus.getLen())); + assertTrue(log.getOutput().contains(\"Checksum not equal\")); Review Comment: asserts to use Assert's assertThat(log.getOutput).contains(...) for better message ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -592,10 +592,14 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( // comparison that took place and return not compatible. // else if matched, return compatible with the matched result. if (sourceChecksum == null || targetChecksum == null) { + LOG.error(\"Checksum incompatible. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); return CopyMapper.ChecksumComparison.INCOMPATIBLE; } else if (sourceChecksum.equals(targetChecksum)) { return CopyMapper.ChecksumComparison.TRUE; } + LOG.error(\"Checksum not equal. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); Review Comment: log checksums at debug maybe anyway, doesn't a checksum mismatch mean \"source file needs copying again\". so really the thing to log is not the mismatch but why the copy is taking place", "created": "2023-05-01T11:17:47.151+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1532347000 The issue we met was not \"source file needs copying again\", but source file uploaded with a different checksum type. Because it's distcp, so the temp file was removed after the exception. We need to first skip the crc check, then run \"hadoop fs -checksum\" on the source file and target file to find out the root cause of this one. The initial idea of this addition of log was to help others with the same issue to save to process of redundant works and just check the log for mismatch reasons. @steveloughran @ayushtkn", "created": "2023-05-03T01:25:14.879+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1532665982 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 34s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 27s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 1s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 24s | [/results-compile-javac-hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | -1 :x: | javac | 0m 21s | [/results-compile-javac-hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 13 unchanged - 0 fixed = 14 total (was 13) | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 15m 7s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 113m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5603 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux cd9be08ad945 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 462b34034693ace140c5dcfcd16bb682fde6c58c | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-03T08:52:12.794+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1532830116 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 2s | | trunk passed | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 52s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 35s | | trunk passed | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 27s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 51s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 43s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 114m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5603 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2b0a37429a9a 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c99bcea56a3e78c3d98e3bcc77a398350a1d8976 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-03T10:57:04.753+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1533526312 Well, I'm afraid your specific problem does not match Dee why do use cases of uploading to stores without checksums. Now, I would I've been happier if distcp's -skipCrc option was required to copy data from an FS with checksums to one without, but it is not and to add it now would break so many people's workflows. So what do we do here? maybe: create counters of why files were copied, specifically * not found at destination * file length different * modtime * checksum Then after a job you can see why files were copied from the host where the job was launched. Then if you want to know why there were issues such as checksums and modtimes, you can log out to debug. Obviously, this will be something to add to the distcp documentation. Now: big warning. I am personally scared of distCp. It is a critical workflow tool and even use programmatically, yet it is surprisingly brittle. It is a running joke that's the last person two add any code to the module gets to field or support calls until someone else comes along. Thank you for volunteering! This also explains why we will be very reluctant/strict about taking on changes. Don't take it personally is as hey everyone gets that same grilling here.", "created": "2023-05-03T18:41:32.340+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1533942745 > Well, I'm afraid your specific problem does not match Dee why do use cases of uploading to stores without checksums. Now, I would I've been happier if distcp's -skipCrc option was required to copy data from an FS with checksums to one without, but it is not and to add it now would break so many people's workflows. > > So what do we do here? > > maybe: create counters of why files were copied, specifically > > * not found at destination > * file length different > * modtime > * checksum > > Then after a job you can see why files were copied from the host where the job was launched. Then if you want to know why there were issues such as checksums and modtimes, you can log out to debug. Obviously, this will be something to add to the distcp documentation. > > Now: big warning. I am personally scared of distCp. It is a critical workflow tool and even use programmatically, yet it is surprisingly brittle. It is a running joke that's the last person two add any code to the module gets to field or support calls until someone else comes along. Thank you for volunteering! This also explains why we will be very reluctant/strict about taking on changes. Don't take it personally is as hey everyone gets that same grilling here. Totally understand. I have removed the error log if checksum is null for sourcePath and targetPath, so a FS without checksum won't be affected. I have also changed the error log to INFO level if both checksum exists but not equal, or a DEBUG level will be more preferred? Personally I think INFO level can save us more time since we don't need to reconfig and restart the job.", "created": "2023-05-04T01:20:13.471+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1185326966 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -596,6 +596,8 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( } else if (sourceChecksum.equals(targetChecksum)) { return CopyMapper.ChecksumComparison.TRUE; } + LOG.info(\"Checksum not equal. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); Review Comment: you aren't putting the paths for which checksum mismatch happened, if there are thousands of file being copied and bunch of them log this. How would you figure out who checksum didn't match", "created": "2023-05-04T17:48:12.782+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1185336347 ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java: ########## @@ -569,6 +572,7 @@ private void testCommitWithChecksumMismatch(boolean skipCrc) fs, new Path(sourceBase + srcFilename), null, fs, new Path(targetBase + srcFilename), sourceCurrStatus.getLen())); + assertThat(log.getOutput(), containsString(\"Checksum not equal\")); Review Comment: wrong import & assertThat. This import ``` import static org.assertj.core.api.Assertions.assertThat; ``` and it works like this ``` assertThat(log.getOutput()).contains(\"Checksum not equal\"); ```", "created": "2023-05-04T17:56:33.592+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1185326966 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -596,6 +596,8 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( } else if (sourceChecksum.equals(targetChecksum)) { return CopyMapper.ChecksumComparison.TRUE; } + LOG.info(\"Checksum not equal. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); Review Comment: you aren't putting the paths for which checksum mismatch happened, if there are thousands of file being copied and bunch of them log this. How would you figure out whose checksum didn't match", "created": "2023-05-04T19:48:24.742+0000"}, {"author": "ASF GitHub Bot", "body": "symious commented on code in PR #5603: URL: https://github.com/apache/hadoop/pull/5603#discussion_r1185646826 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java: ########## @@ -596,6 +596,8 @@ public static CopyMapper.ChecksumComparison checksumsAreEqual( } else if (sourceChecksum.equals(targetChecksum)) { return CopyMapper.ChecksumComparison.TRUE; } + LOG.info(\"Checksum not equal. Source checksum: {}, target checksum: {}\", + sourceChecksum, targetChecksum); Review Comment: In our failed jobs, the mismatch source and target path was printed by https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java#L633. Thanks for pointing that out. The paths have also been added to DistCpUtils. ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyCommitter.java: ########## @@ -569,6 +572,7 @@ private void testCommitWithChecksumMismatch(boolean skipCrc) fs, new Path(sourceBase + srcFilename), null, fs, new Path(targetBase + srcFilename), sourceCurrStatus.getLen())); + assertThat(log.getOutput(), containsString(\"Checksum not equal\")); Review Comment: Copied the hamcrest version from \"org.apache.hadoop.conf.TestReconfiguration\". But changed to the Assertions.assertThat one.", "created": "2023-05-05T02:00:50.706+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-1535840622 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 259m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 33s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 48s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 325m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5603 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6a3f0a39b78f 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c73e1da394e70693d14372c139dc0b2f8f38475b | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 700 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-05T07:28:45.032+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5603: URL: https://github.com/apache/hadoop/pull/5603#issuecomment-3424219709 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-21T00:23:02.900+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5603: HADOOP-18723. Add detail logs if distcp checksum mismatch URL: https://github.com/apache/hadoop/pull/5603", "created": "2025-10-22T00:23:24.759+0000"}], "derived_tasks": {"summary": "Add detail logs if distcp checksum mismatch - We encountered some errors of mismatch checksum during Distcp jobs", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "HADOOP-18706", "title": "Improve S3ABlockOutputStream recovery", "description": "If an application crashes during an S3ABlockOutputStream upload, it's possible to complete the upload if fast.upload.buffer is set to disk by uploading the s3ablock file with putObject as the final part of the multipart upload. If the application has multiple uploads running in parallel though and they're on the same part number when the application fails, then there is no way to determine which file belongs to which object, and recovery of either upload is impossible. If the temporary file name for disk buffering included the s3 key, then every partial upload would be recoverable. h3. Important disclaimer This change does not directly add the Syncable semantics which applications that require {{Syncable.hsync()}} to only return after all pending data has been durably written to the destination path. S3 is not a filesystem and this change does not make it so. What is does do is assist anyone trying to implement some post-crash recovery process which # interrogates s3 to identofy pending uploads to a specific path and get a list of uploaded blocks yet to be committed # scans the local fs.s3a.buffer dir directories to identify in-progress-write blocks for the same target destination. That is those which were being uploaded, queued for uploaded and the single \"new data being written to\" block for an output stream # uploads all those pending blocks # generates a new POST to complete a multipart upload with all the blocks in the correct order All this patch does is ensure the buffered block filenames include the final path and block ID, to aid in identify which blocks need to be uploaded and what order. h2. warning causes HADOOP-18744 -always include the relevant fix when backporting", "status": "Reopened", "priority": "Minor", "reporter": "Chris Bevard", "assignee": "Chris Bevard", "created": "2023-04-17T17:45:52.000+0000", "updated": "2025-10-21T00:22:44.000+0000", "labels": ["pull-request-available"], "components": ["fs/s3"], "comments": [{"author": "ASF GitHub Bot", "body": "cbevard1 opened a new pull request, #5563: URL: https://github.com/apache/hadoop/pull/5563 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR This PR improves the ability to recovery partial S3A uploads. 1. Changed the handleSyncableInvocation() to call flush() after warning that the syncable API isn't supported. This mirrors the downgradeSyncable behavior of BufferedIOStatisticsOutputStream and RawLocalFileSystem. 2. Changed the DiskBlock temporary file names to include the S3 key to allow partial uploads to be recovered. ### How was this patch tested? Unit testsing and regression testing with Accumulo ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-04-17T19:36:38.979+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1513283429 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 31s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 19s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 38s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 40s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 26s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 110m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 65c65fb67bb3 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2d04346bd70249249f274a48521ee9b44d4715a1 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 611 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-18T14:56:44.019+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5563: URL: https://github.com/apache/hadoop/pull/5563#discussion_r1170129401 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -726,7 +726,7 @@ public void hsync() throws IOException { /** * Shared processing of Syncable operation reporting/downgrade. */ - private void handleSyncableInvocation() { + private void handleSyncableInvocation() throws IOException { Review Comment: update javadocs", "created": "2023-04-18T14:57:14.524+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5563: URL: https://github.com/apache/hadoop/pull/5563#discussion_r1170131112 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestDataBlocks.java: ########## @@ -51,7 +51,7 @@ public void testByteBufferIO() throws Throwable { new S3ADataBlocks.ByteBufferBlockFactory(null)) { int limit = 128; S3ADataBlocks.ByteBufferBlockFactory.ByteBufferBlock block - = factory.create(1, limit, null); + = factory.create(\"object/key\", 1, limit, null); Review Comment: add a backslash here too for completeness", "created": "2023-04-18T14:58:04.637+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1513592514 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 2 new + 2 unchanged - 0 fixed = 4 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 22s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 108m 4s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.TestS3ABlockOutputStream | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 7026cd53cc3a 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 23120945595441a326785b0aad3111d1f6ec4c90 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 578 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-18T18:06:23.032+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1513699708 @steveloughran thanks for your feedback. I've added the span ID to the file name as you suggested for better debugging. > If you really want upload to be recoverable then you need to be able to combine blocks on the hard disk with the in-progress multipart upload such that you can build finish the upload, build the list of etags and then POST the complete operation. With the part number and key derived from the local file name, I've been using calls to `list-mulipart-uploads`/`list-parts` to get the uploadID/ETags and complete partial uploads. For single part files I call putObject with the key, and for multipart uploads I use the upload ID and part number returned by `list-mulipart-uploads`/`list-parts` to submit the local file as the final part. The key could exceed an OS's file name char limit though, so I think including the span ID is a very good idea. I know it's not a typical use case to recover a partial upload rather than retry the entire file, but it's very helpful with using S3A as the underlying file system in Accumulo.", "created": "2023-04-18T19:39:56.894+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1513746171 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 22s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 103m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 050c1edeb490 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6b43d44c17303d5a202dedaedd53bfad8e0719e4 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 528 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-18T20:18:27.823+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on code in PR #5563: URL: https://github.com/apache/hadoop/pull/5563#discussion_r1171358622 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -726,7 +726,7 @@ public void hsync() throws IOException { /** * Shared processing of Syncable operation reporting/downgrade. */ - private void handleSyncableInvocation() { + private void handleSyncableInvocation() throws IOException { Review Comment: Done.", "created": "2023-04-19T13:38:02.137+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1514770186 OK. I think the design is incomplete as it is, you would really want to be a bit more sophisticated and * write the .pending multipart manifest to the temp dir as soon as the multipart is created * update it after every block is written * and don't allow more than one block to be written at a time (this is done for S3-CSE) already but this bit seems ready to go in, low risk and potentially useful for others. now, test policy. which aws s3 region did you run the full \"mvn verify\" tests for the hadoop-aws module, and what options did you have on the command line?", "created": "2023-04-19T13:48:26.693+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1515127752 @steveloughran I'm running the integration tests in us-east-2 and the only option I'm setting is the parallel test and thread count (`mvn verify -Dparallel-tests -DtestsThreadCount=8`)", "created": "2023-04-19T17:43:49.744+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1516264118 ok, and they all work? good to know. Don't be afraid to mention any which do fail, as they are often from different developer config, and its good to find out what is wrong with an existing test (or worse, production code) before we ship...", "created": "2023-04-20T12:44:50.630+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1516289775 I did see intermittent failures once with one of the tests in `org.apache.hadoop.fs.contract.s3a.ITestS3AContractRootDir`. I can't remember which test it was, but when I ran just that file immediately after the failure, it passed. Every other time I've run `mvn verify` they've all passed.", "created": "2023-04-20T13:02:39.553+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1516391716 Correction: after rebasing, all of the tests in `org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A` are failing with errors. It looks like these tests are also failing in trunk. Something to do with the HistoryServer failing to start for the MiniMRYarnCluster.", "created": "2023-04-20T14:04:31.463+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1516625886 Nevermind. I was running verify from a new terminal window where JAVA_HOME was set to version 18. That was causing issues with the com.google.inject:guice dependency in hadoop-yarn-server-resourcemanager. It looks like that version of java was also messing up the maven-surefire-plugin because the `ITestTerasortOnS3A` test wasn't being skipped when it should have been. Anyways, rolling back the version of java fixed the issue. I ran the scale tests too this time so `ITestTerasortOnS3A` wasn't skipped and I could make sure it succeeded. All unit and integration tests are passing.", "created": "2023-04-20T16:31:40.837+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1517764238 looks good, just two issues to worry about minor: checkstyle unhappy about line length...please keep at 100 chars or less One bigger issue, which you already mentioned: excessively long filenames. S3 supports 1024 chars of path so this should work through the other block buffers, and MUST work here too. looking at a table of length, there's 255 chars to play with, including block id, span id etc https://www.baeldung.com/linux/bash-filename-limit How about adding a new test case or modifying testRegularUpload() to create a file with a name > 256 chars just see what happens? Oh, and we have to remember about windows too, though as java apis go through the unicode ones, its 255 char limit doesn't always hold. Maybe the solution is to do some cutting down of paths such that first few and final chars are always preserved. along with span ID that should be good, though it does depend on filenames generated...does accumulo generate sufficiently unique ones that the last, say, 128 chars will be something you can map to an upload?", "created": "2023-04-21T12:31:14.247+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1517915320 > minor: checkstyle unhappy about line length...please keep at 100 chars or less No problem, will do. > How about adding a new test case or modifying testRegularUpload() to create a file with a name > 256 chars just see what happens? Initially I had some code in the S3ADataBlock to trim the key if the file name was nearing 255 chars, but after testing what would happen with a massive S3 key passed to `File.createTempFile()` I noticed that it would automatically truncate the end of the prefix to fit within the FS's char limit with the appended the random number and suffix (\".tmp\" if one wasn't specified). I'll add a unit test like you suggested so we can detect if `createTempFile` ever stops truncating names that exceed the FS's max length . Here's a snippet from the javadocs for `createTempFile()` that describes how prefixes are handled > \"To create the new file, the prefix and the suffix may first be adjusted to fit the limitations of the underlying platform. If the prefix is too long then it will be truncated, but its first three characters will always be preserved.\" > Maybe the solution is to do some cutting down of paths such that first few and final chars are always preserved. along with span ID that should be good, though it does depend on filenames generated...does accumulo generate sufficiently unique ones that the last, say, 128 chars will be something you can map to an upload? With Accumulo, it's the WALs that are important to recover and they're named with a UUID, so they're very unique even without a prefix. The full WAL key is built as such `bucket+instance_volume(folder/prefix)+tserver_hostname+UUID`, so avoiding the 255 char limit is pretty doable if you keep your bucket and instance volume names short. The longest file name I've seen so far in my testing with the spanId now included was 175 chars. If I removed the S3 key prefix from the name and kept just the UUID, that would be a little safer for Accumulo, but could cause issues with other systems where prefix is more important contextually than the name. I lean towards leaving the naming convention as it is (s3ablock-PARTNUM-SPAN_ID-ESCAPED_S3_KEY-RANDOM_NUM.tmp). Even if the UUID was truncated, just a few chars along with the part number will almost always be enough to uniquely match it to a pending Multipart Upload. If the key gets trimmed enough that it's not possible to derive the recovery information you need from the file name, then there's the logs and span ID to fall back on. I'll get that test added and correct the style issue. Let me know if you'd like me to make any changes the file name that's generated.", "created": "2023-04-21T14:25:11.997+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1518185036 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 13s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 23s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 104m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e5c10f77ca6a 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d61df9372f1b1bcfe8955acff0d5efda14dd20a8 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-21T18:18:37.644+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5563: URL: https://github.com/apache/hadoop/pull/5563#discussion_r1175384535 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -79,6 +80,42 @@ public void testRegularUpload() throws IOException { verifyUpload(\"regular\", 1024); } + /** + * Test that the DiskBlock's local file doesn't result in error when the S3 key exceeds the max + * char limit of the local file system. Currently + * {@link java.io.File#createTempFile(String, String, File)} is being relied on to handle the + * truncation. + * @throws IOException + */ + @Test + public void testDiskBlockCreate() throws IOException { + S3ADataBlocks.BlockFactory diskBlockFactory = Review Comment: use try-with-resources, even if I doubt this is at risk of leaking things ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -79,6 +80,42 @@ public void testRegularUpload() throws IOException { verifyUpload(\"regular\", 1024); } + /** + * Test that the DiskBlock's local file doesn't result in error when the S3 key exceeds the max + * char limit of the local file system. Currently + * {@link java.io.File#createTempFile(String, String, File)} is being relied on to handle the + * truncation. + * @throws IOException + */ + @Test + public void testDiskBlockCreate() throws IOException { + S3ADataBlocks.BlockFactory diskBlockFactory = + new S3ADataBlocks.DiskBlockFactory(getFileSystem()); + String s3Key = // 1024 char + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key\"; + S3ADataBlocks.DataBlock dataBlock = diskBlockFactory.create(\"spanId\", s3Key, 1, + getFileSystem().getDefaultBlockSize(), null); + LOG.info(dataBlock.toString()); // block file name and location can be viewed in failsafe-report + + // delete the block file + dataBlock.innerClose(); Review Comment: are there any more asserts here, e.g that the file exists afterwards?", "created": "2023-04-24T14:37:57.892+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on code in PR #5563: URL: https://github.com/apache/hadoop/pull/5563#discussion_r1178227622 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -79,6 +80,42 @@ public void testRegularUpload() throws IOException { verifyUpload(\"regular\", 1024); } + /** + * Test that the DiskBlock's local file doesn't result in error when the S3 key exceeds the max + * char limit of the local file system. Currently + * {@link java.io.File#createTempFile(String, String, File)} is being relied on to handle the + * truncation. + * @throws IOException + */ + @Test + public void testDiskBlockCreate() throws IOException { + S3ADataBlocks.BlockFactory diskBlockFactory = + new S3ADataBlocks.DiskBlockFactory(getFileSystem()); + String s3Key = // 1024 char + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key\"; + S3ADataBlocks.DataBlock dataBlock = diskBlockFactory.create(\"spanId\", s3Key, 1, + getFileSystem().getDefaultBlockSize(), null); + LOG.info(dataBlock.toString()); // block file name and location can be viewed in failsafe-report + + // delete the block file + dataBlock.innerClose(); Review Comment: I added an assertion to make sure the tmp file is created.", "created": "2023-04-26T18:04:47.386+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1523955619 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 34s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 22s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 104m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c95c4102e226 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 57eafebd397bd2b968e85c927bece4c9a97922ca | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 531 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-26T19:47:42.452+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5563: URL: https://github.com/apache/hadoop/pull/5563#discussion_r1178915082 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -107,13 +108,18 @@ public void testDiskBlockCreate() throws IOException { \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + \"very_long_s3_key\"; - S3ADataBlocks.DataBlock dataBlock = diskBlockFactory.create(\"spanId\", s3Key, 1, - getFileSystem().getDefaultBlockSize(), null); - LOG.info(dataBlock.toString()); // block file name and location can be viewed in failsafe-report - - // delete the block file - dataBlock.innerClose(); - diskBlockFactory.close(); + long blockSize = getFileSystem().getDefaultBlockSize(); + try (S3ADataBlocks.BlockFactory diskBlockFactory = + new S3ADataBlocks.DiskBlockFactory(getFileSystem()); + S3ADataBlocks.DataBlock dataBlock = + diskBlockFactory.create(\"spanId\", s3Key, 1, blockSize, null); + ) { + boolean created = Arrays.stream( + Objects.requireNonNull(new File(getConfiguration().get(\"hadoop.tmp.dir\")).listFiles())) + .anyMatch(f -> f.getName().contains(\"very_long_s3_key\")); + assertTrue(created); Review Comment: add a message to print if the assert is false: we need to be able to start debugging without having to work back from the first line of the stack trace as to what went wrong. include that hadoop.tmp.dir value in the message too", "created": "2023-04-27T09:58:39.197+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1525952815 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 20s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 49s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 37s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 23s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 105m 30s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 486a49d7ff70 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fd736d01b03bad2889ee11da094efd789d9f7198 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 579 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-27T16:01:37.484+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1533527822 can you stop rebasing this during the review process. it makes it *impossible* for me to use the \"review changes since your last commit\" once you have confirmed that you are going to stop, I will review the PR again.", "created": "2023-05-03T18:42:49.723+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1533531643 Sorry about that. I won't rebase anymore.", "created": "2023-05-03T18:45:41.280+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1534755287 Done. Integration tests were run in us-east-2 with the following options: `-Dparallel-tests -DtestsThreadCount=16 -Dscale`", "created": "2023-05-04T13:10:10.465+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1534918500 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 45s | | trunk passed | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 37s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 15s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 34s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 26s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 28s | | the patch passed | | +1 :green_heart: | javadoc | 0m 13s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 20s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 22s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 100m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1a37e146dbe8 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4f41cf1d0d6427d59e42b58a94b333b326938b83 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 612 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-04T14:49:03.364+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1535241489 thanks for the run. final bit of outstanding style. i know there are lots already, but new code should always start well, whenever possible ``` ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java:114: diskBlockFactory.create(\"spanId\", s3Key, 1, blockSize, null);: 'diskBlockFactory' has incorrect indentation level 12, expected level should be 13. [Indentation] ```", "created": "2023-05-04T18:44:27.056+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1535323312 Shoot. That was in the last checkstyle, and I thought I bumped it out to 113. My IDE says it was indented to 113, but it wasn't. I should have run check style before committing. Anyways, my mistake. I reviewed the check style report before committing this time and it should be good now.", "created": "2023-05-04T19:49:00.122+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1535407973 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 29s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 36s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 11s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 18s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 19s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 101m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5563 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5f590dc76341 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7b5f48a240105c6f8e203780d00e31852a11c92b | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-04T21:02:47.006+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #5563: URL: https://github.com/apache/hadoop/pull/5563", "created": "2023-05-05T10:57:45.685+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5563: URL: https://github.com/apache/hadoop/pull/5563#issuecomment-1536085389 thanks, merged to trunk. can you cherrypick locally to branch-3.3 rerun the tests and then put that up as a new PR? I will then merge that. FWIW I still think what you are trying to do for recovery is \"bold\", the rock climbing \"I wonder if they will survive\" meaning of the word, but the filename tracking could be useful in other ways.", "created": "2023-05-05T11:01:03.185+0000"}, {"author": "Steve Loughran", "body": "[~cbevard1] afraid this is causing a test to fail, and its indicative of a problem which may surface in production. afraid you have to come up with a shorter naming scheme...something with span ID, first few chars, final set of chars, maybe? no file will be written except within the span of the create() call, so if you collect S3 server logs, you get to work out exactly which operation is creating a file -even if >1 process is trying to write to the same path.", "created": "2023-05-18T20:14:12.960+0000"}, {"author": "Steve Loughran", "body": "bad news Chris, had to revert this. Can you do a new pr which has very short filenames (ideally span id and some minimal info for users but enough so we never run out of filename. thanks", "created": "2023-05-24T18:24:45.829+0000"}, {"author": "Chris Bevard", "body": "Sorry. I just noticed your previous comments about the issue. I'll take a look.", "created": "2023-05-24T20:05:32.253+0000"}, {"author": "Chris Bevard", "body": "[~stevel@apache.org] Sorry for introducing the bug. Everywhere I've tested it the block file name was being truncated automatically. I want to patch this change and create a new pull request. I don't think I'll be able to avoid manually limiting the name to 255 chars, but just out of curiosity I'd like to look at the `Java.io.IOException: File name too long` error. I reviewed the comments in [HADOOP-18744|https://issues.apache.org/jira/browse/HADOOP-18744] and it looks like you and two others were able to reproduce it. Can you tell me the OS and JDK you were using to reproduce the error?", "created": "2023-06-15T15:42:45.317+0000"}, {"author": "Steve Loughran", "body": "macos, azul java8", "created": "2023-06-15T22:50:48.435+0000"}, {"author": "Chris Bevard", "body": "I see where I went wrong. Kind of a huge oversight, from bouncing back and forth between Accumulo and Hadoop, and I thought both SNAPSHOT versions were on Java 11, and that's the version I was building and testing with. Hadoop is still on Java 8, and the automatic truncation for java.io.File.createTempFile() was added in Java 9. I've added validation to code similar to what's implemented in newer versions of Java. I had to assume the 255 char limit for all systems, and that the random suffix will always be the max possible length of 19 chars. Can I reopen this pull request with the added validation? [Java 8 generateFile() src (no validation)|https://github.com/bpupadhyaya/openjdk-8/blob/45af329463a45955ea2759b89cb0ebfe40570c3f/jdk/src/share/classes/java/io/File.java#L1902] [Java 9 generateFile() src (validation)|https://github.com/AdoptOpenJDK/openjdk-jdk9/blob/f00b63d24697cce8067f468fe6cd8510374a46f5/jdk/src/java.base/share/classes/java/io/File.java#LL1925C8-L1925C8]", "created": "2023-06-16T14:26:44.220+0000"}, {"author": "Steve Loughran", "body": "yes, new PR with this JIRA ID. thanks", "created": "2023-06-18T17:07:05.166+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 opened a new pull request, #5771: URL: https://github.com/apache/hadoop/pull/5771 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR This PR improves the ability to recovery partial S3A uploads. Changed the handleSyncableInvocation() to call flush() after warning that the syncable API isn't supported. This mirrors the downgradeSyncable behavior of BufferedIOStatisticsOutputStream and RawLocalFileSystem. Changed the DiskBlock temporary file names to include the S3 key to allow partial uploads to be recovered. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-06-23T13:48:30.900+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5771: URL: https://github.com/apache/hadoop/pull/5771#issuecomment-1604454398 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 27s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 8 new + 5 unchanged - 0 fixed = 13 total (was 5) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 104m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5771 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1304a7468aeb 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 95138e0dedfb8934695a766e5b72360a90b44517 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 608 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-06-23T15:34:07.007+0000"}, {"author": "ASF GitHub Bot", "body": "cbevard1 commented on PR #5771: URL: https://github.com/apache/hadoop/pull/5771#issuecomment-1622232578 @steveloughran Can you take a look at this new PR when you get a chance?", "created": "2023-07-05T18:03:10.127+0000"}, {"author": "Chris Bevard", "body": "[~stevel@apache.org]", "created": "2023-07-18T16:41:29.603+0000"}, {"author": "Shilun Fan", "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.", "created": "2024-01-04T09:44:44.902+0000"}, {"author": "Steve Loughran", "body": "catching up on this; i'd forgotten we'd had to revert it. will look at ASAP", "created": "2024-01-25T10:40:41.060+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5771: URL: https://github.com/apache/hadoop/pull/5771#discussion_r1466282702 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1369,12 +1369,69 @@ public S3AEncryptionMethods getS3EncryptionAlgorithm() { File createTmpFileForWrite(String pathStr, long size, Configuration conf) throws IOException { initLocalDirAllocatorIfNotInitialized(conf); - Path path = directoryAllocator.getLocalPathForWrite(pathStr, - size, conf); + Path path = directoryAllocator.getLocalPathForWrite(pathStr, size, conf); File dir = new File(path.getParent().toUri().getPath()); - String prefix = path.getName(); - // create a temp file on this directory - return File.createTempFile(prefix, null, dir); + return safeCreateTempFile(pathStr, null, dir); + } + + // TODO remove this method when hadoop upgrades to a newer version of java than 1.8 + /** + * Ensure that the temp file prefix and suffix don't exceed the maximum number of characters + * allowed by the underlying file system. This validation isn't required in Java 9+ since + * {@link java.io.File#createTempFile(String, String, File)} automatically truncates file names. + * + * @param prefix prefix for the temporary file + * @param suffix suffix for the temporary file + * @param dir directory to create the temporary file in + * @return a unique temporary file + * @throws IOException + */ + static File safeCreateTempFile(String prefix, String suffix, File dir) throws IOException + { + // avoid validating multiple times. + // if the jvm running is version 9+ then defer to java.io.File validation implementation + if(Float.parseFloat(System.getProperty(\"java.class.version\")) >= 53) { + return File.createTempFile(prefix, null, dir); + } + + // if no suffix was defined assume the default + if(suffix == null) { + suffix = \".tmp\"; + } + // Use only the file name from the supplied prefix + prefix = (new File(prefix)).getName(); + + int prefixLength = prefix.length(); + int suffixLength = suffix.length(); + int maxRandomSuffixLen = 19; // Long.toUnsignedString(Long.MAX_VALUE).length() + + String name; + int nameMax = 255; // unable to access the underlying FS directly, so assume 255 Review Comment: make a constant, e.g ASSUMED_MAX_FILENAME ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1369,12 +1369,69 @@ public S3AEncryptionMethods getS3EncryptionAlgorithm() { File createTmpFileForWrite(String pathStr, long size, Configuration conf) throws IOException { initLocalDirAllocatorIfNotInitialized(conf); - Path path = directoryAllocator.getLocalPathForWrite(pathStr, - size, conf); + Path path = directoryAllocator.getLocalPathForWrite(pathStr, size, conf); File dir = new File(path.getParent().toUri().getPath()); - String prefix = path.getName(); - // create a temp file on this directory - return File.createTempFile(prefix, null, dir); + return safeCreateTempFile(pathStr, null, dir); + } + + // TODO remove this method when hadoop upgrades to a newer version of java than 1.8 + /** + * Ensure that the temp file prefix and suffix don't exceed the maximum number of characters + * allowed by the underlying file system. This validation isn't required in Java 9+ since + * {@link java.io.File#createTempFile(String, String, File)} automatically truncates file names. + * + * @param prefix prefix for the temporary file + * @param suffix suffix for the temporary file + * @param dir directory to create the temporary file in + * @return a unique temporary file + * @throws IOException + */ + static File safeCreateTempFile(String prefix, String suffix, File dir) throws IOException + { + // avoid validating multiple times. + // if the jvm running is version 9+ then defer to java.io.File validation implementation + if(Float.parseFloat(System.getProperty(\"java.class.version\")) >= 53) { Review Comment: this should go in org.apache.hadoop.util.Shell; there's already something similar. will need a test somehow. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -798,6 +800,8 @@ public String toString() { * Buffer blocks to disk. */ static class DiskBlockFactory extends BlockFactory { + private static final String ESCAPED_FORWARD_SLASH = \"EFS\"; Review Comment: think I might prefer something more distinguishable from text, e.g \"_FS_\" and \"_BS_\" so its easier to read in a dir listing ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1369,12 +1369,69 @@ public S3AEncryptionMethods getS3EncryptionAlgorithm() { File createTmpFileForWrite(String pathStr, long size, Configuration conf) throws IOException { initLocalDirAllocatorIfNotInitialized(conf); - Path path = directoryAllocator.getLocalPathForWrite(pathStr, - size, conf); + Path path = directoryAllocator.getLocalPathForWrite(pathStr, size, conf); File dir = new File(path.getParent().toUri().getPath()); - String prefix = path.getName(); - // create a temp file on this directory - return File.createTempFile(prefix, null, dir); + return safeCreateTempFile(pathStr, null, dir); + } + + // TODO remove this method when hadoop upgrades to a newer version of java than 1.8 + /** + * Ensure that the temp file prefix and suffix don't exceed the maximum number of characters + * allowed by the underlying file system. This validation isn't required in Java 9+ since + * {@link java.io.File#createTempFile(String, String, File)} automatically truncates file names. + * + * @param prefix prefix for the temporary file + * @param suffix suffix for the temporary file + * @param dir directory to create the temporary file in + * @return a unique temporary file + * @throws IOException + */ + static File safeCreateTempFile(String prefix, String suffix, File dir) throws IOException + { + // avoid validating multiple times. + // if the jvm running is version 9+ then defer to java.io.File validation implementation + if(Float.parseFloat(System.getProperty(\"java.class.version\")) >= 53) { + return File.createTempFile(prefix, null, dir); + } + + // if no suffix was defined assume the default + if(suffix == null) { + suffix = \".tmp\"; + } + // Use only the file name from the supplied prefix + prefix = (new File(prefix)).getName(); + + int prefixLength = prefix.length(); + int suffixLength = suffix.length(); + int maxRandomSuffixLen = 19; // Long.toUnsignedString(Long.MAX_VALUE).length() + + String name; + int nameMax = 255; // unable to access the underlying FS directly, so assume 255 + int excess = prefixLength + maxRandomSuffixLen + suffixLength - nameMax; + + // shorten the prefix length if the file name exceeds 255 chars Review Comment: and replace explicit size with \"too long\" ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -79,6 +82,46 @@ public void testRegularUpload() throws IOException { verifyUpload(\"regular\", 1024); } + /** + * Test that the DiskBlock's local file doesn't result in error when the S3 key exceeds the max + * char limit of the local file system. Currently + * {@link java.io.File#createTempFile(String, String, File)} is being relied on to handle the + * truncation. + * @throws IOException + */ + @Test + public void testDiskBlockCreate() throws IOException { + String s3Key = // 1024 char + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key\"; + long blockSize = getFileSystem().getDefaultBlockSize(); + try (S3ADataBlocks.BlockFactory diskBlockFactory = + new S3ADataBlocks.DiskBlockFactory(getFileSystem()); + S3ADataBlocks.DataBlock dataBlock = + diskBlockFactory.create(\"spanId\", s3Key, 1, blockSize, null); + ) { + String tmpDir = getConfiguration().get(\"hadoop.tmp.dir\"); + boolean created = Arrays.stream( + Objects.requireNonNull(new File(tmpDir).listFiles())) + .anyMatch(f -> f.getName().contains(\"very_long_s3_key\")); + assertTrue(String.format(\"tmp file should have been created locally in %s\", tmpDir), created); Review Comment: * use AssertJ. it should actually be possible to add an assert that the array matches the requirement. * should be exactly one; test suite setup should delete everything with \"very_long_s3_key\" in it; so should teardown too, but to support things like IDE debugging, its best to do both ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3ABlockOutputStream.java: ########## @@ -59,6 +63,9 @@ private S3ABlockOutputStream.BlockOutputStreamBuilder mockS3ABuilder() { mock(S3ADataBlocks.BlockFactory.class); long blockSize = Constants.DEFAULT_MULTIPART_SIZE; WriteOperationHelper oHelper = mock(WriteOperationHelper.class); + AuditSpan auditSpan = mock(AuditSpan.class); Review Comment: you can just use `org.apache.hadoop.fs.s3a.audit.impl.NoopSpan` here; one less thing to mock. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -79,6 +82,46 @@ public void testRegularUpload() throws IOException { verifyUpload(\"regular\", 1024); } + /** + * Test that the DiskBlock's local file doesn't result in error when the S3 key exceeds the max + * char limit of the local file system. Currently + * {@link java.io.File#createTempFile(String, String, File)} is being relied on to handle the + * truncation. + * @throws IOException + */ + @Test + public void testDiskBlockCreate() throws IOException { + String s3Key = // 1024 char Review Comment: 1. you could do this with a shorter string and simply concatenate it a few times 2. its duplicated in TestS3aFilesystem...it should be a constant in S3ATestConstants ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystem.java: ########## @@ -0,0 +1,110 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import org.junit.After; +import org.junit.Assert; +import org.junit.Before; +import org.junit.Rule; +import org.junit.Test; +import org.junit.rules.Timeout; + +import java.io.File; +import java.io.IOException; +import java.nio.file.Files; +import java.util.regex.Pattern; + +/** + * Unit tests for {@link S3AFileSystem}. + */ +public class TestS3AFileSystem extends Assert { + final File TEMP_DIR = new File(\"target/build/test/TestS3AFileSystem\"); + final String longStr = // 1024 char + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key\"; + final String longStrTruncated = \"very_long_s3_key__very_long_s3_key__\"; + + @Rule + public Timeout testTimeout = new Timeout(30 * 1000); + + @Before + public void init() throws IOException { + Files.createDirectories(TEMP_DIR.toPath()); + } + + @After + public void teardown() throws IOException { + File[] testOutputFiles = TEMP_DIR.listFiles(); + for(File file: testOutputFiles) { + Files.delete(file.toPath()); + } + Files.deleteIfExists(TEMP_DIR.toPath()); + } + + @Before + public void nameThread() { + Thread.currentThread().setName(\"JUnit\"); + } + + /** + * Test the {@link S3AFileSystem#safeCreateTempFile(String, String, File)}. + * The code verifies that the input prefix and suffix don't exceed the file system's max name + * length and cause an exception. + * + * This test verifies the basic contract of the process. + */ + @Test + public void testSafeCreateTempFile() throws Throwable { + // fitting name isn't changed + File noChangesRequired = S3AFileSystem.safeCreateTempFile(\"noChangesRequired\", \".tmp\", TEMP_DIR); + assertTrue(noChangesRequired.exists()); + String noChangesRequiredName = noChangesRequired.getName(); + assertTrue(noChangesRequiredName.startsWith(\"noChangesRequired\")); + assertTrue(noChangesRequiredName.endsWith(\".tmp\")); + + // a long prefix should be truncated + File excessivelyLongPrefix = S3AFileSystem.safeCreateTempFile(longStr, \".tmp\", TEMP_DIR); Review Comment: each of these should be split into their own test method so they can fail independently. I know, we ignore that in ITests, but that is because they're integration tests with longer setup overhead ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystem.java: ########## @@ -0,0 +1,110 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import org.junit.After; +import org.junit.Assert; +import org.junit.Before; +import org.junit.Rule; +import org.junit.Test; +import org.junit.rules.Timeout; + +import java.io.File; Review Comment: nit: ordering...these should come first. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystem.java: ########## @@ -0,0 +1,110 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import org.junit.After; +import org.junit.Assert; +import org.junit.Before; +import org.junit.Rule; +import org.junit.Test; +import org.junit.rules.Timeout; + +import java.io.File; +import java.io.IOException; +import java.nio.file.Files; +import java.util.regex.Pattern; + +/** + * Unit tests for {@link S3AFileSystem}. + */ +public class TestS3AFileSystem extends Assert { + final File TEMP_DIR = new File(\"target/build/test/TestS3AFileSystem\"); + final String longStr = // 1024 char + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key\"; + final String longStrTruncated = \"very_long_s3_key__very_long_s3_key__\"; + + @Rule + public Timeout testTimeout = new Timeout(30 * 1000); + + @Before + public void init() throws IOException { + Files.createDirectories(TEMP_DIR.toPath()); + } + + @After + public void teardown() throws IOException { + File[] testOutputFiles = TEMP_DIR.listFiles(); + for(File file: testOutputFiles) { + Files.delete(file.toPath()); + } + Files.deleteIfExists(TEMP_DIR.toPath()); + } + + @Before + public void nameThread() { + Thread.currentThread().setName(\"JUnit\"); + } + + /** + * Test the {@link S3AFileSystem#safeCreateTempFile(String, String, File)}. + * The code verifies that the input prefix and suffix don't exceed the file system's max name + * length and cause an exception. + * + * This test verifies the basic contract of the process. + */ + @Test + public void testSafeCreateTempFile() throws Throwable { + // fitting name isn't changed + File noChangesRequired = S3AFileSystem.safeCreateTempFile(\"noChangesRequired\", \".tmp\", TEMP_DIR); + assertTrue(noChangesRequired.exists()); Review Comment: bad news, use AssertJ. more verbose but the assertions it raised include details about the failure. which is what I insist on, sorry. If a yetus build fails, i want more than a line number. if this isn't done automatically, used .describedAs() which takes String.format() string + varargs list ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1369,12 +1369,69 @@ public S3AEncryptionMethods getS3EncryptionAlgorithm() { File createTmpFileForWrite(String pathStr, long size, Configuration conf) throws IOException { initLocalDirAllocatorIfNotInitialized(conf); - Path path = directoryAllocator.getLocalPathForWrite(pathStr, - size, conf); + Path path = directoryAllocator.getLocalPathForWrite(pathStr, size, conf); File dir = new File(path.getParent().toUri().getPath()); - String prefix = path.getName(); - // create a temp file on this directory - return File.createTempFile(prefix, null, dir); + return safeCreateTempFile(pathStr, null, dir); + } + + // TODO remove this method when hadoop upgrades to a newer version of java than 1.8 + /** + * Ensure that the temp file prefix and suffix don't exceed the maximum number of characters + * allowed by the underlying file system. This validation isn't required in Java 9+ since + * {@link java.io.File#createTempFile(String, String, File)} automatically truncates file names. + * + * @param prefix prefix for the temporary file + * @param suffix suffix for the temporary file + * @param dir directory to create the temporary file in + * @return a unique temporary file + * @throws IOException + */ + static File safeCreateTempFile(String prefix, String suffix, File dir) throws IOException + { + // avoid validating multiple times. + // if the jvm running is version 9+ then defer to java.io.File validation implementation + if(Float.parseFloat(System.getProperty(\"java.class.version\")) >= 53) { + return File.createTempFile(prefix, null, dir); + } + + // if no suffix was defined assume the default + if(suffix == null) { + suffix = \".tmp\"; + } + // Use only the file name from the supplied prefix + prefix = (new File(prefix)).getName(); + + int prefixLength = prefix.length(); + int suffixLength = suffix.length(); + int maxRandomSuffixLen = 19; // Long.toUnsignedString(Long.MAX_VALUE).length() + + String name; + int nameMax = 255; // unable to access the underlying FS directly, so assume 255 + int excess = prefixLength + maxRandomSuffixLen + suffixLength - nameMax; + + // shorten the prefix length if the file name exceeds 255 chars + if (excess > 0) { + // Attempt to shorten the prefix length to no less than 3 + prefixLength = shortenSubName(prefixLength, excess, 3); + prefix = prefix.substring(0, prefixLength); + } + // shorten the suffix if the file name still exceeds 255 chars + excess = prefixLength + maxRandomSuffixLen + suffixLength - nameMax; + if (excess > 0) { + // Attempt to shorten the suffix length to no less than 3 + suffixLength = shortenSubName(suffixLength, excess, 3); + suffix = suffix.substring(0, suffixLength); + } + + return File.createTempFile(prefix, suffix, dir); + } + + private static int shortenSubName(int subNameLength, int excess, int nameMin) { Review Comment: add javadocs. a unit test would be good too and straightforward to add. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AFileSystem.java: ########## @@ -0,0 +1,110 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a; + +import org.junit.After; +import org.junit.Assert; +import org.junit.Before; +import org.junit.Rule; +import org.junit.Test; +import org.junit.rules.Timeout; + +import java.io.File; +import java.io.IOException; +import java.nio.file.Files; +import java.util.regex.Pattern; + +/** + * Unit tests for {@link S3AFileSystem}. + */ +public class TestS3AFileSystem extends Assert { + final File TEMP_DIR = new File(\"target/build/test/TestS3AFileSystem\"); + final String longStr = // 1024 char + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key__very_long_s3_key__very_long_s3_key__very_long_s3_key__\" + + \"very_long_s3_key\"; + final String longStrTruncated = \"very_long_s3_key__very_long_s3_key__\"; + + @Rule + public Timeout testTimeout = new Timeout(30 * 1000); + + @Before + public void init() throws IOException { + Files.createDirectories(TEMP_DIR.toPath()); Review Comment: use `S3ATestUtils` code here for handling of parallel test cases and ease of future maintenance ``` Configuration conf = new Configuration(false) S3ATestUtils.prepareTestConfiguration(conf) conf.get(BUFFER_DIR) ``` ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1369,12 +1369,69 @@ public S3AEncryptionMethods getS3EncryptionAlgorithm() { File createTmpFileForWrite(String pathStr, long size, Configuration conf) throws IOException { initLocalDirAllocatorIfNotInitialized(conf); - Path path = directoryAllocator.getLocalPathForWrite(pathStr, - size, conf); + Path path = directoryAllocator.getLocalPathForWrite(pathStr, size, conf); File dir = new File(path.getParent().toUri().getPath()); - String prefix = path.getName(); - // create a temp file on this directory - return File.createTempFile(prefix, null, dir); + return safeCreateTempFile(pathStr, null, dir); + } + + // TODO remove this method when hadoop upgrades to a newer version of java than 1.8 + /** + * Ensure that the temp file prefix and suffix don't exceed the maximum number of characters + * allowed by the underlying file system. This validation isn't required in Java 9+ since + * {@link java.io.File#createTempFile(String, String, File)} automatically truncates file names. + * + * @param prefix prefix for the temporary file + * @param suffix suffix for the temporary file + * @param dir directory to create the temporary file in + * @return a unique temporary file + * @throws IOException + */ + static File safeCreateTempFile(String prefix, String suffix, File dir) throws IOException + { + // avoid validating multiple times. + // if the jvm running is version 9+ then defer to java.io.File validation implementation + if(Float.parseFloat(System.getProperty(\"java.class.version\")) >= 53) { + return File.createTempFile(prefix, null, dir); + } + + // if no suffix was defined assume the default + if(suffix == null) { + suffix = \".tmp\"; + } + // Use only the file name from the supplied prefix + prefix = (new File(prefix)).getName(); + + int prefixLength = prefix.length(); + int suffixLength = suffix.length(); + int maxRandomSuffixLen = 19; // Long.toUnsignedString(Long.MAX_VALUE).length() + + String name; + int nameMax = 255; // unable to access the underlying FS directly, so assume 255 + int excess = prefixLength + maxRandomSuffixLen + suffixLength - nameMax; + + // shorten the prefix length if the file name exceeds 255 chars + if (excess > 0) { + // Attempt to shorten the prefix length to no less than 3 + prefixLength = shortenSubName(prefixLength, excess, 3); Review Comment: again, make a constant and use in both places ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java: ########## @@ -79,6 +82,46 @@ public void testRegularUpload() throws IOException { verifyUpload(\"regular\", 1024); } + /** + * Test that the DiskBlock's local file doesn't result in error when the S3 key exceeds the max + * char limit of the local file system. Currently + * {@link java.io.File#createTempFile(String, String, File)} is being relied on to handle the Review Comment: this comment is out of date", "created": "2024-01-25T12:43:51.461+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5771: URL: https://github.com/apache/hadoop/pull/5771#issuecomment-3419090777 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-19T00:24:56.139+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5771: HADOOP-18706: S3ABlockOutputStream recovery, and downgrade syncable will call flush rather than no-op. URL: https://github.com/apache/hadoop/pull/5771", "created": "2025-10-21T00:22:44.288+0000"}], "derived_tasks": {"summary": "Improve S3ABlockOutputStream recovery - If an application crashes during an S3ABlockOutputStream upload, it's possible to complete the upload if fast", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-18701", "title": "Generic Build Improvements", "description": "Some proposed build changes. * Add\u00a0 {{surefire.failIfNoSpecifiedTests}} as false in POM, else it fails if the test specified in -Dtest isn't there in that module, creates problem when you plan to run multiple tests across multiple sub-projects from the root of the project. (https://maven.apache.org/surefire/maven-surefire-plugin/test-mojo.html#failifnospecifiedtests)   * Disable Concurrent builds: Folks push multiple commits in 5-10 mins while pre-commit is running already, so good to discourage this. * Add threshold to number of builds per day, saves resources for genuine PR's against someone pushing multiple commits. (This & the above one: Copied idea from Hive) * Leverage Github Actions to delegate some of the tasks to them, so a bit of parallel execution and might save time, may be explore pushing JDK-11 related stuff to Github Actions (We don't run tests as of now for both JDK-11 & 8, tests are for 8 only in precommit)", "status": "Open", "priority": "Major", "reporter": "Ayush Saxena", "assignee": null, "created": "2023-04-13T07:35:41.000+0000", "updated": "2025-10-24T00:20:02.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "Ayush Saxena", "body": "failIfNoSpecifiedTests looks very straight forward and I feel worth doing for sure, others are just my thoughts, looking at PRs and some ideas here & there from other projects.", "created": "2023-04-13T07:37:57.593+0000"}, {"author": "Steve Loughran", "body": "also make sure that surefire prints the full stack trace. I had to enable that in a couple of surefire invocations in HADOOP-18637; parallel hadoop-aws failsafe tests did, but not single test execution", "created": "2023-04-13T13:29:33.409+0000"}, {"author": "Ayush Saxena", "body": "Now in the latest surefire version it is by default false([https://maven.apache.org/surefire/maven-surefire-plugin/test-mojo.html#trimStackTrace]) https://issues.apache.org/jira/browse/SUREFIRE-1432, did that [~svaughan] has a PR at\u00a0HADOOP-18417, if we merge that this stack trace problem should get solved. I have retriggered the build there", "created": "2023-04-14T14:10:30.944+0000"}, {"author": "Steve Loughran", "body": "not sure if that surefire jira is the one, as our build is still at 3.0.0-M1 and i saw it on trunk. maybe some other aspect of the build has changed now homebrew put me on to maven 3.3.9. anyway, we should pre-emptively fix our build so a surefire upgrade won't lose the output we depend on.", "created": "2023-04-18T20:36:20.161+0000"}, {"author": "ASF GitHub Bot", "body": "ayushtkn opened a new pull request, #5567: URL: https://github.com/apache/hadoop/pull/5567 ### Description of PR **WIP:** Attempting basic improvements ### How was this patch tested? **WIP** Tried failIfNoSpecifiedTests prop by now ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-04-18T23:57:49.584+0000"}, {"author": "Ayush Saxena", "body": "yeps, should add it in hadoop-main. Some confusion: you too added the *trimStackTrace* as false explicitly here: https://github.com/apache/hadoop/pull/5543/files#diff-22b9fbf2d456e024bad08d789df2f55744cef1c8a8c585209b0f0a52a068350dR276 and they made it by default false in M6 in that Surefire ticket, we are on M1 that is why you had to explicitly add trying some wip stuff, yet to validate stuff.", "created": "2023-04-18T23:58:53.289+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5567: URL: https://github.com/apache/hadoop/pull/5567#issuecomment-1514212026 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 11m 33s | | Docker failed to build run-specific yetus/hadoop:tp-10388}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5567 | | Console output | [CI_URL] | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-19T06:46:44.163+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5567: URL: https://github.com/apache/hadoop/pull/5567#issuecomment-1515382608 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 40s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 47s | | trunk passed | | +1 :green_heart: | compile | 23m 7s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | mvnsite | 25m 22s | | trunk passed | | +1 :green_heart: | javadoc | 8m 7s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 7m 15s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | shadedclient | 34m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 41s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 22m 34s | | the patch passed | | +1 :green_heart: | compile | 22m 28s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 28s | | the patch passed | | +1 :green_heart: | compile | 20m 41s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 41s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 19m 53s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 7m 50s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 7m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | shadedclient | 35m 58s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 567m 37s | [/patch-unit-root.txt]([CI_URL] | root in the patch failed. | | +0 :ok: | asflicense | 0m 49s | | ASF License check generated no output? | | | | 841m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5567 | | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs compile javac javadoc mvninstall mvnsite unit shadedclient xmllint | | uname | Linux 2f9dad57855b 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6abf97bb449da6be6bfc7f4f41f576d7bb08fe44 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3098 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws . U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-19T21:08:24.408+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5567: URL: https://github.com/apache/hadoop/pull/5567#issuecomment-3434644646 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-23T00:22:40.845+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5567: HADOOP-18701. Generic Build Improvements. URL: https://github.com/apache/hadoop/pull/5567", "created": "2025-10-24T00:20:02.073+0000"}], "derived_tasks": {"summary": "Generic Build Improvements - Some proposed build changes", "classifications": ["wish", "improvement"], "qa_pairs": []}}
{"id": "HADOOP-18692", "title": "User in staticPriorities cost also shouldn't be accumulated to totalDecayedCallCost and totalRawCallCost.", "description": "After HADOOP-17165, we can avoid to restrict some important user who has many request. In HADOOP-17346, the solution is similar to HADOOP-17165, but the user is service user. And another improvement in HADOOP-17346 is that we can avoid backoff by response time. HADOOP-17280 solve the problem \"Service-user cost shouldn't be accumulated to totalDecayedCallCost and totalRawCallCost.\" for HADOOP-17165. HADOOP-17346 also fix it. I think some code is redundancy, we should reconstruct.", "status": "Open", "priority": "Major", "reporter": "Chenyu Zheng", "assignee": null, "created": "2023-04-07T01:43:58.000+0000", "updated": "2025-10-24T00:20:05.000+0000", "labels": ["pull-request-available"], "components": ["common"], "comments": [{"author": "ASF GitHub Bot", "body": "zhengchenyu opened a new pull request, #5538: URL: https://github.com/apache/hadoop/pull/5538 ### Description of PR https://issues.apache.org/jira/browse/HADOOP-18692", "created": "2023-04-07T03:01:49.417+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5538: URL: https://github.com/apache/hadoop/pull/5538#issuecomment-1499986873 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 24s | | trunk passed | | +1 :green_heart: | compile | 23m 9s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 43s | | trunk passed | | +1 :green_heart: | javadoc | 3m 15s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 1s | | the patch passed | | +1 :green_heart: | compile | 22m 30s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 30s | | the patch passed | | +1 :green_heart: | compile | 20m 32s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 6s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 6s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 23s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 210m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5538 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e0cfea3295e4 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1f612f728b564aeb310002eac04343093477e72e | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2183 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-07T06:33:26.577+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5538: URL: https://github.com/apache/hadoop/pull/5538#issuecomment-1519196266 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 40s | | trunk passed | | +1 :green_heart: | compile | 23m 12s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 41s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 1m 13s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 44s | | trunk passed | | +1 :green_heart: | javadoc | 1m 15s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 51s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 59s | | the patch passed | | +1 :green_heart: | compile | 22m 27s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 27s | | the patch passed | | +1 :green_heart: | compile | 20m 35s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 35s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 7s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 43s | | the patch passed | | +1 :green_heart: | javadoc | 1m 7s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 37s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 18s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 36s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 208m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5538 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux efe3363f43cb 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1f612f728b564aeb310002eac04343093477e72e | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3150 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-23T23:39:23.025+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5538: URL: https://github.com/apache/hadoop/pull/5538#issuecomment-3434644723 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-23T00:22:44.094+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5538: HADOOP-18692. User in staticPriorities cost also shouldn't be accumul\u2026 URL: https://github.com/apache/hadoop/pull/5538", "created": "2025-10-24T00:20:05.347+0000"}], "derived_tasks": {"summary": "User in staticPriorities cost also shouldn't be accumulated to totalDecayedCallCost and totalRawCallCost. - After HADOOP-17165, we can avoid to res...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-18675", "title": "CachedSASToken noisy log errors when SAS token has YYYY-MM-DD expiration", "description": "Error Description: When using SAS tokens with expiration dates in the format YYYY-MM-DD, a frequent error appears in the logs related to the date format. The error expects an ISO_DATE_TIME. See [existing implementation|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/CachedSASToken.java#LL112-L119]. The error is noisy in the logs, but does not cause issues. Example stacktrace:  23/03/23 15:40:06 ERROR CachedSASToken: Error parsing se query parameter (2023-11-05) from SAS. Desired Resolution: Expiration code can read YYYY-MM-DD format as well as existing format.", "status": "Open", "priority": "Major", "reporter": "Gil Cottle", "assignee": null, "created": "2023-03-23T15:59:30.000+0000", "updated": "2025-10-24T00:20:09.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "redcape opened a new pull request, #5508: URL: https://github.com/apache/hadoop/pull/5508 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR When using SAS tokens with expiration dates in the format YYYY-MM-DD, a frequent error appears in the logs. The cause is that the code expects ISO_DATE_TIME, but ISO_DATE type formats are acceptable as well. ### How was this patch tested? Added tests that the expiration is properly parsed using the previous failing format. Ran `mvn test -Dtest=\"TestCachedSASToken\"` ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-03-23T17:05:36.536+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5508: URL: https://github.com/apache/hadoop/pull/5508#issuecomment-1481719590 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 10s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 98m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5508 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f905e8ef34ab 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c67ca8ab8c798f9674b112a3ce94fc0c671e7d3d | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-23T18:44:57.654+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5508: URL: https://github.com/apache/hadoop/pull/5508#discussion_r1146691034 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TestCachedSASToken.java: ########## @@ -78,6 +78,31 @@ public void testUpdateAndGet() throws IOException { Assert.assertTrue(token3 == cachedToken); } + @Test + public void testValidExpirationParsing() { + CachedSASToken cachedSasToken = new CachedSASToken(); + String[] values = { + \"2123-03-24T00:06:46Z\", // sample timestamp from azure portal + \"2124-03-30\", // sample YYYY-MM-DD date format generated from az cli + \"2125-03-30Z\", // sample YYYY-MM-DD[offset] date format + }; + + for (String se : values) { + cachedSasToken.setForTesting(null, null); + String token = \"se=\" + se; + + // set first time and ensure reference equality + cachedSasToken.update(token); + String cachedToken = cachedSasToken.get(); + Assert.assertTrue(token == cachedToken); Review Comment: assertEquals(expected, actual) or AssertJ.asserThat(). thanks ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/CachedSASToken.java: ########## @@ -114,8 +125,20 @@ private static OffsetDateTime getExpiry(String token) { OffsetDateTime seDate = OffsetDateTime.MIN; try { seDate = OffsetDateTime.parse(seValue, DateTimeFormatter.ISO_DATE_TIME); - } catch (DateTimeParseException ex) { - LOG.error(\"Error parsing se query parameter ({}) from SAS.\", seValue, ex); + } catch (DateTimeParseException dateTimeException) { + try { + TemporalAccessor dt = isoDateMidnight.parseBest(seValue, OffsetDateTime::from, LocalDateTime::from); + if (dt instanceof OffsetDateTime) { + seDate = (OffsetDateTime) dt; + } else if (dt instanceof LocalDateTime) { + seDate = ((LocalDateTime) dt).atOffset(ZoneOffset.UTC); + } else { + throw dateTimeException; + } + } catch (DateTimeParseException dateOnlyException) { + // log original exception + LOG.error(\"Error parsing se query parameter ({}) from SAS as ISO_DATE_TIME or ISO_DATE.\", seValue, dateTimeException); Review Comment: now, if there's a problem here it is still going to log a lot, isn't it? might be best to use a LogExactlyOnce log here for the error line, and log at debug the rest of the time ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TestCachedSASToken.java: ########## @@ -78,6 +78,31 @@ public void testUpdateAndGet() throws IOException { Assert.assertTrue(token3 == cachedToken); } + @Test + public void testValidExpirationParsing() { Review Comment: is there a test for an invalid date time parsing now?", "created": "2023-03-23T19:04:49.700+0000"}, {"author": "ASF GitHub Bot", "body": "redcape commented on code in PR #5508: URL: https://github.com/apache/hadoop/pull/5508#discussion_r1146715092 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TestCachedSASToken.java: ########## @@ -78,6 +78,31 @@ public void testUpdateAndGet() throws IOException { Assert.assertTrue(token3 == cachedToken); } + @Test + public void testValidExpirationParsing() { + CachedSASToken cachedSasToken = new CachedSASToken(); + String[] values = { + \"2123-03-24T00:06:46Z\", // sample timestamp from azure portal + \"2124-03-30\", // sample YYYY-MM-DD date format generated from az cli + \"2125-03-30Z\", // sample YYYY-MM-DD[offset] date format + }; + + for (String se : values) { + cachedSasToken.setForTesting(null, null); + String token = \"se=\" + se; + + // set first time and ensure reference equality + cachedSasToken.update(token); + String cachedToken = cachedSasToken.get(); + Assert.assertTrue(token == cachedToken); Review Comment: was following the existing tests which seem to want to assert reference equality a lot, I will change it for the new test.", "created": "2023-03-23T19:22:16.639+0000"}, {"author": "ASF GitHub Bot", "body": "redcape commented on code in PR #5508: URL: https://github.com/apache/hadoop/pull/5508#discussion_r1146716906 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TestCachedSASToken.java: ########## @@ -78,6 +78,31 @@ public void testUpdateAndGet() throws IOException { Assert.assertTrue(token3 == cachedToken); } + @Test + public void testValidExpirationParsing() { Review Comment: there's an existing one testing invalid `se=abc` in `testUpdateAndGetWithInvalidToken`. I'll add another method specifically for invalid expiration.", "created": "2023-03-23T19:23:16.360+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5508: URL: https://github.com/apache/hadoop/pull/5508#issuecomment-1481809803 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 28s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 14s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 5s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 59s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 110m 35s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5508 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6fa47ce06a59 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 77444f6f02d3eddc393d14fb96cff7f8525ebfa0 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 530 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-23T19:56:23.282+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5508: URL: https://github.com/apache/hadoop/pull/5508#issuecomment-1481861371 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 96m 30s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5508 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 99c5b56892ad 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ac12f4be11773c4e6290cae43124149f9683bb75 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-23T20:36:06.942+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5508: URL: https://github.com/apache/hadoop/pull/5508#discussion_r1147315190 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/CachedSASToken.java: ########## @@ -114,8 +125,20 @@ private static OffsetDateTime getExpiry(String token) { OffsetDateTime seDate = OffsetDateTime.MIN; try { seDate = OffsetDateTime.parse(seValue, DateTimeFormatter.ISO_DATE_TIME); - } catch (DateTimeParseException ex) { - LOG.error(\"Error parsing se query parameter ({}) from SAS.\", seValue, ex); + } catch (DateTimeParseException dateTimeException) { + try { + TemporalAccessor dt = ISO_DATE_MIDNIGHT.parseBest(seValue, OffsetDateTime::from, LocalDateTime::from); + if (dt instanceof OffsetDateTime) { + seDate = (OffsetDateTime) dt; + } else if (dt instanceof LocalDateTime) { + seDate = ((LocalDateTime) dt).atOffset(ZoneOffset.UTC); + } else { + throw dateTimeException; + } + } catch (DateTimeParseException dateOnlyException) { Review Comment: Can we have something like: `private static final DateTimeFormatter[] formatters;` ``` private OffsetDateTime getParsedDateTime(String dateTime) { for(DateTimeFormatter formatter : formatters) { try { TemporalAccessor temporalAccessor = formatter.parseBest(dateTime); if(temporalAccessor instanceof OffsetDateTime) { return (OffsetDateTime) temporalAccessor; } if(temporalAccessor instanceof LocalDateTime) { return ((LocalDateTime) temporalAccessor).atOffset(ZoneOffset.UTC); } } catch (DateTimeParseException e) { } } return null; } ``` And from this method we will just call `seDate = getParsedDateTime(seValue);` This way I feel we can prevent nested code.", "created": "2023-03-24T09:28:53.198+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5508: URL: https://github.com/apache/hadoop/pull/5508#discussion_r1149159845 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TestCachedSASToken.java: ########## @@ -78,6 +78,31 @@ public void testUpdateAndGet() throws IOException { Assert.assertTrue(token3 == cachedToken); } + @Test + public void testValidExpirationParsing() { + CachedSASToken cachedSasToken = new CachedSASToken(); + String[] values = { + \"2123-03-24T00:06:46Z\", // sample timestamp from azure portal + \"2124-03-30\", // sample YYYY-MM-DD date format generated from az cli + \"2125-03-30Z\", // sample YYYY-MM-DD[offset] date format + }; + + for (String se : values) { + cachedSasToken.setForTesting(null, null); + String token = \"se=\" + se; + + // set first time and ensure reference equality + cachedSasToken.update(token); + String cachedToken = cachedSasToken.get(); + Assert.assertTrue(token == cachedToken); Review Comment: yeah, old code...just because it was done that way doesn't mean we should carry on", "created": "2023-03-27T11:14:37.392+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5508: URL: https://github.com/apache/hadoop/pull/5508#issuecomment-3434644866 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-23T00:22:48.900+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5508: HADOOP-18675: Fix CachedSASToken noisy log errors when SAS token has YYYY-MM-DD expiration URL: https://github.com/apache/hadoop/pull/5508", "created": "2025-10-24T00:20:09.043+0000"}], "derived_tasks": {"summary": "CachedSASToken noisy log errors when SAS token has YYYY-MM-DD expiration - Error Description: When using SAS tokens with expiration dates in the fo...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-18657", "title": "Tune ABFS create() retry logic", "description": "Based on experience trying to debug this happening # add debug statements when create() fails # generated exception text to reference string shared with tests, path and error code # generated exception to include inner exception for full stack trace Currently the retry logic is # create(overwrite=false) # if HTTP_CONFLICT/409 raised; call HEAD # use etag in create(path, overwrite=true, etag) # special handling of error HTTP_PRECON_FAILED = 412 There's a race condition here, which is if between 1 and 2 the file which exists is deleted. The retry should succeed, but currently a 404 from the head is escalated to a failure proposed changes # if HEAD is 404, leave etag == null and continue # special handling of 412 also to handle 409", "status": "Open", "priority": "Major", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "created": "2023-03-08T14:23:29.000+0000", "updated": "2025-10-25T00:22:17.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "steveloughran opened a new pull request, #5462: URL: https://github.com/apache/hadoop/pull/5462 ### Description of PR Tunes how abfs handles a failure during create which may be due to concurrency *or* load-related retries happening in the store. * better logging * happy with the confict being resolved by the file being deleted * more diagnostics in failure raised ### How was this patch tested? lease test run already; doing full hadoop-azure test run ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-03-08T15:16:11.469+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1460326545 fyi @saxenapranav @mehakmeet as well as improving diagnostics, this patch also changes the recovery code by handling a deletion of the target file between the first failure and the retry.", "created": "2023-03-08T15:23:51.827+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1460491719 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 57s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 93m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5462 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0304206b7a96 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a4122e276ad2264c6303eecc3584b63f865dd353 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 627 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-08T16:51:40.170+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1130438266 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { // Is a parallel access case, as file which was found to be // present went missing by this request. - throw new ConcurrentWriteOperationDetectedException( - \"Parallel access to the create path detected. Failing request \" - + \"to honor single writer semantics\"); + // this means the other thread deleted it and the conflict + // has implicitly been resolved. + LOG.debug(\"File at {} has been deleted; creation can continue\", relativePath); } else { throw ex; } } - String eTag = op.getResult() - .getResponseHeader(HttpHeaderConfigurations.ETAG); + String eTag = op != null + ? op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG) + : null; + LOG.debug(\"Attempting to create file {} with etag of {}\", relativePath, eTag); try { - // overwrite only if eTag matches with the file properties fetched befpre - op = client.createPath(relativePath, true, true, permission, umask, + // overwrite only if eTag matches with the file properties fetched or the file + // was deleted and there is no etag. + // if the etag was not retrieved, overwrite is still false, so will fail + // if another process has just created the file + op = client.createPath(relativePath, true, eTag != null, permission, umask, isAppendBlob, eTag, tracingContext); } catch (AbfsRestOperationException ex) { - if (ex.getStatusCode() == HttpURLConnection.HTTP_PRECON_FAILED) { + final int sc = ex.getStatusCode(); + LOG.debug(\"Failed to create file {} with etag {}; status code={}\", + relativePath, eTag, sc, ex); + if (sc == HttpURLConnection.HTTP_PRECON_FAILED + || sc == HttpURLConnection.HTTP_CONFLICT) { Review Comment: good that we have taken care of 409 which can come when due to `etag!=null` -> overwrite argument to `client.createPath` = false. would be awesome if we can put it in comments, and also have log according to it. log1: about some file is there whose eTag is with our process. When we went back to createPath with the same eTag, some other process had replaced that file which would lead to 412, which is present in the added code: ``` final ConcurrentWriteOperationDetectedException ex2 = new ConcurrentWriteOperationDetectedException( AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED + \" Path =\\\"\" + relativePath+ \"\\\"\" + \"; Status code =\" + sc + \"; etag = \\\"\" + eTag + \"\\\"\" + \"; error =\" + ex.getErrorMessage()); ``` suggestion to add log2: where in when we searched for etag, there was no file, now when we will try to createPath with overWrite = false, if it will give 409 in case some other process created a file on same path. Also, in case of 409, it is similar to the case we started with in this method. Should we get into 409 control as in https://github.com/apache/hadoop/blob/7f9ca101e2ae057a42829883596085732f8d5fa6/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java#L624 for a number of times. Like if we keep threshold as 2. If it happens that it gets 409 at this line, we will try once again to handle 409, post that we fail. @snvijaya @anmolanmol1234 @sreeb-msft, what you feel.", "created": "2023-03-09T05:05:35.705+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1130757159 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { // Is a parallel access case, as file which was found to be // present went missing by this request. - throw new ConcurrentWriteOperationDetectedException( - \"Parallel access to the create path detected. Failing request \" - + \"to honor single writer semantics\"); + // this means the other thread deleted it and the conflict Review Comment: There is a race condition in the job, and developer should be informed about the same. @snvijaya @anmolanmol1234 @sreeb-msft , what you feel.", "created": "2023-03-09T10:06:40.318+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1137732758 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { // Is a parallel access case, as file which was found to be // present went missing by this request. - throw new ConcurrentWriteOperationDetectedException( - \"Parallel access to the create path detected. Failing request \" - + \"to honor single writer semantics\"); + // this means the other thread deleted it and the conflict Review Comment: will do; text will indicate this may be due to a lease on the parent dir too.", "created": "2023-03-15T20:22:33.515+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1470924983 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 19s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 105m 15s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5462 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 24e1da3b49dc 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1853a46ecbb41baf82035664e30cf03584b77a64 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-15T22:23:38.477+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1508743152 any updates on this? big issue is whether to retry on 409 or not?", "created": "2023-04-14T15:07:27.547+0000"}, {"author": "ASF GitHub Bot", "body": "snvijaya commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1131060581 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { Review Comment: Hi @steveloughran, Given Hadoop is single writer semantic, would it be correct to expect that as part of job parallelization only one worker process should try to create a file ? As this check for FileNotFound is post an attempt to create the file with overwrite=false, which inturn failed with conflict indicating file was just present, concurrent operation on the file is indeed confirmed. Its quite possible that if we let this create proceed, some other operation such as delete can kick in later on as well. Below code that throws exception at the first indication of parallel activity would be the right thing to do ? As the workload pattern is not honoring the single writer semantic I feel we should retain the logic to throw ConcurrentWriteOperationDetectedException.", "created": "2023-04-18T07:15:51.664+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1545445339 reviewing this; too many other things have got in my way. I agree, with create overwrite=false, we must fail with a concurrency error what we don't want to do is overreact if we are doing overwrite=true and something does happen partway. I'll look at this in more detail, maybe focus purely on being meaningful on errors, in particular making sure that if the file is deleted before the error is raised, keep raising that concurrency error.", "created": "2023-05-12T09:22:34.376+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-3440030100 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-24T00:20:30.717+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5462: HADOOP-18657. Tune ABFS create() retry logic URL: https://github.com/apache/hadoop/pull/5462", "created": "2025-10-25T00:22:17.219+0000"}], "derived_tasks": {"summary": "Tune ABFS create() retry logic - Based on experience trying to debug this happening # add debug statements when create() fails # generated exceptio...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-18656", "title": "ABFS: Support for Pagination in Recursive Directory Delete", "description": "Today, when a recursive delete is issued for a large directory in ADLS Gen2 (HNS) account, the directory deletion happens in O(1) but in backend ACL Checks are done recursively for each object inside that directory which in case of large directory could lead to request time out. Pagination is introduced in the Azure Storage Backend for these ACL checks. More information on how pagination works can be found on public documentation of\u00a0[Azure Delete Path API|https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete?view=rest-storageservices-datalakestoragegen2-2019-12-12]. This PR contains changes to support this from client side. To trigger pagination, client needs to add a new query parameter \"paginated\" and set it to true along with recursive set to true. In return if the directory is large, server might return a continuation token back to the caller. If caller gets back a continuation token, it has to call the delete API again with continuation token along with recursive and pagination set to true. This is similar to directory delete of FNS account. Pagination is available only in versions \"2023-08-03\" onwards. PR also contains functional tests to verify driver works well with different combinations of recursive and pagination features for HNS. Full E2E testing of pagination requires large dataset to be created and hence not added as part of driver test suite. But extensive E2E testing has been performed.", "status": "Resolved", "priority": "Minor", "reporter": "Sree Bhattacharyya", "assignee": "Anuj Modi", "created": "2023-03-08T05:31:52.000+0000", "updated": "2025-10-22T00:23:15.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "Steve Loughran", "body": "isn't it an O(1) operation on a HNS store?", "created": "2023-03-08T16:23:09.043+0000"}, {"author": "Sree Bhattacharyya", "body": "Delete in itself is a constant time operation. However, a recursive delete of a directory would be preceded by ACL checks that need to parse the entire directory structure, to check permissions on every single resource. This is what requires pagination. Again, in case the user in question has superuser privileges, the entire operation would stay O(1).", "created": "2023-03-10T06:40:55.385+0000"}, {"author": "Steve Loughran", "body": "aah, i see. yes, we have hit that ACL issue; it's why the manifest committer can delete task attempt dirs in parallel, rather than just try to delete the base job dir", "created": "2023-03-13T13:02:07.294+0000"}, {"author": "Shilun Fan", "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.", "created": "2024-01-04T09:45:07.176+0000"}, {"author": "Descifrado", "body": "[HADOOP-18656: [ABFS] Adding Support for Paginated Delete for Large Directories in HNS Account by anujmodi2021 \u00b7 Pull Request #6409 \u00b7 apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/6409]", "created": "2024-01-04T14:45:15.666+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1877373746 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 42m 27s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 2 new + 6 unchanged - 0 fixed = 8 total (was 6) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 45s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 1s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 123m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d12aa8aef27c 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4580d3b12e620c2185f9096dac18f1fe60c90949 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 562 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-04T16:17:22.925+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1877455693 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 9s | | trunk passed | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 4s | | trunk passed | | +1 :green_heart: | shadedclient | 38m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 6 unchanged - 0 fixed = 7 total (was 6) | | +1 :green_heart: | mvnsite | 0m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 46s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 59s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 138m 40s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 4853f5102c8b 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e5605ac3b7beff404cf6147413c3cecac8925b54 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 603 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-04T17:03:26.867+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1442522138 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { Review Comment: Lets explain in comments why this block is required and why hns test should use `firstTestUserFs ` ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); Review Comment: Lets isolate this configSet to a given thread. Other thread in parallel should not be affected. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { Review Comment: agree string comparison helps here. But I was thinking if enums can be more useful here, wherein each enum shall have an index set to it, and the comparison can be between he set-index. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + + AbfsRestOperation op = client.deletePath(testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AUGUST_2023_API_VERSION); + } else if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) >= 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } else { Review Comment: Lets simplify this. Common block is on isPaginatedDeleteEnabled, and while asserting xMsVersionUsed, we can compare with AUGUST_2023_API_VERSION ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: lets have a package-protected method `getIsPaginated()` for `abfsConfiguration.isPaginatedDeleteEnabled()` which can be spied in the test. This would remove the need of set method in abfsConfiguration. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1191,7 +1195,12 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { - renameResilience = actualResilience; + public boolean isPaginatedDeleteEnabled() { + return isPaginatedDeleteEnabled; + } + + @VisibleForTesting Review Comment: As discussed in the client comment, lets remove this public method. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: should hns check also be added here. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + + AbfsRestOperation op = client.deletePath(testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AUGUST_2023_API_VERSION); + } else if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) >= 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testNonRecursiveDeleteWithPaginationInternal(boolean isPaginatedDeleteEnabled) throws Exception{ + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath = new Path(\"/emptyPath\"); + fs.mkdirs(testPath); + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + AbfsRestOperation op = client.deletePath(testPath.toString(), false, null, testTracingContext); + + // Getting the url that was used to make the request + String urlUsed = op.getUrl().toString(); + + // Assert that paginated query param was not set to make request + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testRecursiveDeleteWithInvalidCTInternal(boolean isPaginatedEnabled) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + Path smallDirPath = createSmallDir(); + String randomCT = \"randomContinuationToken1234\"; + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedEnabled); + + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.deletePath(smallDirPath.toString(), true, randomCT, testTracingContext)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Request Should fail with 400\").isEqualTo(HTTP_BAD_REQUEST); + } + + private void setFirstTestUserFsAuth() throws IOException { + if (this.firstTestUserFs != null) { + return; + } + checkIfConfigIsSet(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT + + \".\" + getAccountName()); + Configuration conf = getRawConfiguration(); + setTestFsConf(FS_AZURE_BLOB_FS_CLIENT_ID, FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID); + setTestFsConf(FS_AZURE_BLOB_FS_CLIENT_SECRET, Review Comment: let add clientId and secret in the method args. Reason being, we always want `setFirstTestUserFsAuth` and `setDefaultAclOnRoot` in sync. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + requestHeaders.removeIf(header -> header.getName().equalsIgnoreCase(X_MS_VERSION)); Review Comment: This would be kind of O(n) check on each delete invocation. What if we set the right version in `createDefaultHeaders`.", "created": "2024-01-05T06:42:50.293+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1442539222 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: lets have a package-protected method `getIsPaginatedDeleteEnabled()` for `abfsConfiguration.isPaginatedDeleteEnabled()` which can be spied in the test. This would remove the need of set method in abfsConfiguration.", "created": "2024-01-05T06:44:00.322+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1453027686 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: Is the support added only for hns accounts ?", "created": "2024-01-16T07:37:19.983+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1453028138 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + requestHeaders.removeIf(header -> header.getName().equalsIgnoreCase(X_MS_VERSION)); Review Comment: +1 on this", "created": "2024-01-16T07:37:55.043+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1453033220 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: yes: https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete?view=rest-storageservices-datalakestoragegen2-2019-12-12#:~:text=Optional%20and%20valid%20only%20if%20Hierarchical%20Namespace%20is%20enabled%20for%20the%20account%20and%20resource%20is%20a%20directory%20with%20%22recursive%22%20query%20parameter%20set%20to%20%22true%22.", "created": "2024-01-16T07:44:00.761+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1453051679 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); Review Comment: Is the true true combination missing for AUGUST_2023 and true false for DECEMBER 2019 or not added intentionally ? false false as well for AUGUST_2019", "created": "2024-01-16T08:04:17.944+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1456987897 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + + AbfsRestOperation op = client.deletePath(testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AUGUST_2023_API_VERSION); + } else if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) >= 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testNonRecursiveDeleteWithPaginationInternal(boolean isPaginatedDeleteEnabled) throws Exception{ + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath = new Path(\"/emptyPath\"); + fs.mkdirs(testPath); + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + AbfsRestOperation op = client.deletePath(testPath.toString(), false, null, testTracingContext); + + // Getting the url that was used to make the request + String urlUsed = op.getUrl().toString(); + + // Assert that paginated query param was not set to make request + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testRecursiveDeleteWithInvalidCTInternal(boolean isPaginatedEnabled) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + Path smallDirPath = createSmallDir(); + String randomCT = \"randomContinuationToken1234\"; + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedEnabled); + + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.deletePath(smallDirPath.toString(), true, randomCT, testTracingContext)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Request Should fail with 400\").isEqualTo(HTTP_BAD_REQUEST); Review Comment: Rather than 400 we should mention Bad Request", "created": "2024-01-18T06:53:28.788+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1456987897 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + + AbfsRestOperation op = client.deletePath(testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AUGUST_2023_API_VERSION); + } else if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) >= 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testNonRecursiveDeleteWithPaginationInternal(boolean isPaginatedDeleteEnabled) throws Exception{ + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath = new Path(\"/emptyPath\"); + fs.mkdirs(testPath); + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + AbfsRestOperation op = client.deletePath(testPath.toString(), false, null, testTracingContext); + + // Getting the url that was used to make the request + String urlUsed = op.getUrl().toString(); + + // Assert that paginated query param was not set to make request + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testRecursiveDeleteWithInvalidCTInternal(boolean isPaginatedEnabled) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + Path smallDirPath = createSmallDir(); + String randomCT = \"randomContinuationToken1234\"; + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedEnabled); + + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.deletePath(smallDirPath.toString(), true, randomCT, testTracingContext)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Request Should fail with 400\").isEqualTo(HTTP_BAD_REQUEST); Review Comment: Rather than 400 we should mention Bad Request error message", "created": "2024-01-18T06:53:43.852+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464712357 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); Review Comment: Agreed... Created a spied Client here and doing config set on the spied client only.", "created": "2024-01-24T10:41:07.116+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464712819 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: Added check for HNS as well", "created": "2024-01-24T10:41:27.105+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464713380 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { Review Comment: Nice suggestion... Added a new enum class to define all the versions currently in use ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + requestHeaders.removeIf(header -> header.getName().equalsIgnoreCase(X_MS_VERSION)); Review Comment: Taken", "created": "2024-01-24T10:41:52.156+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464714707 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1191,7 +1195,12 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { - renameResilience = actualResilience; + public boolean isPaginatedDeleteEnabled() { + return isPaginatedDeleteEnabled; + } + + @VisibleForTesting Review Comment: What is the issue with having a public method @VisibleForTesting here?? I see we have it for other configs as well.", "created": "2024-01-24T10:42:52.242+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464716396 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: I have defined a function in client which determines if pagination has to be enabled based o 3 things: 1. config set 2. account type 3. recursive delete or not. Spying that will not allow me test the logic for determining if pagination should be enabled or not.", "created": "2024-01-24T10:44:17.317+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464722105 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); Review Comment: They are not added intentionally. These combinations are enough to test if xMsVersion is set properly or not.", "created": "2024-01-24T10:48:12.743+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1908060057 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 57s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 3 new + 6 unchanged - 0 fixed = 9 total (was 6) | | +1 :green_heart: | mvnsite | 0m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 122m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ad3a04c648ae 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1b0e9a30f9d9016671c0b33dbc61c09c0530b006 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 676 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-24T12:48:53.566+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464894704 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1191,7 +1195,12 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { - renameResilience = actualResilience; + public boolean isPaginatedDeleteEnabled() { + return isPaginatedDeleteEnabled; + } + + @VisibleForTesting Review Comment: Problem is that its a setter. And, it can be invoked from outside abfs codebase : `fs.getAbfsStore(). getAbfsConfiguration().setIsPaginatedDeleteEnabled()`. Outside abfs should not be able to change the config. Other setters (except one or two) are non-public. Requesting you to kindly remove it. The same behavior that you want to set / unset can be done by change in AbfsClient as suggested in: https://github.com/apache/hadoop/pull/6409#discussion_r1442539222 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { Review Comment: It is still string comparison. My original proposal was to assign an index to each enum, and then we compare on the basis of the index. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: Great you have added private method `isPaginatedDeleteEnabled` which computes the conditions. What I am proposing is that there be a package-protected method `getIsPaginatedDeleteEnabled()` which you can refactor with `abfsConfiguration.isPaginatedDeleteEnabled()`. Something like: ``` private Boolean isPaginatedDeleteEnabled(TracingContext tracingContext, boolean isRecursiveDelete) throws AzureBlobFileSystemException { return getIsPaginatedDeleteEnabled() && getIsNamespaceEnabled(tracingContext) && isRecursiveDelete; } ... Boolean getIsPaginatedDeleteEnabled() { return abfsConfiguration.isPaginatedDeleteEnabled(); } ``` ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,279 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.AUGUST_2023_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DECEMBER_2019_API_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + private String firstTestUserGuid; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + this.firstTestUserGuid = getConfiguration() + .get(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + + if(isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(); + setDefaultAclOnRoot(this.firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, true, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(false, false, AUGUST_2023_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, true, DECEMBER_2019_API_VERSION); + testRecursiveDeleteWithPaginationInternal(true, false, AUGUST_2023_API_VERSION); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + public void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, boolean isPaginatedDeleteEnabled, + String xMsVersion) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = ITestAbfsClient.setAbfsClientField( + fs.getAbfsStore().getClient(), \"xMsVersion\", xMsVersion); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + + AbfsRestOperation op = client.deletePath(testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AUGUST_2023_API_VERSION); + } else if (isPaginatedDeleteEnabled && xMsVersion.compareTo(AUGUST_2023_API_VERSION) >= 0) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testNonRecursiveDeleteWithPaginationInternal(boolean isPaginatedDeleteEnabled) throws Exception{ + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + Path testPath = new Path(\"/emptyPath\"); + fs.mkdirs(testPath); + + // Set the paginated enabled value and xMsVersion at client level. + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedDeleteEnabled); + AbfsRestOperation op = client.deletePath(testPath.toString(), false, null, testTracingContext); + + // Getting the url that was used to make the request + String urlUsed = op.getUrl().toString(); + + // Assert that paginated query param was not set to make request + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); + } + + public void testRecursiveDeleteWithInvalidCTInternal(boolean isPaginatedEnabled) throws Exception { + final AzureBlobFileSystem fs = isHnsEnabled ? this.firstTestUserFs : getFileSystem(); + Path smallDirPath = createSmallDir(); + String randomCT = \"randomContinuationToken1234\"; + TracingContext testTracingContext = getTestTracingContext(this.firstTestUserFs, true); + + AbfsClient client = fs.getAbfsStore().getClient(); + client.getAbfsConfiguration().setIsPaginatedDeleteEnabled(isPaginatedEnabled); + + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + client.deletePath(smallDirPath.toString(), true, randomCT, testTracingContext)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Request Should fail with 400\").isEqualTo(HTTP_BAD_REQUEST); + } + + private void setFirstTestUserFsAuth() throws IOException { + if (this.firstTestUserFs != null) { + return; + } + checkIfConfigIsSet(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT + + \".\" + getAccountName()); + Configuration conf = getRawConfiguration(); + setTestFsConf(FS_AZURE_BLOB_FS_CLIENT_ID, FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID); + setTestFsConf(FS_AZURE_BLOB_FS_CLIENT_SECRET, Review Comment: Are UID, clientId, and secret not related. if yes, requesting you to supply clientId and secret from the `setup()` method.", "created": "2024-01-24T13:33:06.741+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464989125 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { Review Comment: This is not string comparison, this is ordinal comparison. Every enum defined has an inbuild index called ordinal which is nothing but its position in the enum set. The comparison here is comparing the position of enum in enum set. That's why we have to add versions in chronological order.", "created": "2024-01-24T14:24:06.321+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464714707 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1191,7 +1195,12 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { - renameResilience = actualResilience; + public boolean isPaginatedDeleteEnabled() { + return isPaginatedDeleteEnabled; + } + + @VisibleForTesting Review Comment: What is the issue with having a public method @ VisibleForTesting here?? I see we have it for other configs as well.", "created": "2024-01-24T14:24:51.360+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464996429 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1191,7 +1195,12 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { - renameResilience = actualResilience; + public boolean isPaginatedDeleteEnabled() { + return isPaginatedDeleteEnabled; + } + + @VisibleForTesting Review Comment: Taken", "created": "2024-01-24T14:28:56.477+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1464997079 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { Review Comment: Taken", "created": "2024-01-24T14:29:21.483+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1465791387 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1053,12 +1053,24 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (abfsConfiguration.isPaginatedDeleteEnabled() && recursive) { + // Change the x-ms-version to \"2023-08-03\" if its less than that. + if (xMsVersion.compareTo(AUGUST_2023_API_VERSION) < 0) { Review Comment: Got it. Nice way to achieve the required need. Thank you for the clarification!", "created": "2024-01-25T03:14:52.004+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1909584344 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 4 new + 6 unchanged - 0 fixed = 10 total (was 6) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 8s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 59s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 123m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c984caf97208 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 42010aa466746fe4b6d2ebd7db0d1e6167ca7764 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 551 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-25T08:00:41.288+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1909680123 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 29s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 38m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 6 unchanged - 0 fixed = 7 total (was 6) | | +1 :green_heart: | mvnsite | 0m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 37m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 58s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 139m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 29935bf1492c 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / db957389a25f246b4fff0c6b399d56453f338232 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 563 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-25T08:50:54.918+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1909702726 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 50m 25s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 37s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 38m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 6 unchanged - 0 fixed = 7 total (was 6) | | +1 :green_heart: | mvnsite | 0m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 143m 3s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6c13c25f24bd 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 197f3bcbbec9d21c62f063dcee893d7cb1c547ce | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 707 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-25T09:04:54.084+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1910010681 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 58s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 30s | | trunk passed | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 8s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 6 unchanged - 0 fixed = 7 total (was 6) | | +1 :green_heart: | mvnsite | 0m 28s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 22s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 1s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 121m 59s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9339a5fbd571 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1ab45fa6343269d01eef5a64682eb10b22586ee3 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 702 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-01-25T11:44:06.614+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1971141450 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 54s | | trunk passed | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 32m 55s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 33m 16s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 12s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 124m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 12c3a833abd7 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 169e1d7e6effb65c77abded66191e50f106c5cdb | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 620 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-02-29T13:28:35.632+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-1972931402 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 17s | | trunk passed | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 35m 30s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 35m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 129m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.44 ServerAPI=1.44 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0fd962d8422b 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2b502e4f127af68c0b381f14101b8af0ffb9b72b | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 698 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-03-01T10:29:26.843+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2014318061 Hi @steveloughran @mukund-thakur Requesting you review on this PR. Thanks", "created": "2024-03-22T04:08:27.765+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2015255222 hey, in #6494 i'm drafting a bulk delete API where the caller (iceberg etc) can give a list of file paths for deletion with no guarantees about safety checks, parent dirs existing afterwards etc. Would this work here too?", "created": "2024-03-22T14:42:55.033+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1537887411 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1117,12 +1117,22 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { - final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - + final List<AbfsHttpHeader> requestHeaders + = (isPaginatedDelete(tracingContext, recursive) + && xMsVersion.compareTo(ApiVersion.AUG_03_2023) < 0) + ? createDefaultHeaders(ApiVersion.AUG_03_2023) + : createDefaultHeaders(xMsVersion); final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (isPaginatedDelete(tracingContext, recursive)) { Review Comment: what if this is a paginated delete but the L1126 api version test doesn't hold? Is that ok? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean( + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + + // Test User Credentials. + String firstTestUserGuid = getConfiguration().get( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + String clientId = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID, \"\"); + String clientSecret = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET, \"\"); + + if (isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(clientId, clientSecret); + setDefaultAclOnRoot(firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(true, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(true, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + private void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, Review Comment: javadocs ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; Review Comment: nit: import group ordering. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { Review Comment: can you add javadocs on the class, fields and methods so that the next person that comes to maintain this can understand it. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -121,8 +121,34 @@ public final class AbfsHttpConstants { public static final char CHAR_EQUALS = '='; public static final char CHAR_STAR = '*'; public static final char CHAR_PLUS = '+'; - public static final String DECEMBER_2019_API_VERSION = \"2019-12-12\"; - public static final String APRIL_2021_API_VERSION = \"2021-04-10\"; + + /** + * Specifies the version of the REST protocol used for processing the request. + * Versions should be added in enum list in ascending chronological order. + * Latest one should be added last in the list. + * When upgrading the version for whole driver, update the getCurrentVersion; + */ + public enum ApiVersion { + + DEC_12_2019(\"2019-12-12\"), + APR_10_2021(\"2021-04-10\"), + AUG_03_2023(\"2023-08-03\"); + + private final String xMsApiVersion; + + ApiVersion(String xMsApiVersion) { + this.xMsApiVersion = xMsApiVersion; + } + + @Override + public String toString() { + return xMsApiVersion; + } + + public static ApiVersion getCurrentVersion() { + return DEC_12_2019; Review Comment: that's the oldest one, not the latest. is it still \"current?\" ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean( + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + + // Test User Credentials. + String firstTestUserGuid = getConfiguration().get( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + String clientId = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID, \"\"); + String clientSecret = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET, \"\"); + + if (isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(clientId, clientSecret); + setDefaultAclOnRoot(firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(true, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(true, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + private void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, + boolean isPaginatedDeleteEnabled, + AbfsHttpConstants.ApiVersion xMsVersion) throws Exception { + final AzureBlobFileSystem fs = getUserFileSystem(); + TracingContext testTracingContext = getTestTracingContext(fs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at spiedClient level. + AbfsClient spiedClient = Mockito.spy(fs.getAbfsStore().getClient()); + ITestAbfsClient.setAbfsClientField(spiedClient, \"xMsVersion\", xMsVersion); + Mockito.doReturn(isPaginatedDeleteEnabled).when(spiedClient).getIsPaginatedDeleteEnabled(); + + AbfsRestOperation op = spiedClient.deletePath( + testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) < 0) { + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AbfsHttpConstants.ApiVersion.AUG_03_2023.toString()); + } else if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) >= 0) { + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion.toString()); + } + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion.toString()); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + spiedClient.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); Review Comment: * Factor this assertion out into a method here and below, something like `assertStatusCode(AbfsRestOperationException, int)` * include e.toString() in message, or just rethrow the exception with its full stack trace. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientUtils.java: ########## @@ -31,4 +33,13 @@ public static void setIsNamespaceEnabled(final AbfsClient abfsClient, final Bool public static void setEncryptionContextProvider(final AbfsClient abfsClient, final EncryptionContextProvider provider) { abfsClient.setEncryptionContextProvider(provider); } + + public static String getHeaderValue(List<AbfsHttpHeader> reqHeaders, String headerName) { + for (AbfsHttpHeader header : reqHeaders) { Review Comment: you could probably do something involving java 8 streaming/filtering here if you wanted to... ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1117,12 +1117,22 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { - final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - + final List<AbfsHttpHeader> requestHeaders Review Comment: add a comment explaining what is happening, and move the = sign to this line, leaving ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -259,9 +259,9 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } - List<AbfsHttpHeader> createDefaultHeaders() { + List<AbfsHttpHeader> createDefaultHeaders(ApiVersion xMsVersion) { Review Comment: 1. Can you add a Javadoc here? 2. as most invocations use the xMsVersion value, why not retain the original signature as an overloaded invocation -avoids having to change so much of the code. 3. is this package private just for testing? if so, let's mark it as @VisibleForTesting ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -121,8 +121,34 @@ public final class AbfsHttpConstants { public static final char CHAR_EQUALS = '='; public static final char CHAR_STAR = '*'; public static final char CHAR_PLUS = '+'; - public static final String DECEMBER_2019_API_VERSION = \"2019-12-12\"; Review Comment: this is a `@InterfaceAudience.Public` class, you can't remove this. What you can do here and below is * declare the value underneath the new enum and set it from the appropriate enum * tag as deprecated", "created": "2024-03-25T16:45:50.763+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1546222398 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -121,8 +121,34 @@ public final class AbfsHttpConstants { public static final char CHAR_EQUALS = '='; public static final char CHAR_STAR = '*'; public static final char CHAR_PLUS = '+'; - public static final String DECEMBER_2019_API_VERSION = \"2019-12-12\"; - public static final String APRIL_2021_API_VERSION = \"2021-04-10\"; + + /** + * Specifies the version of the REST protocol used for processing the request. + * Versions should be added in enum list in ascending chronological order. + * Latest one should be added last in the list. + * When upgrading the version for whole driver, update the getCurrentVersion; + */ + public enum ApiVersion { + + DEC_12_2019(\"2019-12-12\"), + APR_10_2021(\"2021-04-10\"), + AUG_03_2023(\"2023-08-03\"); + + private final String xMsApiVersion; + + ApiVersion(String xMsApiVersion) { + this.xMsApiVersion = xMsApiVersion; + } + + @Override + public String toString() { + return xMsApiVersion; + } + + public static ApiVersion getCurrentVersion() { + return DEC_12_2019; Review Comment: Yes, this is the default current version used in driver. This PR does not have scope to change the default version for all the Rest operations. Here using the required version for paginated delete and retaining the version for all other APIs. Upgrading the API version throughout the driver can be taken up in a separate PR as it can affect all the APIs and will require proper testing.", "created": "2024-04-01T11:11:26.698+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1546230961 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1117,12 +1117,22 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { - final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - + final List<AbfsHttpHeader> requestHeaders + = (isPaginatedDelete(tracingContext, recursive) + && xMsVersion.compareTo(ApiVersion.AUG_03_2023) < 0) + ? createDefaultHeaders(ApiVersion.AUG_03_2023) + : createDefaultHeaders(xMsVersion); final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder(); + + if (isPaginatedDelete(tracingContext, recursive)) { Review Comment: If this is a paginated delete, then API version change condition will only fail if current version is greater than AUG_03_2023. In that case we can go ahead with Current API Version only as Azure APIs are backward compatible,", "created": "2024-04-01T11:23:35.978+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547534548 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -121,8 +121,34 @@ public final class AbfsHttpConstants { public static final char CHAR_EQUALS = '='; public static final char CHAR_STAR = '*'; public static final char CHAR_PLUS = '+'; - public static final String DECEMBER_2019_API_VERSION = \"2019-12-12\"; Review Comment: Added it back as deprecated. Thanks for pointing this out", "created": "2024-04-02T09:58:06.983+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547545849 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { Review Comment: Added javadocs", "created": "2024-04-02T10:05:52.377+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547547655 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean( + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + + // Test User Credentials. + String firstTestUserGuid = getConfiguration().get( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + String clientId = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID, \"\"); + String clientSecret = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET, \"\"); + + if (isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(clientId, clientSecret); + setDefaultAclOnRoot(firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(true, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(true, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + private void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, + boolean isPaginatedDeleteEnabled, + AbfsHttpConstants.ApiVersion xMsVersion) throws Exception { + final AzureBlobFileSystem fs = getUserFileSystem(); + TracingContext testTracingContext = getTestTracingContext(fs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at spiedClient level. + AbfsClient spiedClient = Mockito.spy(fs.getAbfsStore().getClient()); + ITestAbfsClient.setAbfsClientField(spiedClient, \"xMsVersion\", xMsVersion); + Mockito.doReturn(isPaginatedDeleteEnabled).when(spiedClient).getIsPaginatedDeleteEnabled(); + + AbfsRestOperation op = spiedClient.deletePath( + testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) < 0) { + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AbfsHttpConstants.ApiVersion.AUG_03_2023.toString()); + } else if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) >= 0) { + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion.toString()); + } + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion.toString()); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + spiedClient.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); Review Comment: Makes sense. Factored out to new method. Added the e.toString() in message so that it will be a part of assertion error in case assertion fails.", "created": "2024-04-02T10:06:52.437+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547547976 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; Review Comment: Fixed", "created": "2024-04-02T10:07:07.661+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547560589 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -1117,12 +1117,22 @@ public AbfsRestOperation read(final String path, return op; } - public AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, + public AbfsRestOperation deletePath(final String path, final boolean recursive, + final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException { - final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders(); - + final List<AbfsHttpHeader> requestHeaders Review Comment: Added a comment", "created": "2024-04-02T10:15:02.876+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547563402 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -259,9 +259,9 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } - List<AbfsHttpHeader> createDefaultHeaders() { + List<AbfsHttpHeader> createDefaultHeaders(ApiVersion xMsVersion) { Review Comment: Taken all", "created": "2024-04-02T10:17:22.995+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2031631438 > how does the pagination work here? do repeated calls need to be made? if so, where is this done? it wasn't immediately obvious to me. Paginated delete will work somehow similar to how recursive delete works for FNS accounts. For HNS Accounts, recursive delete is supposed to be a O(1) operation i.e deleting the folder itself. But before deleting the whole folder, server needs to do ACL checks on all the children of that folder which is not O(1). If the directory is large, this ACL check can take some time and request can timeout. To avoid this, server will return a continuation token if ACL check is still pending. Client need to loop delete on this continuation token similar to how it loop delete for FNS account recursive delete. Difference is that for FNS every call does delete some objects, in HNS it only performs ACL checks and actual delete of directory happens on last delete call of loop. Hope that explains. Repeated calls are made in abfsStore.deletePath() where continuation token check is present. This is common code for FNS and HNS.", "created": "2024-04-02T10:25:25.529+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547583537 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; Review Comment: Fixed", "created": "2024-04-02T10:29:58.677+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547583929 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean( + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + + // Test User Credentials. + String firstTestUserGuid = getConfiguration().get( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + String clientId = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID, \"\"); + String clientSecret = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET, \"\"); + + if (isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(clientId, clientSecret); + setDefaultAclOnRoot(firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(true, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(true, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + private void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, Review Comment: Added from where it is called ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + private AzureBlobFileSystem superUserFs; + private AzureBlobFileSystem firstTestUserFs; + + private boolean isHnsEnabled; + public ITestAbfsPaginatedDelete() throws Exception { + } + + @Override + public void setup() throws Exception { + isHnsEnabled = this.getConfiguration().getBoolean( + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + loadConfiguredFileSystem(); + super.setup(); + this.superUserFs = getFileSystem(); + + // Test User Credentials. + String firstTestUserGuid = getConfiguration().get( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + String clientId = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID, \"\"); + String clientSecret = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET, \"\"); + + if (isHnsEnabled) { + // setting up ACL permissions for test user + setFirstTestUserFsAuth(clientId, clientSecret); + setDefaultAclOnRoot(firstTestUserGuid); + } + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(true, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(true, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + private void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, + boolean isPaginatedDeleteEnabled, + AbfsHttpConstants.ApiVersion xMsVersion) throws Exception { + final AzureBlobFileSystem fs = getUserFileSystem(); + TracingContext testTracingContext = getTestTracingContext(fs, true); + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + fs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at spiedClient level. + AbfsClient spiedClient = Mockito.spy(fs.getAbfsStore().getClient()); + ITestAbfsClient.setAbfsClientField(spiedClient, \"xMsVersion\", xMsVersion); + Mockito.doReturn(isPaginatedDeleteEnabled).when(spiedClient).getIsPaginatedDeleteEnabled(); + + AbfsRestOperation op = spiedClient.deletePath( + testPath.toString(), true, null, testTracingContext); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) < 0) { + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(AbfsHttpConstants.ApiVersion.AUG_03_2023.toString()); + } else if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) >= 0) { + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion.toString()); + } + } else { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must not have paginated = true as query param\") + .doesNotContain(QUERY_PARAM_PAGINATED); + Assertions.assertThat(xMsVersionUsed) + .describedAs(\"Request was made with wrong x-ms-version\") + .isEqualTo(xMsVersion.toString()); + } + + // Assert that deletion was successful in every scenario. + AbfsRestOperationException e = intercept(AbfsRestOperationException.class, () -> + spiedClient.getPathStatus(testPath.toString(), false, testTracingContext, null)); + Assertions.assertThat(e.getStatusCode()) + .describedAs(\"Path should have been deleted\").isEqualTo(HTTP_NOT_FOUND); Review Comment: Taken", "created": "2024-04-02T10:30:13.713+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1547584244 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,289 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import org.assertj.core.api.Assertions; +import org.junit.Assume; +import org.junit.Test; +import org.mockito.Mockito; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { Review Comment: Javadocs Added ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java: ########## @@ -121,8 +121,34 @@ public final class AbfsHttpConstants { public static final char CHAR_EQUALS = '='; public static final char CHAR_STAR = '*'; public static final char CHAR_PLUS = '+'; - public static final String DECEMBER_2019_API_VERSION = \"2019-12-12\"; Review Comment: Taken", "created": "2024-04-02T10:30:28.694+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2031731782 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 51s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 6 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 25s | | trunk passed | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 33m 16s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 33m 37s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 18 unchanged - 0 fixed = 19 total (was 18) | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 33m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 14s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 128m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9a61f5531419 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d7d9d065dd66ab82a5685f36531b1c91e84265dd | | Default Java | Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 730 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-02T11:14:32.527+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2031900895 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 6 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 49m 42s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 7s | | trunk passed | | +1 :green_heart: | shadedclient | 39m 59s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 40m 20s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 18 unchanged - 0 fixed = 19 total (was 18) | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 12s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 146m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux dde2b05887b6 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f500632b9b690fc139fb724da63bccbca64f5d2f | | Default Java | Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 577 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-02T12:22:28.770+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2031907051 > hey, in #6494 i'm drafting a bulk delete API where the caller (iceberg etc) can give a list of file paths for deletion with no guarantees about safety checks, parent dirs existing afterwards etc. Would this work here too? From what I understood from your PR, this seems a bit different from bulk delete. Paginated delete will be supported here but caller won't be able to specify a list of paths that they want to delete. Only one path that too a directory path can be passes and everything inside that directory will be deleted. Pagination here is only for performing ACL checks and not actual delete. Delete will still be a single operation performed after ACL check is completed on whole directory listing. In case ACL checks ail in between after a few pages, whole delete operation will fail it won't delete any object. This way this is an atomic delete.", "created": "2024-04-02T12:25:10.922+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2031973745 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 6 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 49m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 29s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 1s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 40m 22s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 26s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 18 unchanged - 0 fixed = 19 total (was 18) | | +1 :green_heart: | mvnsite | 0m 29s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 146m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 68fb2c7d0f69 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a31d3ae0722b103c16f10e6861850245ee64fd40 | | Default Java | Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 569 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-02T12:57:03.250+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2034847753 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 6 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 36s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | +1 :green_heart: | javadoc | 0m 38s | | trunk passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 5s | | trunk passed | | +1 :green_heart: | shadedclient | 42m 40s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 43m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 38m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 149m 42s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6409 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 78700f0fcb82 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 47f6623fc6c1a0fa8947a231a078b25a1c2befb8 | | Default Java | Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 528 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-03T14:54:58.337+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1552243088 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1240,8 +1244,8 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { Review Comment: any reason to cut this? I presume it means no tests are using it... ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsPaginatedDelete.java: ########## @@ -0,0 +1,333 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import java.io.IOException; +import java.util.List; +import java.util.UUID; + +import org.assertj.core.api.Assertions; +import org.junit.Test; +import org.mockito.Mockito; + +import org.apache.commons.lang3.StringUtils; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException; +import org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider; +import org.apache.hadoop.fs.azurebfs.utils.AclTestHelpers; +import org.apache.hadoop.fs.azurebfs.utils.TracingContext; +import org.apache.hadoop.fs.permission.AclEntry; +import org.apache.hadoop.fs.permission.AclEntryScope; +import org.apache.hadoop.fs.permission.AclEntryType; +import org.apache.hadoop.fs.permission.FsAction; +import org.apache.hadoop.util.Lists; + +import static java.net.HttpURLConnection.HTTP_BAD_REQUEST; +import static java.net.HttpURLConnection.HTTP_NOT_FOUND; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT; +import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemUriSchemes.ABFS_SECURE_SCHEME; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.X_MS_VERSION; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_PAGINATED; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_ID; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_BLOB_FS_CLIENT_SECRET; +import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT; +import static org.apache.hadoop.fs.azurebfs.services.AbfsClientUtils.getHeaderValue; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +/** + * Tests to verify server side pagination feature is supported from driver. + */ +public class ITestAbfsPaginatedDelete extends AbstractAbfsIntegrationTest { + + /** + * File system using super-user OAuth, used to create the directory. + */ + private AzureBlobFileSystem superUserFs; + + /** + * File system using NoRBAC user OAuth, used to delete the directory. + * This user will have default ACL permissions set on root path including delete. + * Since this is not a super-user, azure servers will trigger recursive ACL + * checks on root path when delete is called using this user OAuth token. + */ + private AzureBlobFileSystem testUserFs; + + /** + * Service supports Pagination only for HNS Accounts. + */ + private boolean isHnsEnabled; + + public ITestAbfsPaginatedDelete() throws Exception { + } + + /** + * Create file system instances for both super-user and test user. + * @throws Exception + */ + @Override + public void setup() throws Exception { + super.setup(); + this.superUserFs = getFileSystem(); + + assumeValidTestConfigPresent(this.getRawConfiguration(), + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT); + isHnsEnabled = this.getConfiguration().getBoolean( + FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false); + + assumeTestUserCredentialsConfigured(); + this.testUserFs = isHnsEnabled ? createTestUserFs() : null; + } + + private AzureBlobFileSystem createTestUserFs() throws IOException { + // Test User Credentials. + String firstTestUserGuid = getConfiguration().get( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID); + String clientId = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID, \"\"); + String clientSecret = getConfiguration().getString( + FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET, \"\"); + + Configuration testUserConf = new Configuration(getRawConfiguration()); + setTestUserConf(testUserConf, FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.OAuth.name()); + setTestUserConf(testUserConf, FS_AZURE_BLOB_FS_CLIENT_ID, clientId); + setTestUserConf(testUserConf, FS_AZURE_BLOB_FS_CLIENT_SECRET, clientSecret); + setTestUserConf(testUserConf, FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME, + ClientCredsTokenProvider.class.getName()); + + testUserConf.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, false); + testUserConf.setBoolean(String.format(\"fs.%s.impl.disable.cache\", ABFS_SECURE_SCHEME), true); + + setDefaultAclOnRoot(firstTestUserGuid); + return (AzureBlobFileSystem) FileSystem.newInstance(testUserConf); + } + + private void setTestUserConf(Configuration conf, String key, String value) { + conf.set(key, value); + conf.set(key + \".\" + getAccountName(), value); + } + + /** + * Test to check that recursive deletePath works with paginated enabled and + * disabled for both empty and non-empty directory. + * When enabled appropriate xMsVersion should be used. + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithPagination() throws Exception { + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, true, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(false, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + testRecursiveDeleteWithPaginationInternal(true, true, + AbfsHttpConstants.ApiVersion.DEC_12_2019); + testRecursiveDeleteWithPaginationInternal(true, false, + AbfsHttpConstants.ApiVersion.AUG_03_2023); + } + + /** + * Test to check that non-recursive delete works with both paginated enabled + * and disabled only for empty directories. + * Pagination should not be set when recursive is false. + * @throws Exception + */ + @Test + public void testNonRecursiveDeleteWithPagination() throws Exception { + testNonRecursiveDeleteWithPaginationInternal(true); + testNonRecursiveDeleteWithPaginationInternal(false); + } + + /** + * Test to check that with pagination enabled, invalid CT will fail + * @throws Exception + */ + @Test + public void testRecursiveDeleteWithInvalidCT() throws Exception { + testRecursiveDeleteWithInvalidCTInternal(true); + testRecursiveDeleteWithInvalidCTInternal(false); + } + + private void testRecursiveDeleteWithPaginationInternal(boolean isEmptyDir, + boolean isPaginatedDeleteEnabled, AbfsHttpConstants.ApiVersion xMsVersion) + throws Exception { + final AzureBlobFileSystem fs = getUserFileSystem(); + TracingContext testTC = getTestTracingContext(fs, true); + + Path testPath; + if (isEmptyDir) { + testPath = new Path(\"/emptyPath\" + StringUtils.right( + UUID.randomUUID().toString(), 10)); + superUserFs.mkdirs(testPath); + } else { + testPath = createSmallDir(); + } + + // Set the paginated enabled value and xMsVersion at spiedClient level. + AbfsClient spiedClient = Mockito.spy(fs.getAbfsStore().getClient()); + ITestAbfsClient.setAbfsClientField(spiedClient, \"xMsVersion\", xMsVersion); + Mockito.doReturn(isPaginatedDeleteEnabled).when(spiedClient).getIsPaginatedDeleteEnabled(); + + AbfsRestOperation op = spiedClient.deletePath( + testPath.toString(), true, null, testTC, isHnsEnabled); + + // Getting the xMsVersion that was used to make the request + String xMsVersionUsed = getHeaderValue(op.getRequestHeaders(), X_MS_VERSION); + String urlUsed = op.getUrl().toString(); + + // Assert that appropriate xMsVersion and query param was used to make request + if (isPaginatedDeleteEnabled && isHnsEnabled) { + Assertions.assertThat(urlUsed) + .describedAs(\"Url must have paginated = true as query param\") + .contains(QUERY_PARAM_PAGINATED); + if (xMsVersion.compareTo(AbfsHttpConstants.ApiVersion.AUG_03_2023) < 0) { Review Comment: this is going to need revision in future. just be aware", "created": "2024-04-04T18:47:47.049+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #6409: URL: https://github.com/apache/hadoop/pull/6409", "created": "2024-04-04T18:48:27.655+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #6409: URL: https://github.com/apache/hadoop/pull/6409#issuecomment-2037966009 merged to trunk. @anujmodi2021 can you do a PR and retest with branch -then I will merge it there too.", "created": "2024-04-04T18:49:35.420+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on code in PR #6409: URL: https://github.com/apache/hadoop/pull/6409#discussion_r1559208012 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -1240,8 +1244,8 @@ public boolean getRenameResilience() { return renameResilience; } - void setRenameResilience(boolean actualResilience) { Review Comment: Yes this code was not used any where so removed it", "created": "2024-04-10T10:27:22.456+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 opened a new pull request, #6718: URL: https://github.com/apache/hadoop/pull/6718 Description of PR Jira Ticket: https://issues.apache.org/jira/browse/HADOOP-18656 Today, when a recursive delete is issued for a large directory in ADLS Gen2 (HNS) account, the directory deletion happens in O(1) but in backend ACL Checks are done recursively for each object inside that directory which in case of large directory could lead to request time out. Pagination is introduced in the Azure Storage Backend for these ACL checks. More information on how pagination works can be found on public documentation of [Azure Delete Path API](https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete?view=rest-storageservices-datalakestoragegen2-2019-12-12). This PR contains changes to support this from client side. To trigger pagination, client needs to add a new query parameter \"paginated\" and set it to true along with recursive set to true. In return if the directory is large, server might return a continuation token back to the caller. If caller gets back a continuation token, it has to call the delete API again with continuation token along with recursive and pagination set to true. This is similar to directory delete of FNS account. Pagination is available only in versions \"2023-08-03\" onwards. PR also contains functional tests to verify driver works well with different combinations of recursive and pagination features for both HNS and FNS account. Full E2E testing of pagination requires large dataset to be created and hence not added as part of driver test suite. But extensive E2E testing has been performed.", "created": "2024-04-10T11:06:52.817+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2047520884 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 6 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 29s | | branch-3.4 passed | | +1 :green_heart: | compile | 0m 38s | | branch-3.4 passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | compile | 0m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 32s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 0m 41s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 0m 39s | | branch-3.4 passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 7s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 33m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 33m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 128m 24s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6718 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9920691b15e4 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / b96fbd74711a7b31adb90c2e15b7d7bdefb6ce4a | | Default Java | Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 629 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-10T13:16:33.521+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2049229760 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth ============================================================ [ERROR] testListPathWithValueGreaterThanServerMaximum(org.apache.hadoop.fs.azurebfs.ITestAbfsClient) Time elapsed: 290.912 s <<< FAILURE! [ERROR] test_120_terasort(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort) Time elapsed: 4.531 s <<< ERROR! [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 2 [ERROR] Tests run: 623, Failures: 1, Errors: 0, Skipped: 73 [ERROR] Tests run: 380, Failures: 0, Errors: 1, Skipped: 55 ============================================================ HNS-SharedKey ============================================================ [ERROR] testListPathWithValueGreaterThanServerMaximum(org.apache.hadoop.fs.azurebfs.ITestAbfsClient) Time elapsed: 237.663 s <<< FAILURE! [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 3 [ERROR] Tests run: 623, Failures: 1, Errors: 0, Skipped: 28 [WARNING] Tests run: 380, Failures: 0, Errors: 0, Skipped: 41 ============================================================ NonHNS-SharedKey ============================================================ [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 607, Failures: 0, Errors: 0, Skipped: 269 [WARNING] Tests run: 380, Failures: 0, Errors: 0, Skipped: 44 ============================================================ AppendBlob-HNS-OAuth ============================================================ [ERROR] testCloseOfDataBlockOnAppendComplete(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAppend) Time elapsed: 9.219 s <<< FAILURE! [ERROR] testListPathWithValueGreaterThanServerMaximum(org.apache.hadoop.fs.azurebfs.ITestAbfsClient) Time elapsed: 226.54 s <<< FAILURE! [ERROR] testAbfsStreamOps(org.apache.hadoop.fs.azurebfs.ITestAbfsStreamStatistics) Time elapsed: 5.942 s <<< FAILURE! [ERROR] testExpect100ContinueFailureInAppend(org.apache.hadoop.fs.azurebfs.services.ITestAbfsOutputStream) Time elapsed: 5.002 s <<< ERROR! [ERROR] testAppendWithChecksumAtDifferentOffsets(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemChecksum) Time elapsed: 6.037 s <<< ERROR! [ERROR] testTwoWritersCreateAppendNoInfiniteLease(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemLease) Time elapsed: 3.717 s <<< ERROR! [ERROR] test_120_terasort(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort) Time elapsed: 4.503 s <<< ERROR! [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 2 [ERROR] Tests run: 623, Failures: 2, Errors: 3, Skipped: 73 [ERROR] Tests run: 380, Failures: 1, Errors: 1, Skipped: 79 Time taken: 60 mins 25 secs.", "created": "2024-04-11T08:53:02.893+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2049231207 @steveloughran This is good to merge. The test failures here are known and fixed in PR: https://github.com/apache/hadoop/pull/6676", "created": "2024-04-11T08:53:52.775+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2061597117 @steveloughran, @mukund-thakur... Requesting you to please get this merged.", "created": "2024-04-17T15:41:49.012+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2061899177 there are conflicts here after the test patch has been merged.", "created": "2024-04-17T18:00:07.564+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2068572764 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth ============================================================ [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 2 [WARNING] Tests run: 623, Failures: 0, Errors: 0, Skipped: 73 [WARNING] Tests run: 380, Failures: 0, Errors: 0, Skipped: 54 ============================================================ HNS-SharedKey ============================================================ [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 623, Failures: 0, Errors: 0, Skipped: 28 [WARNING] Tests run: 380, Failures: 0, Errors: 0, Skipped: 41 ============================================================ NonHNS-SharedKey ============================================================ [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 607, Failures: 0, Errors: 0, Skipped: 268 [WARNING] Tests run: 380, Failures: 0, Errors: 0, Skipped: 44 ============================================================ AppendBlob-HNS-OAuth ============================================================ [WARNING] Tests run: 137, Failures: 0, Errors: 0, Skipped: 2 [WARNING] Tests run: 623, Failures: 0, Errors: 0, Skipped: 75 [WARNING] Tests run: 380, Failures: 0, Errors: 0, Skipped: 78 Time taken: 57 mins 2 secs.", "created": "2024-04-22T06:17:37.058+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2068573436 > there are conflicts here after the test patch has been merged. Resolved conflicts. Would be good to have it merged. Thanks a lot.", "created": "2024-04-22T06:18:12.843+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #6718: URL: https://github.com/apache/hadoop/pull/6718#issuecomment-2068631006 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 11m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ branch-3.4 Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 11s | | branch-3.4 passed | | +1 :green_heart: | compile | 0m 38s | | branch-3.4 passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | compile | 0m 35s | | branch-3.4 passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 32s | | branch-3.4 passed | | +1 :green_heart: | mvnsite | 0m 42s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 0m 40s | | branch-3.4 passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | branch-3.4 passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 7s | | branch-3.4 passed | | +1 :green_heart: | shadedclient | 34m 31s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 34m 52s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 33m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 140m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.45 ServerAPI=1.45 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/6718 | | JIRA Issue | HADOOP-18656 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1d45262834f9 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / a9f4e105633a6e7b5e977bc98c1842cef60e4f77 | | Default Java | Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.22+7-post-Ubuntu-0ubuntu220.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_402-8u402-ga-2ubuntu1~20.04-b06 | | Test Results | [CI_URL] | | Max. process+thread count | 705 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2024-04-22T07:00:23.722+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur merged PR #6718: URL: https://github.com/apache/hadoop/pull/6718", "created": "2024-04-22T16:40:41.579+0000"}, {"author": "Descifrado", "body": "Commit in trunk: [HADOOP-18656. [ABFS] Add Support for Paginated Delete for Large Direc\u2026 \u00b7 apache/hadoop@6ed7389|https://github.com/apache/hadoop/commit/6ed73896f6e8b4b7c720eff64193cb30b3e77fb2] Commit in 3.4.1: [HADOOP-18656. [ABFS] Add Support for Paginated Delete for Large Direc\u2026 \u00b7 apache/hadoop@4e96b8e|https://github.com/apache/hadoop/commit/4e96b8e88438d7c2dbd6bf2b4b024e27d378fb0b]", "created": "2024-11-12T11:39:19.571+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5661: URL: https://github.com/apache/hadoop/pull/5661#issuecomment-3424219266 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-21T00:22:50.156+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5661: [ABFS] HADOOP-18656: Paginated Delete Driver Testing URL: https://github.com/apache/hadoop/pull/5661", "created": "2025-10-22T00:23:15.309+0000"}], "derived_tasks": {"summary": "ABFS: Support for Pagination in Recursive Directory Delete - Today, when a recursive delete is issued for a large directory in ADLS Gen2 (HNS) acco...", "classifications": ["sub-task"], "qa_pairs": [{"question": "com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete?", "answer": "isn't it an O(1) operation on a HNS store?"}]}}
{"id": "HADOOP-18640", "title": "ABFS: Enabling Client-side Backoff only for new requests", "description": "Enabling backoff only for new requests that happen, and disabling for retried requests.", "status": "Open", "priority": "Minor", "reporter": "Sree Bhattacharyya", "assignee": "Sree Bhattacharyya", "created": "2023-02-23T09:50:25.000+0000", "updated": "2025-10-26T00:24:15.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "sreeb-msft opened a new pull request, #5446: URL: https://github.com/apache/hadoop/pull/5446 This PR introduces two changes that allows client-side throttling and backoff only for new requests, and increases the level of control through new configs in AbfsClientThrottlingAnalyzer. For the first change, it checks for whether the current rest operation corresponds to a new or retried request. In case of a new request, it calls the necessary methods to apply throttling and backoff at the client side if necessary. In case of a retried request, these methods are skipped and no backoff is applied, or even checked if necessary. This, however, does not affect any other flow such as updating metrics, which happens for each request, irrespective of whether retried or new. In code, the check for whether it is a retried request or not, and the subsequent call for CST is moved to a separate new method, which directly takes input of the current retry count, and makes the decision based on that. #### Tests Added: Two separate tests have been added as part of this change, one for the read and the other for write/append requests - which are the only cases where client-side throttling is applied. Each test does the following - 1. Validates for a new request: - The method for CST (sendingRequest) is called. - The counters for throttling (read/append) are incremented by 1. 2. Validates for a retried request: - The CST method (sendingRequest) call is skipped. - The counters for throttling (read/append) are not incremented and are the same before and after the apply throttling backoff call happens. For the second change, new configs are introduced for the following - 1. `fs.azure.min.acceptable.error.percentage` 2. `fs.azure.max.equilibrium.error.percentage` 3. `fs.azure.rapid.sleep.decrease.factor` 4. `fs.azure.rapid.sleep.decrease.transition.ms` 5. `fs.azure.sleep.decrease.factor` 6. `fs.azure.sleep.increase.factor` All of these are meant to replace the static values that were in place, by allowing the user more control to configure these directly. The static values for these variables, earlier present in code itself, now would be used as default values for the configs.", "created": "2023-03-02T07:20:42.590+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1122694142 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -272,6 +273,30 @@ DefaultValue = DEFAULT_ANALYSIS_PERIOD_MS) private int analysisPeriod; + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MIN_ACCEPTABLE_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE) + private double minAcceptableErrorPercentage; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MAX_EQUILIBRIUM_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MAX_EQUILIBRIUM_ERROR_PERCENTAGE) + private double maxEquilibriumErrorPercentage; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_FACTOR, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_FACTOR) + private double rapidSleepDecreaseFactor; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_TRANSITION_MS, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) + private double rapidSleepDecreaseTransitionPeriodMs; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_SLEEP_DECREASE_FACTOR, + DefaultValue = DEFAULT_SLEEP_DECREASE_FACTOR) + private double sleepDecreaseFactor; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_SLEEP_INCREASE_FACTOR, + DefaultValue = DEFAULT_SLEEP_INCREASE_FACTOR) + private double sleepIncreaseFactor; Review Comment: lets add documentation of what each field means. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +293,67 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { +@Test +public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); Review Comment: why sleep is there? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -334,6 +337,21 @@ private boolean executeHttpOperation(final int retryCount, return true; } + /** + * Makes a call for client side throttling based on + * the request count. + * @param operationType operation type of current request + * @param abfsCounters AbfsCounters instance + */ + @VisibleForTesting + boolean applyThrottlingBackoff(int retryCount, AbfsRestOperationType operationType, AbfsCounters abfsCounters) { + if (retryCount == 0) { + intercept.sendingRequest(operationType, abfsCounters); + return true; + } + return false; + } + Review Comment: once test is refactored. lets keep ``` if (retryCount == 0) { intercept.sendingRequest(operationType, abfsCounters); return true; } ``` inline to line 286. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +293,67 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { +@Test +public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); Review Comment: lets not just test this method. lets test the whole executeRequest method. Have httpOperation mocked. And change the behaviour of httpOperation.processResponse.. Fail it for 2-3 times. and assert.", "created": "2023-03-02T07:39:17.800+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1451533058 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 2s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 2 new + 55 unchanged - 0 fixed = 57 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 7 new + 4 unchanged - 0 fixed = 11 total (was 4) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 56s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 11s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 35s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 107m 9s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 794084458b10 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0011b04d75d2908ea36540b8958d43d988d3d864 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 751 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-02T09:09:15.354+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1453847787 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 24s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 2 new + 55 unchanged - 0 fixed = 57 total (was 55) | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 9 new + 4 unchanged - 0 fixed = 13 total (was 4) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 13s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 97m 52s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux dc39661540a8 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 91c932722d3e92aa48d19a5d275e4721a29e48f6 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-03T17:14:58.501+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1125941455 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -50,6 +50,14 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + // Throttling Analysis defaults. + public static final double DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE = .1; Review Comment: nit:`0.1` ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } Review Comment: what is happening in throttlingIntercept is out of scope for the pr. its not testing what the change we want to have. What its touching is that if the if-else block is working correct or not. even if we want to to test it, lets have intercept mocked and just check if `intercept.sendingRequest` is being called or not. What i want to say is that, lets not get into whats happening inside throttlingIntercept. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -50,6 +50,14 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + // Throttling Analysis defaults. + public static final double DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE = .1; Review Comment: please check other double values in default configs added. nit comment. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -334,6 +337,25 @@ private boolean executeHttpOperation(final int retryCount, return true; } + /** + * Makes a call for client side throttling based on + * the request count. + * @param operationType operation type of current request + * @param abfsCounters AbfsCounters instance + */ + @VisibleForTesting + boolean applyThrottlingBackoff(int retryCount, AbfsRestOperationType operationType, AbfsCounters abfsCounters) { + if (retryCount == 0) { + intercept.sendingRequest(operationType, abfsCounters); + return true; + } + return false; + } + + public AbfsHttpOperation createHttpOperationInstance() throws IOException { Review Comment: dont have it public. have it like: ``` AbfsHttpOperation createHttpOperationInstance() throws IOException { ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -268,10 +269,55 @@ DefaultValue = DEFAULT_ACCOUNT_OPERATION_IDLE_TIMEOUT_MS) private int accountOperationIdleTimeout; + /* + Analysis Period for client-side throttling + */ @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ANALYSIS_PERIOD, DefaultValue = DEFAULT_ANALYSIS_PERIOD_MS) private int analysisPeriod; + /* Review Comment: lets have it in javadocs? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { Review Comment: nit: lets remove this line. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); Review Comment: same comment as above. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List<AbfsHttpHeader> requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); + Mockito.doReturn(-1) + .doReturn(-1) + .doReturn(-1) + .doReturn(HTTP_OK) + .when(mockHttpOp).getStatusCode(); + Mockito.doNothing().when(mockHttpOp).setRequestProperty(nullable(String.class), nullable(String.class)); + Mockito.doNothing().when(mockHttpOp).sendRequest(nullable(byte[].class), nullable(int.class), nullable(int.class)); + Mockito.doNothing().when(mockHttpOp).processResponse(nullable(byte[].class), nullable(int.class), nullable(int.class)); + + Mockito.doReturn(mockHttpOp).when(mockRestOp).createHttpOperationInstance(); + Mockito.doReturn(mockHttpOp).when(mockRestOp).getResult(); + + mockRestOp.execute(getTestTracingContext(fs, false)); + Mockito.verify(intercept, times(1)).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); Review Comment: since you plan to keep `applyThrottlingBackoff` in your code, let check if `applyThrottlingBackoff` is being called 4 times. it would be great addition. What you feel? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List<AbfsHttpHeader> requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); + Mockito.doReturn(-1) + .doReturn(-1) + .doReturn(-1) + .doReturn(HTTP_OK) + .when(mockHttpOp).getStatusCode(); + Mockito.doNothing().when(mockHttpOp).setRequestProperty(nullable(String.class), nullable(String.class)); + Mockito.doNothing().when(mockHttpOp).sendRequest(nullable(byte[].class), nullable(int.class), nullable(int.class)); + Mockito.doNothing().when(mockHttpOp).processResponse(nullable(byte[].class), nullable(int.class), nullable(int.class)); + + Mockito.doReturn(mockHttpOp).when(mockRestOp).createHttpOperationInstance(); + Mockito.doReturn(mockHttpOp).when(mockRestOp).getResult(); + + mockRestOp.execute(getTestTracingContext(fs, false)); + Mockito.verify(intercept, times(1)).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + } + + @Test + public void testWriteThrottleNewRequest() throws IOException { Review Comment: this is not different than `testReadThrottleNewRequest`, just that api is changing. we can remove it, since its not adding any significance. What you feel?", "created": "2023-03-06T06:00:08.240+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1455548802 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 43s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 2 new + 55 unchanged - 0 fixed = 57 total (was 55) | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 9 new + 4 unchanged - 0 fixed = 13 total (was 4) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 37s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 96m 43s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 0f2865589c72 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b036f9285b4b2124273dda736bf44f002f7e27d1 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 558 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-06T06:53:56.916+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1126166721 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List<AbfsHttpHeader> requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: No need to do spy. ``` AbfsHttpOperation mockHttpOp = Mockito.mock(AbfsHttpOperation.class); Mockito.doReturn(mockHttpOp).when(mockRestOp).createHttpOperationInstance(); ```", "created": "2023-03-06T09:47:42.427+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1126325114 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List<AbfsHttpHeader> requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: Just Mocking is leading to NPE under executeHttpOperation method. Keeping it as a spy.", "created": "2023-03-06T12:04:46.443+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1456190858 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 12 new + 7 unchanged - 0 fixed = 19 total (was 7) | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 94m 44s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 6cabcb9c56bb 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 94bab6c212bb2af7792ae25595a1582f68132269 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 603 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-06T14:01:35.466+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1127328623 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -268,10 +269,62 @@ DefaultValue = DEFAULT_ACCOUNT_OPERATION_IDLE_TIMEOUT_MS) private int accountOperationIdleTimeout; + /** + * Analysis Period for client-side throttling + */ @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ANALYSIS_PERIOD, DefaultValue = DEFAULT_ANALYSIS_PERIOD_MS) private int analysisPeriod; + /** + * Lower limit of acceptable error percentage + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MIN_ACCEPTABLE_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE) + private double minAcceptableErrorPercentage; + + /** + * Maximum equilibrium error percentage + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MAX_EQUILIBRIUM_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MAX_EQUILIBRIUM_ERROR_PERCENTAGE) + private double maxEquilibriumErrorPercentage; + + /** + * Rapid sleep decrease factor to increase throughput + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_FACTOR, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_FACTOR) + private double rapidSleepDecreaseFactor; + + /** + * Rapid sleep decrease transition period in milliseconds + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_TRANSITION_MS, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) + private double rapidSleepDecreaseTransitionPeriodMs; + + /** + * Sleep decrease factor to increase throughput + */ Review Comment: fix spacing in all javadocs wherever required. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -20,28 +20,42 @@ import static java.net.HttpURLConnection.HTTP_INTERNAL_ERROR; +import static java.net.HttpURLConnection.HTTP_OK; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_GET; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_PUT; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_BACKOFF_INTERVAL; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_MAX_BACKOFF_INTERVAL; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_MAX_IO_RETRIES; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_MIN_BACKOFF_INTERVAL; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_LEVEL_THROTTLING_ENABLED; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_AUTOTHROTTLING; import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MIN_BUFFER_SIZE; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.IF_MATCH; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.RANGE; import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_ABFS_ACCOUNT1_NAME; import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_ACCOUNT_NAME; import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.TEST_CONFIGURATION_FILE_NAME; import static org.junit.Assume.assumeTrue; -import static org.mockito.Mockito.mock; -import static org.mockito.Mockito.when; +import static org.mockito.Mockito.*; Review Comment: fix wildcard. You can switch off wildcard imports on IDE so in future it doesnt come. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List<AbfsHttpHeader> requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: lets resolve to not do this. Some issue in our mocking. The way described should work. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: why is it in abfsClient. What risk is there to keep it in abfsRestOperation.", "created": "2023-03-07T04:36:13.385+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1127368834 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List<AbfsHttpHeader> requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: everything is possible :). Bit of debug is required. Please merge https://github.com/sreeb-msft/hadoop/pull/1 in your pr.", "created": "2023-03-07T05:47:01.992+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1458113928 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 23s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 11 new + 7 unchanged - 0 fixed = 18 total (was 7) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 13s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 1 ASF License warnings. | | | | 94m 2s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 133ddfe34150 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4c6f2ae89f22a3d525ec7b7c50ddc9b13687d638 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 688 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-07T12:47:59.398+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1129256701 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: Within AbfsRestOperation, would have to do client.getAbfsConfiguration.getShouldThrottleRetries. Would that be more preferable?", "created": "2023-03-08T10:53:47.022+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1130357512 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: Either of two is fine. if we keep in abfsClient, it will be stored account level, and we dont need to check anything till abfsClient object is alived. In abfsRestOp, new field will be created as new object is created for each api call. if we keep it in abfsRestOperation, it is something which is actually requried in abfsRestOperation. Though it doesn't matter. You may please resolve this comment.", "created": "2023-03-09T03:18:18.798+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1130357920 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -268,10 +269,62 @@ DefaultValue = DEFAULT_ACCOUNT_OPERATION_IDLE_TIMEOUT_MS) private int accountOperationIdleTimeout; + /** + * Analysis Period for client-side throttling + */ Review Comment: nit: spacing.", "created": "2023-03-09T03:18:43.821+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1130546200 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: Right. That makes sense. We can directly invoke it from within AbfsRestOperation.", "created": "2023-03-09T06:39:42.037+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1461580972 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 30s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 50s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 94m 54s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux bc0f9d906cc6 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9bea2eff81d5f897a138d59cb5bb0575e4f6ade6 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 705 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-09T08:46:38.507+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1462216813 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 53s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 12s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 38s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7) | | +1 :green_heart: | mvnsite | 0m 36s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 15s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 97m 10s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 137e902f552a 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9c5e993d71f861abc6145cd9d2ca1a78343c9474 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-09T15:08:19.337+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1462328124 it seems yetus is giving wrong line number. there should be a space on 839.", "created": "2023-03-09T16:06:20.610+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1462348876 error was due to space on empty line: try merging https://github.com/sreeb-msft/hadoop/pull/2.", "created": "2023-03-09T16:18:25.903+0000"}, {"author": "Shilun Fan", "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.", "created": "2024-01-04T09:45:50.869+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-3445309605 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-25T00:22:21.499+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5446: HADOOP-18640: [ABFS] Enabling Client-side Backoff only for new requests URL: https://github.com/apache/hadoop/pull/5446", "created": "2025-10-26T00:24:15.550+0000"}], "derived_tasks": {"summary": "ABFS: Enabling Client-side Backoff only for new requests - Enabling backoff only for new requests that happen, and disabling for retried requests", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "HADOOP-18639", "title": "DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed.", "description": "YARN NodeManager's deletion service has two types of deletion tasks: the FileDeletionTask for deleting log, usercache, appcache files and the DockerContainerDeletionTask for deleting Docker containers. The FileDeletionTask is removed from the statestore when the task is completed, but the DockerContainerDeletionTask is not. Therefore, the DockerContainerDeletionTask accumulates continuously in the statestore. This causes the NodeManager's deletion service to run the accumulated DockerContainerDeletionTask in the statestore when the NodeManager restarts. As a result, the FileDeletionTask and DockerContainerDeletionTask are delayed unnecessarily while processing accumulated tasks, which can cause disk full issues in environments where a large number of containers are allocated and released. I will attach a patch soon", "status": "Open", "priority": "Major", "reporter": "Sejin Hwang", "assignee": null, "created": "2023-02-22T13:43:55.000+0000", "updated": "2025-10-26T00:24:19.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "ASF GitHub Bot", "body": "Sejin-Hwang opened a new pull request, #5425: URL: https://github.com/apache/hadoop/pull/5425 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR When the DockerContainerDeletionTask is completed, it should be deleted from the NodeManager's statestore, just like the FileDeletionTask. JIRA : https://issues.apache.org/jira/browse/HADOOP-18639 ### How was this patch tested? Unit test added", "created": "2023-02-22T13:52:56.141+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5425: URL: https://github.com/apache/hadoop/pull/5425#issuecomment-1440331866 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 19s | | trunk passed | | +1 :green_heart: | compile | 1m 31s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 1m 27s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 37s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 34s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 1m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 1m 25s | | the patch passed | | +1 :green_heart: | compile | 1m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 1m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 23s | [/results-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt]([CI_URL] | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager: The patch generated 2 new + 7 unchanged - 0 fixed = 9 total (was 7) | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 35s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 31s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 139m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5425 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 71ed66f96066 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ab08810d2c5e73b2a362d9007e83b1a4ec7e33f8 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 606 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-22T16:13:49.375+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5425: URL: https://github.com/apache/hadoop/pull/5425#issuecomment-1440644469 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 32s | | trunk passed | | +1 :green_heart: | compile | 1m 30s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 1m 24s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 42s | | the patch passed | | +1 :green_heart: | compile | 1m 24s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 1m 24s | | the patch passed | | +1 :green_heart: | compile | 1m 17s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 1m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 23s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 30s | | the patch passed | | +1 :green_heart: | shadedclient | 26m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 30s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 139m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5425 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 4cd6557679cc 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b9662f3160da82f744a48ac298eb0e68d1a57a33 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-22T19:04:54.048+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5425: URL: https://github.com/apache/hadoop/pull/5425#issuecomment-3445309935 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-25T00:22:28.002+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5425: HADOOP-18639 DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed URL: https://github.com/apache/hadoop/pull/5425", "created": "2025-10-26T00:24:19.014+0000"}], "derived_tasks": {"summary": "DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed. - YARN NodeManager's deletion service has ...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-18637", "title": "S3A to support upload of files greater than 2 GB using DiskBlocks", "description": "Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. cc: [~mthakur] [~stevel@apache.org] [~mehakmeet]", "status": "Resolved", "priority": "Major", "reporter": "Harshit Gupta", "assignee": "Harshit Gupta", "created": "2023-02-21T09:05:42.000+0000", "updated": "2025-10-25T00:22:13.000+0000", "labels": ["pull-request-available"], "components": ["fs/s3"], "comments": [{"author": "Harshit Gupta", "body": "S3ABlockOutputStream creates three different types of DataBlock depending upon the\u00a0{{fs.s3a.fast.upload.buffer}}\u00a0which defaults to disk, we can create an empty file for the same size and limit the Buffer size to\u00a0{{Integer.MAX_VALUE}}\u00a0.\u00a0*For other buffer types should we deny uploads larger than 2 Gigs or should we add the support there as well?*\u00a0like for\u00a0{{ByteArrayBlock}}\u00a0which writes directly to the\u00a0{{S3AByteArrayOutputStream}}\u00a0which will be again initialized with\u00a0{{Integer.MAX_Value}}\u00a0.The same goes for\u00a0{{ByteBufferBlock}} as well. One thing to make sure of here is that it's never gonna write something larger than {{Integer.MAX_VALUE}}\u00a0as the calling function to write has the signature\u00a0{{public synchronized void write(byte[] source, int offset, int len)}}\u00a0(S3ABlockOutputStream). *This is just for compatibility with non-AWS s3 stores.*", "created": "2023-02-21T10:52:33.696+0000"}, {"author": "Mukund Thakur", "body": "As discussed offline, following changes will be required. * Introduce a new config to disable multipart upload everywhere and enable just a large file upload. * Error in public S3AFS.createMultipartUploader based on above config. * Error in staging committer based on above config. * Error in magic committer based on above config. * Error in write operations helper based on above config. * Add hasCapability(isMultiPartAllowed, path) use config. * If multipart upload is disabled we only upload via Disk. Add check for this.", "created": "2023-02-21T23:39:28.177+0000"}, {"author": "ASF GitHub Bot", "body": "HarshitGupta11 opened a new pull request, #5481: URL: https://github.com/apache/hadoop/pull/5481 ### Description of PR Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. ### How was this patch tested? The patch was tested against us-west-2 ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-03-15T09:46:48.496+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1469829840 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 11s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 33s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 96m 36s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5e32eadfab76 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a2d25f629c809071990645855774528e52103cfe | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 727 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-15T11:25:15.190+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1469831351 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 37s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 47s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 27s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 32s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 98m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 035b98662ea9 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a2d25f629c809071990645855774528e52103cfe | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-15T11:26:27.413+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1146257205 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1255,4 +1255,8 @@ private Constants() { */ public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + + public static final String ALLOW_MULTIPART_UPLOADS = \"fs.s3a.allow.multipart.uploads\"; + + public static final boolean IS_ALLOWED_MULTIPART_UPLOADS_DEFAULT = true; Review Comment: change to MULTIPART_UPLOAD_ENABLED_DEFAULT; ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -564,11 +564,12 @@ class ByteBufferBlock extends DataBlock { * @param statistics statistics to update */ ByteBufferBlock(long index, - int bufferSize, + long bufferSize, BlockOutputStreamStatistics statistics) { super(index, statistics); - this.bufferSize = bufferSize; - blockBuffer = requestBuffer(bufferSize); + this.bufferSize = bufferSize > Integer.MAX_VALUE ? + Integer.MAX_VALUE : (int) bufferSize; + blockBuffer = requestBuffer((int) bufferSize); Review Comment: use this.bufferSize rather than casting again. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -169,6 +169,9 @@ class S3ABlockOutputStream extends OutputStream implements /** Thread level IOStatistics Aggregator. */ private final IOStatisticsAggregator threadIOStatisticsAggregator; + /**Is multipart upload allowed? */ + private final boolean isMultipartAllowed; Review Comment: isMultipartEnabled ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,9 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + if (!isMultipartAllowed){ + return; Review Comment: throw Exception. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1255,4 +1255,8 @@ private Constants() { */ public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + + public static final String ALLOW_MULTIPART_UPLOADS = \"fs.s3a.allow.multipart.uploads\"; Review Comment: Change to MULTIPART_UPLOADS_ENABLED = \"fs.s3a.multipart.uploads.enabled\"; ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -516,6 +516,7 @@ public void initialize(URI name, Configuration originalConf) maxKeys = intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1); partSize = getMultipartSizeProperty(conf, MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE); + LOG.warn(\"Patcchhhh: The part size is : {}\", partSize); Review Comment: delete ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1832,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ Review Comment: just add a method validateOutputStreamConfiguration() and throw exception in the implementation only. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,19 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + public static boolean checkDiskBuffer(Configuration conf){ + boolean isAllowedMultipart = conf.getBoolean(ALLOW_MULTIPART_UPLOADS, + IS_ALLOWED_MULTIPART_UPLOADS_DEFAULT); + if (isAllowedMultipart) { Review Comment: this is wrong here I guess. if isAllowedMultipart is enabled then FAST_UPLOAD_BUFFER must be disk else we throw an error right? ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java: ########## @@ -269,8 +269,8 @@ public PutObjectRequest createPutObjectRequest( String dest, File sourceFile, final PutObjectOptions options) { - Preconditions.checkState(sourceFile.length() < Integer.MAX_VALUE, - \"File length is too big for a single PUT upload\"); + //Preconditions.checkState(sourceFile.length() < Integer.MAX_VALUE, Review Comment: remove, no unnecessary comments. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -436,11 +436,11 @@ static class ByteArrayBlock extends DataBlock { private Integer dataSize; ByteArrayBlock(long index, - int limit, + long limit, BlockOutputStreamStatistics statistics) { super(index, statistics); - this.limit = limit; - buffer = new S3AByteArrayOutputStream(limit); + this.limit = (limit > Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int) limit; + buffer = new S3AByteArrayOutputStream((int) limit); Review Comment: use this.limit. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -595,7 +596,7 @@ public void initialize(URI name, Configuration originalConf) } blockOutputBuffer = conf.getTrimmed(FAST_UPLOAD_BUFFER, DEFAULT_FAST_UPLOAD_BUFFER); - partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize); + //partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize); Review Comment: cut", "created": "2023-03-23T14:35:24.296+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1481310374 @steveloughran could you review this please. thanks.", "created": "2023-03-23T14:35:38.018+0000"}, {"author": "ASF GitHub Bot", "body": "HarshitGupta11 commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1149003025 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,9 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + if (!isMultipartAllowed){ + return; Review Comment: Why do we need to throw exception here, since this just meant to skip the initialisation? cc: @mehakmeet @steveloughran", "created": "2023-03-27T08:58:25.708+0000"}, {"author": "ASF GitHub Bot", "body": "HarshitGupta11 commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1149011685 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,19 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + public static boolean checkDiskBuffer(Configuration conf){ + boolean isAllowedMultipart = conf.getBoolean(ALLOW_MULTIPART_UPLOADS, + IS_ALLOWED_MULTIPART_UPLOADS_DEFAULT); + if (isAllowedMultipart) { Review Comment: If multipart is disabled and the FAST_UPLOAD_BUFFER is not disk then we throw an error.", "created": "2023-03-27T09:05:26.552+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1485290836 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 1s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 40s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 40s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 31s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 98m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux cdba540f5bf0 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f381b88a9260facd94616e0cf62c426b44575f45 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 721 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-27T15:07:29.544+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1150546662 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -558,19 +565,21 @@ public String toString() { } /** - * Upload the current block as a single PUT request; if the buffer - * is empty a 0-byte PUT will be invoked, as it is needed to create an - * entry at the far end. - * @throws IOException any problem. - * @return number of bytes uploaded. If thread was interrupted while - * waiting for upload to complete, returns zero with interrupted flag set - * on this thread. + * Upload the current block as a single PUT request; if the buffer is empty a + * 0-byte PUT will be invoked, as it is needed to create an entry at the far Review Comment: no need to reformat the entire javadoc, you don't want your IDE set to do this as it only makes cherrypicking harder ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -169,6 +169,9 @@ class S3ABlockOutputStream extends OutputStream implements /** Thread level IOStatistics Aggregator. */ private final IOStatisticsAggregator threadIOStatisticsAggregator; + /**Is multipart upload allowed? */ Review Comment: nit, add a space after the ** ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -1126,6 +1135,11 @@ public static final class BlockOutputStreamBuilder { */ private IOStatisticsAggregator ioStatisticsAggregator; + /** + * Is Multipart Uploads enabled for the given upload Review Comment: add a . to keep javadoc happy ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -1126,6 +1135,11 @@ public static final class BlockOutputStreamBuilder { */ private IOStatisticsAggregator ioStatisticsAggregator; + /** + * Is Multipart Uploads enabled for the given upload + */ + private boolean isMultipartAllowed; Review Comment: rename isMultipartEnabled ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -1276,5 +1290,11 @@ public BlockOutputStreamBuilder withIOStatisticsAggregator( ioStatisticsAggregator = value; return this; } + + public BlockOutputStreamBuilder withMultipartAllowed( Review Comment: again, rename ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; Review Comment: and add java.* block above the others. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override + protected Configuration createScaleConfiguration() { + Configuration configuration = super.createScaleConfiguration(); + configuration.setBoolean(Constants.MULTIPART_UPLOADS_ENABLED, false); + configuration.setLong(MULTIPART_SIZE, 53687091200L); + configuration.setInt(KEY_TEST_TIMEOUT, 36000); + configuration.setInt(IO_CHUNK_BUFFER_SIZE, 655360); + configuration.set(\"fs.s3a.connection.request.timeout\", \"1h\"); Review Comment: use the relevant constant ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,19 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + public static boolean checkDiskBuffer(Configuration conf){ Review Comment: 1. javadocs 2. add a space after the ) and { ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override + protected Configuration createScaleConfiguration() { + Configuration configuration = super.createScaleConfiguration(); + configuration.setBoolean(Constants.MULTIPART_UPLOADS_ENABLED, false); + configuration.setLong(MULTIPART_SIZE, 53687091200L); + configuration.setInt(KEY_TEST_TIMEOUT, 36000); + configuration.setInt(IO_CHUNK_BUFFER_SIZE, 655360); + configuration.set(\"fs.s3a.connection.request.timeout\", \"1h\"); + return configuration; + } + + @Test + public void uploadFileSinglePut() throws IOException { + LOG.info(\"Creating file with size : {}\", fileSize); + ContractTestUtils.createAndVerifyFile(getFileSystem(), Review Comment: after the upload, use the iostatistics of the fs to verify that only one PUT operation took place, and therefore that the operation worked ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; Review Comment: is this going to be configurable? as i might want to make the size smaller so my tests don't time out as much (in fact: I absolutely WILL because I run with -Dscale most of the time) look at AbstractSTestS3AHugeFiles.setup() to see how it picks up the size. use the same configuration option to control the size in this test case. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1830,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ + throw new IOException(\"The filesystem conf is not \" + Review Comment: that's not a particularly useful error message. better to say which options are inconsistent ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1255,4 +1255,8 @@ private Constants() { */ public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + + public static final String MULTIPART_UPLOADS_ENABLED = \"fs.s3a.multipart.uploads.enabled\"; Review Comment: add javadocs for these constants with `{@value}` in them, so IDEs will show what they mean and the generated docs show the strings also add a mention in the \"How S3A writes data to S3\" in hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; Review Comment: split import blocks, into apache and non apache, put the non-apache one first ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override Review Comment: nit: newline between these two ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ Review Comment: * add a javadoc to see what it is doing. * give it a name to indicate it is doing a single put, e.g `ITestS3AHugeFileUploadSinglePut` * add a space before { ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,9 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + if (!isMultipartAllowed){ + return; Review Comment: good q. we should throw it because if something does get as far as calling it, something has gone very wrong. I would just use ``` Preconditions.checkState(!isMultipartEnabled, \"multipart upload is disabled\") ```", "created": "2023-03-28T12:54:09.559+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1157069838 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,13 +1031,22 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } - public static boolean checkDiskBuffer(Configuration conf){ + /** + * Check whether the configuration for S3ABlockOutputStream is + * consistent or not. Multipart uploads allow all kinds of fast buffers to + * be supported. When the option is disabled only disk buffers are allowed to + * be used as the file size might be bigger than the buffer size that can be + * allocated. + * @param conf + * @return Review Comment: nit: document conf argument and retrun value ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1256,7 +1256,16 @@ private Constants() { public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + /** + * Option to enable or disable the multipart uploads. Review Comment: nit: still needs an {@value} element ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUploadSinglePut.java: ########## @@ -42,14 +51,23 @@ protected Configuration createScaleConfiguration() { configuration.setLong(MULTIPART_SIZE, 53687091200L); configuration.setInt(KEY_TEST_TIMEOUT, 36000); configuration.setInt(IO_CHUNK_BUFFER_SIZE, 655360); - configuration.set(\"fs.s3a.connection.request.timeout\", \"1h\"); + configuration.set(REQUEST_TIMEOUT, \"1h\"); + fileSize = getTestPropertyBytes(configuration, KEY_HUGE_FILESIZE, + DEFAULT_HUGE_FILESIZE); return configuration; } @Test public void uploadFileSinglePut() throws IOException { LOG.info(\"Creating file with size : {}\", fileSize); - ContractTestUtils.createAndVerifyFile(getFileSystem(), - getTestPath(), fileSize ); + S3AFileSystem fs = getFileSystem(); + ContractTestUtils.createAndVerifyFile(fs, + getTestPath(), fileSize); + //No more than three put requests should be made during the upload of the file + //First one being the creation of test/ directory marker + //Second being the creation of the file with tests3ascale/<file-name> + //Third being the creation of directory marker tests3ascale/ on the file delete + assertEquals(3L, Review Comment: use `IOStatisticAssertions` here; it generates AssertJ assertion chains from lookups with automatic generation of error text. ```java assertThatStatisticCounter(fs.getIOStatistics(), OBJECT_PUT_REQUESTS.getSymbol()) .isEqualTo(3); ``` ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1859,7 +1859,7 @@ private FSDataOutputStream innerCreateFile( .withPutOptions(putOptions) .withIOStatisticsAggregator( IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator()) - .withMultipartAllowed(getConf().getBoolean( + .withMultipartEnabled(getConf().getBoolean( Review Comment: i think the multipart enabled flag should be made a field and stored during initialize(), so we can save on scanning the conf map every time a file is created.", "created": "2023-04-04T10:55:08.986+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1495990200 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 13s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 30s | | trunk passed | | +1 :green_heart: | compile | 25m 59s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 43s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 50s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 55s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 19s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 32s | | the patch passed | | +1 :green_heart: | compile | 22m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 25s | | the patch passed | | +1 :green_heart: | compile | 20m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 31s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 30s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 3 new + 9 unchanged - 0 fixed = 12 total (was 9) | | +1 :green_heart: | mvnsite | 2m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 44s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 47s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | spotbugs | 4m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 22s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 47s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 230m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 728c0caced16 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ea0007fa8ac7aba823faab608539cf133cb27323 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1259 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-04T13:36:18.161+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1496209366 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 54s | | trunk passed | | +1 :green_heart: | compile | 23m 9s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 44s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 53s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 3m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 54s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 17s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 30s | | the patch passed | | +1 :green_heart: | compile | 22m 31s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 31s | | the patch passed | | +1 :green_heart: | compile | 22m 38s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 22m 38s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 11s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 2 new + 9 unchanged - 0 fixed = 11 total (was 9) | | +1 :green_heart: | mvnsite | 2m 33s | | the patch passed | | +1 :green_heart: | javadoc | 1m 39s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 14s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 20s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 43s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 229m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 0016822b3a9d 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1f56e2a27ba37e1896ccf9b86e346278d149f617 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 3159 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-04T15:46:08.483+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1498651485 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 46m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 12s | | trunk passed | | +1 :green_heart: | compile | 23m 1s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 40s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 38s | | trunk passed | | +1 :green_heart: | javadoc | 1m 53s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 4s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 0s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 24s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 33s | | the patch passed | | +1 :green_heart: | compile | 22m 32s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 32s | | the patch passed | | +1 :green_heart: | compile | 20m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 25s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 34s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 44s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 47s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 4m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 31s | | hadoop-common in the patch passed. | | -1 :x: | unit | 2m 25s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 1s | | The patch does not generate ASF License warnings. | | | | 258m 23s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 4db319f228cd 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4e922b4a81e4c6abc85d43267e74eb3b9827d197 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 2152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-06T08:03:49.141+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1499070067 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 4s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 45s | | trunk passed | | +1 :green_heart: | compile | 23m 10s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 57s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 42s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 34s | | trunk passed | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 23s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 1s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 49s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 12s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 30s | | the patch passed | | +1 :green_heart: | compile | 22m 28s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 28s | | the patch passed | | +1 :green_heart: | compile | 20m 36s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 36s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 31s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 45s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 46s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 4m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 30s | | hadoop-common in the patch passed. | | -1 :x: | unit | 2m 19s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 1s | | The patch does not generate ASF License warnings. | | | | 226m 22s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 2cbac9757e23 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 13fc2d5d4f4ae17b7701a897969bdb8e0643f8bd | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1291 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-06T13:31:30.823+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1159979341 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -767,6 +781,18 @@ public RequestFactoryBuilder withRequestPreparer( this.requestPreparer = value; return this; } + + /** + * Multipart enabled Review Comment: always add a trailing \".\" on javadocs. it'll save review iterations ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUploadSinglePut.java: ########## @@ -0,0 +1,74 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import java.io.IOException; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; +import org.junit.Test; + +import org.apache.hadoop.fs.s3a.S3AFileSystem; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.REQUEST_TIMEOUT; +import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestPropertyBytes; +import static org.apache.hadoop.fs.s3a.Statistic.OBJECT_PUT_REQUESTS; +import static org.apache.hadoop.fs.statistics.IOStatisticAssertions.assertThatStatisticCounter; + +/** + * Test a file upload using a single PUT operation. Multipart uploads will + * be disabled in the test. + */ +public class ITestS3AHugeFileUploadSinglePut extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUploadSinglePut.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override + protected Configuration createScaleConfiguration() { + Configuration configuration = super.createScaleConfiguration(); + configuration.setBoolean(Constants.MULTIPART_UPLOADS_ENABLED, false); + configuration.setLong(MULTIPART_SIZE, 53687091200L); Review Comment: is this some special value? if so: make a constant, explain what it is. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestRequestFactory.java: ########## @@ -173,7 +174,11 @@ private void createFactoryObjects(RequestFactory factory) { a(factory.newListObjectsV1Request(path, \"/\", 1)); a(factory.newListNextBatchOfObjectsRequest(new ObjectListing())); a(factory.newListObjectsV2Request(path, \"/\", 1)); - a(factory.newMultipartUploadRequest(path, null)); + try { Review Comment: just change have the method throw IOE and cut the try/catch ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java: ########## @@ -99,6 +99,11 @@ public interface StreamCapabilities { */ String IOSTATISTICS_CONTEXT = \"fs.capability.iocontext.supported\"; + /** Review Comment: as this is s3a only, put it in org.apache.hadoop.fs.s3a.Constants with the MULTIPART_UPLOADS_ENABLED definition and using the same prefix `fs.s3a.capability.` as `STORE_CAPABILITY_DIRECTORY_MARKER*` probes ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -460,7 +466,10 @@ public AbortMultipartUploadRequest newAbortMultipartUploadRequest( @Override public InitiateMultipartUploadRequest newMultipartUploadRequest( final String destKey, - @Nullable final PutObjectOptions options) { + @Nullable final PutObjectOptions options) throws IOException { + if(!isMultipartEnabled){ Review Comment: nit: spacing ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,57 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, CommitConstants.COMMITTER_NAME_MAGIC); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); + assertThrows(PathCommitException.class, Review Comment: prefer LambdaTestUtils.intercept. why so? * the move to jupiter API is a PITA for backporting etc * intercept() will include the toString() value of whatever was returned (here: the committer) in the exception raised. hence: automatic diagnostics", "created": "2023-04-06T16:05:52.998+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1160262354 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -124,6 +124,11 @@ public class RequestFactoryImpl implements RequestFactory { */ private final StorageClass storageClass; + /** + * Is Multipart Enabled Review Comment: . in the end for javadoc. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestStagingCommitProtocolFailure.java: ########## @@ -0,0 +1,58 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.staging.integration; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.InternalCommitterConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestStagingCommitProtocolFailure extends AbstractS3ATestBase { + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, InternalCommitterConstants.COMMITTER_NAME_STAGING); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); + assertThrows(PathCommitException.class, Review Comment: same intercept. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -124,6 +124,11 @@ public class RequestFactoryImpl implements RequestFactory { */ private final StorageClass storageClass; + /** + * Is Multipart Enabled Review Comment: Other config names clearly mentions it is multipart upload only. Why are we not using isMultipartUploadEnabled here as well? ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,57 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, CommitConstants.COMMITTER_NAME_MAGIC); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); + assertThrows(PathCommitException.class, Review Comment: and it is beautifully integrated with other parts of tests well. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -124,6 +124,11 @@ public class RequestFactoryImpl implements RequestFactory { */ private final StorageClass storageClass; + /** + * Is Multipart Enabled Review Comment: m small, is multipart upload enabled. As this is only for uploads not downloads as well. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,57 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, CommitConstants.COMMITTER_NAME_MAGIC); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); Review Comment: log what are you printing. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1832,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ Review Comment: This is still pending. I don't really mind leaving it as it is but I think my suggestion is consistent with other parts of the code and is more readable. CC @steveloughran ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -767,6 +781,18 @@ public RequestFactoryBuilder withRequestPreparer( this.requestPreparer = value; return this; } + + /** + * Multipart enabled Review Comment: +1", "created": "2023-04-06T21:25:48.214+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1160269091 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1832,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ Review Comment: > just add a method validateOutputStreamConfiguration() and throw exception in the implementation only. This is still pending. I don't really mind leaving it as it is but I think my suggestion is consistent with other parts of the code and is more readable. CC @steveloughran", "created": "2023-04-06T21:28:58.613+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1501718145 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 43s | | trunk passed | | +1 :green_heart: | compile | 23m 6s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 41s | | trunk passed | | +1 :green_heart: | javadoc | 1m 53s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 58s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 22s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 31s | | the patch passed | | +1 :green_heart: | compile | 22m 21s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 21s | | the patch passed | | +1 :green_heart: | compile | 20m 33s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 33s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 38s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 3 new + 9 unchanged - 0 fixed = 12 total (was 9) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 45s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 48s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 4m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 21s | | hadoop-common in the patch passed. | | -1 :x: | unit | 2m 22s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 227m 52s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 665b3a783820 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1476424088e2d20e5cf139dfb7e075d5000218d2 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1259 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-10T11:34:48.701+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1501730716 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 45s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 25s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 38s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt]([CI_URL] | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 3 new + 9 unchanged - 0 fixed = 12 total (was 9) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 5s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 93m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 97ba7bf3ee91 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f18c0cb9c0ca8baa6eb911f9c1c753124a8a9785 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 758 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-10T11:53:22.657+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162165006 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,8 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + Preconditions.checkState(!isMultipartUploadEnabled, Review Comment: this is wrong.", "created": "2023-04-10T23:28:01.027+0000"}, {"author": "ASF GitHub Bot", "body": "mehakmeet commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162432674 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java: ########## @@ -217,6 +217,10 @@ protected AbstractS3ACommitter( LOG.debug(\"{} instantiated for job \\\"{}\\\" ID {} with destination {}\", role, jobName(context), jobIdString(context), outputPath); S3AFileSystem fs = getDestS3AFS(); + if (!fs.isMultipartUploadEnabled()) { Review Comment: So we want to fail for any s3a committer initialization if the multipart is disabled? iirc magic committer does require multipart but should we be failing for others as well? CC @steveloughran also seems like alot of tests are failing when I run the suite on default props(by default this should be true and not fail here) could be due to UTs using \"MockS3AFileSystem\" which doesn't actually initialize and set the variable. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,8 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + Preconditions.checkState(!isMultipartUploadEnabled, Review Comment: +1", "created": "2023-04-11T09:25:13.101+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162568369 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java: ########## @@ -217,6 +217,10 @@ protected AbstractS3ACommitter( LOG.debug(\"{} instantiated for job \\\"{}\\\" ID {} with destination {}\", role, jobName(context), jobIdString(context), outputPath); S3AFileSystem fs = getDestS3AFS(); + if (!fs.isMultipartUploadEnabled()) { Review Comment: they all use multiparts as that is how they write-but-don't-commit the data. this is something harshit and I worked on", "created": "2023-04-11T09:47:40.511+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162598087 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -414,6 +414,11 @@ public class S3AFileSystem extends FileSystem implements StreamCapabilities, */ private ArnResource accessPoint; + /** + * Is this S3A FS instance has multipart uploads enabled? Review Comment: grammar nit \"is multipart upload enabled?\" ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,40 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + /** + * Validates the output stream configuration + * @param conf : configuration object for the given context + * @throws IOException : throws an IOException on config mismatch + */ + public static void validateOutputStreamConfiguration(Configuration conf) throws IOException { + if(!checkDiskBuffer(conf)){ + throw new IOException(\"Unable to create OutputStream with the given\" + + \" multipart upload and buffer configuration.\"); + } + } + + /** + * Check whether the configuration for S3ABlockOutputStream is + * consistent or not. Multipart uploads allow all kinds of fast buffers to + * be supported. When the option is disabled only disk buffers are allowed to + * be used as the file size might be bigger than the buffer size that can be + * allocated. + * @param conf : configuration object for the given context + * @return true if the disk buffer and the multipart settings are supported + */ + public static boolean checkDiskBuffer(Configuration conf) { + boolean isMultipartUploadEnabled = conf.getBoolean(MULTIPART_UPLOADS_ENABLED, + MULTIPART_UPLOAD_ENABLED_DEFAULT); + if (isMultipartUploadEnabled) { + return true; + } else if (!isMultipartUploadEnabled && conf.get(FAST_UPLOAD_BUFFER) Review Comment: can be simplified to ``` return isMultipartUploadEnabled || FAST_UPLOAD_BUFFER_DISK.equals(conf.get(FAST_UPLOAD_BUFFER, DEFAULT_FAST_UPLOAD_BUFFER)); ``` that default in conf.get is critical to prevent NPEs if the option is unset, moving the constant first even more rigorous ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/api/RequestFactory.java: ########## @@ -199,7 +200,7 @@ AbortMultipartUploadRequest newAbortMultipartUploadRequest( */ Review Comment: document the @throws IOException, stating when it is raised. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1854,7 +1863,8 @@ private FSDataOutputStream innerCreateFile( .withCSEEnabled(isCSEEnabled) .withPutOptions(putOptions) .withIOStatisticsAggregator( - IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator()); + IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator()) + .withMultipartEnabled(isMultipartUploadEnabled); Review Comment: nit, indentation. should be aligned with the `.with` above ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestStagingCommitProtocolFailure.java: ########## @@ -0,0 +1,59 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.staging.integration; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.InternalCommitterConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestStagingCommitProtocolFailure extends AbstractS3ATestBase { + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); Review Comment: again clear any overrides anyone has set. ``` removeBucketOverrides(getTestBucketName(conf), conf, S3A_COMMITTER_FACTORY_KEY, FS_S3A_COMMITTER_NAME, MULTIPART_UPLOADS_ENABLED); } ``` ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -460,7 +466,10 @@ public AbortMultipartUploadRequest newAbortMultipartUploadRequest( @Override public InitiateMultipartUploadRequest newMultipartUploadRequest( final String destKey, - @Nullable final PutObjectOptions options) { + @Nullable final PutObjectOptions options) throws IOException { + if (!isMultipartUploadEnabled) { + throw new IOException(\"Multipart uploads are disabled on the given filesystem.\"); Review Comment: make a PathIOException and include destkey. This gives a bit more detail. ``` throw new PathIOException(destKey, \"Multipart uploads are disabled\"); ``` ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,58 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); Review Comment: tests need to make sure that they've removed any per-bucket settings as they can trigger false failures. There's a method in S3ATestUtils to do this - `AbstractCommitITest` shows its use ``` removeBucketOverrides(getTestBucketName(conf), conf, MAGIC_COMMITTER_ENABLED, S3A_COMMITTER_FACTORY_KEY, FS_S3A_COMMITTER_NAME, MULTIPART_UPLOADS_ENABLED); } ```", "created": "2023-04-11T10:28:49.539+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1503339214 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 42s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 20s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 35s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 53s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 5s | [/patch-unit-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 93m 39s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 412427820e6b 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7207fddd59f637e3bc05ae2603f9855457c049d8 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 626 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-11T13:19:27.101+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1504044502 added a change in #5543 to pull in here. now, what do we do for large file renames. currently the transfer manager using that part size to trigger use of MPUs in renames; it doesn't use our request factory so it won't surface. we could add a modified auditor which would trigger an exception on any MPU initialisation POST, then make sure the huge file renames don't trigger it...", "created": "2023-04-11T20:24:11.958+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1504045199 or just bypass the xfer manager entirely in this world and do a single copy request?", "created": "2023-04-11T20:24:56.091+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1504053513 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 20s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 43s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt]([CI_URL] | hadoop-tools/hadoop-aws: The patch generated 2 new + 9 unchanged - 0 fixed = 11 total (was 9) | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 31s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 94m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5543 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux dbd69a77967d 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d990781d2b5e08d81afeaf22f96e9af8f06850e6 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 737 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-11T20:33:05.754+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5543: URL: https://github.com/apache/hadoop/pull/5543#discussion_r1163358833 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -849,14 +855,29 @@ long dataSize() { return bytesWritten; } + /** + * Does this block have unlimited space? + * @return true if a block with no size limit was created. + */ + private boolean unlimited() { + return limit < 0; + } + @Override boolean hasCapacity(long bytes) { - return dataSize() + bytes <= limit; + return unlimited() || dataSize() + bytes <= limit; } + /** + * {@inheritDoc}. + * If there is no limit to capacity, return MAX_VALUE. + * @return capacity in the block. + */ @Override long remainingCapacity() { - return limit - bytesWritten; + return unlimited() Review Comment: remainingCapacity is long so shouldn't it be long.MAX_VALUE", "created": "2023-04-11T21:38:48.362+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5543: URL: https://github.com/apache/hadoop/pull/5543#discussion_r1163383275 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -849,14 +855,29 @@ long dataSize() { return bytesWritten; } + /** + * Does this block have unlimited space? + * @return true if a block with no size limit was created. + */ + private boolean unlimited() { + return limit < 0; + } + @Override boolean hasCapacity(long bytes) { - return dataSize() + bytes <= limit; + return unlimited() || dataSize() + bytes <= limit; } + /** + * {@inheritDoc}. + * If there is no limit to capacity, return MAX_VALUE. + * @return capacity in the block. + */ @Override long remainingCapacity() { - return limit - bytesWritten; + return unlimited() Review Comment: Although I see we are always casting to int. So should fine. I am assuming it is like that as we are writing the big file in disk in loop.", "created": "2023-04-11T22:11:46.489+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur commented on code in PR #5543: URL: https://github.com/apache/hadoop/pull/5543#discussion_r1163383275 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -849,14 +855,29 @@ long dataSize() { return bytesWritten; } + /** + * Does this block have unlimited space? + * @return true if a block with no size limit was created. + */ + private boolean unlimited() { + return limit < 0; + } + @Override boolean hasCapacity(long bytes) { - return dataSize() + bytes <= limit; + return unlimited() || dataSize() + bytes <= limit; } + /** + * {@inheritDoc}. + * If there is no limit to capacity, return MAX_VALUE. + * @return capacity in the block. + */ @Override long remainingCapacity() { - return limit - bytesWritten; + return unlimited() Review Comment: Although I see we are always casting to int. So should fine. I think it is like that as we are writing the big file in disk in loop.", "created": "2023-04-11T23:03:06.761+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1504240307 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 21s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 39s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 94m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5543 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux 5a06c41cf424 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9f07eba3bddc23d8f08aa87731f7c0e8e0a27b38 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 729 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-11T23:09:02.492+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1504241884 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 23s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 34s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 53s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 35s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 95m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5543 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux 5ccf2de415b0 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9f07eba3bddc23d8f08aa87731f7c0e8e0a27b38 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 700 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-11T23:10:47.729+0000"}, {"author": "ASF GitHub Bot", "body": "mukund-thakur merged PR #5543: URL: https://github.com/apache/hadoop/pull/5543", "created": "2023-04-11T23:47:48.782+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1505272396 I have a followup for this feature, primarily to reject multipart copy requests when disabled, test to verify that for a large enough threshold, calls don't get rejected.", "created": "2023-04-12T13:23:59.975+0000"}, {"author": "Steve Loughran", "body": "[~harshit.gupta] can you do a PR of the trunk commit against branch-3.3 while I do a followup?", "created": "2023-04-12T13:25:07.524+0000"}, {"author": "Harshit Gupta", "body": "Sure, I will get on it.", "created": "2023-04-12T15:02:47.920+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1516269415 @HarshitGupta11 create a new PR with your change for yetus to review, then we can merge through the github ui. No need code reviews, unless related to the backport itself", "created": "2023-04-20T12:48:44.293+0000"}, {"author": "Steve Loughran", "body": "resolved in 3.4.0; harshit -if you want to backport, submit a followup pr", "created": "2023-04-27T10:01:46.024+0000"}, {"author": "ASF GitHub Bot", "body": "HarshitGupta11 opened a new pull request, #5630: URL: https://github.com/apache/hadoop/pull/5630 ### Description of PR Backport of HADOOP-18637 ### How was this patch tested? The patch was tested against uswest-2 ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-05-08T10:11:21.455+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5630: URL: https://github.com/apache/hadoop/pull/5630#issuecomment-1541917118 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ branch-3.3 Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 12s | | branch-3.3 passed | | +1 :green_heart: | compile | 0m 34s | | branch-3.3 passed | | +1 :green_heart: | checkstyle | 0m 30s | | branch-3.3 passed | | +1 :green_heart: | mvnsite | 0m 42s | | branch-3.3 passed | | +1 :green_heart: | javadoc | 0m 36s | | branch-3.3 passed | | +1 :green_heart: | spotbugs | 1m 18s | | branch-3.3 passed | | +1 :green_heart: | shadedclient | 25m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 103m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5630 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux d88a42d9ca82 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.3 / b8d6365b9f60a1978f8fca0a1f4b245a9d20fe06 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~18.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 641 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-10T10:50:00.018+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran merged PR #5630: URL: https://github.com/apache/hadoop/pull/5630", "created": "2023-05-10T14:58:59.844+0000"}, {"author": "ASF GitHub Bot", "body": "HarshitGupta11 opened a new pull request, #5641: URL: https://github.com/apache/hadoop/pull/5641 ### Description of PR Backport of HADOOP-18637 ### How was this patch tested? The patch is tested against uswest-2 ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-05-11T03:20:19.692+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5641: URL: https://github.com/apache/hadoop/pull/5641#issuecomment-1543338764 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 2m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ branch-3.3 Compile Tests _ | | +1 :green_heart: | mvninstall | 36m 33s | | branch-3.3 passed | | +1 :green_heart: | compile | 0m 39s | | branch-3.3 passed | | +1 :green_heart: | checkstyle | 0m 35s | | branch-3.3 passed | | +1 :green_heart: | mvnsite | 0m 46s | | branch-3.3 passed | | +1 :green_heart: | javadoc | 0m 41s | | branch-3.3 passed | | +1 :green_heart: | spotbugs | 1m 21s | | branch-3.3 passed | | +1 :green_heart: | shadedclient | 24m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 34s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 99m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5641 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux 026421debab8 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.3 / 3361f432b5fe7678305460b0a37140bf2dfcf512 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~18.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | [CI_URL] | | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-05-11T05:00:50.314+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5641: URL: https://github.com/apache/hadoop/pull/5641#issuecomment-1543792390 1. what was missing from the previous cherrypick? 2. what were the full commands passed in to maven?", "created": "2023-05-11T11:05:35.681+0000"}, {"author": "ASF GitHub Bot", "body": "HarshitGupta11 commented on PR #5641: URL: https://github.com/apache/hadoop/pull/5641#issuecomment-1545412081 Hi @steveloughran, 1. Due to the merge resolution, the S3ABlockOutputStreamBuilder was not being properly initialised, also the threadpool was not being initialised with the prefetching threads. 2. The commands that were used : `mvn -Dscale -Dparallel-tests -DtestsThreadCount=8 -fae clean verify`", "created": "2023-05-12T08:58:16.351+0000"}, {"author": "Shilun Fan", "body": "-3.3.6 release has been fixed, fix version removed 3.4.0-", "created": "2024-01-21T03:12:35.030+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-3440029664 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-24T00:20:19.590+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5481: HADOOP-18637:S3A to support upload of files greater than 2 GB using DiskBlocks URL: https://github.com/apache/hadoop/pull/5481", "created": "2025-10-25T00:22:12.737+0000"}], "derived_tasks": {"summary": "S3A to support upload of files greater than 2 GB using DiskBlocks - Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskB...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-18635", "title": "Expose distcp counters to user via config parameter and distcp contants", "description": "Currently users or application such as Hive cannot access directly the distcp counters such as total number of bytes copied by distcp operation. This Jira is to enable this functionality in distcp tool.", "status": "Open", "priority": "Major", "reporter": "Amit Saonerkar", "assignee": null, "created": "2023-02-15T14:19:19.000+0000", "updated": "2025-10-26T00:24:24.000+0000", "labels": ["pull-request-available"], "components": ["tools/distcp"], "comments": [{"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1431585282 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 58s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 36s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-tools/hadoop-distcp: The patch generated 3 new + 18 unchanged - 0 fixed = 21 total (was 18) | | +1 :green_heart: | mvnsite | 0m 25s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 15m 58s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 117m 35s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1c80ac3f44b1 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a3dcbd611b0d06f06431e4128c96bcfacf9c3866 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 704 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-15T15:51:05.954+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1432634125 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 56s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 36s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-tools/hadoop-distcp: The patch generated 5 new + 29 unchanged - 0 fixed = 34 total (was 29) | | +1 :green_heart: | mvnsite | 0m 26s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 57s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 15m 21s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 120m 24s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6e382f48f842 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2bce2f6885de6486f50e21fab63479fb77308225 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-16T07:21:38.611+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1434208005 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 42m 15s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 37s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 16s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 26s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 53s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 16m 0s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 119m 28s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ac77e5e911bb 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3b4574246a6cbf9d3f04c7a15aa1fd5f7f7fe61c | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 572 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-17T07:16:33.612+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5402: URL: https://github.com/apache/hadoop/pull/5402#discussion_r1109612693 ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java: ########## @@ -452,7 +454,12 @@ public void testDistcpLargeFile() throws Exception { }; LOG.info(\"_____ running distcp: \" + args[0] + \" \" + args[1]); - ToolRunner.run(conf, new DistCp(), args); + DistCp distcpTool = new DistCp(); + ToolRunner.run(conf, distcpTool, args); + final long bytesCopied = NumberUtils.toLong(distcpTool.getConf(). + get(CONF_LABEL_DISTCP_TOTAL_BYTES_COPIED), 0); + assertEquals(\"Bytes copied by distcp tool should match source file length\", Review Comment: assert args are the wrong way round for the generated error messages.", "created": "2023-02-17T10:48:58.041+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1441367924 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 5s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 49s | | the patch passed | | +1 :green_heart: | shadedclient | 26m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 52s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 123m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a45725ac2cee 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 15669c18810037abf5204f0fd868e23d253532c8 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 585 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-23T08:28:14.590+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1443788914 I really don't like how the results come back. I'm going to propose adding IOStatistics support to distcp so lined up for future work and to not modify the source config to suddenly become two way exchange of data 1. DistCp to implement IOStatisticsSource 1. until job finishes, getIOStatistics() to return null 3. when job finished, ``` // to create a builder IOStatisticsStore iostats = IOStatisticsBinding.iostatisticsStore() .withCounter(DISTCP_TOTAL_BYTES_COPIED) .build() // then set the counter to the retrieved value iostats.setCounter(DISTCP_TOTAL_BYTES_COPIED, <counter>) ``` This is extra work and you have to learn a new api, but * IOStatisticsAssertions has the asserts * IOStatisticsLogging has pretty printing * you can take an IOStatisticsSnapshot and send over the wire as json or java serialized object * lines it up perfectly for us collecting more detailed stats, not just from the workers (trickier...) but also cost of directory scanning, cleanup etc.", "created": "2023-02-24T14:48:28.567+0000"}, {"author": "Shilun Fan", "body": "I deleted 3.4.0 from the fix version. When PR is merged, we will set the fix version again.", "created": "2024-01-16T08:30:52.752+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-3447890992 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-26T00:24:24.203+0000"}], "derived_tasks": {"summary": "Expose distcp counters to user via config parameter and distcp contants - Currently users or application such as Hive cannot access directly the di...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-18632", "title": "ABFS: Customize and optimize timeouts made based on each separate request", "description": "In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success.", "status": "Open", "priority": "Major", "reporter": "Sree Bhattacharyya", "assignee": "Sree Bhattacharyya", "created": "2023-02-15T09:21:31.000+0000", "updated": "2025-10-26T00:24:25.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "ASF GitHub Bot", "body": "sreeb-msft opened a new pull request, #5399: URL: https://github.com/apache/hadoop/pull/5399 <!-- Thanks for sending a pull request! 1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute 2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'. --> ### Description of PR In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-02-15T09:23:22.003+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431009246 ---", "created": "2023-02-15T09:28:42.094+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431015489 ------------------------ :::: AGGREGATED TEST RESULT :::: HNS-OAuth ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:329 \u00bb TestTimedOut test timed o... [INFO] [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected:<1> but was:<0> [INFO] [ERROR] Tests run: 336, Failures: 1, Errors: 0, Skipped: 41 HNS-SharedKey ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:334 \u00bb TestTimedOut test timed o... [INFO] [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54 [INFO] Results: [INFO] [WARNING] Tests run: 336, Failures: 0, Errors: 0, Skipped: 41 NonHNS-SharedKey ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAzureBlobFileSystemRandomRead.testValidateSeekBounds:269->Assert.assertTrue:42->Assert.fail:89 There should not be any network I/O (elapsedTimeMs=526). [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:336 \u00bb TestTimedOut test timed o... [INFO] [ERROR] Tests run: 569, Failures: 1, Errors: 1, Skipped: 278 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAbfsTerasort.test_110_teragen:244->executeStage:211->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 teragen(1000, abfs://testcontainer@sreebcitestnonhns.dfs.core.windows.net/ITestAbfsTerasort/sortin) failed expected:<0> but was:<1> [ERROR] ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected:<1> but was:<0> [ERROR] Errors: [ERROR] ITestAbfsJobThroughManifestCommitter.test_0420_validateJob \u00bb OutputValidation ... [ERROR] ITestAbfsManifestCommitProtocol.testCommitLifecycle \u00bb OutputValidation `abfs:/... [ERROR] ITestAbfsManifestCommitProtocol.testCommitterWithDuplicatedCommit \u00bb OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testConcurrentCommitTaskWithSubDir \u00bb OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testMapFileOutputCommitter \u00bb OutputValidation ... [ERROR] ITestAbfsManifestCommitProtocol.testOutputFormatIntegration \u00bb OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testParallelJobsToAdjacentPaths \u00bb OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testTwoTaskAttemptsCommit \u00bb OutputValidation `... [INFO] [ERROR] Tests run: 336, Failures: 2, Errors: 8, Skipped: 46 AppendBlob-HNS-OAuth ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:344->lambda$testAcquireRetry$6:345 \u00bb TestTimedOut [INFO] [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected:<1> but was:<0> [INFO] [ERROR] Tests run: 336, Failures: 1, Errors: 0, Skipped: 41 Time taken: 39 mins 34 secs.", "created": "2023-02-15T09:33:39.918+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431176861 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 34s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 14 new + 55 unchanged - 0 fixed = 69 total (was 55) | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 2 unchanged - 0 fixed = 12 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 24s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 37s | [/results-asflicense.txt]([CI_URL] | The patch generated 3 ASF License warnings. | | | | 105m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 379ced44d885 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c9e53a79ec634a3f9a496425cafe215625784690 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 563 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-15T11:10:07.693+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1107015958 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } Review Comment: Lets add it inside else block above. Reason being, if block is always having this key false. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: should RuntimeException be thrown here? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java: ########## @@ -0,0 +1,155 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.junit.Test; + +import java.io.IOException; +import java.net.URL; +import java.util.HashMap; +import java.util.Map; + + +public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest { + + private boolean optimizeTimeout; + private int maxRequestTimeout; + private int requestTimeoutIncRate; + private HashMap<AbfsRestOperationType, Integer> opMap = new HashMap<AbfsRestOperationType, Integer>(); + + public ITestAbfsCustomTimeout() throws Exception { + super(); + initOpTypeRequestTimeout(); + } + + @Test + public void testOptimizer() throws IOException, IllegalAccessException { + + AbfsConfiguration abfsConfig = getConfiguration(); + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"true\"); + abfsConfig.set(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT, \"90\"); + abfsConfig.set(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE, \"2\"); + optimizeTimeout =true; + maxRequestTimeout = 90; + requestTimeoutIncRate = 2; + AbfsConfiguration newConfig = new AbfsConfiguration(abfsConfig.getRawConfiguration(), getAccountName()); + + for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + config = ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + config = ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + config = ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + config = ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.ListPaths) { + config = ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.CreatePath) { + config = ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.RenamePath) { + config = ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetAcl) { + config = ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + config = ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + config = ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetAcl) { + config = ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetOwner) { + config = ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetPermissions) { + config = ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.CheckAccess) { + config = ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + config = ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT; + } + abfsConfig.set(config, Integer.toString(timeout)); + testInitTimeoutOptimizer(opType, 3, timeout, newConfig); + abfsConfig.unset(config); + } + + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + + } + + public void testInitTimeoutOptimizer(AbfsRestOperationType opType, int maxRetryCount, int expectedReqTimeout, AbfsConfiguration abfsConfig) throws IOException { + + AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration()); + AbfsClient client = fs.getAbfsStore().getClient(); + String query = client.createDefaultUriQueryBuilder().toString(); + URL url = client.createRequestUrl(\"/testPath\", query); + TimeoutOptimizer opt = new TimeoutOptimizer(url, opType, client.getRetryPolicy(), getConfiguration()); + int retryCount = 0; + while (retryCount <= maxRetryCount) { Review Comment: Great test. This would be a unit-test for the functionality of TimeoutOptimizer. Can we add a test where-in, processResponse() of abfsHttpOperation fails, and we assert how abfsRestOperation and TimeoutOptimizer is working. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: lets use switch case. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); Review Comment: Let have it as Integer and not primitive int. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: readTimeout and connTimeout are in ms, requestTimeout is in second. so if requestTimeout is 90 it will set readTimeout as 90 which JDK understands 90 MILLISECONDS and NOT seconds. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: requestTimeout == -1? What would be the implications. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Lets have a NPE check. Agree that abfsclient adds the param. But if in future, some other class wants to use this class. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { Review Comment: timeout == null || timeout.isEmpty() ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: Also, what if we have a field in AbfsRestOperationType enum containing the config name?", "created": "2023-02-16T04:48:24.334+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1432508287 Please add the test class in https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/pom.xml#L601-L608 and https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/pom.xml#L644-L652, else it will break the runTest script runs.", "created": "2023-02-16T04:51:32.504+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108011727 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -276,14 +280,15 @@ public AbfsHttpOperation(final URL url, final String method, final List<AbfsHttp } } - this.connection.setConnectTimeout(CONNECT_TIMEOUT); - this.connection.setReadTimeout(READ_TIMEOUT); + this.connection.setConnectTimeout(timeoutOptimizer.getConnTimeout(CONNECT_TIMEOUT)); + this.connection.setReadTimeout(timeoutOptimizer.getReadTimeout(READ_TIMEOUT)); this.connection.setRequestMethod(method); for (AbfsHttpHeader header : requestHeaders) { this.connection.setRequestProperty(header.getName(), header.getValue()); } + Review Comment: Can we remove the extra lines, makes it difficult to backport", "created": "2023-02-16T05:03:50.913+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108012054 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -555,6 +560,7 @@ public static class AbfsHttpOperationWithFixedResult extends AbfsHttpOperation { public AbfsHttpOperationWithFixedResult(final URL url, final String method, final int httpStatus) { + Review Comment: Remove extra line.", "created": "2023-02-16T05:04:05.939+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108030121 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -117,9 +125,10 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: URL can be made to final in timeoutoptimizer also", "created": "2023-02-16T05:27:38.220+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108031419 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { Review Comment: Add javadoc for the class and comments.", "created": "2023-02-16T05:30:14.173+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108034619 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); Review Comment: should we add a null check here as well ?", "created": "2023-02-16T05:37:03.885+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108030121 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -117,9 +125,10 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: URL can be made to final in timeoutoptimizer also", "created": "2023-02-16T05:38:13.830+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108034619 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); Review Comment: should we add a null check here as well or should we have default values for this as we are taking dependency on some config ? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); Review Comment: Same as above.", "created": "2023-02-16T05:38:38.869+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108035699 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } Review Comment: Line break.", "created": "2023-02-16T05:39:18.916+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108037262 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { Review Comment: Since we are taking dependency on configs everywhere we should add a NP check or add default value for each config.", "created": "2023-02-16T05:42:29.083+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108037646 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: should we throw back the exception in catch block ?", "created": "2023-02-16T05:43:19.112+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108048351 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } Review Comment: Are you suggesting moving just the if block code to the above else block? Or including the if check and the following code in the block together in the above else block?", "created": "2023-02-16T06:04:10.337+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108048702 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: Can try to have an enum with the AbfsRestOperationType and corresponding ConfigurationKey", "created": "2023-02-16T06:05:00.360+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108049244 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: Had kept this empty because an already formed URL will be used (just modifying one query parameter), but can throw RuntimeException over here.", "created": "2023-02-16T06:06:00.441+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108070379 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: Thanks for pointing this out. Have made the necessary changes.", "created": "2023-02-16T06:43:12.275+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108079007 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: Setting request timeout (and all other timeouts) to -1 can be thought of as a flag value that is being used. Although the value for request timeout does not get checked, the other timeout values get checked (getReadTimeout and getConnTimeout calls). So to keep with the other timeouts initializations this is also set to -1. Would you suggest changing this in any way?", "created": "2023-02-16T06:56:58.045+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108081097 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); Review Comment: Added the change.", "created": "2023-02-16T07:00:13.258+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108083553 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Are you referring to a check on the url.getQuery() or the timeout parameter itself?", "created": "2023-02-16T07:03:59.124+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109277491 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: i understand that url will always be correct on which we would do manipulation, but still better to throw RuntimeException. maybe in future, some other class wants to use this timeoutOptimizer, it can use it. This class should be agnostic to whatever flowing in.", "created": "2023-02-17T03:56:24.210+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109278671 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: what does server understands from requestTimeout == -1?", "created": "2023-02-17T03:58:14.352+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109279454 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } Review Comment: ``` if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { this.shouldOptimizeTimeout = false; } else { this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); if (this.shouldOptimizeTimeout) { this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get( ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get( ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); initTimeouts(); updateUrl(); } } ```", "created": "2023-02-17T03:59:29.401+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109283269 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: We understand that the request can take maximum of (connTimeout + requestTimeout) and JDK will not throw any issue? Feel that connTimeout can be kept more aggressive. Why is readTimeout, connTimeout a function of requestTimeout? Feel that diff types of exception should change diff parameters. CC: @snvijaya @anmolanmol1234", "created": "2023-02-17T04:05:49.769+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109284406 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Suggestion: lets have timeoutPos as Integer and not primitive. Then: `if(timeoutPos != null && timeoutPos < 0)`", "created": "2023-02-17T04:07:49.877+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109316607 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: ReadTimeout can be a function of requestTimeout, but connectionTimeout is totally independent to requestTimeout. Until connection is not established, there is no significance of requestTimeout as server would not have started processing.", "created": "2023-02-17T05:25:10.491+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109318466 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { Review Comment: This I understand is per-call level. Should we use the feedback loop across the lifetime of abfsClient. ex: 10th Read call should use the intellegence it achieved in the first Read call. Also, since we say that we will start from aggressive value, the retries would be same across all API calls in the lifetime of AbfsClient object. CC: @snvijaya @anmolanmol1234", "created": "2023-02-17T05:28:46.068+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109318466 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { Review Comment: This I understand is per-call level. Should we use the feedback loop across the lifetime of abfsClient. ex: 10th Read call should use the intellegence it achieved in the first Read call. Also, since we say that we will start from aggressive value, the retries would be same across all API calls in the lifetime of AbfsClient object. ex: lets take out connTimeout is aggresive say 100 ms. And network is not able to connect in that time. And through our heuristic lets say on 10th retry, connTimeout become 1 sec which is good with network.\u00a0Now all API call would have to fail nearly 10 times to work on that network CC: @snvijaya @anmolanmol1234", "created": "2023-02-17T05:38:41.869+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1111514936 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: Have added RuntimeException for both the catch blocks in TimeoutOptimizer class.", "created": "2023-02-20T06:42:36.296+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1111563391 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { Review Comment: Null Pointer Check is added at the end, in case any of the configs are not set. At present in case any of the configs are not set, manually setting this timeout value to the default of 90. Setting a default value for the config can also be considered.", "created": "2023-02-20T07:22:18.320+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1436762577 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 41s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | -1 :x: | javadoc | 0m 42s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 23s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 1m 8s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 23m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 3 ASF License warnings. | | | | 105m 0s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:[line 137] | | | Switch statement found in org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() where default case is missing At TimeoutOptimizer.java:where default case is missing At TimeoutOptimizer.java:[lines 149-205] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 5f0464b0b1b3 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 10fd6c2ac7054d7e306286dc18e1824e26c3da47 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 711 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-20T11:07:38.872+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1112771893 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Understood, thanks!", "created": "2023-02-21T09:11:48.678+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1439558099 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 29s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 31s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 31s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | -1 :x: | mvnsite | 0m 30s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 24s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 0m 29s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 26m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 35s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 3 ASF License warnings. | | | | 105m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux e74be7b85008 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0761741b5e6964ee63964d308bdd596c69eecf52 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 662 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-22T07:38:21.931+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1115313395 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: This value of the request timeout would never reach the server. This is just used as a flag value. In the case where request timeout / read timeout stays set as -1, the optimizer can be considered to not be doing any work (ie, when either the config for whether optimization should happen or not is set to false, or when the request in consideration does not have a timeout value in its query parameters, like create Filesystem, or when any of the configs are incorrectly set).", "created": "2023-02-23T07:28:28.239+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1115320710 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + updateUrl(); + } + + private void updateTimeouts(int retryCount) { + if (retryCount == 0) { + return; + } + int maxRetryCount = retryPolicy.getRetryCount(); + if (retryCount <= maxRetryCount && timeoutIncRate > 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: Made the necessary changes.", "created": "2023-02-23T07:39:34.356+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1115418467 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -167,6 +168,10 @@ public String getResponseHeader(String httpHeader) { return connection.getHeaderField(httpHeader); } + public TimeoutOptimizer getTimeoutOptimizer() { Review Comment: seems unused. Lets remove it. In case needed in test, you may have package-protected access and have @visibleForTesting annotation. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -94,6 +95,8 @@ public URL getUrl() { return url; } + public TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; } Review Comment: lets have package-protected access and @visibleForTesting annotation. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { Review Comment: initTimeouts can happen only if shouldOptimizeTimeout is true. Lets remove this if-block? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; Review Comment: lets say we did requestTimeout = -1 and returned, the constructor then calls updateUrl(), it will set -1. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + switch(opType) { + case CreateFileSystem: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + break; + case GetFileSystemProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + break; + case SetFileSystemProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + break; + case DeleteFileSystem: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + break; + case ListPaths: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + break; + case CreatePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + break; + case RenamePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + break; + case GetAcl: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + break; + case GetPathProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + break; + case SetPathProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + break; + case SetAcl: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + break; + case SetOwner: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + break; + case SetPermissions: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + break; + case Append: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + break; + case CheckAccess: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + break; + case GetPathStatus: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + break; + case Flush: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + break; + case ReadFile: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + break; + case LeasePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + break; + } + if (timeout == null || timeout.isEmpty()) { + // if any of the timeout values are not set + // despite optimize config set to true + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); Review Comment: can we set max value for requestTimeout. Reason being, what if config-setter, thought its in ms and give something like 10000 thinking it 10s but is actually 10000sec, ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { Review Comment: its future works. lets resolve it. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java: ########## @@ -0,0 +1,218 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; + +import org.junit.Test; +import org.mockito.Mockito; +import org.mockito.invocation.InvocationOnMock; +import org.mockito.stubbing.Answer; + +import java.io.IOException; +import java.net.URL; +import java.util.ArrayList; +import java.util.Arrays; +import java.util.HashMap; +import java.util.Map; + +import static java.net.HttpURLConnection.HTTP_OK; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FILESYSTEM; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_HEAD; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RESOURCE; +import static org.mockito.ArgumentMatchers.nullable; + + +public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest { + private int maxRequestTimeout; + private int requestTimeoutIncRate; + private HashMap<AbfsRestOperationType, Integer> opMap = new HashMap<AbfsRestOperationType, Integer>(); + private HashMap<AbfsRestOperationType, String> opTimeoutConfigMap = new HashMap<AbfsRestOperationType, String>(); + + public ITestAbfsCustomTimeout() throws Exception { + super(); + initOpTypeConfigs(); + } + + @Test + public void testOptimizer() throws IOException, IllegalAccessException { + + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + + for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + testInitTimeoutOptimizer(opType, 3, timeout, abfsConfig); + abfsConfig.unset(config); + } + + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + + } + + /** + * Test to verify working of timeout optimization with AbfsRestOperation execute calls + * Currently tests only for a single API + * @throws IOException + * @throws IllegalAccessException + */ + @Test + public void testOptimizationInRestCall() throws IOException, IllegalAccessException { + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + AzureBlobFileSystem newFs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration()); + for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + AbfsRestOperation op = getMockAbfsRestOp(opType, newFs); + final int[] finalTimeout = {timeout}; + Mockito.doAnswer(new Answer() { + int requestCount = 4; Review Comment: Lets keep it outside Mockito.doAnswer. something like: ``` int[] request = new int[1]; request[0]=4; ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well Review Comment: why? i feel timeout=90 is added everywhere. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java: ########## @@ -0,0 +1,218 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; + +import org.junit.Test; +import org.mockito.Mockito; +import org.mockito.invocation.InvocationOnMock; +import org.mockito.stubbing.Answer; + +import java.io.IOException; +import java.net.URL; +import java.util.ArrayList; +import java.util.Arrays; +import java.util.HashMap; +import java.util.Map; + +import static java.net.HttpURLConnection.HTTP_OK; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FILESYSTEM; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_HEAD; +import static org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams.QUERY_PARAM_RESOURCE; +import static org.mockito.ArgumentMatchers.nullable; + + +public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest { + private int maxRequestTimeout; + private int requestTimeoutIncRate; + private HashMap<AbfsRestOperationType, Integer> opMap = new HashMap<AbfsRestOperationType, Integer>(); + private HashMap<AbfsRestOperationType, String> opTimeoutConfigMap = new HashMap<AbfsRestOperationType, String>(); + + public ITestAbfsCustomTimeout() throws Exception { + super(); + initOpTypeConfigs(); + } + + @Test + public void testOptimizer() throws IOException, IllegalAccessException { + + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + + for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + testInitTimeoutOptimizer(opType, 3, timeout, abfsConfig); + abfsConfig.unset(config); + } + + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + + } + + /** + * Test to verify working of timeout optimization with AbfsRestOperation execute calls + * Currently tests only for a single API + * @throws IOException + * @throws IllegalAccessException + */ + @Test + public void testOptimizationInRestCall() throws IOException, IllegalAccessException { + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + AzureBlobFileSystem newFs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration()); + for (Map.Entry<AbfsRestOperationType, Integer> it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + AbfsRestOperation op = getMockAbfsRestOp(opType, newFs); + final int[] finalTimeout = {timeout}; + Mockito.doAnswer(new Answer() { + int requestCount = 4; + + public Object answer(InvocationOnMock invocation) { + if (requestCount > 0) { + requestCount--; + assertEquals(finalTimeout[0], op.getTimeoutOptimizer().getRequestTimeout()); + if (finalTimeout[0] * requestTimeoutIncRate > maxRequestTimeout) { + finalTimeout[0] = maxRequestTimeout; + } else { + finalTimeout[0] *= requestTimeoutIncRate; + } + } + return op.getResult(); + } + }).when(op).createHttpOperationInstance(); + op.execute(getTestTracingContext(newFs, true)); + abfsConfig.unset(config); + } + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + } + + private AbfsRestOperation getMockAbfsRestOp(AbfsRestOperationType opType, AzureBlobFileSystem fs) throws IOException { + + AbfsClient spyClient = Mockito.spy(getAbfsClient(fs.getAbfsStore())); + + // creating the parameters (Url and request headers) to initialize AbfsRestOperation + AbfsUriQueryBuilder queryBuilder = spyClient.createDefaultUriQueryBuilder(); + URL url = spyClient.createRequestUrl(\"/\", queryBuilder.toString()); + + AbfsRestOperation spyRestOp = Mockito.spy(new AbfsRestOperation(opType, spyClient, HTTP_METHOD_HEAD, url, new ArrayList<>())); + + AbfsHttpOperation mockHttpOp = Mockito.spy(spyRestOp.createHttpOperationInstance()); + Mockito.doAnswer(new Answer() { + private int count = 0; Review Comment: let keep count outside mockito.", "created": "2023-02-23T09:55:44.561+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1441540148 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 17s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 13s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 10 new + 2 unchanged - 0 fixed = 12 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 1m 6s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 23m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 105m 5s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:[line 132] | | | Switch statement found in org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() where default case is missing At TimeoutOptimizer.java:where default case is missing At TimeoutOptimizer.java:[lines 143-199] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 8525889b434d 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 75c6d196ae6cdac9d45872cabe034648bee7342d | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-23T10:43:57.974+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116842887 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well Review Comment: Not added in cases like createFilesystem", "created": "2023-02-24T11:12:32.372+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116849374 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; Review Comment: updated this accordingly, removing any init to -1.", "created": "2023-02-24T11:18:37.995+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116879406 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -167,6 +168,10 @@ public String getResponseHeader(String httpHeader) { return connection.getHeaderField(httpHeader); } + public TimeoutOptimizer getTimeoutOptimizer() { Review Comment: Keeping it for any test cases that might be added in the future. Making package protected.", "created": "2023-02-24T11:47:40.979+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116885652 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { Review Comment: Updated!", "created": "2023-02-24T11:54:21.742+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1443690838 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | -1 :x: | javadoc | 0m 42s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 17s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 12 new + 2 unchanged - 0 fixed = 14 total (was 2) | | -1 :x: | mvnsite | 0m 18s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 17s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 17s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | spotbugs | 0m 18s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 22m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 21s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 89m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 56921c473dfe 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a00f099b26b7592853c26deabbfb237994c10766 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 597 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-24T13:35:31.720+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118282237 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -167,6 +169,11 @@ public String getResponseHeader(String httpHeader) { return connection.getHeaderField(httpHeader); } + @VisibleForTesting + protected TimeoutOptimizer getTimeoutOptimizer() { Review Comment: We have it package-protected: ``` TimeoutOptimizer getTimeoutOptimizer() ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -94,6 +96,9 @@ public URL getUrl() { return url; } + @VisibleForTesting + protected TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; } Review Comment: we have it package-protected: ``` TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; } ```", "created": "2023-02-27T04:49:58.351+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118295742 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: Resolving for future addition.", "created": "2023-02-27T05:22:51.364+0000"}, {"author": "ASF GitHub Bot", "body": "sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118303110 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + switch(opType) { + case CreateFileSystem: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + break; + case GetFileSystemProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + break; + case SetFileSystemProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + break; + case DeleteFileSystem: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + break; + case ListPaths: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + break; + case CreatePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + break; + case RenamePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + break; + case GetAcl: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + break; + case GetPathProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + break; + case SetPathProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + break; + case SetAcl: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + break; + case SetOwner: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + break; + case SetPermissions: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + break; + case Append: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + break; + case CheckAccess: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + break; + case GetPathStatus: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + break; + case Flush: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + break; + case ReadFile: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + break; + case LeasePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + break; + } + if (timeout == null || timeout.isEmpty()) { + // if any of the timeout values are not set + // despite optimize config set to true + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); Review Comment: Added a check for this.", "created": "2023-02-27T05:39:18.924+0000"}, {"author": "ASF GitHub Bot", "body": "saxenapranav commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118348558 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -134,7 +140,7 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: same comment for final. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -117,9 +122,10 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: it can be left final. Any reason to remove it?", "created": "2023-02-27T07:02:06.384+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1445810167 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 4s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | -1 :x: | javadoc | 0m 43s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 18s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 15s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 15s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 12 new + 2 unchanged - 0 fixed = 14 total (was 2) | | -1 :x: | mvnsite | 0m 17s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 15s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | spotbugs | 0m 17s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 23m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 19s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 35s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 92m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux d974df9dd342 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6ded5e25c369a53787cbdf2950a5d751d0921842 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 761 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-27T07:06:12.102+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1446163972 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 34s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 23s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt]([CI_URL] | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 1m 9s | [/new-spotbugs-hadoop-tools_hadoop-azure.html]([CI_URL] | hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 21m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 36s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 94m 7s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:[line 126] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux c6eee2203c75 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9390100e60530e7c19139900d27b9d22d0d65b66 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 664 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-27T11:27:03.930+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1451568775 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 38m 4s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 39s | [/results-asflicense.txt]([CI_URL] | The patch generated 4 ASF License warnings. | | | | 105m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 76f35de8c855 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 08000d1a7f2db671247d677ebed9be4f0f364c07 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-02T09:34:36.591+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1134213829 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,243 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.util.Preconditions; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + String query = url.getQuery(); + Integer timeoutPos = Integer.valueOf(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + switch(opType) { + case CreateFileSystem: Review Comment: why not add a field to AbfsRestOperationType giving the string prefix for all parameters, e.g. \"createfilesystem\" which is then mapped to fs.azure.request.createfilesystem.timeout and would allow for any new per-request options to be added. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -46,6 +46,32 @@ public final class ConfigurationKeys { public static final String AZURE_BACKOFF_INTERVAL = \"fs.azure.io.retry.backoff.interval\"; public static final String AZURE_MAX_IO_RETRIES = \"fs.azure.io.retry.max.retries\"; public static final String AZURE_CUSTOM_TOKEN_FETCH_RETRY_COUNT = \"fs.azure.custom.token.fetch.retry.count\"; + public static final String AZURE_REQUEST_TIMEOUT_INCREASE_RATE = \"fs.azure.timeout.increase.rate\"; + public static final String AZURE_MAX_REQUEST_TIMEOUT = \"fs.azure.max.request.timeout\"; + + // API-specific request timeout configurations + public static final String AZURE_CREATE_FS_REQUEST_TIMEOUT = \"fs.azure.createfs.request.timeout\"; Review Comment: prefer fs.azure.request.createfs.timeout why? isolates all requests under the \"fs.azure.request\" prefix and avoids mixing them with any other config option", "created": "2023-03-13T15:58:16.829+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1469788058 currently the max timeout is 90s, right? is there any way to extend this for those operations we know may be extra slow (directory delete...)", "created": "2023-03-15T10:56:33.494+0000"}, {"author": "Shilun Fan", "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.", "created": "2024-01-04T09:48:55.532+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-3447891002 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-26T00:24:25.097+0000"}], "derived_tasks": {"summary": "ABFS: Customize and optimize timeouts made based on each separate request - In present day ABFS Driver functioning, all API request calls use the s...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "HADOOP-18629", "title": "Hadoop DistCp supports specifying favoredNodes for data copying", "description": "When importing large scale data to HBase, we always generate the hfiles with other Hadoop cluster, use the Distcp tool to copy the data to the HBase cluster, and bulkload data to HBase table. However, the data locality is rather low which may result in high query latency. After taking a compaction it will recover. Therefore, we can increase the data locality by specifying the favoredNodes in Distcp. Could I submit a pull request to optimize it?", "status": "Open", "priority": "Major", "reporter": "zhuyaogai", "assignee": null, "created": "2023-02-14T03:12:19.000+0000", "updated": "2025-10-26T00:24:27.000+0000", "labels": ["pull-request-available"], "components": ["tools/distcp"], "comments": [{"author": "ASF GitHub Bot", "body": "zhuyaogai opened a new pull request, #5391: URL: https://github.com/apache/hadoop/pull/5391 ### Description of PR Hadoop DistCp supports specifying favoredNodes for data copying. ### How was this patch tested? Add new UT. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-02-14T08:00:44.590+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429555546 -1 to anything exposing internal hdfs implementation methods. Sorry People start using them and expect them to be stable and maintained. There is also the little detail that in cloud deployments do not always have hdfs jars on the class path; this PR would break those deployments. What would make sense would be to use createFile() and for hdfs to add a .opt() option for those favoured nodes, createFile() is the public api, .opt() options can be ignorred by other filesystems, *or reimplemented*. There is a lot more in terms of design and wiring up but the benefit is that portability and maintainability.", "created": "2023-02-14T11:16:12.460+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429650998 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 17m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 31m 33s | | trunk passed | | +1 :green_heart: | compile | 23m 39s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 20m 40s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 6s | | trunk passed | | +1 :green_heart: | javadoc | 1m 49s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 54s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 29s | | the patch passed | | +1 :green_heart: | compile | 22m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 41s | | the patch passed | | +1 :green_heart: | compile | 20m 43s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 43s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 37s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 9 new + 54 unchanged - 1 fixed = 63 total (was 55) | | +1 :green_heart: | mvnsite | 2m 4s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 27m 55s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 1s | | The patch does not generate ASF License warnings. | | | | 251m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestDistCpSync | | | hadoop.tools.TestDistCpWithRawXAttrs | | | hadoop.tools.TestDistCpSystem | | | hadoop.tools.TestDistCpWithAcls | | | hadoop.tools.TestDistCpWithXAttrs | | | hadoop.tools.contract.TestLocalContractDistCp | | | hadoop.tools.contract.TestHDFSContractDistCp | | | hadoop.tools.TestDistCpSyncReverseFromTarget | | | hadoop.tools.TestDistCpSyncReverseFromSource | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 060e891702b0 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / be67a5984771d510d7cfd96439215bccc3d5baca | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-14T12:13:35.375+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429680256 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 13s | | trunk passed | | +1 :green_heart: | compile | 23m 10s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 20m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 6s | | trunk passed | | +1 :green_heart: | javadoc | 1m 47s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 56s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 27s | | the patch passed | | +1 :green_heart: | compile | 22m 26s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 26s | | the patch passed | | +1 :green_heart: | compile | 20m 28s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 37s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 8 new + 55 unchanged - 1 fixed = 63 total (was 56) | | +1 :green_heart: | mvnsite | 1m 59s | | the patch passed | | +1 :green_heart: | javadoc | 1m 39s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 29m 24s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 247m 24s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestDistCpSync | | | hadoop.tools.TestDistCpWithRawXAttrs | | | hadoop.tools.TestDistCpSystem | | | hadoop.tools.TestDistCpSyncReverseFromTarget | | | hadoop.tools.TestDistCpWithAcls | | | hadoop.tools.contract.TestHDFSContractDistCp | | | hadoop.tools.TestDistCpSyncReverseFromSource | | | hadoop.tools.contract.TestLocalContractDistCp | | | hadoop.tools.TestDistCpWithXAttrs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a25803cbcfde 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 08c0cafef93ba76b6b69613bdfcc2542467ca5d3 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 735 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-14T12:35:44.529+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1430118718 > @steveloughran hi, I have fixed some problems, could you please review it again? and please correct me if I'm wrong. Thank you :)", "created": "2023-02-14T17:29:59.661+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1430405499 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 17m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 31m 37s | | trunk passed | | +1 :green_heart: | compile | 24m 53s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 22m 10s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 44s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 4s | | trunk passed | | +1 :green_heart: | javadoc | 1m 49s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 28s | | the patch passed | | +1 :green_heart: | compile | 22m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 25s | | the patch passed | | +1 :green_heart: | compile | 20m 25s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 36s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 3s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 45s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 14m 46s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 240m 40s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.contract.TestLocalContractDistCp | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d770bec9be9c 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1182895b78d4d92012d09f1fa11d88abd50ef6b7 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 667 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-02-14T21:31:40.621+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1432392281 @steveloughran hi, thanks for your suggestion:) I know what you mean, but I find that it also uses the hdfs public/stable API in source code. https://github.com/apache/hadoop/blob/723535b788070f6b103be3bae621fefe3b753081/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java#L230 I just refer to its practice in the if branch, and if you think my code changes affect too much, can I just change the else branch code and add favoredNodes option in it? Please correct me if I'm wrong. Thank you :)", "created": "2023-02-16T02:33:57.344+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1442901558 @steveloughran @toddlipcon @phunt hi, could you give me some advice? Thank you!", "created": "2023-02-24T07:12:07.805+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1442901997 @steveloughran @toddlipcon @phunt hi, could you give me some advice? Thank you!", "created": "2023-02-24T07:12:38.654+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1443728823 This is interesting. I know there are deployments without hdfs around (e.g. azure clusters), but do see that the import is there for snapshot updates (hdfs only) with explicit imports of SnapshotDiffReport. tracing it back, if you use \"-diff\" on the command line then hdfs *must* be on the classpath. your link flags up that it is already in copymapper; looking at that I don't see it in branch-3.3; it came in with HADOOP-14254. Given it is in there on a codepath hit with the option to copy erasure policy (and skipped if not), then again, provided the change goes in such that it is optional, your patch isn't going to force in a new run-time dependency, is it? let me look at the new patch some more without worrying about that detail...the builder API is public. Be aware that we are always very nervous about touching distcp because it is fairly old and brittle code that is used incredibly broadly -not just on the command line but actually at the Java API from applications like hive. I think this is fairly low risk but will highlight the JIRA on the HDFS mailing list so they can review it to.", "created": "2023-02-24T14:05:31.577+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1117059919 ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java: ########## @@ -594,4 +595,49 @@ public void testUpdateRoot() throws Exception { assertEquals(srcStatus.getModificationTime(), destStatus2.getModificationTime()); } + + @Test + public void testFavoredNodes() throws Exception { + final String testRoot = \"/testdir\"; + final String testSrc = testRoot + \"/\" + SRCDAT; + final String testDst = testRoot + \"/\" + DSTDAT; + + String nnUri = FileSystem.getDefaultUri(conf).toString(); + String rootStr = nnUri + testSrc; + String tgtStr = nnUri + testDst; + + FileEntry[] srcFiles = { + new FileEntry(SRCDAT, true), + new FileEntry(SRCDAT + \"/file10\", false) + }; + + DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(URI.create(nnUri), conf); + createFiles(fs, testRoot, srcFiles, -1); + + // sad path + assertNotEquals(ToolRunner.run(conf, new DistCp(), Review Comment: the args are the wrong way round fro these assertions. switch to AssertJ assertions and add good .descriptions, so if a jenkins runs fails we know what went wrong, rather than just what line. Right ########## hadoop-tools/hadoop-distcp/src/site/markdown/DistCp.md.vm: ########## @@ -364,6 +364,7 @@ Command Line Options | `-direct` | Write directly to destination paths | Useful for avoiding potentially very expensive temporary file rename operations when the destination is an object store | | `-useiterator` | Uses single threaded listStatusIterator to build listing | Useful for saving memory at the client side. Using this option will ignore the numListstatusThreads option | | `-updateRoot` | Update root directory attributes (eg permissions, ownership ...) | Useful if you need to enforce root directory attributes update when using distcp | +| `-favoredNodes` | Specify favored nodes (Desired option input format: host1:port1,host2:port2,...) | Useful if you need to specify favored nodes when using distcp | Review Comment: +. Requires the destination to be an hdfs filesystem ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java: ########## @@ -32,6 +32,7 @@ import java.util.List; import java.util.Random; +import org.apache.hadoop.hdfs.server.datanode.DataNode; Review Comment: nit: must go in the right place in the org.apache.hadoop imports. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List<InetSocketAddress> result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { + String[] split = hostAndPort.split(\":\"); Review Comment: log at debug, or maybe even at info. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java: ########## @@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args) } } + if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) { + String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim()); + if (StringUtils.isEmpty(favoredNodesStr)) { Review Comment: if you pull this out to a @VisibleForTesting package scoped method then unit tests could to try to break it through invalid args, e.g * trailing , * empty string invalid node/valid node bad port. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List<InetSocketAddress> result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { + String[] split = hostAndPort.split(\":\"); + if (split.length != 2) { + throw new IllegalArgumentException(\"Illegal favoredNodes parameter: \" + hostAndPort); Review Comment: prefer `org.apache.hadoop.util.Preconditions` here, e.g ``` checkArgument(split.length == 2, \"\"Illegal favoredNodes parameter: %s\", hostAndPort) ``` ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java: ########## @@ -164,6 +164,8 @@ public final class DistCpOptions { private final boolean updateRoot; + private final String favoredNodes; + Review Comment: nit: add javadocs. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { Review Comment: javadocs to explain what happens, when exceptions are raised. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List<InetSocketAddress> result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { Review Comment: what happens if an empty string is passed in? it should be an error, right? so add a test ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpOptions.java: ########## @@ -574,4 +574,15 @@ public void testUpdateRoot() { .build(); Assert.assertTrue(options.shouldUpdateRoot()); } + + @Test + public void testFavoredNodes() { + final DistCpOptions options = new DistCpOptions.Builder( + Collections.singletonList( + new Path(\"hdfs://localhost:8020/source\")), + new Path(\"hdfs://localhost:8020/target/\")) + .withFavoredNodes(\"localhost:50010\") + .build(); + Assert.assertNotNull(options.getFavoredNodes()); Review Comment: prefer assertJ for new tests, with a message, and ideally verification the node value went all the way through ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS, FSDataOutputStream out; ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum); if (!preserveEC || ecPolicy == null) { - out = targetFS.create(targetPath, permission, - EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, - repl, blockSize, context, checksumOpt); + if (targetFS instanceof DistributedFileSystem Review Comment: There should be one path if preserving ec or favoredNodes is set, so switch to hdfsbuilder, leaving the other path as create()", "created": "2023-02-24T14:32:55.075+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1455156173 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 28s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 50s | | trunk passed | | +1 :green_heart: | compile | 23m 12s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 49s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 5s | | trunk passed | | +1 :green_heart: | javadoc | 1m 50s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 15s | | the patch passed | | +1 :green_heart: | compile | 22m 29s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 29s | | the patch passed | | +1 :green_heart: | compile | 20m 27s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 41s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 1s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 39s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 14m 7s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 222m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e50831db0918 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 662045ad8a6c5d2c5dff9907a0ac7e6842844153 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 686 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-05T17:45:32.092+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125831678 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List<InetSocketAddress> result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { Review Comment: I have checked if `favoredNodesStr ` is an empty string above, and will skip `favoredNodesStr` if empty.", "created": "2023-03-06T03:01:01.395+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125837868 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java: ########## @@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args) } } + if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) { + String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim()); + if (StringUtils.isEmpty(favoredNodesStr)) { Review Comment: Add new UT in `TestOptionsParser`, and I think it's ok that YARN cluster can resolve favored nodes. BTW, it seems that DN does not care about `port` but `hostname`?", "created": "2023-03-06T03:11:42.135+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125837868 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java: ########## @@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args) } } + if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) { + String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim()); + if (StringUtils.isEmpty(favoredNodesStr)) { Review Comment: Add new UT in `TestOptionsParser`, and I think it's ok that YARN cluster can resolve favored nodes. BTW, it seems that DN does not care about `port` but `hostname`? https://github.com/apache/hadoop/blob/2a0dc2ab2f5fb46dc540ed440d6c8b2896dd195b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java#L703", "created": "2023-03-06T03:13:27.540+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125843372 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS, FSDataOutputStream out; ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum); if (!preserveEC || ecPolicy == null) { - out = targetFS.create(targetPath, permission, - EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, - repl, blockSize, context, checksumOpt); + if (targetFS instanceof DistributedFileSystem Review Comment: If I only change else branch, some UT will be fail when I have set `favoredNodes`, because in my UT it just goes into the if branch (ec settings if not set). Is there a better solution?", "created": "2023-03-06T03:20:02.565+0000"}, {"author": "ASF GitHub Bot", "body": "zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125847429 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS, FSDataOutputStream out; ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum); if (!preserveEC || ecPolicy == null) { - out = targetFS.create(targetPath, permission, - EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, - repl, blockSize, context, checksumOpt); + if (targetFS instanceof DistributedFileSystem Review Comment: Thank you for your code review and so many valuable comments\uff01", "created": "2023-03-06T03:25:12.628+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1455526533 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 17m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 36s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 40s | | trunk passed | | +1 :green_heart: | compile | 23m 0s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 26s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 45s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 3s | | trunk passed | | +1 :green_heart: | javadoc | 1m 49s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 15s | [/patch-mvninstall-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch failed. | | -1 :x: | compile | 12m 30s | [/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1. | | -1 :x: | javac | 12m 30s | [/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | root in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1. | | -1 :x: | compile | 11m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09. | | -1 :x: | javac | 11m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | root in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09. | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 18s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 1 new + 63 unchanged - 0 fixed = 64 total (was 63) | | -1 :x: | mvnsite | 0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch failed. | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt]([CI_URL] | hadoop-distcp in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1. | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt]([CI_URL] | hadoop-distcp in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09. | | -1 :x: | spotbugs | 0m 23s | [/patch-spotbugs-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch failed. | | +1 :green_heart: | shadedclient | 21m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 33s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 0m 21s | [/patch-unit-hadoop-tools_hadoop-distcp.txt]([CI_URL] | hadoop-distcp in the patch failed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 198m 42s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6b1c4e22c346 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b20d228c6939cded9fb2ca92ef667feb84b5397d | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 648 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-06T06:23:12.917+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1456058542 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 42s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 13s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 49s | | trunk passed | | +1 :green_heart: | compile | 22m 59s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 26s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 4s | | trunk passed | | +1 :green_heart: | javadoc | 1m 48s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 3m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 15s | | the patch passed | | +1 :green_heart: | compile | 22m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 24s | | the patch passed | | +1 :green_heart: | compile | 20m 33s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 38s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 1 new + 62 unchanged - 0 fixed = 63 total (was 62) | | +1 :green_heart: | mvnsite | 2m 3s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 42s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 13m 56s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 221m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux fecad139efb7 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c906c78e3e7aecd12a818fcb76d281f713387323 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 731 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-06T12:37:26.267+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-3447891027 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-26T00:24:27.476+0000"}], "derived_tasks": {"summary": "Hadoop DistCp supports specifying favoredNodes for data copying - When importing large scale data to HBase, we always generate the hfiles with othe...", "classifications": ["new feature"], "qa_pairs": [{"question": "Could I submit a pull request to optimize it?", "answer": "zhuyaogai opened a new pull request, #5391: URL: https://github.com/apache/hadoop/pull/5391 ### Description of PR Hadoop DistCp supports specifying favoredNodes for data copying. ### How was this patch tested? Add new UT. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}]}}
{"id": "HADOOP-18603", "title": "NPE in LdapAuthenticationHandler as disableHostNameVerification is never initialized", "description": "Steps to reproduce this issue: Enable ldap auth with tls by configuring these in core-site.xml 1. hadoop.http.authentication.multi-scheme-auth-handler.schemes = basic 2. hadoop.http.authentication.multi-scheme-auth-handler.schemes.basic.handler = ldap 3. hadoop.http.authentication.ldap.enablestarttls = true Trace:", "status": "Open", "priority": "Major", "reporter": "Bilwa S T", "assignee": "Bilwa S T", "created": "2023-01-25T06:35:20.000+0000", "updated": "2025-10-23T00:22:35.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "Bilwa S T", "body": "cc [~hgadre]", "created": "2023-01-25T06:38:29.976+0000"}, {"author": "ASF GitHub Bot", "body": "BilwaST opened a new pull request, #5581: URL: https://github.com/apache/hadoop/pull/5581 \u2026ication is never initialized.", "created": "2023-04-22T11:04:35.480+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5581: URL: https://github.com/apache/hadoop/pull/5581#issuecomment-1518671506 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 57s | | trunk passed | | +1 :green_heart: | compile | 25m 14s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 21m 43s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | +1 :green_heart: | javadoc | 0m 39s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 24m 37s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 24m 37s | | the patch passed | | +1 :green_heart: | compile | 21m 45s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 21m 45s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 28s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 32s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 0m 59s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 22s | | hadoop-auth in the patch passed. | | +1 :green_heart: | asflicense | 0m 57s | | The patch does not generate ASF License warnings. | | | | 197m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5581 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 76cd0460e684 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / bb7c1ea3c6dbcb1b48102a87b22fd3fd8d59692a | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 532 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth U: hadoop-common-project/hadoop-auth | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-22T14:23:53.610+0000"}, {"author": "Bilwa S T", "body": "cc [~brahmareddy]", "created": "2023-04-24T05:47:29.902+0000"}, {"author": "ASF GitHub Bot", "body": "simbadzina commented on PR #5581: URL: https://github.com/apache/hadoop/pull/5581#issuecomment-1524300983 Could you please add a unit test that fails without your change and passes with the change.", "created": "2023-04-27T00:37:03.148+0000"}, {"author": "ASF GitHub Bot", "body": "BilwaST commented on PR #5581: URL: https://github.com/apache/hadoop/pull/5581#issuecomment-1858380654 Thank you for the review @ayushtkn @simbadzina. I have added testcase for this.", "created": "2023-12-15T19:24:35.793+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5581: URL: https://github.com/apache/hadoop/pull/5581#issuecomment-1858569516 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 12s | | trunk passed | | +1 :green_heart: | compile | 16m 26s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 55s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 41s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 33m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 15m 27s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 27s | | the patch passed | | +1 :green_heart: | compile | 14m 50s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 14m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 36s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 42s | | the patch passed | | +1 :green_heart: | javadoc | 0m 39s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 39s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 32m 48s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 3m 24s | | hadoop-auth in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 189m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5581 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6dda81a4f9b7 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 15a4a26fdd8b65bfe7c783f9f756f852ea05e4f9 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-auth U: hadoop-common-project/hadoop-auth | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-12-15T22:33:41.837+0000"}, {"author": "Bilwa S T", "body": "can we merge this [~ayushsaxena] [~simbadzina] ?", "created": "2024-05-30T07:16:45.010+0000"}, {"author": "ASF GitHub Bot", "body": "simbadzina commented on code in PR #5581: URL: https://github.com/apache/hadoop/pull/5581#discussion_r1622586601 ########## hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/LdapAuthenticationHandler.java: ########## @@ -100,6 +100,12 @@ public class LdapAuthenticationHandler implements AuthenticationHandler { */ public static final String ENABLE_START_TLS = TYPE + \".enablestarttls\"; + /** + * Constant for disabling the host name verification for this handler. + */ + private static final String DISABLE_HOSTNAME_VERIFICATION = TYPE + + \".hostname.verification.disable\"; Review Comment: Could you please also add this new config in core-site default.", "created": "2024-05-31T15:23:46.030+0000"}, {"author": "ASF GitHub Bot", "body": "BilwaST commented on PR #5581: URL: https://github.com/apache/hadoop/pull/5581#issuecomment-2142633137 @simbadzina None of the configs added in this file are added to core-default.xml. I'm thinking we can plan to add it as part of new jira. Would it be ok? Thanks", "created": "2024-05-31T16:46:47.628+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5581: URL: https://github.com/apache/hadoop/pull/5581#issuecomment-3430009446 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-22T00:23:33.405+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5581: HADOOP-18603 NPE in LdapAuthenticationHandler as disableHostNameVerif\u2026 URL: https://github.com/apache/hadoop/pull/5581", "created": "2025-10-23T00:22:35.642+0000"}], "derived_tasks": {"summary": "NPE in LdapAuthenticationHandler as disableHostNameVerification is never initialized - Steps to reproduce this issue: Enable ldap auth with tls by ...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-18450", "title": "JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail", "description": "Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store.\u00a0 The calls to:  renameOrFail(Path src, Path dest) throws IOException\u00a0 ... fails with an IOException when it attempts to rename a file.\u00a0 The calling code catches FileNotFoundException since the src file may not exist. Example:  $ hadoop key create sample Update the implementation to check for the file, throwing a FileNotFoundException.", "status": "Open", "priority": "Critical", "reporter": "Steve Vaughan", "assignee": "Steve Vaughan", "created": "2022-09-12T01:43:46.000+0000", "updated": "2025-10-25T00:22:18.000+0000", "labels": ["pull-request-available"], "components": ["common"], "comments": [{"author": "ASF GitHub Bot", "body": "snmvaughan opened a new pull request, #5452: URL: https://github.com/apache/hadoop/pull/5452 \u2026 in renameOrFail ### Description of PR Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store. The calls to: ``` renameOrFail(Path src, Path dest) throws IOException ``` ... fails with an IOException when it attempts to rename a file. The calling code catches FileNotFoundException since the src file may not exist. Example: ``` $ hadoop key create sample", "created": "2023-03-03T14:07:08.270+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5452: URL: https://github.com/apache/hadoop/pull/5452#issuecomment-1453873533 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 46s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 28s | | trunk passed | | +1 :green_heart: | compile | 23m 17s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 1m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 47s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 22m 23s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 23s | | the patch passed | | +1 :green_heart: | compile | 20m 36s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 6s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 8s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 24s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 209m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5452 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 523383cac356 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9027fa0f097af13bcee9c54033e400d6994059a5 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 1418 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-03-03T17:38:23.167+0000"}, {"author": "ASF GitHub Bot", "body": "virajjasani commented on code in PR #5452: URL: https://github.com/apache/hadoop/pull/5452#discussion_r1132896769 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java: ########## @@ -640,6 +640,10 @@ private void revertFromOld(Path oldPath, boolean fileExisted) private void renameOrFail(Path src, Path dest) throws IOException { if (!fs.rename(src, dest)) { + if (!fs.exists(src)) { + throw new FileNotFoundException(src.toUri().toString()); + } Review Comment: How about keeping this check just before `fs#rename`? Could save efforts of doing rename in the first place?", "created": "2023-03-10T21:18:25.893+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5452: URL: https://github.com/apache/hadoop/pull/5452#issuecomment-3440030224 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-24T00:20:34.241+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5452: HADOOP-18450. JavaKeyStoreProvider should throw FileNotFoundException\u2026 URL: https://github.com/apache/hadoop/pull/5452", "created": "2025-10-25T00:22:18.476+0000"}], "derived_tasks": {"summary": "JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail - Attempting to create a key a KMS is configured with the JavaKeystoreProvi...", "classifications": ["new feature", "bug"], "qa_pairs": []}}
{"id": "HADOOP-18142", "title": "Increase precommit job timeout from 24 hr to 30 hr", "description": "As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here|https://github.com/apache/hadoop/pull/4000] where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.", "status": "Patch Available", "priority": "Minor", "reporter": "Viraj Jasani", "assignee": null, "created": "2022-02-24T07:14:20.000+0000", "updated": "2025-10-25T00:26:24.000+0000", "labels": ["pull-request-available"], "components": [], "comments": [{"author": "Zoltan Haindrich", "body": "[~vjasani] I've noticed this ticket by chance and would like to mention a few things I've done in Hive to avoid kinda like the same issues: * [use rateLimit|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L21] to avoid building the same PR multiple time a day ; this naturally adds a 6 hour wait before the next would start * use a global lock to [limit the number|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L150] of concurrently running * [disable concurrent builds|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L23] as there is no point running the tests for someone who pushed new changes while it was still running => the contributor most likely will push more commits anyway which could launch even more builds...not starting a new build means it could pick up multiple trigger events while the one executing is still running * auto-kill the build in case the PR was updated while it waiting/running ; by calling [this method|https://github.com/apache/hive/blob/master/Jenkinsfile#L30-L45] at a few key points in the build", "created": "2022-02-24T14:30:20.326+0000"}, {"author": "Viraj Jasani", "body": "Thanks [~kgyrtkirk] for the nice suggestions! If you would like to take up this work, please feel free to go ahead with creating PR.", "created": "2022-02-24T18:18:25.899+0000"}, {"author": "Viraj Jasani", "body": "[~kgyrtkirk] I went through the changes you have mentioned above, nice work indeed! Based on my recent observation with [PR#4000|https://github.com/apache/hadoop/pull/4000], Hadoop full build definitely exceeds current timeout of 24 hr (regardless of whether we rate limit i.e. run only 1 build or run multiple builds concurrently) hence increasing timeout to 30 hr is a certain requirement for the entire hadoop build to be finished. For the improvements that you have mentioned above (specifically disabling concurrent builds and auto-kill for updated PR), will create a new follow-up Jira.", "created": "2022-03-08T16:20:33.536+0000"}, {"author": "Takanobu Asanuma", "body": "It seems the recent QBTs finished within 20 hours. Maybe we should wait and see for a while. [CI_URL]", "created": "2022-03-30T02:49:58.286+0000"}, {"author": "Viraj Jasani", "body": "[~tasanuma] It seems QBTs run only single JDK version builds, whereas PR builds run full QA with both Java 8 and 11 (for trunk PRs), hence 24 hr is not sufficient for many runs.", "created": "2022-05-08T18:49:35.469+0000"}, {"author": "Viraj Jasani", "body": "One recent example of build timeout after 24 hr [CI_URL]", "created": "2022-05-18T23:40:35.189+0000"}, {"author": "Shilun Fan", "body": "Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0.", "created": "2024-01-04T09:51:01.404+0000"}, {"author": "ASF GitHub Bot", "body": "virajjasani closed pull request #4056: HADOOP-18142. Increase precommit job timeout from 24 hr to 30 hr URL: https://github.com/apache/hadoop/pull/4056", "created": "2025-10-25T00:26:24.741+0000"}], "derived_tasks": {"summary": "Increase precommit job timeout from 24 hr to 30 hr - As per some recent precommit build results, full build QA is not getting completed in 24 hr (r...", "classifications": ["task"], "qa_pairs": []}}
{"id": "HADOOP-18033", "title": "Upgrade fasterxml Jackson to 2.13.0", "description": "Spark 3.2.0 depends on Jackson 2.12.3. Let's upgrade to 2.12.5 (2.12.x latest as of now) or upper. h2. this has been reverted. we had to revert this as it broke tez.", "status": "Resolved", "priority": "Major", "reporter": "Akira Ajisaka", "assignee": "Viraj Jasani", "created": "2021-12-03T14:54:23.000+0000", "updated": "2025-10-25T00:26:09.000+0000", "labels": ["pull-request-available"], "components": ["build"], "comments": [{"author": "Viraj Jasani", "body": "Let's upgrade to 2.13.0? It already seems to have good number of usages [here|https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core/2.13.0/usages].", "created": "2021-12-03T19:37:39.521+0000"}, {"author": "Viraj Jasani", "body": "Build fails with 2.13.0  [WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message: Duplicate classes found: Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile Duplicate classes: META-INF/versions/11/module-info.class Will get back to this after some time.", "created": "2021-12-03T20:05:15.745+0000"}, {"author": "Viraj Jasani", "body": "With this upgrade, we will also need to explicitly add new dependency {_}javax.ws.rs:javax.ws.rs-api{_}. We also need to exclude it from shading, else we will get multiple duplicate class clash with existing javax.ws.rs dependencies. For the record, let me provide duplicate class details:  Duplicate classes found:\u00a0 Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile Duplicate classes: org/apache/hadoop/shaded/javax/ws/rs/POST.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbLink.class org/apache/hadoop/shaded/javax/ws/rs/NotFoundException.class org/apache/hadoop/shaded/javax/ws/rs/container/PreMatching.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestContext.class org/apache/hadoop/shaded/javax/ws/rs/core/FeatureContext.class org/apache/hadoop/shaded/javax/ws/rs/core/HttpHeaders.class org/apache/hadoop/shaded/javax/ws/rs/PATCH.class org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent$Builder.class org/apache/hadoop/shaded/javax/ws/rs/core/GenericType.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseBroadcaster.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$2.class org/apache/hadoop/shaded/javax/ws/rs/core/StreamingOutput.class org/apache/hadoop/shaded/javax/ws/rs/core/GenericEntity.class org/apache/hadoop/shaded/javax/ws/rs/core/PathSegment.class org/apache/hadoop/shaded/javax/ws/rs/BadRequestException.class org/apache/hadoop/shaded/javax/ws/rs/ext/ExceptionMapper.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientBuilder.class org/apache/hadoop/shaded/javax/ws/rs/Priorities.class org/apache/hadoop/shaded/javax/ws/rs/HeaderParam.class org/apache/hadoop/shaded/javax/ws/rs/core/Context.class org/apache/hadoop/shaded/javax/ws/rs/container/ResourceContext.class org/apache/hadoop/shaded/javax/ws/rs/ConstrainedTo.class org/apache/hadoop/shaded/javax/ws/rs/Encoded.class org/apache/hadoop/shaded/javax/ws/rs/core/AbstractMultivaluedMap.class org/apache/hadoop/shaded/javax/ws/rs/client/Entity.class org/apache/hadoop/shaded/javax/ws/rs/client/SyncInvoker.class org/apache/hadoop/shaded/javax/ws/rs/NameBinding.class org/apache/hadoop/shaded/javax/ws/rs/client/Invocation$Builder.class org/apache/hadoop/shaded/javax/ws/rs/ext/MessageBodyReader.class org/apache/hadoop/shaded/javax/ws/rs/client/ResponseProcessingException.class org/apache/hadoop/shaded/javax/ws/rs/sse/FactoryFinder.class org/apache/hadoop/shaded/javax/ws/rs/client/FactoryFinder.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestFilter.class org/apache/hadoop/shaded/javax/ws/rs/ext/RuntimeDelegate$HeaderDelegate.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$Status$Family.class org/apache/hadoop/shaded/javax/ws/rs/ext/ReaderInterceptor.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerResponseContext.class org/apache/hadoop/shaded/javax/ws/rs/ApplicationPath.class org/apache/hadoop/shaded/javax/ws/rs/ext/WriterInterceptorContext.class org/apache/hadoop/shaded/javax/ws/rs/PUT.class org/apache/hadoop/shaded/javax/ws/rs/container/ResourceInfo.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$ResponseBuilder.class org/apache/hadoop/shaded/javax/ws/rs/ext/MessageBodyWriter.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSource.class org/apache/hadoop/shaded/javax/ws/rs/FormParam.class org/apache/hadoop/shaded/javax/ws/rs/PathParam.class org/apache/hadoop/shaded/javax/ws/rs/core/Application.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$Builder.class org/apache/hadoop/shaded/javax/ws/rs/NotAcceptableException.class org/apache/hadoop/shaded/javax/ws/rs/NotAllowedException.class org/apache/hadoop/shaded/javax/ws/rs/ext/InterceptorContext.class org/apache/hadoop/shaded/javax/ws/rs/container/ConnectionCallback.class org/apache/hadoop/shaded/javax/ws/rs/container/TimeoutHandler.class org/apache/hadoop/shaded/javax/ws/rs/core/Request.class org/apache/hadoop/shaded/javax/ws/rs/WebApplicationException.class org/apache/hadoop/shaded/javax/ws/rs/ext/WriterInterceptor.class org/apache/hadoop/shaded/javax/ws/rs/RedirectionException.class org/apache/hadoop/shaded/javax/ws/rs/ext/RuntimeDelegate.class org/apache/hadoop/shaded/javax/ws/rs/CookieParam.class org/apache/hadoop/shaded/javax/ws/rs/container/CompletionCallback.class org/apache/hadoop/shaded/javax/ws/rs/Path.class org/apache/hadoop/shaded/javax/ws/rs/client/Invocation.class org/apache/hadoop/shaded/javax/ws/rs/core/EntityTag.class org/apache/hadoop/shaded/javax/ws/rs/core/UriBuilder.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSource$Builder.class org/apache/hadoop/shaded/javax/ws/rs/DefaultValue.class org/apache/hadoop/shaded/javax/ws/rs/client/Client.class org/apache/hadoop/shaded/javax/ws/rs/ext/FactoryFinder.class org/apache/hadoop/shaded/javax/ws/rs/NotSupportedException.class org/apache/hadoop/shaded/javax/ws/rs/HEAD.class org/apache/hadoop/shaded/javax/ws/rs/core/Link.class org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverter$Lazy.class org/apache/hadoop/shaded/javax/ws/rs/QueryParam.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$StatusType.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientResponseFilter.class org/apache/hadoop/shaded/javax/ws/rs/client/RxInvoker.class org/apache/hadoop/shaded/javax/ws/rs/core/MultivaluedHashMap.class org/apache/hadoop/shaded/javax/ws/rs/core/UriBuilderException.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientRequestFilter.class org/apache/hadoop/shaded/javax/ws/rs/client/RxInvokerProvider.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEvent.class org/apache/hadoop/shaded/javax/ws/rs/DELETE.class org/apache/hadoop/shaded/javax/ws/rs/Produces.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType.class org/apache/hadoop/shaded/javax/ws/rs/core/NoContentException.class org/apache/hadoop/shaded/javax/ws/rs/OPTIONS.class org/apache/hadoop/shaded/javax/ws/rs/ext/Provider.class org/apache/hadoop/shaded/javax/ws/rs/BeanParam.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientRequestContext.class org/apache/hadoop/shaded/javax/ws/rs/core/Feature.class org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverter.class org/apache/hadoop/shaded/javax/ws/rs/core/Form.class org/apache/hadoop/shaded/javax/ws/rs/Consumes.class org/apache/hadoop/shaded/javax/ws/rs/ClientErrorException.class org/apache/hadoop/shaded/javax/ws/rs/client/CompletionStageRxInvoker.class org/apache/hadoop/shaded/javax/ws/rs/core/MultivaluedMap.class org/apache/hadoop/shaded/javax/ws/rs/ext/ReaderInterceptorContext.class org/apache/hadoop/shaded/javax/ws/rs/sse/InboundSseEvent.class org/apache/hadoop/shaded/javax/ws/rs/core/NewCookie.class org/apache/hadoop/shaded/javax/ws/rs/core/Variant$VariantListBuilder.class org/apache/hadoop/shaded/javax/ws/rs/client/WebTarget.class org/apache/hadoop/shaded/javax/ws/rs/core/Configuration.class org/apache/hadoop/shaded/javax/ws/rs/ForbiddenException.class org/apache/hadoop/shaded/javax/ws/rs/RuntimeType.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$1.class org/apache/hadoop/shaded/javax/ws/rs/MatrixParam.class org/apache/hadoop/shaded/javax/ws/rs/client/InvocationCallback.class org/apache/hadoop/shaded/javax/ws/rs/container/Suspended.class org/apache/hadoop/shaded/javax/ws/rs/ext/Providers.class org/apache/hadoop/shaded/javax/ws/rs/InternalServerErrorException.class org/apache/hadoop/shaded/javax/ws/rs/container/DynamicFeature.class org/apache/hadoop/shaded/javax/ws/rs/ext/ContextResolver.class org/apache/hadoop/shaded/javax/ws/rs/core/Cookie.class org/apache/hadoop/shaded/javax/ws/rs/HttpMethod.class org/apache/hadoop/shaded/javax/ws/rs/ServiceUnavailableException.class org/apache/hadoop/shaded/javax/ws/rs/GET.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSink.class org/apache/hadoop/shaded/javax/ws/rs/sse/Sse.class org/apache/hadoop/shaded/javax/ws/rs/container/AsyncResponse.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerResponseFilter.class org/apache/hadoop/shaded/javax/ws/rs/core/Configurable.class org/apache/hadoop/shaded/javax/ws/rs/core/Response.class org/apache/hadoop/shaded/javax/ws/rs/ServerErrorException.class org/apache/hadoop/shaded/javax/ws/rs/core/Form$1.class org/apache/hadoop/shaded/javax/ws/rs/ProcessingException.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientResponseContext.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$Status.class org/apache/hadoop/shaded/javax/ws/rs/NotAuthorizedException.class org/apache/hadoop/shaded/javax/ws/rs/core/Variant.class org/apache/hadoop/shaded/javax/ws/rs/core/CacheControl.class org/apache/hadoop/shaded/javax/ws/rs/core/UriInfo.class org/apache/hadoop/shaded/javax/ws/rs/client/AsyncInvoker.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbAdapter.class org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverterProvider.class org/apache/hadoop/shaded/javax/ws/rs/core/SecurityContext.class org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent.class", "created": "2021-12-06T14:36:01.098+0000"}, {"author": "Viraj Jasani", "body": "With a quick search, I realized we already have Jira HADOOP-16908 to remove all org.codehaus.jackson usages from the codebase. Let me link it with this Jira. I can take up that work once this is resolved. Thanks", "created": "2021-12-07T14:24:45.794+0000"}, {"author": "Akira Ajisaka", "body": "Merged the PR into trunk. Hi [~vjasani], would you create a PR for branch-3.3?", "created": "2021-12-08T07:53:28.333+0000"}, {"author": "Viraj Jasani", "body": "Sure [~aajisaka], I am on it.", "created": "2021-12-08T08:43:03.853+0000"}, {"author": "Akira Ajisaka", "body": "Committed to branch-3.3. Thank you [~vjasani]", "created": "2021-12-13T04:53:26.629+0000"}, {"author": "Ayush Saxena", "body": "guess, this broke the way for tez to upgrade post 3.3.1 and there after for hive as well. Have been discussing the upgrade stuff for Tez internally and I think this is the one, any way we have decided to settle for at 3.3.1. and that works.... -> {quote}With this upgrade, we will also need to explicitly add new dependency javax.ws.rs:javax.ws.rs-api. We also need to exclude it from shading, else we will get multiple duplicate class clash with existing javax.ws.rs dependencies. {quote} Most probably this is the reason, I will create a Jira and try to see what can be done or if something in trunk already sorted this, I see a couple of Jiras linked to this ticket", "created": "2022-06-17T08:38:20.037+0000"}, {"author": "Steve Loughran", "body": "what is the problem here?", "created": "2022-06-17T19:00:09.261+0000"}, {"author": "Ayush Saxena", "body": "[~stevel@apache.org] It added javax.ws.rs:javax.ws.rs-api and that isn't shaded also and is conflicting with jsr311-api in tez. Can see the error here as well: [[CI_URL] and someone quoted some problem here as well(I didn't check what is that though): https://github.com/apache/hadoop/pull/3764#issuecomment-1158641569", "created": "2022-06-17T19:04:13.634+0000"}, {"author": "Viraj Jasani", "body": "[~ayushtkn] Thanks for posting your findings, just had a high level glance at the above failure stacktrace in Tez. {quote}at org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager.createTimelineDomain(ATSHistoryACLPolicyManager.java:127) {quote} For this one, is it be convenient to include javax.ws.rs-api in [https://github.com/apache/tez/blob/master/tez-plugins/tez-yarn-timeline-history-with-acls/pom.xml] ? Or does that also conflict with jsr311-api? I understand the pain with minor release upgrade when it has to deal with such issues. FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line, given that 3.3 is the latest release line. Let me also check if something can be done in the meantime. (As you already noticed, the problem here is that with shading, we get multiple duplicate class clashes for javax.ws.rs dependencies and hence we have no choice but to remove it from shading)", "created": "2022-06-18T01:38:39.491+0000"}, {"author": "Viraj Jasani", "body": "Ah I see, HADOOP-18178 has also bumped Jackson to 2.13.2 in light of fixing CVE-2020-36518, and it has made it's way to 3.3.2 release but I guess the pain related to javax.ws.rs-api remains the same.", "created": "2022-06-18T01:45:24.952+0000"}, {"author": "Ayush Saxena", "body": "{quote}For this one, is it be convenient to include javax.ws.rs-api in [https://github.com/apache/tez/blob/master/tez-plugins/tez-yarn-timeline-history-with-acls/pom.xml] ? Or does that also conflict with jsr311-api? {quote} The actual error is : and this is due to conflict with jsr311-api, \u00a0javax.ws.rs-api already got included as transitive dependency, if I exclude javax.ws.rs-api in Tez it can make the test pass, but we don't want to play with exclusions as we aren't sure what runtime issues it can create", "created": "2022-06-18T06:39:49.349+0000"}, {"author": "Ayush Saxena", "body": "Spent some time checking if we have any quick solution or not and see how things are: Both {{jsr311-api}} and {{javax.ws.rs-api}} have couple of similar classes and different implementations, That is why this duplicate classes issue started surfacing, I guess Jackson 2 requires implementation classes from {{javax.ws.rs-api}} at runtime or so. In ideal situation we should either have {{javax.ws.rs-api}} or {{js311-api}} in our code, when adding {{javax.ws.rs-api}} if we could have got rid of {{js311-api}} then everything would have been sorted for the shading part. But I guess we have some dependencies on {{{}js311-api{}}}, and it is coming from some other thirdparty libs as well, so may be we have to explore and upgrade them to a version, where they ditch {{js311-api}} for {{{}javax.ws.rs-api{}}}. Then our shading jar should get sorted. How tough is that we don't know, a normal exclude of {{js311-api}} as a transitive dependency isn't a solution because {{javax.ws.rs-api}} has different implementation of methods. The duplicate class exception that we saw here was actually an alarm here that these two dependencies can't stay in peace together, but we got away with that by an exclude... Now coming for Tez, Tez still has {{js311-api}} as a dependency, if we some how ditch that and move to {{javax.ws.rs-api}} in hadoop, I am not very sure if Tez too have to adapt to our Jackson version and do the same to get things working.. {quote}FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line {quote} Revert isn't an option now, HADOOP-18178 got its way clear only because of this, else it would have been facing this same issue and would have crashed. Now we have a CVE fixed in 3.3.2 & 3.3.3, we can't get it back in 3.3.4, We won't fix a thirdparty CVE we could have said, but after fixing and claiming we have fixed one, we can't get it back AFAIK, this issue only somehow we have to fix. BTW. I am not sure what Spark and Kyubi issues are exactly, that also seems class conflicts may be.. [~pan3793] can you share some more information about that here", "created": "2022-06-18T14:20:15.713+0000"}, {"author": "Cheng Pan", "body": "> BTW. I am not sure what Spark and Kyuubi issues are exactly, that also seems class conflicts may be Yes, I think it's because Jackson requires some classes which only exist in javax.ws.rs-api, which are not bundled into the shaded client. Have a brief look, js311-api is only required by jersey 1.x? If yes, I think upgrading the jersey to 2.x which depends on javax.ws.rs-api and dropping js311-api may be the right direction. And I also see that Hadoop 3.3.2 mixed use jersey 1.x and 2.x in module hadoop-yarn-applications-catalog-webapp, not sure if it's a good practice.", "created": "2022-06-18T18:45:20.205+0000"}, {"author": "Viraj Jasani", "body": "{quote}The duplicate class exception that we saw here was actually an alarm here that these two dependencies can't stay in peace together, but we got away with that by an exclude... {quote} I don't think it was as simple as \"completely removing jsr311-api from Hadoop\" would allow us to exclude shading javax.ws.rs-api from both hadoop-client-minicluster and hadoop-client-runtime. I have already tried this before, it doesn't work AFAIK. At least, one of them would have to keep the exclusion on. HADOOP-15983 has upgraded all com.sun.jersey dependencies (jersey-core, jersey-servlet etc) to the latest version in 1.x line and the latest version of jersey-core pulls-in jsr311-api with it:  [INFO] | +- com.sun.jersey:jersey-core:jar:1.19.4:compile [INFO] | | \\- javax.ws.rs:jsr311-api:jar:1.1.1:compile  I don't think without exclusion (and maybe some additional code change, if JAX-RS 1.x and 2.x incompatibilities are in use), we might be able to get rid of jsr311-api. I would expect the same for Tez and other dependencies as well. Tez and other dependencies also can explore the similar path of excluding jsr311-api completely and only rely on JAX-RS 2.x based javax.ws.rs-api (specifically if already using jersey 2.x release versions). jsr311-api is the official spec jar for the JAX-RS 1.x line and the latest central release available is from Nov, 2009 (too old) [https://mvnrepository.com/artifact/javax.ws.rs/jsr311-api] whereas javax.ws.rs-api is jar for JAX-RS 2.x line [https://mvnrepository.com/artifact/javax.ws.rs/javax.ws.rs-api] (latest version from 2018), hence we can expect more upgraded thirdparty libraries (just like Jackson2) having dependency on javax.ws.rs-api and less on jsr311-api. So all downstreamers (Tez, Hadoop, Spark) should try to get rid of jsr311-api anyways, totally agree here. One dependency doesn't necessarily have to wait for another to remove it, for instance, Tez can go ahead with exclusion of jsr311-api even before upgrading to Hadoop 3.3.4 because if not Hadoop, some of it's other dependencies (like jersey-core latest version, as mentioned above) would likely anyways pull it in transitively. On the other hand, let Hadoop also get rid of jsr311-api. But I am pretty sure, removing it won't solve shading issue completely. Will come up with patch because I do recall I have already tried this as part of this Jira only. We can also run full build QA (all modules) and I can manually verify HDFS, MapReduce and ATSv2 working on pseudo-distributed mode.", "created": "2022-06-18T20:10:38.066+0000"}, {"author": "Cheng Pan", "body": "The mvnrepository[1] tips that com.sun.jersey:jersey-core was moved to [org.glassfish.jersey.core|https://mvnrepository.com/artifact/org.glassfish.jersey.core] Upgrading(or migrating) to glassfish jersey 2.x should help. [1] [https://mvnrepository.com/artifact/com.sun.jersey/jersey-core]", "created": "2022-06-18T20:21:28.491+0000"}, {"author": "Viraj Jasani", "body": "[~pan3793] Thanks for your comments. {quote}And I also see that Hadoop 3.3.2 mixed use jersey 1.x and 2.x in module hadoop-yarn-applications-catalog-webapp, not sure if it's a good practice. {quote} Do you mean jersey-json or\u00a0jersey-media-json-jackson? HADOOP-15983 has been a recent work and jersey-media-json-jackson is a test dependency.", "created": "2022-06-18T20:25:53.187+0000"}, {"author": "Viraj Jasani", "body": "[~pan3793] HADOOP-15984 has migration to Jersey 2.x related efforts going on.", "created": "2022-06-18T20:28:37.981+0000"}, {"author": "Viraj Jasani", "body": "Remove jsr311-api dependency: [https://github.com/apache/hadoop/pull/4460] (to see how QA results go for now)", "created": "2022-06-18T20:35:18.008+0000"}, {"author": "Ayush Saxena", "body": "Removing jsr311-api from hadoop, will not cause duplicate file exception in shading, because when I tried it didn't... But whether we can do that? because jsr311-api\u00a0 & javax.ws.rs-api aren't compatible with each other. That is one thing I am sure, because that only caused Tez to give that AbstractMethod Error... The build will pass I think, If test fails it is good, at least we will come to know what is broken and what needs to be fixed and we can some how figure out how.\u00a0 It might come green as well, because couple of tests which I tried were passing.(Running whole Hadoop test suite isn't easy to run locally) Else if jersey is using jsr311-api, that can create runtime issues, earlier we had jsr311-api in our client jar, now we won't be having that, what impact to downstream projects, that will stay a mystery... Atleast we have to try we don't need have any transitive dependency of jsr311-api I think downstream projects have to get rid of jsr311-api and upgrade Jackson to adapt to this even if we sort this FWIW. Tez is already going ahead with 3.3.1 for the current release: [https://lists.apache.org/thread/7sw84rcc729fgw31g0w9h9y9r61tok9d]", "created": "2022-06-18T21:25:31.326+0000"}, {"author": "Viraj Jasani", "body": "Thanks [~ayushtkn], yes we should hopefully get the full build QA results in ~24 hrs. On the shading side, I meant: {quote}I don't think it was as simple as \"completely removing jsr311-api from Hadoop\" would allow us to exclude shading javax.ws.rs-api from both hadoop-client-minicluster and hadoop-client-runtime {quote} I tried this again and the build fails with the same error that I faced earlier:  [INFO] -------< org.apache.hadoop:hadoop-client-check-test-invariants >-------- [INFO] Building Apache Hadoop Client Packaging Invariants for Test 3.4.0-SNAPSHOT [105/112] [INFO] --------------------------------[ pom ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-client-check-test-invariants --- [INFO] Deleting /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants/target [INFO] Deleting /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants (includes = [dependency-reduced-pom.xml], excludes = []) [INFO] [INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-client-check-test-invariants --- [INFO] Executing tasks main: [mkdir] Created dir: /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants/target/test-dir [INFO] Executed tasks [INFO] [INFO] --- maven-enforcer-plugin:3.0.0:enforce (enforce-banned-dependencies) @ hadoop-client-check-test-invariants --- [INFO] Adding ignore: module-info [INFO] Adding ignore: META-INF/versions/*/module-info [INFO] Adding ignorable dependency: org.apache.hadoop:hadoop-annotations:null [INFO] \u00a0 Adding ignore: * [WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message: Duplicate classes found: Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile Duplicate classes: org/apache/hadoop/shaded/javax/ws/rs/POST.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbLink.class org/apache/hadoop/shaded/javax/ws/rs/NotFoundException.class org/apache/hadoop/shaded/javax/ws/rs/container/PreMatching.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestContext.class org/apache/hadoop/shaded/javax/ws/rs/core/FeatureContext.class org/apache/hadoop/shaded/javax/ws/rs/core/HttpHeaders.class org/apache/hadoop/shaded/javax/ws/rs/PATCH.class org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent$Builder.class org/apache/hadoop/shaded/javax/ws/rs/core/GenericType.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseBroadcaster.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$2.class org/apache/hadoop/shaded/javax/ws/rs/core/StreamingOutput.class ... ... ...  Hence, with the above PR, I have removed exclusion only from hadoop-client-runtime shade. Now we can confirm that these classes are present in hadoop-client-runtime but not on hadoop-client-minicluster jar:  $ jar tf hadoop-client-modules/hadoop-client-runtime/target/hadoop-client-runtime-3.4.0-SNAPSHOT.jar | grep \"AbstractMultivaluedMap\" org/apache/hadoop/shaded/javax/ws/rs/core/AbstractMultivaluedMap.class $ jar tf hadoop-client-modules/hadoop-client-minicluster/target/hadoop-client-minicluster-3.4.0-SNAPSHOT.jar | grep \"AbstractMultivaluedMap\" (no-output) One question for you: how do we determine which client module to shade the new dependency in? Is it always hadoop-client-runtime (for downstreamers)?", "created": "2022-06-18T21:58:44.216+0000"}, {"author": "Viraj Jasani", "body": "Actually, javax.ws.rs-api is not even clashing with jsr311-api (weird, didn't expect this). I just applied this patch, and the build is successful:  diff --git a/hadoop-client-modules/hadoop-client-runtime/pom.xml b/hadoop-client-modules/hadoop-client-runtime/pom.xml index 35fbd7665fb..0879ce1e3bc 100644 --- a/hadoop-client-modules/hadoop-client-runtime/pom.xml +++ b/hadoop-client-modules/hadoop-client-runtime/pom.xml @@ -163,7 +163,6 @@ <exclude>org.bouncycastle:*</exclude> <!-- Leave snappy that includes native methods which cannot be relocated. --> <exclude>org.xerial.snappy:*</exclude> -\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>javax.ws.rs:javax.ws.rs-api</exclude> </excludes> </artifactSet> <filters>  Created PR [https://github.com/apache/hadoop/pull/4461] for full build results. We can compare QA results for both PRs tomorrow.", "created": "2022-06-18T22:20:47.665+0000"}, {"author": "Ayush Saxena", "body": "{quote}Actually, javax.ws.rs-api is not even clashing with jsr311-api (weird, didn't expect this). {quote} The duplicate error that you were quoting was conflict in between hadoop-client-runtime vs hadoop-minicluster :( Looks like yes:  Duplicate classes found:\u00a0 Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile AFAIK Whatever gets shaded in hadoop-client-runtime we anyway have to exclude from hadoop-minicluster, we can't shade it in both,\u00a0 there are bunch of lines like. : {{exclude everything that comes in via the shaded runtime and api}} {{exclude things that came in via transitive in shaded runtime and api}} the problem that [~pan3793] quoted for spark & kyubi should get sorted atleast by this", "created": "2022-06-18T22:32:37.450+0000"}, {"author": "Viraj Jasani", "body": "Yeah that problem should be at least resolved by #4461 even if #4460 turns out to be complicated. If #4460 is good, maybe we can pursue some testing in ATSv2 and Yarn to verify the basic functionalities with REST APIs. {quote}The duplicate error that you were quoting was conflict in between hadoop-client-runtime vs hadoop-minicluster {quote} Unfortunately, yes i also realized this now :(", "created": "2022-06-18T22:54:22.292+0000"}, {"author": "Viraj Jasani", "body": "Once QA results are available, will create new Jira to link both PRs #4460 and #4461.", "created": "2022-06-18T22:57:30.002+0000"}, {"author": "Cheng Pan", "body": "Looks reasonable, thanks [~ayushtkn] and [~vjasani] for investigating this issue. cc [~csun]", "created": "2022-06-18T23:02:15.143+0000"}, {"author": "Ayush Saxena", "body": "Shading issue we might get rid of, because the issue wasn't in the same jar, so that isn't a problem & that might solve problem for some. Tez doesn't pull in hadoop-client dependency AFAIK, Now after this it is pulling in both javax.ws.rs-api and jsr311-api which is creating runtime issues like URIBuilder Class is there in both the packages and so. Excluding/Removing javax.ws.rs-api will make Jackson cry. I tried this as well a lot of test fails. Excluding/Removing jsr311-api makes Jersey cry. Like TestHttpServer#testJersey fails if we exclude it. I tried this only but some more can fail as well. This gives the same error what Tez is getting:  2022-06-19 17:53:24,623 WARN server.HttpChannel (HttpChannel.java:handleException(689)) - /jersey/foo", "created": "2022-06-19T12:26:29.714+0000"}, {"author": "Viraj Jasani", "body": "{quote}Excluding/Removing javax.ws.rs-api will make Jackson cry. {quote} {quote}Excluding/Removing jsr311-api makes Jersey cry. {quote} That is so true. Unfortunately we have not received a single QA result on PR#4460 so far. Although Jenkins is getting shut down it seems, at least we have bunch of test results available on PR#4461, hence perhaps some test failures are making builds difficult on 4460. Anyways, we might have to spend another day to see the full results. But we are sure we will have test failures (likely more than expected, as Ayush already mentioned about TestHttpServer#testJersey). Overall, it seems we are back to HADOOP-15984 (upgrading Jersey to 2.x) because so long as we have \"com.sun.jersey:jersey-core\", things will stay complicated.", "created": "2022-06-19T19:55:19.072+0000"}, {"author": "Akira Ajisaka", "body": "Thank you [~ayushtkn] [~pan3793] [~vjasani] for your discussion. I'm really sorry for not caching up with. In my past experience, Jersey 2.x upgrade takes a lot of time and I think it will cause some incompatible changes. Therefore I think we should revert the patch and update to 2.12.x latest to avoid the above issue for (at least) Hadoop 3.3.x. What do you think? Jackson 2.12.x would work because I think the change in https://github.com/FasterXML/jackson-jaxrs-providers/issues/134 caused the issue and it is only in Jackson 2.13.0 and upper.", "created": "2022-07-10T10:25:47.769+0000"}, {"author": "Ayush Saxena", "body": "Thanx [~aajisaka] for checking, I am +1 on revert", "created": "2022-07-10T10:41:34.068+0000"}, {"author": "Cheng Pan", "body": "Reverting Jackson in the 3.3 branch looks reasonable to me, since Kyuubi and Spark use Hadoop shaded client, downgrading Jackson from 2.13 to 2.12 should not cause another dependency issue.", "created": "2022-07-10T16:34:26.971+0000"}, {"author": "Viraj Jasani", "body": "{quote}In my past experience, Jersey 2.x upgrade takes a lot of time and I think it will cause some incompatible changes. {quote} I agree that 3.3 subsequent releases should not wait for Jersey 2 because of the sheer volume of changes and incompatibility with Jersey 1. From my previous comment: {quote}FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line, given that 3.3 is the latest release line. {quote} we might have to call out on the Jackson CVE that we claimed to have fixed with 3.3.2 and 3.3.3 and now 3.3.4 would get it exposed with the revert. IIRC, Jersey 1.19 is not flagged by security for active CVEs but Jackson versions <= 2.12 are? But I can understand that since it is breaking downstreamers, it might be worth reverting this and HADOOP-18178 at the expense of known CVE exposure.", "created": "2022-07-10T18:31:30.033+0000"}, {"author": "Akira Ajisaka", "body": "bq. we might have to call out on the Jackson CVE The CVE is fixed in 2.12.6.1 or upper (https://github.com/FasterXML/jackson-databind/issues/2816), therefore we should change the version to 2.12.7 (the latest 2.12.x as of now). That way the vulnerability will be still fixed.", "created": "2022-07-11T02:27:50.100+0000"}, {"author": "Akira Ajisaka", "body": "Note: When reverting this issue, I recommend to use a separate JIRA because the change was released in 3.3.2 and 3.3.3. That way we can easily track what change is in the specific release.", "created": "2022-07-11T03:50:45.910+0000"}, {"author": "Viraj Jasani", "body": "[~aajisaka] do you recommend downgrading to 2.12.7 only for branch-3.3 or for trunk as well? Can trunk continue using Jackson 2.13.2 because Jersey upgrade work is in progress? For 3.4.0 release, HADOOP-15984 can be treated as blocker that way?", "created": "2022-07-11T04:25:42.991+0000"}, {"author": "Akira Ajisaka", "body": "[~vjasani] Currently I recommend downgrading to 2.12.7 in both trunk and branch-3.3. That way we don't need to treat HADOOP-15984 as a blocker for 3.4.0. (maybe HADOOP-15984 is still a blocker for 3.4.0 regardless of this issue as it blocks compile Hadoop with Java 11)", "created": "2022-07-11T04:35:42.779+0000"}, {"author": "Ayush Saxena", "body": "Well, Jersey upgrade is good to have, for Java-11, kind of new feature support. If we have it in a good and safe manor, But yes I too believe it will have a bunch of incompatible changes, need to see, how to handle that... Reverting for now in both trunk & branch-3.3, makes sense, it will allow us time and won't be blocking any of the release lines. I reverted locally these two commits and pushed a PR. Hopefully it shouldn't break any tests. https://github.com/apache/hadoop/pull/4544", "created": "2022-07-11T08:09:41.391+0000"}, {"author": "Steve Loughran", "body": "ok. someone do the 3.3.x revert and i will get it into the 3.3.4 release i will kick off once it is in", "created": "2022-07-11T11:36:38.122+0000"}, {"author": "PJ Fanning", "body": "[~ayushtkn] would https://issues.apache.org/jira/browse/HADOOP-18332 be worth trying first before looking to undo jackson/rs-api changes? I've been doing a build locally and so far, at least, things look ok (that jsr311-api dependency can be removed).", "created": "2022-07-11T11:58:02.960+0000"}, {"author": "Ayush Saxena", "body": "[~pj.fanning] I think that was tried: https://github.com/apache/hadoop/pull/4460 Try TestHttpServer#testJersey if it passes with your code change", "created": "2022-07-11T12:44:13.569+0000"}, {"author": "PJ Fanning", "body": "So the Tez issue seems (possibly) to be caused by https://github.com/FasterXML/jackson-jaxrs-providers/issues/134 - is it ok to downgrade jackson to 2.12.7? - has latest CVE fixes but not this change", "created": "2022-07-11T15:06:39.347+0000"}, {"author": "Ayush Saxena", "body": "{quote}is it ok to downgrade jackson to 2.12.7? - has latest CVE fixes but not this change {quote} Sounds good to me , if we get rid of javax.ws.rs-api dependency without compromising on the CVE, I think there isn't anything better which we can think of. [~aajisaka] too pointed that we can explore moving to 2.12.7. Initially this Jira too was raised to move Jackson to 2.12.x latest. I think if the build doesn't complain post removing javax.ws.rs-api and moving to 2.12.7, then we are sorted", "created": "2022-07-11T16:13:10.022+0000"}, {"author": "Viraj Jasani", "body": "{quote}Currently I recommend downgrading to 2.12.7 in both trunk and branch-3.3. That way we don't need to treat\u00a0HADOOP-15984\u00a0as a blocker for 3.4.0. {quote} I understand that if we are doing the revert with a new Jira, the new Jira should ideally land on trunk before making it's way to active release branches, but Jackson downgrade to 2.12.7 and removal of javax.ws.rs-api would also likely need to be reverted as part of HADOOP-15984, so for HADOOP-15984 it will be too much work staying upto date with trunk (it's already struggling to do so btw with whatever progress is made), and now it will have to reintroduce javax.ws.rs-api and remove jsr311-api. So far I have jsr311-api removed from the current local patch, but if trunk removes javax.ws.rs-api as part of revert of HADOOP-18033 on trunk, there will be rework (basically, revert of revert of HADOOP-18033 for HADOOP-15984 to make progress) that would make the overall progress for HADOOP-15984 even more complicated. Hence, I am requesting if we could only restrict the revert of HADOOP-18033 for branch-3.3 to unblock 3.3.4 release. IIUC, we are anyways not ready for 3.4.0 release anytime soon?", "created": "2022-07-11T18:46:38.793+0000"}, {"author": "Steve Loughran", "body": "i want to kick off a 3.3 3+ cve-only release this week, with the real \"branch-3.3\" coming later. what do we do here? as the longer we think about this the more PRs to update even more jars will surface", "created": "2022-07-12T18:46:48.261+0000"}, {"author": "PJ Fanning", "body": "[~stevel@apache.org] I've had to make a change to https://github.com/apache/hadoop/pull/4547 - there is also https://github.com/apache/hadoop/pull/4544 (which builds ok). The difference is that 4544 uses an older version of Jackson - but both PRs involve downgrading Jackson. Is there a branch you would prefer us to target for your 3.3.3+ cve-only release?", "created": "2022-07-12T19:26:13.648+0000"}, {"author": "Ayush Saxena", "body": "[~stevel@apache.org] I have a PR which reverts the two commits here: [https://github.com/apache/hadoop/pull/4544] So, initial thought was to revert those commits and unblock the releases. Then\u00a0HADOOP-18332 came up with revert 2 + move to Jackson 2.12.7, so we don't expose the CVE as well and remove the new jar which is creating problems. (Let me know if need separate commits, like 2 different revert commits & one upgrade, will do some CLI stuff with HADOOP-18332) Both revert PR & the new PR have green builds, unfortunately I have a review comment on the new one but that is no big stuff and to me that is the final solution, unless other people come and block us. The plan was to try the Tez stuff as well with that change & ask the other folks who flagged Spark issues to try that as well, but considering the timelines, lets not spend too much time there... {*}So, in best case should unblock the release by day after{*}, considering the build will take some 24 hours, if updated tomorrow. Regarding trunk vs only branch-3.3, in favour of keeping all the branches in sync for now, otherwise if some change comes in trunk which uses this new jar, then we would be doing this revert exercise again and with new set of problems. Moreover no point in keeping the trunk also in broken state. [~vjasani] regarding the effort due to this revert activity and so. The best offer I have is \"I can help or worst get some help\", may be with some rebase effort, so this revert activity doesn't become an overhead for you.", "created": "2022-07-12T19:37:06.269+0000"}, {"author": "Ayush Saxena", "body": "I have approved the PR at HADOOP-18332, Tried the two Tez tests which failed with 3.3.3. They pass locally with those changes. Haven't run all the tests though... It is kind of revert of these changes, So, once folks involved here are convinced with the new changes. It can be merged. Nothing blocking from my side now.", "created": "2022-07-14T01:51:54.664+0000"}, {"author": "Viraj Jasani", "body": "Thanks [~ayushtkn], HADOOP-18332 PRs (trunk and 3.3) seem good enough to unblock 3.3.4.", "created": "2022-07-14T02:46:21.918+0000"}, {"author": "Steve Loughran", "body": "transient CVE issues (snakeyaml) are generating motivation for upgrading hadoop jackson. anyone got an idea about how to do this in a way which could work downstream?", "created": "2023-06-12T13:45:50.161+0000"}, {"author": "PJ Fanning", "body": "We're stuck on Jackson 2.12 because of jersey v1. Jackson 2.13 has a change that drops support for jersey v1. Options include: * forking the jackson module for jaxrs to undo the change that drops jersey v1 support * or removing the dependence on that jackson module by doing https://issues.apache.org/jira/browse/HADOOP-18619 * or completing the move to jersey 2 (https://issues.apache.org/jira/browse/HADOOP-15984)", "created": "2023-06-12T15:09:45.404+0000"}, {"author": "Ayush Saxena", "body": "I have almost lost track of this and honestly didn't return back once Tez-Hive upgrade got sorted. :( Jersey upgrade is the best thing to do, but that is stuck, we need that anyway for JDK-11 compile time support as well. but if thats not working, HADOOP-18619 could be a way out, forking would be a trouble during next upgrades and all.", "created": "2023-06-12T16:39:00.971+0000"}, {"author": "Steve Loughran", "body": "java11, please please please", "created": "2023-06-21T09:59:11.773+0000"}, {"author": "ASF GitHub Bot", "body": "virajjasani closed pull request #4460: HADOOP-18033. [WIP] Remove jsr311-api dependency URL: https://github.com/apache/hadoop/pull/4460", "created": "2025-10-25T00:26:09.530+0000"}], "derived_tasks": {"summary": "Upgrade fasterxml Jackson to 2.13.0 - Spark 3", "classifications": ["improvement", "performance"], "qa_pairs": []}}
{"id": "HADOOP-17725", "title": "Improve error message for token providers in ABFS", "description": "It would be good to improve error messages for token providers in ABFS. Currently, when a configuration key is not found or mistyped, the error is not very clear on what went wrong. It would be good to indicate that the key was required but not found in Hadoop configuration when creating a token provider. For example, when running the following code:  import org.apache.hadoop.conf._ import org.apache.hadoop.fs._ val conf = new Configuration() conf.set(\"fs.azure.account.auth.type\", \"OAuth\") conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") conf.set(\"fs.azure.account.oauth2.client.id\", \"my-client-id\") // conf.set(\"fs.azure.account.oauth2.client.secret.my-account.dfs.core.windows.net\", \"my-secret\") conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"my-endpoint\") val path = new Path(\"abfss://container@my-account.dfs.core.windows.net/\") val fs = path.getFileSystem(conf) fs.getFileStatus(path) The following exception is thrown:  TokenAccessProviderException: Unable to load OAuth token provider class. ... ... which does not tell what configuration key was not loaded. IMHO, it would be good if the exception was something like this:  TokenAccessProviderException: Unable to load OAuth token provider class. ...", "status": "Resolved", "priority": "Major", "reporter": "Ivan Sadikov", "assignee": "Viraj Jasani", "created": "2021-05-21T11:07:38.000+0000", "updated": "2025-10-25T00:25:36.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure", "hadoop-thirdparty"], "comments": [{"author": "Viraj Jasani", "body": "[~stevel@apache.org]\u00a0Would you like to take a look at https://github.com/apache/hadoop/pull/3041? Thanks", "created": "2021-05-23T12:13:48.802+0000"}, {"author": "Steve Loughran", "body": "This seems to be triggering some needless failures. We should look hard and see if it is asking for properties which do not always need to be set", "created": "2021-12-06T10:58:36.526+0000"}, {"author": "Viraj Jasani", "body": "The only parameters that were not covered by Preconditions.checkNotNull are clientId and tenantGuid used by MsiTokenProvider:  } else if (tokenProviderClass == MsiTokenProvider.class) { String authEndpoint = getTrimmedPasswordString( FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); String tenantGuid = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT); String clientId = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID); String authority = getTrimmedPasswordString( FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); authority = appendSlashIfNeeded(authority); tokenProvider = new MsiTokenProvider(authEndpoint, tenantGuid, clientId, authority); LOG.trace(\"MsiTokenProvider initialized\"); }  If we replace them back to getPasswordString() and not encounter for precise error message for missing configs, perhaps the failures you are now seeing would be gone. But for any other params that are covered by getMandatoryPasswordString(), they are all also covered by Preconditions.checkNotNull meaning, they would fail with NPE anyways. Hence, are the needless failures relevant to MsiTokenProvider? If yes, this patch would revert the behaviour for them:  diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java index 9719da7dc16..d43f1d99a77 100644 --- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java +++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java @@ -812,9 +812,9 @@ public AccessTokenProvider getTokenProvider() throws TokenAccessProviderExceptio FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); String tenantGuid = -\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT); +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT); String clientId = -\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID); +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 getPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID); String authority = getTrimmedPasswordString( FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);", "created": "2021-12-06T11:34:09.560+0000"}, {"author": "Viraj Jasani", "body": "[~ivan.sadikov] Would you like to verify above info? If you still have your account setup, could you also create PR and run tests against the accessible zone? {quote}We should look hard and see if it is asking for properties which do not always need to be set {quote} I have confirmed that the above mentioned props (fs.azure.account.oauth2.msi.tenant and\u00a0fs.azure.account.oauth2.client.id) are the ones that are not always required to be set.", "created": "2021-12-07T16:13:57.761+0000"}, {"author": "Mehakmeet Singh", "body": "Re-opening as it caused some unexpected failures in our environment. Since, MSI tenant ID and client ID are optional, as stated in: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html#OAuth_2.0_Client_Credentials and also discussed above by Viraj and steve, I think we should not mandate them to be set.", "created": "2021-12-11T05:28:46.581+0000"}, {"author": "Viraj Jasani", "body": "[~mehakmeetSingh] I can create addendum PR if you would like but I don't have setup to run against an endpoint as of now. Would you like to run tests against any endpoint you have access to? Edit: Managed to get an endpoint to run tests against, but OAuth is not yet setup.", "created": "2021-12-11T05:45:33.535+0000"}, {"author": "Mehakmeet Singh", "body": "I can run the tests against an endpoint(on sharedKey auth setup), but I think for this to verify we need OAuth creds, which unfortunately I don't have set up as of now. If [~ivan.sadikov]already have that setup, that's good. We did have some problems setting OAuth tests on our local machines the last time we tried. You can open up a PR in the meantime, and we can start reviewing it.", "created": "2021-12-11T05:59:52.299+0000"}, {"author": "Viraj Jasani", "body": "Opened the addendum [PR|https://github.com/apache/hadoop/pull/3788]. Unfortunately, without this addendum, 3.3.2 RC0 might need to sink. However, let [~stevel@apache.org] comment for the final decision. FYI [~csun] [~mehakmeetSingh] [~ivan.sadikov]", "created": "2021-12-11T07:33:23.830+0000"}, {"author": "Viraj Jasani", "body": "{quote}We did have some problems setting OAuth tests on our local machines the last time we tried. {quote} I am also facing issues with tests using OAuth providers as mentioned on the PR: [https://github.com/apache/hadoop/pull/3788]", "created": "2021-12-12T14:27:23.832+0000"}, {"author": "Steven Swor", "body": "[~vjasani] [~stevel@apache.org] Can this be bumped to a later release so that 3.3.2 can go out? This seems to be the only remaining open issue marked for that release.", "created": "2022-03-01T00:14:52.030+0000"}, {"author": "Viraj Jasani", "body": "[~stevel@apache.org] Since the first commit already landed on 3.3.2, I believe we can't remove 3.3.2 from fixVersion. Could you please confirm?", "created": "2022-03-01T14:45:50.277+0000"}, {"author": "Steve Loughran", "body": "3.3.2 has this patch in, so closing as fixed in that version.", "created": "2022-05-04T10:40:57.792+0000"}, {"author": "Carl", "body": "The changes related to this ticket force to set optional fields and prevent from relying on Azure's metadata service to set them automatically. Both [~vjasani] 's PR ([https://github.com/apache/hadoop/pull/3788)] and mine fix it ([https://github.com/apache/hadoop/pull/4262|https://github.com/apache/hadoop/pull/4262)]). Can we get one of these merged please ?", "created": "2022-06-23T13:56:01.194+0000"}, {"author": "ASF GitHub Bot", "body": "CLevasseur commented on PR #4262: URL: https://github.com/apache/hadoop/pull/4262#issuecomment-1327466165 Hi @pranavsaxena-microsoft, I've tried to follow[ this section ](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/site/markdown/testing_azure.md#generating-test-run-configurations-and-test-triggers-over-various-config-combinations)of the documentation that you mentioned, but it's outdated: - The folder `./src/test/resources/accountSettings/` doesn't exist, nor the template file that I should use to create my account settings file - `dev-support/testrun-scripts/runtests.sh` should prompt a menu, but in my case it runs `AppendBlob-HNS-OAuth` and gives me no choice I followed the rest of the documentation by copying `./src/test/resources/azure-auth-keys.xml.template` to `./src/test/resources/azure-auth-keys.xml` and replacing those variables by the right values in the xml file: - `{ABFS_ACCOUNT_NAME}` - `{ACCOUNT_ACCESS_KEY}` - `{TENANTID}` - `{WASB_ACCOUNT_NAME}` - `{WASB_FILESYSTEM}` - `{CONTAINER_NAME}` - `{ACCOUNT_NAME}` Then I ran ``` dev-support/testrun-scripts/runtests.sh -c \"NonHNS-SharedKey\" [...] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 11.030 s [INFO] Finished at: 2022-11-25T13:08:18Z [INFO] ------------------------------------------------------------------------ Running the combination: NonHNS-SharedKey... # Then it terminates without running anything ``` So it doesn't run the tests, same for `HNS-OAuth` and `HNS-SharedKey`. Do you know what I am missing for those tests to run ?", "created": "2022-11-25T13:14:31.964+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on PR #4262: URL: https://github.com/apache/hadoop/pull/4262#issuecomment-1328485053 Hi @CLevasseur , the testing section requires you to have latest trunk commits into your branch (as the testing section has been updated for the trunk). Hence requesting you to kindly back-merge apache:trunk into your branch. Thanks.", "created": "2022-11-28T03:22:37.022+0000"}, {"author": "ASF GitHub Bot", "body": "CLevasseur commented on PR #4262: URL: https://github.com/apache/hadoop/pull/4262#issuecomment-1328860096 I have pulled the latest changes. It now fails when running the tests. Note that it also fails when I run the tests from the trunk branch of the `apache/hadoop` repository. I haven't setup OAuth2, is that alright if we just run those tests using the Shared Key ? Also, it looks like those tests create a lot of containers in the storage account, is there an easy way to clean those ? **Tests Output (Both apache/hadoop:trunk and CLevasseur/hadoop:trunk give the same errors)** **NonHNS-SharedKey**: ``` Choose action: [Note - SET_ACTIVE_TEST_CONFIG will help activate the config for IDE/single test class runs] 1) SET_ACTIVE_TEST_CONFIG 2) RUN_TEST 3) CLEAN_UP_OLD_TEST_CONTAINERS 4) SET_OR_CHANGE_TEST_ACCOUNT 5) PRINT_LOG4J_LOG_PATHS_FROM_LAST_RUN #? 2 Enter parallel test run process count [default - 8]: Set the active test combination to run the action: 1) HNS-OAuth 2) HNS-SharedKey 3) nonHNS-SharedKey 4) AppendBlob-HNS-OAuth 5) AllCombinationsTestRun 6) Quit #? 3 Combination specific property setting: [ key=fs.azure.account.auth.type , value=SharedKey ] Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: dataenginfraus3prod for combination NonHNS-SharedKey Running test for combination NonHNS-SharedKey on account dataenginfraus3prod [ProcessCount=8] Test run report can be seen in dev-support/testlogs/2022-11-28_10-12-22/Test-Logs-NonHNS-SharedKey.txt pcregrep: pcre_exec() gave error -27 while matching text that starts: [ERROR] ITestAzureBlobFileSystemMainOperation>FSMainOperationsBaseTest.testGlobStatusFilterWithEmptyPathResults:492 ? AbfsRestOperation [ERROR] ITestAzureBlobF pcregrep: Error -8, -21 or -27 means that a resource limit was exceeded. pcregrep: Check your regex for nested unlimited loops. ---", "created": "2022-11-28T10:36:34.101+0000"}, {"author": "ASF GitHub Bot", "body": "virajjasani closed pull request #3788: HADOOP-17725. Keep MSI tenant ID and client ID optional (ADDENDUM) URL: https://github.com/apache/hadoop/pull/3788", "created": "2025-10-25T00:25:35.921+0000"}], "derived_tasks": {"summary": "Improve error message for token providers in ABFS - It would be good to improve error messages for token providers in ABFS", "classifications": ["improvement", "bug"], "qa_pairs": []}}
{"id": "HADOOP-17377", "title": "ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service", "description": "*Summary* The instance metadata service has its own guidance for error handling and retry which are different from the Blob store. [https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#error-handling] In particular, it responds with HTTP 429 if request rate is too high. Whereas Blob store will respond with HTTP 503. The retry policy used only accounts for the latter as it will retry any status >=500. This can result in job instability when running multiple processes on the same host. *Environment* * Spark talking to an ABFS store * Hadoop 3.2.1 * Running on an Azure VM with user-assigned identity, ABFS configured to use MsiTokenProvider * 6 executor processes on each VM *Example* Here's an example error message and stack trace. It's always the same stack trace. This appears in logs a few hundred to low thousands of times a day. It's luckily skating by since the download operation is wrapped in 3 retries.  AADToken: HTTP connection failed for getting token from AzureAD. Http response: 429 null Content-Type: application/json; charset=utf-8 Content-Length: 90 Request ID: Proxies: none First 1K of Body: {\"error\":\"invalid_request\",\"error_description\":\"Temporarily throttled, too many requests\"} CC [~mackrorysd], [~stevel@apache.org]", "status": "Open", "priority": "Major", "reporter": "Brandon", "assignee": null, "created": "2020-11-13T03:16:31.000+0000", "updated": "2025-10-24T08:10:35.000+0000", "labels": ["pull-request-available"], "components": ["fs/azure"], "comments": [{"author": "Steve Loughran", "body": "[~snvijaya]", "created": "2020-11-16T10:42:04.413+0000"}, {"author": "Brandon", "body": "Another note. Very rarely, I've also seen HTTP 410 errors from the Instance Metadata Service. ABFS currently doesn't retry those. Azure documentation suggests 410 and 500 response codes should be retried: [https://docs.microsoft.com/en-in/azure/virtual-machines/linux/instance-metadata-service?tabs=windows#errors-and-debugging] Here's the full error message and stack trace for reference:  AADToken: HTTP connection failed for getting token from AzureAD. Http response: 410 Gone Content-Type: text/html Content-Length: 35 Request ID: Proxies: none First 1K of Body: The page you requested was removed.", "created": "2021-02-12T19:54:36.420+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370941112 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 14s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | -1 :x: | javadoc | 0m 39s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 36s | | the patch passed | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 103m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6b243f21df19 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56c6f7de6ead9500d2c449ba768d2301fc7e4d85 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 627 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-01-04T13:39:16.845+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370942634 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 33s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | -1 :x: | javadoc | 0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | -1 :x: | javadoc | 0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 14s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 2s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 103m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ab9041391762 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56c6f7de6ead9500d2c449ba768d2301fc7e4d85 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-01-04T13:40:25.598+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370943150 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | -1 :x: | javadoc | 0m 40s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 1s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 94m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6f4158513412 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2c25b39b727a259ca0feb754705adbcfe5e80331 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 694 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-01-04T13:40:49.808+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1371868477 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | -1 :x: | javadoc | 0m 40s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 14s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 29s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 40s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 93m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7099b9bc66b4 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3be0b00a59ca2e0286bdd44dbb20e3d0f76b8d4d | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 560 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-01-05T07:26:14.127+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1371873722 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 39m 22s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | -1 :x: | javadoc | 0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 16s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 2s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 99m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 8916fc660cd7 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3be0b00a59ca2e0286bdd44dbb20e3d0f76b8d4d | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 586 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-01-05T07:34:38.124+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1372011908 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | -1 :x: | javadoc | 0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | -1 :x: | javadoc | 0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 2s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 102m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7d84852c9f33 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e04011ca7ca13cb0ab3146ee54e5c9bbf7aefddb | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | [CI_URL] | | Max. process+thread count | 534 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-01-05T10:03:27.516+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1063226881 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) { return retryCount < this.retryCount && (statusCode == -1 || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT + || statusCode == HttpURLConnection.HTTP_GONE + || statusCode == HTTP_TOO_MANY_REQUESTS Review Comment: Should we keep this in AzureADAuthentication condition. Reason being, now if any API in AbfsHttpOperation get 429 or 410, it will be retried 30 times. Right now what would happen in 429 / 410: `executeHttpOperation` would give true and in completeExecute after very first call: ``` if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) { throw new AbfsRestOperationException(result.getStatusCode(), result.getStorageErrorCode(), result.getStorageErrorMessage(), null, result); } ``` would throw exception.", "created": "2023-01-06T08:19:46.987+0000"}, {"author": "ASF GitHub Bot", "body": "pranavsaxena-microsoft commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1064481413 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) { return retryCount < this.retryCount && (statusCode == -1 || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT + || statusCode == HttpURLConnection.HTTP_GONE + || statusCode == HTTP_TOO_MANY_REQUESTS Review Comment: Removing from ExponentialRetryPolicy, we can have https://github.com/apache/hadoop/pull/5273/files#diff-dff9c93d1668203c206aa1c092aef9d2921dc6e20af8888d06fae34778991531R320-R321 as ``` !succeeded && isRecoverableFailure && (tokenFetchRetryPolicy.shouldRetry(retryCount, httperror)||httpError==429 ||httpError==410); ``` reason being, this check is only required in ADAuthenticator.", "created": "2023-01-09T10:25:43.178+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1066593607 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) { return retryCount < this.retryCount && (statusCode == -1 || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT + || statusCode == HttpURLConnection.HTTP_GONE + || statusCode == HTTP_TOO_MANY_REQUESTS Review Comment: We plan to retry for these status codes, if they come as a response from backend as well. Hence adding these into a centralized exponential retry policy class.", "created": "2023-01-11T05:47:35.642+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1494098831 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 1s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 23s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 37s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 56s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 94m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 78e6cbb88986 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 62834104a7e75dfb14c6774e9ef4dfc6e803e10e | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | [CI_URL] | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-04-03T10:51:21.565+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1164119014 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> + <version>4.11.0</version> Review Comment: sorry, hadoop-project defines the version, and through properties. revert this ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> + <version>4.11.0</version> <scope>test</scope> </dependency> + + <dependency> + <groupId>org.mockito</groupId> + <artifactId>mockito-inline</artifactId> + <version>4.11.0</version> Review Comment: if this is new to hadoop, declare it in hadoop-project/pom.xml, with versions and exclusions, then declare here without those ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -40,13 +47,16 @@ import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT; +import static org.mockito.Mockito.times; /** * Test MsiTokenProvider. */ public final class ITestAbfsMsiTokenProvider extends AbstractAbfsIntegrationTest { + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: refer to the value in the src/main code ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -58,6 +58,13 @@ public class ExponentialRetryPolicy { */ private static final double MAX_RANDOM_RATIO = 1.2; + /** + * Qualifies for retry based on Review Comment: needs a . in it, maybe split into \"qualifies for retry.\" and \"see...\" ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + * @throws Exception + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, Review Comment: what if these are undefined? skip the test? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + * @throws Exception Review Comment: cut this @throws ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java: ########## @@ -341,7 +341,7 @@ private static boolean isRecoverableFailure(IOException e) { || e instanceof FileNotFoundException); } - private static AzureADToken getTokenSingleCall(String authEndpoint, + public static AzureADToken getTokenSingleCall(String authEndpoint, Review Comment: now it is public, add a javadoc ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -58,6 +58,13 @@ public class ExponentialRetryPolicy { */ private static final double MAX_RANDOM_RATIO = 1.2; + /** + * Qualifies for retry based on + * https://learn.microsoft.com/en-us/azure/active-directory/ + * managed-identities-azure-resources/how-to-use-vm-token#error-handling + */ + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: make public and refer from tests, maybe put in a different file for this", "created": "2023-04-12T13:18:42.131+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1687757407 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 12s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 50s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 19s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 37s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 54s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 29s | | The patch does not generate ASF License warnings. | | | | 95m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux d2957048b6f1 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cf73a70fc72b27a28f18058b8399784e3d7e891e | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-22T08:51:16.197+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1687925288 @anmolanmol1234 still need those (minor) changes -otherwise it is ready to merge", "created": "2023-08-22T10:29:25.793+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1301867025 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + * @throws Exception + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, Review Comment: these are hardcoded here, no need for skipping", "created": "2023-08-22T16:01:07.074+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1302010099 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsHandlerImpl.java: ########## @@ -192,7 +192,7 @@ public void testMountController() throws IOException { assertTrue(\"cgroup dir should be cerated\", cgroup.mkdirs()); //Since we enabled (deferred) cgroup controller mounting, no interactions //should have occurred, with this mock - verifyZeroInteractions(privilegedOperationExecutorMock); + Mockito.verifyNoInteractions(privilegedOperationExecutorMock); Review Comment: use static import for consistency with the others. ########## hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java: ########## @@ -24,7 +24,6 @@ import static org.junit.Assert.assertNotNull; import static org.junit.Assert.assertTrue; import static org.junit.Assert.fail; -import static org.mockito.Matchers.any; Review Comment: reinstate so this file doesn't change ########## hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java: ########## @@ -423,7 +423,7 @@ public void testSubclusterDown() throws Exception { FSNamesystem ns0 = nn0.getNamesystem(); HAContext nn0haCtx = (HAContext)getInternalState(ns0, \"haContext\"); HAContext mockCtx = mock(HAContext.class); - doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(any()); + doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(Mockito.any()); Review Comment: return to the existing any() static import ########## hadoop-project/pom.xml: ########## @@ -1308,7 +1308,20 @@ <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> - <version>2.28.2</version> + <version>4.11.0</version> Review Comment: add a new property mockito.version and reference in both places ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/gpu/TestGpuResourceAllocator.java: ########## @@ -210,7 +210,7 @@ private void assertAllocatedGpus(int gpus, int deniedGpus, private void assertNoAllocation(GpuAllocation allocation) { assertEquals(1, allocation.getDeniedGPUs().size()); assertEquals(0, allocation.getAllowedGPUs().size()); - verifyZeroInteractions(nmStateStore); + Mockito.verifyNoInteractions(nmStateStore); Review Comment: use static import", "created": "2023-08-22T18:09:28.358+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689282173 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 16 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 41s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 48s | | trunk passed | | +1 :green_heart: | compile | 10m 41s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 9m 40s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 2m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 8m 56s | | trunk passed | | +1 :green_heart: | javadoc | 6m 53s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 6m 22s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +0 :ok: | spotbugs | 0m 19s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 21m 15s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 35s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 5m 35s | | the patch passed | | +1 :green_heart: | compile | 10m 1s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 10m 1s | | the patch passed | | +1 :green_heart: | compile | 9m 39s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 9m 39s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 2m 24s | [/results-checkstyle-root.txt]([CI_URL] | root: The patch generated 3 new + 86 unchanged - 0 fixed = 89 total (was 86) | | +1 :green_heart: | mvnsite | 8m 48s | | the patch passed | | -1 :x: | javadoc | 0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +0 :ok: | spotbugs | 0m 20s | | hadoop-project has no data from spotbugs | | -1 :x: | shadedclient | 21m 14s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 22s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 16m 5s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 192m 15s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 86m 6s | | hadoop-yarn-server-resourcemanager in the patch passed. | | -1 :x: | unit | 22m 15s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt]([CI_URL] | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 19m 25s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 20m 26s | | hadoop-yarn-services-core in the patch passed. | | -1 :x: | unit | 208m 56s | [/patch-unit-hadoop-yarn-project.txt]([CI_URL] | hadoop-yarn-project in the patch passed. | | +1 :green_heart: | unit | 2m 26s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 778m 13s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.linux.runtime.TestDockerContainerRuntime | | | hadoop.yarn.server.nodemanager.containermanager.linux.runtime.TestDockerContainerRuntime | | | hadoop.yarn.client.TestRMFailover | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint | | uname | Linux 8b3e21c22fc4 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0688809c43766c8d001a39c972e263d102b2df82 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 3325 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project hadoop-tools/hadoop-azure U: . | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-23T04:57:59.506+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689324456 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 18s | | https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Console output | [CI_URL] | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-23T05:55:57.854+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689328770 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 18s | | https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Console output | [CI_URL] | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-23T06:01:23.818+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689330087 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 17s | | https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Console output | [CI_URL] | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-23T06:02:58.232+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689440680 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 29s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 53s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 15s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 33s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 11s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 9s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 11s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 10s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 11s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 11s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | shadedclient | 2m 25s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 10s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 12s | | ASF License check generated no output? | | | | 68m 5s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 1b921f5b2a8a 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8b5e8839d108e3d02d0b1b0f872701bd79d8354c | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 625 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-23T07:35:21.784+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689442903 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 58s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 50s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 9s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 26s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 10s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 9s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 10s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 10s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 11s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 11s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | shadedclient | 2m 27s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 11s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 12s | | ASF License check generated no output? | | | | 68m 8s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 5b0ba2fb9772 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8b5e8839d108e3d02d0b1b0f872701bd79d8354c | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 552 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-23T07:37:07.565+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1697434060 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 35m 9s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 33s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 53s | | trunk passed | | -1 :x: | shadedclient | 21m 27s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 42s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 15s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | -1 :x: | shadedclient | 21m 21s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 29s | | The patch does not generate ASF License warnings. | | | | 90m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 04bde9dbd212 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ba573ff39a6fa1ac0a78391d345bd2219fc9c97 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 412 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-29T13:20:31.853+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1698648099 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 36s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | -1 :x: | shadedclient | 25m 29s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 19s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | -1 :x: | shadedclient | 24m 24s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 47s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 27s | | The patch does not generate ASF License warnings. | | | | 96m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux e4005f0bafa0 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 462a3b6a0c7aadefe39960218092651bad10c979 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 455 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-30T07:34:18.219+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1310529985 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( Review Comment: this kind of stuff is trouble as it makes maintenance a nightmare; you can't see where the field is access, all you have is a mocking test failing. proposed: add a static setter to AzureADAuthenticator; mark as @VisibleForTesting.", "created": "2023-08-30T16:29:27.852+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1310534958 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( + \"tokenFetchRetryPolicy\"); + tokenFetchRetryPolicy.setAccessible(true); + tokenFetchRetryPolicy.set(ExponentialRetryPolicy.class, + exponentialRetryPolicy); + + AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint, + tenantGuid, clientId, authority); + AzureADToken token = null; + intercept(AzureADAuthenticator.HttpException.class, + tokenProvider::getToken); + + // If the status code doesn't qualify for retry shouldRetry returns false and the loop ends. + // It being called multiple times verifies that the retry was done for the throttling status code 429. + Mockito.verify(exponentialRetryPolicy, Review Comment: so ExponentialRetryPolicy.getRetryCount() is there to let you pass a non-mocked policy in and then assert on it. how about using that here? it probably needs making the accessors public, rather than package scoped, but that's all. The less use we make of mockito, the less things will break with every mockito upgrade", "created": "2023-08-30T16:29:42.859+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1310523986 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> + <version>4.11.0</version> Review Comment: again, cut this now; the version in hadoop project is the one you now expect", "created": "2023-08-30T16:32:58.056+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1701148507 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 50s | | trunk passed | | -1 :x: | shadedclient | 27m 31s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 27m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | -1 :x: | javadoc | 0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | -1 :x: | shadedclient | 26m 8s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 54s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 31s | | The patch does not generate ASF License warnings. | | | | 102m 59s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7a1812c7f588 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 78329dece95a9e2d53d4f523a95c3325b09a2d6d | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 455 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-08-31T14:21:16.718+0000"}, {"author": "Agnes Tevesz", "body": "[~stevel@apache.org] [~brandonvin] Can you help to move this change forward? Who should be the owner of this task? The ticket is not assigned to anyone and there is no activity on the change since end of August. This fix should land in hadoop. The pod identity in azure was deprecated: [https://github.com/Azure/aad-pod-identity] If we get the token directly from the instance metadata service we hit this HTTP 429 issue with tpcds tests very frequently: [https://azure.github.io/azure-workload-identity/docs/] The pod identity component most likely provided the retry logic before, but we cannot install depreciated components on an AKS cluster. Can this change get finished?", "created": "2023-11-03T18:08:37.050+0000"}, {"author": "ASF GitHub Bot", "body": "nandorKollar commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1808268461 I think this this PR is great, however there's still one related open problem: the default values (2) for `fs.azure.oauth.token.fetch.retry.delta.backoff` is incorrect. The value of 2 is consistent with MS recommendation (https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/how-to-use-vm-token#retry-guidance), but it is assumed in **seconds**, but as this is used in Thread.sleep [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L326), it will be measured in **milliseconds**. I think we should change the default to 2000. @steveloughran @anmolanmol1234 do you think we can implement this minimal change in this PR, or we should open a separate one?", "created": "2023-11-13T14:28:06.614+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1809005084 I'll go with whatever @saxenapranav thinks here...we have seen this ourselves and need a fix. However, that PR to update mockito bounced, so either 1. another attempt is made to update mockito, including the shaded client 2. this PR can be done without updating mockito (easier)", "created": "2023-11-13T20:20:23.334+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811834929 > I'll go with whatever @saxenapranav thinks here...we have seen this ourselves and need a fix. > > However, that PR to update mockito bounced, so either > > 1. another attempt is made to update mockito, including the shaded client > 2. this PR can be done without updating mockito (easier) The mockito upgrade was needed as part of this PR to mock static methods. So would it be fine if we remove that test method or if not I will attempt to upgrade mockito, including the shaded client.", "created": "2023-11-15T05:31:10.213+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811839555 > I think this this PR is great, however there's still one related open problem: the default values (2) for `fs.azure.oauth.token.fetch.retry.delta.backoff` is incorrect. The value of 2 is consistent with MS recommendation (https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/how-to-use-vm-token#retry-guidance), but it is assumed in **seconds**, but as this is used in Thread.sleep [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L326), it will be measured in **milliseconds**. I think we should change the default to 2000. @steveloughran @anmolanmol1234 do you think we can implement this minimal change in this PR, or we should open a separate one? Will update this change as an iteration of this PR, but will some time for the mockito upgrade PR.", "created": "2023-11-15T05:37:11.864+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811896245 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 20s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -0 :warning: | checkstyle | 0m 18s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 22s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 20s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 21s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 21s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 2m 21s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 2m 42s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 20s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 20s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 20s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 3m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 20s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 20s | | ASF License check generated no output? | | | | 12m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 58c5a67f711c 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b0563a19157c485d782faa96d8300020c408ea8e | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 24 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-15T06:50:10.145+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811906117 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 20s | [/branch-mvninstall-root.txt]([CI_URL] | root in trunk failed. | | -1 :x: | compile | 0m 20s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -0 :warning: | checkstyle | 0m 18s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 20s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 20s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 4m 44s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 7m 10s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 7m 32s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 21s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 24s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 9s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 9s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 9s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | -1 :x: | shadedclient | 2m 52s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 9s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 11s | | ASF License check generated no output? | | | | 15m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 5348f26896a5 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 225541ef7a765d856eb28966b7340744fe049654 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | [CI_URL] | | Max. process+thread count | 88 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2023-11-15T07:00:48.461+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1419336218 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -323,6 +323,13 @@ <artifactId>mockito-core</artifactId> <scope>test</scope> </dependency> + + <dependency> Review Comment: is this needed? because its not in the base project pom. I would rather this PR doesn't need that mockito upgrade as mockito upgrades are always a painful piece of work which never gets backported. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -48,7 +48,7 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS = 5; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL = 0; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; - public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2 * 1000; Review Comment: use 2_000", "created": "2023-12-07T17:33:07.968+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3432466591 @anujmodi2021 can you revisit this so I can get it in? we do appear to have been using it internally since december 2023, so I'm happy it works.", "created": "2025-10-22T13:49:38.403+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3435471034 @steveloughran will backport the PR for merge", "created": "2025-10-23T07:10:25.605+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437618030 fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`) ``` [ERROR] ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 \u00bb AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 \u00bb FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 \u00bb FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [INFO] [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13 [INFO] ``` This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h", "created": "2025-10-23T15:23:57.701+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437685484 When these uploads fail we do leave incomplete uploads in progress: ``` Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found. ``` Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed. ``` [ERROR] ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190 \u00bb AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) ``` Yet the uploads list afterwards finds it ``` job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 ``` I have to conclude that the list of pending uploads was briefly offline/inconsistent. This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute. Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\".", "created": "2025-10-23T15:34:46.844+0000"}, {"author": "ASF GitHub Bot", "body": "steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437730498 sorry, commenting on wrong PR. will cut.", "created": "2025-10-23T15:43:21.320+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438000237 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 57s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 51s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 35s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 30m 20s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 30m 40s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 41s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | spotbugs | 1m 26s | | the patch passed | | +1 :green_heart: | shadedclient | 28m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 58s | [/patch-unit-hadoop-tools_hadoop-azure.txt]([CI_URL] | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 31s | | The patch does not generate ASF License warnings. | | | | 116m 35s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 8088428d1bb5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c7d46718136c7e1585bcd3f1d82becc8446f0b50 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 611 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T16:36:18.506+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456024943 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> + <version>4.11.0</version> Review Comment: taken ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> + <version>4.11.0</version> <scope>test</scope> </dependency> + + <dependency> + <groupId>org.mockito</groupId> + <artifactId>mockito-inline</artifactId> + <version>4.11.0</version> Review Comment: removed dependency", "created": "2025-10-23T16:49:17.443+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456032575 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -58,6 +58,13 @@ public class ExponentialRetryPolicy { */ private static final double MAX_RANDOM_RATIO = 1.2; + /** + * Qualifies for retry based on + * https://learn.microsoft.com/en-us/azure/active-directory/ + * managed-identities-azure-resources/how-to-use-vm-token#error-handling + */ + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: taken ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -40,13 +47,16 @@ import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT; +import static org.mockito.Mockito.times; /** * Test MsiTokenProvider. */ public final class ITestAbfsMsiTokenProvider extends AbstractAbfsIntegrationTest { + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: taken", "created": "2025-10-23T16:50:42.784+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456039983 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( Review Comment: taken", "created": "2025-10-23T16:52:12.402+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456041107 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic<AzureADAuthenticator> adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( + \"tokenFetchRetryPolicy\"); + tokenFetchRetryPolicy.setAccessible(true); + tokenFetchRetryPolicy.set(ExponentialRetryPolicy.class, + exponentialRetryPolicy); + + AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint, + tenantGuid, clientId, authority); + AzureADToken token = null; + intercept(AzureADAuthenticator.HttpException.class, + tokenProvider::getToken); + + // If the status code doesn't qualify for retry shouldRetry returns false and the loop ends. + // It being called multiple times verifies that the retry was done for the throttling status code 429. + Mockito.verify(exponentialRetryPolicy, Review Comment: taken", "created": "2025-10-23T16:52:26.382+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456045985 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -48,7 +48,7 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS = 5; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL = 0; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; - public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2 * 1000; Review Comment: taken", "created": "2025-10-23T16:53:23.884+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456046384 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -323,6 +323,13 @@ <artifactId>mockito-core</artifactId> <scope>test</scope> </dependency> + + <dependency> Review Comment: removed dependency", "created": "2025-10-23T16:53:38.055+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438376772 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 48s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 16m 49s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 17m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | -1 :x: | javadoc | 0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | spotbugs | 0m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 16m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 12s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 69m 12s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux df85de01587f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fd260666ade501f736fe7af3476e96f6231f2f0b | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 613 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T17:55:35.975+0000"}, {"author": "ASF GitHub Bot", "body": "hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438560491 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 50s | | trunk passed | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 26s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html]([CI_URL] | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 27m 54s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 28m 14s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 40s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt]([CI_URL] | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | spotbugs | 1m 23s | | the patch passed | | +1 :green_heart: | shadedclient | 28m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 55s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 110m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: [CI_URL] | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c145c3f5b775 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fd260666ade501f736fe7af3476e96f6231f2f0b | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | [CI_URL] | | Max. process+thread count | 629 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | [CI_URL] | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated.", "created": "2025-10-23T18:38:58.039+0000"}, {"author": "ASF GitHub Bot", "body": "anujmodi2021 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3441679165 Thanks @anmolanmol1234 for refreshing up this. @steveloughran this LGTM now and should be ready to merge. The spotbugs adnd javadoc warnings are due to https://issues.apache.org/jira/browse/HADOOP-19731", "created": "2025-10-24T08:04:04.099+0000"}, {"author": "ASF GitHub Bot", "body": "anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3441710243 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 873, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 169 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 715, Failures: 0, Errors: 0, Skipped: 282 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 873, Failures: 0, Errors: 0, Skipped: 228 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 722, Failures: 0, Errors: 0, Skipped: 140 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 712, Failures: 0, Errors: 0, Skipped: 284 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 719, Failures: 0, Errors: 0, Skipped: 152 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 198 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 747, Failures: 0, Errors: 0, Skipped: 226 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 712, Failures: 0, Errors: 0, Skipped: 281 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24", "created": "2025-10-24T08:10:35.531+0000"}], "derived_tasks": {"summary": "ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service - *Summary* The instance metadata service has its own guidance for...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "HADOOP-16964", "title": "Modify constant for AbstractFileSystem", "description": "Modify constant for AbstractFileSystem", "status": "Open", "priority": "Minor", "reporter": "bianqi", "assignee": null, "created": "2020-04-09T04:17:47.000+0000", "updated": "2025-10-20T16:40:10.000+0000", "labels": [], "components": ["fs"], "comments": [{"author": "bianqi", "body": "Change hard coding to constant", "created": "2020-04-09T04:19:33.790+0000"}, {"author": "Steve Loughran", "body": "a good little cleanup. Could you submit it as a github PR against trunk? That will kick off the automated testing", "created": "2020-04-09T18:27:18.952+0000"}, {"author": "bianqi", "body": "[~stevel@apache.org]\u00a0Thank review.\u00a0\u00a0I have submit it as a github PR against trunk.", "created": "2020-04-10T02:00:46.751+0000"}], "derived_tasks": {"summary": "Modify constant for AbstractFileSystem", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "HADOOP-13184", "title": "Add \"Apache\" to Hadoop project logo", "description": "Many ASF projects include \"Apache\" in their logo. We should add it to Hadoop.", "status": "Resolved", "priority": "Major", "reporter": "Christopher Douglas", "assignee": "Abhishek", "created": "2016-05-19T18:42:06.000+0000", "updated": "2025-10-23T17:04:47.000+0000", "labels": [], "components": ["site"], "comments": [{"author": "Christopher Douglas", "body": "Our current set of logos are [here|http://svn.apache.org/repos/asf/hadoop/logos/].", "created": "2016-05-19T18:48:05.005+0000"}, {"author": "Abhishek", "body": "Options for the new logo with APACHE included: *Option 1* !https://ske0hq-bn1305.files.1drv.com/y3mfuiXQm9OGG3-dQGy2hTzYQPu1Xdv0C7wAA2rAA0uoEHS08BQxlReQoL_sLPyy_1JNi04LFfrpEGzyhLUpNJXXfY8mkSDezepfzY2rfo_aw3sEYesjbb_DTdJP3xCmSZ4X66UrJM85HPIa4AoTxjl_Nbjzx4V6HQFVv64rmqPSYc?width=1000&height=267&cropmode=none! \\\\ \\\\ *Option 2* !https://skevhq-bn1305.files.1drv.com/y3m3jGYvgeu2uIHc5C98M3dGA5-pUo_ZgNDCANBWJYEQqZeYdyGFwV1UWOIFrpD56FnUNAJkUJOywicSIG_nBdG6v1RvI3BGGkEBlnLcbH5Kz7QoU5j7gI6vghNkDD3HSTSaNDK2PVMqivI005IRdrqTJfduaImaVy4ZyTn_CaJMNY?width=1000&height=291&cropmode=none! \\\\ \\\\ *Option 3* !https://skeuhq-bn1305.files.1drv.com/y3mwENxSi1zzJ6g0hX9wyZu-7wFj77cz6NWYKuhvFyn67Uo7boeqbqw4YPCP8DW05h8lQAEt4XDyC9c_yNspOkwuPnMqFeK_chXzjZBGVPAD7t1UP5iw7TtGmmMn70H1W7hjR-kByyJHvuA3Y4Gjbm6ZzQv8peMLxvggE6dUSVMIZc?width=1000&height=267&cropmode=none! \\\\ \\\\ *Option 4* !https://ske1hq-bn1305.files.1drv.com/y3msqsFXMBqxWYj8kk_-_ShZb1spGcfIzuYD5ShOT4oQB-EMVE2_18GrQPS8rc8K4Gh4Zo6dP76dGkSfvEj7vNoAOU3FAe3HNcpJxl5MZPA8hOx-1aLus_UtzE62bviKkmp9-NcHY_eVWf4EmKtJ5aMcy5jiDk7tBUXRl27YUSvy8Q?width=1000&height=298&cropmode=none! \\\\ \\\\ *Option 5* !https://r6eshq-bn1305.files.1drv.com/y3myRrrctLCNgHqbkjN85nDVzZUwKsGnN3mFMVlJf3uKnMFtMEzrkR4mI8A2bOZfiF3tPXrYkw5DOcYtfbnbwolbjwGusgc3kjovtmiCR8yYElqj6H3uLzeFSNxSgcAA0mAQLkGJOTH4fR89xCWGUSrRaw9vDToaWIGGaY662nE0MA?width=1000&height=291&cropmode=none! \\\\", "created": "2016-06-07T03:10:40.706+0000"}, {"author": "Thomas Graves", "body": "my vote would be option 4.", "created": "2016-06-07T13:40:46.956+0000"}, {"author": "Junping Du", "body": "+1 on option 4 too.", "created": "2016-06-07T13:45:16.108+0000"}, {"author": "Akira Ajisaka", "body": "+1 on option 4.", "created": "2016-06-07T14:02:09.341+0000"}, {"author": "Karthik Kambatla", "body": "+1 on option 4.", "created": "2016-06-07T14:42:35.236+0000"}, {"author": "Daniel Templeton", "body": "+1 on #4.", "created": "2016-06-07T16:40:46.762+0000"}, {"author": "Bikas Saha", "body": "+1 on 5", "created": "2016-06-07T17:27:14.451+0000"}, {"author": "Christopher Douglas", "body": "+1 on option 1", "created": "2016-06-08T00:26:00.881+0000"}, {"author": "Craig L Russell", "body": "Great idea to add Apache to the logo. While you're at it, how about adding (R) to Hadoop to show that it's a registered trademark?", "created": "2016-06-08T00:56:26.803+0000"}, {"author": "Abhishek", "body": "I'll add that in the final version. Thanks!", "created": "2016-06-08T05:04:34.266+0000"}, {"author": "Xiao Chen", "body": "+1 on option 1", "created": "2016-06-08T05:08:06.392+0000"}, {"author": "Yi Liu", "body": "option 1 is more beautiful, +1.", "created": "2016-06-08T05:13:49.912+0000"}, {"author": "Naganarasimha G R", "body": "+1 on option 1", "created": "2016-06-08T07:08:58.185+0000"}, {"author": "Rohith Sharma K S", "body": "+1 for option 1", "created": "2016-06-08T07:09:20.067+0000"}, {"author": "Varun Saxena", "body": "+1 for option 1", "created": "2016-06-08T07:22:47.036+0000"}, {"author": "Bibin Chundatt", "body": "+1 for option 1", "created": "2016-06-08T07:45:46.865+0000"}, {"author": "Vinayakumar B", "body": "+1 for option 1. Looks beautiful.", "created": "2016-06-08T07:57:36.048+0000"}, {"author": "Kai Zheng", "body": "+1 for option 1. Pretty cool.", "created": "2016-06-08T08:08:39.959+0000"}, {"author": "Steve Loughran", "body": "Option 5", "created": "2016-06-09T19:54:48.352+0000"}, {"author": "Shane Curcuru", "body": "Note that technically HADOOP as a name (i.e. without any reference to fonts, colors, or the like) is registered; the logo as a graphical element is not registered. So legally the logo should have a \u2122, not an \u00ae. If the PMC updates the logo, they could certainly request registering the logo (since this is such a major project), which would let us put the \u00ae there as well.", "created": "2016-06-09T20:49:30.399+0000"}, {"author": "Shane Curcuru", "body": "I love the idea of adding the \"APACHE\" to the logo; obviously decisions here are up to the PMC (so my opinion is non-binding). It's a little odd with the feather, since it's a tilted & mirror image feather, but I understand that's the only way it works with your horizontal logo format. But again; up to the PMC.", "created": "2016-06-09T20:52:57.410+0000"}, {"author": "Christopher Douglas", "body": "Good to know. The yellow elephant is registered, right? Do we need to update that graphic? We should probably generate both \u2122 and \u00ae, while this is active. If/when it's registered, we can swap in the correct logo.", "created": "2016-06-09T21:42:07.488+0000"}, {"author": "Shane Curcuru", "body": "No, the only US registration the ASF holds is on HADOOP. The PMC needs to request registration via tm-registrations@ for me to take action. We don't typically register logos due to the additional cost and budget limitations. However given Hadoop's reach I'd certainly process a registration for the elephant. Note that you register an *exact* graphical logo (modulo any TM/(R), which don't matter for the application). So when you request registration, be sure to include the specific graphic file you want to register (i.e. either just the elephant, or the elephant & word). Separately, does the PMC have records from the original logo author showing they're granting trademark rights to the ASF? We can work on that on private@/tm@.", "created": "2016-06-10T12:31:26.467+0000"}, {"author": "Sangjin Lee", "body": "Belated +1 for option 4.", "created": "2016-06-24T16:46:46.434+0000"}, {"author": "Andrew Wang", "body": "I have the same feedback as Shane about the feather. +1 for option 4.", "created": "2016-06-29T00:02:33.888+0000"}, {"author": "Akira Ajisaka", "body": "When the vote ends?", "created": "2016-07-08T22:08:31.685+0000"}, {"author": "Christopher Douglas", "body": "bq. When the vote ends? I'd hoped to avoid a vote on JIRA, but here we are. I count 9 votes for #1, 7 votes for #4, 2 for #5. [~kspk], could you create the final version of #1? [~curcuru], just to be clear, should the logo have a TM or an (R)?", "created": "2016-07-19T01:08:15.867+0000"}, {"author": "Shane Curcuru", "body": "The logo should have a TM symbol; we do not currently hold a registration on the logo itself. The mere use of the stylized HADOOP word within the logo does not mean the logo should be an (R), even though HADOOP as a word is registered in several countries. Trademark law generally treats words separately from obviously stylized or graphical displays of words, so while the word (in any normal typeface) is registered, the logo depicting it is not. Also, trademark law is often very specific about exact logos, or officially recognizing part of a logo vs. the entirety of the design. Thus if the PMC wants to request registration of a new logo, we must provide the *exact* logo image we want to register. While there are some trademark protections for similar designs, the strongest legal protection (and ability to use (R)) only extends to the same or virtually the same graphical image (although I'm pretty sure we could (R) a black & white version of the same image, if we registered the normal color version).", "created": "2016-08-12T13:25:40.135+0000"}, {"author": "Abhishek", "body": "Uploading all files for the new logo. Various sizes and formats. List of files: * Original PSD * PDF generated with Adobe's Color profile. * TIFF version of original file * PNGs with white background and transparent background (multiple sizes). All logos contain the (TM) symbol. I can generate more sizes and formats if needed. Thanks, Abhishek", "created": "2016-09-15T16:19:06.570+0000"}, {"author": "Christopher Douglas", "body": "I committed this. Thanks Abhishek!", "created": "2016-09-26T07:02:09.815+0000"}, {"author": "Steve Loughran", "body": "I want to roll back to the older version without the feather. Does anyone have any ideas about how best to do this?", "created": "2025-10-23T17:04:47.461+0000"}], "derived_tasks": {"summary": "Add \"Apache\" to Hadoop project logo - Many ASF projects include \"Apache\" in their logo", "classifications": ["feature", "task"], "qa_pairs": []}}
{"id": "HADOOP-12677", "title": "DecompressorStream throws IndexOutOfBoundsException when calling skip(long)", "description": "DecompressorStream.skip(long) throws an IndexOutOfBoundException when using a long bigger than Integer.MAX_VALUE This is because of this cast from long to int: https://github.com/apache/hadoop-common/blob/HADOOP-3628/src/core/org/apache/hadoop/io/compress/DecompressorStream.java#L125 The fix is probably to do the cast after applying Math.min: in that case, it should not be an issue since it should not be bigger than the buffer size (512)", "status": "Patch Available", "priority": "Major", "reporter": "Laurent Goujon", "assignee": "Wei-Chiu Chuang", "created": "2015-12-24T02:38:03.000+0000", "updated": "2025-10-19T00:24:48.000+0000", "labels": ["pull-request-available"], "components": ["io"], "comments": [{"author": "Wei-Chiu Chuang", "body": "Thanks [~laurentgo] for reporting this issue! I was able to reproduce the exception and then created a patch and a regression test case. Attaching my rev01 patch: * In DecompressorStream.skip(), declare both skipped and len as long to avoid overflowing the number. * Create a test case in TestBlockDecompressorStream. BlockDecompressorStream inherits DecompressorStream, and DecompressorStream does not have its own test file.", "created": "2015-12-24T06:26:25.989+0000"}, {"author": "Hadoop QA", "body": "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} | | {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} | | {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} | | {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 39s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 7m 57s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 37s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 3s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 46s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 50s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 1s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 45s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 7m 48s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} javac {color} | {color:green} 7m 48s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 34s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} javac {color} | {color:green} 8m 34s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 0s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} | | {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 54s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 51s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 1s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} | | {color:red}-1{color} | {color:red} unit {color} | {color:red} 6m 12s {color} | {color:red} hadoop-common in the patch failed with JDK v1.8.0_66. {color} | | {color:red}-1{color} | {color:red} unit {color} | {color:red} 6m 25s {color} | {color:red} hadoop-common in the patch failed with JDK v1.7.0_91. {color} | | {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 23s {color} | {color:red} Patch generated 1 ASF License warnings. {color} | | {color:black}{color} | {color:black} {color} | {color:black} 66m 49s {color} | {color:black} {color} | \\\\ \\\\ || Reason || Tests || | JDK v1.8.0_66 Failed junit tests | hadoop.test.TestTimedOutTestsListener | | JDK v1.7.0_91 Failed junit tests | hadoop.security.TestShellBasedIdMapping | \\\\ \\\\ || Subsystem || Report/Notes || | Docker | Image:yetus/hadoop:0ca8df7 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12779391/HADOOP-12677.001.patch | | JIRA Issue | HADOOP-12677 | | Optional Tests | asflicense compile javac javadoc mvninstall mvnsite unit findbugs checkstyle | | uname | Linux 809b3b049d92 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh | | git revision | trunk / a308e86 | | Default Java | 1.7.0_91 | | Multi-JDK versions | /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 | | findbugs | v3.0.0 | | unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_66.txt | | unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_91.txt | | unit test logs | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_91.txt | | JDK v1.7.0_91 Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/testReport/ | | asflicense | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/artifact/patchprocess/patch-asflicense-problems.txt | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Max memory used | 75MB | | Powered by | Apache Yetus 0.2.0-SNAPSHOT http://yetus.apache.org | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/8310/console | This message was automatically generated.", "created": "2015-12-24T07:35:34.053+0000"}, {"author": "Wei-Chiu Chuang", "body": "Test failures are unrelated. ASF license warning is false positive.", "created": "2015-12-24T07:57:50.349+0000"}, {"author": "Laurent Goujon", "body": "The unit test is incorrect. According to http://docs.oracle.com/javase/7/docs/api/java/io/InputStream.html#skip%28long%29, it doesn't throw EOFException and should return how many bytes were skipped. So the catch clause should be removed, and instead the return value should be checked (it should be 4). It looks also that the following check  if (bufLen > 0) is not needed since you set bufLen to 4 just before. Regarding the fix, it looks correct. Style-wise, some parenthesis and casts are probably not needed, and I believe it could be on a single line:  // safe to cast to int as between between 0 and skipBytes.length int len = (int) Math.min(n - skipped, skipBytes.length);", "created": "2015-12-24T15:29:59.187+0000"}, {"author": "Wei-Chiu Chuang", "body": "[~laurentgo] Thank you for the comments and reviews! I have posted patch #2 to address comment#2. Regarding comment#1, even though DecompressorStream inherits InputStream, its skip() implementation does not call InputStream.skip(). Instead, its internal implementation uses InputStream.read(), which returns -1 if end of stream. If InputStream.end() returns -1, an EOFException is thrown.", "created": "2015-12-28T15:57:11.213+0000"}, {"author": "Laurent Goujon", "body": "It's not about invoking InputStream.skip() but following the same behavior, as part of the API contract. Before your patch, DecompressorStream would also return the number of bytes skipped until EOF, changing it would mean introducing an incompatible change.", "created": "2015-12-28T16:15:08.637+0000"}, {"author": "Hadoop QA", "body": "| (/) *{color:green}+1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} | | {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} | | {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} | | {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 50s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 41s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 9m 20s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 5s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 54s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 57s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 7s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 38s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 37s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} javac {color} | {color:green} 8m 37s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 9m 18s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} javac {color} | {color:green} 9m 18s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 5s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} | | {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 2s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 55s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 6s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} | | {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 15s {color} | {color:green} hadoop-common in the patch passed with JDK v1.8.0_66. {color} | | {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 14s {color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_91. {color} | | {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} Patch does not generate ASF License warnings. {color} | | {color:black}{color} | {color:black} {color} | {color:black} 74m 47s {color} | {color:black} {color} | \\\\ \\\\ || Subsystem || Report/Notes || | Docker | Image:yetus/hadoop:0ca8df7 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12779660/HADOOP-12677.002.patch | | JIRA Issue | HADOOP-12677 | | Optional Tests | asflicense compile javac javadoc mvninstall mvnsite unit findbugs checkstyle | | uname | Linux 734aa7180e1a 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh | | git revision | trunk / a0249da | | Default Java | 1.7.0_91 | | Multi-JDK versions | /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 | | findbugs | v3.0.0 | | JDK v1.7.0_91 Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/8317/testReport/ | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Max memory used | 76MB | | Powered by | Apache Yetus 0.2.0-SNAPSHOT http://yetus.apache.org | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/8317/console | This message was automatically generated.", "created": "2015-12-28T17:23:41.108+0000"}, {"author": "Xiang Li", "body": "We exposed this problem when trying to read Spark executor log when it is greater than 2G (2^31 - 1, as Integer.MAX_VALUE) and would like to try the patch on our test environment. Thanks for the fix [~weichiu]!", "created": "2023-07-18T15:00:57.882+0000"}, {"author": "ASF GitHub Bot", "body": "jojochuang opened a new pull request, #5886: URL: https://github.com/apache/hadoop/pull/5886 ### Description of PR Cast DecompressorStream.skip() properly. ### How was this patch tested? Unit test ### For code changes: - [ X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "created": "2023-07-24T22:10:04.844+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] commented on PR #5886: URL: https://github.com/apache/hadoop/pull/5886#issuecomment-3413330729 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution.", "created": "2025-10-17T00:22:37.872+0000"}, {"author": "ASF GitHub Bot", "body": "github-actions[bot] closed pull request #5886: HADOOP-12677. DecompressorStream throws IndexOutOfBoundsException when calling skip(long) URL: https://github.com/apache/hadoop/pull/5886", "created": "2025-10-19T00:24:48.661+0000"}], "derived_tasks": {"summary": "DecompressorStream throws IndexOutOfBoundsException when calling skip(long) - DecompressorStream", "classifications": ["bug"], "qa_pairs": []}}
