{"id": "SPARK-54031", "title": "Add golden files for Analyzer edge cases", "description": "New golden files for edge-cases discovered during the Analyzer support and development.", "status": "Open", "priority": "Major", "reporter": "Vladimir Golubev", "assignee": null, "created": "2025-10-25T21:20:48.000+0000", "updated": "2025-10-25T21:37:53.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Add golden files for Analyzer edge cases - New golden files for edge-cases discovered during the Analyzer support and development", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "SPARK-54030", "title": "Add detailed error message for corrupted view metadata with mismatched column counts", "description": "Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch.", "status": "Open", "priority": "Major", "reporter": "Ganesha S", "assignee": null, "created": "2025-10-25T16:11:19.000+0000", "updated": "2025-10-25T16:16:29.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Add detailed error message for corrupted view metadata with mismatched column counts - Enhance error reporting for corrupted view metadata by addin...", "classifications": ["feature", "task", "bug"], "qa_pairs": []}}
{"id": "SPARK-54029", "title": "Display a detailed error message when table metadata is corrupted", "description": "Currently, no meaningful error message appears when the table metadata is corrupted due to a mismatch between the partition column names in the table schema and the declared partition columns. To facilitate debugging, a detailed error message should be shown in this situation.", "status": "Open", "priority": "Major", "reporter": "Ganesha S", "assignee": null, "created": "2025-10-25T14:57:47.000+0000", "updated": "2025-10-25T15:04:25.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Display a detailed error message when table metadata is corrupted - Currently, no meaningful error message appears when the table metadata is corru...", "classifications": ["task", "bug"], "qa_pairs": []}}
{"id": "SPARK-54028", "title": "Use empty schema when altering a view which is not Hive compatible", "description": "Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails. However, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation.", "status": "Open", "priority": "Major", "reporter": "Chiran Ravani", "assignee": null, "created": "2025-10-25T12:47:26.000+0000", "updated": "2025-10-25T12:57:26.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Chiran Ravani", "body": "I am working on a Fix, Can I assign to myself?", "created": "2025-10-25T12:48:25.032+0000"}], "derived_tasks": {"summary": "Use empty schema when altering a view which is not Hive compatible - Spark attempts to save views in a Hive-compatible format and only sets the sch...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-54027", "title": "Kafka Source RTM support", "description": "", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "created": "2025-10-25T00:06:42.000+0000", "updated": "2025-10-25T00:16:00.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [], "derived_tasks": {"summary": "Kafka Source RTM support", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54026", "title": "Use BOM for AWS Java SDK V2 dependency management", "description": "", "status": "Open", "priority": "Major", "reporter": "Vlad Rozov", "assignee": null, "created": "2025-10-24T23:39:36.000+0000", "updated": "2025-10-25T00:06:53.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [], "derived_tasks": {"summary": "Use BOM for AWS Java SDK V2 dependency management", "classifications": ["task"], "qa_pairs": []}}
{"id": "SPARK-54025", "title": "Support recaching when a table is written via a different table implementation (V1 or V2)", "description": "When a table is cached using one table implementation (e.g., V2) and written through the other (e.g., V1), Spark may not automatically trigger recaching. As a result, the cached data can become stale even though the underlying table content has changed. This issue arises because the current recaching mechanism does not consistently handle cross-implementation writes. Given that the community is actively working on Data Source V2 (DSV2), many data sources are expected to have both V1 and V2 implementations for a period of time, making this issue more likely to occur in practice. *Proposed Fix:* Enhance the cache invalidation logic to detect writes that occur through a different table implementation (V1 \u2194 V2) and trigger recaching accordingly. {*}Expected Outcome:{*}{*}{*} * Cached data remains up to date when a table is written through either V1 or V2 paths. * Both logical-plan-based and file-path-based recaching continue to work as expected for V1&V2 connectors", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-24T20:25:04.000+0000", "updated": "2025-10-24T20:25:31.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "cc [~vli-databricks]", "created": "2025-10-24T20:25:31.062+0000"}], "derived_tasks": {"summary": "Support recaching when a table is written via a different table implementation (V1 or V2) - When a table is cached using one table implementation (e", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "SPARK-54024", "title": "add sbt-dependency-graph to SBT plugins", "description": "The plugin adds few useful commands to browser dependencies tree in SBT.", "status": "Open", "priority": "Minor", "reporter": "Vlad Rozov", "assignee": null, "created": "2025-10-24T19:52:00.000+0000", "updated": "2025-10-24T20:03:06.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [], "derived_tasks": {"summary": "add sbt-dependency-graph to SBT plugins - The plugin adds few useful commands to browser dependencies tree in SBT", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "SPARK-54023", "title": "Support `AUTO` Netty IO Mode", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-24T18:42:53.000+0000", "updated": "2025-10-26T00:53:30.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52724 [https://github.com/apache/spark/pull/52724]", "created": "2025-10-26T00:53:30.646+0000"}], "derived_tasks": {"summary": "Support `AUTO` Netty IO Mode", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54022", "title": "Make DSv2 table resolution aware of cached tables", "description": "Make DSv2 table resolution aware of cached tables. If the user executes CACHE TABLE t, the subsequent resolution calls must use the cached table metadata, instead of potentially allowing the connectors to refresh and break the cache lookup.", "status": "Open", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": null, "created": "2025-10-24T17:40:06.000+0000", "updated": "2025-10-24T17:40:06.000+0000", "labels": [], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Make DSv2 table resolution aware of cached tables", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-54021", "title": "Implement Geography and Geometry accessors across Catalyst", "description": "", "status": "Open", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": null, "created": "2025-10-24T17:09:51.000+0000", "updated": "2025-10-24T23:29:44.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Uro\u0161 Bojani\u0107", "body": "Work in progress: https://github.com/apache/spark/pull/52723.", "created": "2025-10-24T17:10:21.354+0000"}], "derived_tasks": {"summary": "Implement Geography and Geometry accessors across Catalyst", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-54020", "title": "Support spark.sql(\"SELECT ...\") inside Pipelines query functions", "description": "", "status": "Open", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Jessie Luo", "created": "2025-10-24T16:22:23.000+0000", "updated": "2025-10-24T16:22:23.000+0000", "labels": [], "components": ["Declarative Pipelines"], "comments": [], "derived_tasks": {"summary": "Support spark.sql(\"SELECT ...\") inside Pipelines query functions", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-54019", "title": "Prevent spark.sql session state mutation within Python pipeline files", "description": "We should ban all uses of spark.sql except for select statements.", "status": "Open", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Jessie Luo", "created": "2025-10-24T16:19:35.000+0000", "updated": "2025-10-24T16:19:35.000+0000", "labels": [], "components": ["Declarative Pipelines"], "comments": [], "derived_tasks": {"summary": "Prevent spark.sql session state mutation within Python pipeline files - We should ban all uses of spark", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54018", "title": "Upgrade Volcano to 1.13.0", "description": "", "status": "In Progress", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-24T16:06:39.000+0000", "updated": "2025-10-24T16:11:26.000+0000", "labels": ["pull-request-available"], "components": ["Documentation", "k8s", "Project Infra"], "comments": [], "derived_tasks": {"summary": "Upgrade Volcano to 1.13.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54017", "title": "Audit test dependencies in Spark 4.1.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-24T15:35:21.000+0000", "updated": "2025-10-24T15:46:38.000+0000", "labels": [], "components": ["Build", "Tests"], "comments": [], "derived_tasks": {"summary": "Audit test dependencies in Spark 4.1.0", "classifications": ["umbrella"], "qa_pairs": []}}
{"id": "SPARK-54016", "title": "Improve K8s support in Spark 4.1.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-24T15:06:06.000+0000", "updated": "2025-10-24T15:29:24.000+0000", "labels": ["releasenotes"], "components": ["Kubernetes"], "comments": [], "derived_tasks": {"summary": "Improve K8s support in Spark 4.1.0", "classifications": ["umbrella", "improvement"], "qa_pairs": []}}
{"id": "SPARK-54015", "title": "Relex Py4J requirement to 0.10.9.7+", "description": "JVM are compatible with 0.10.9.7+ and above versions have some correctness fixes", "status": "Resolved", "priority": "Major", "reporter": "Hyukjin Kwon", "assignee": "Hyukjin Kwon", "created": "2025-10-24T12:54:21.000+0000", "updated": "2025-10-25T23:56:27.000+0000", "labels": ["pull-request-available"], "components": ["PySpark"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52721 [https://github.com/apache/spark/pull/52721]", "created": "2025-10-24T14:48:37.929+0000"}], "derived_tasks": {"summary": "Relex Py4J requirement to 0.10.9.7+ - JVM are compatible with 0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54014", "title": "Support setMaxRows for SparkConnectStatement", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-10-24T08:51:13.000+0000", "updated": "2025-10-24T08:51:13.000+0000", "labels": [], "components": ["Connect"], "comments": [], "derived_tasks": {"summary": "Support setMaxRows for SparkConnectStatement", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54013", "title": "Implement SparkConnectDatabaseMetaData simple methods", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-10-24T08:48:36.000+0000", "updated": "2025-10-24T08:51:24.000+0000", "labels": [], "components": ["Connect"], "comments": [], "derived_tasks": {"summary": "Implement SparkConnectDatabaseMetaData simple methods", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-54012", "title": "Improve Netty usage patterns", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Kent Yao", "created": "2025-10-24T05:16:15.000+0000", "updated": "2025-10-24T15:02:44.000+0000", "labels": ["releasenotes"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Hi, [~yao]. As a recognition of your contribution, I made this umbrella JIRA issue with `releasenotes` label. Although I have more ideas for this topic and will add some, I believe we are able to mark this as a *resolved* item for Apache Spark 4.1.0 by containing only what we did and what we want to add until the feature freeze. WDYT?", "created": "2025-10-24T05:27:19.110+0000"}, {"author": "Kent Yao", "body": "Thank you for making this umbrella, [~dongjoon]. Making it resolved sounds good to me", "created": "2025-10-24T06:02:54.498+0000"}, {"author": "Dongjoon Hyun", "body": "Thank you, [~yao].", "created": "2025-10-24T15:02:44.121+0000"}], "derived_tasks": {"summary": "Improve Netty usage patterns", "classifications": ["umbrella", "improvement"], "qa_pairs": []}}
{"id": "SPARK-54011", "title": "Switch direct ctors to IoHandlerFactories for EventLoopGroups", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "created": "2025-10-24T03:03:56.000+0000", "updated": "2025-10-24T14:47:07.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52719 [https://github.com/apache/spark/pull/52719]", "created": "2025-10-24T14:47:07.891+0000"}], "derived_tasks": {"summary": "Switch direct ctors to IoHandlerFactories for EventLoopGroups", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54010", "title": "Support restart counter in Spark app", "description": "", "status": "Open", "priority": "Major", "reporter": "Zhou JIANG", "assignee": null, "created": "2025-10-23T23:50:13.000+0000", "updated": "2025-10-24T22:53:06.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [], "derived_tasks": {"summary": "Support restart counter in Spark app", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54009", "title": "Support `spark.io.mode.default`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-23T22:21:19.000+0000", "updated": "2025-10-24T13:45:50.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52717 [https://github.com/apache/spark/pull/52717]", "created": "2025-10-24T01:55:03.315+0000"}], "derived_tasks": {"summary": "Support `spark.io.mode.default`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54008", "title": "Skip query execution for DESCRIBE QUERY", "description": "DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis. Therefore, we can skip the unnecessary execution portion for the command.", "status": "Resolved", "priority": "Major", "reporter": "Gene Pang", "assignee": "Gene Pang", "created": "2025-10-23T20:11:47.000+0000", "updated": "2025-10-24T18:57:40.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "Issue resolved by pull request 52713 [https://github.com/apache/spark/pull/52713]", "created": "2025-10-24T17:38:06.396+0000"}], "derived_tasks": {"summary": "Skip query execution for DESCRIBE QUERY - DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54007", "title": "Use Java `Set.of` instead of `Collections.emptySet`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-23T19:08:20.000+0000", "updated": "2025-10-24T04:59:46.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core", "SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52711 [https://github.com/apache/spark/pull/52711]", "created": "2025-10-24T04:59:46.193+0000"}], "derived_tasks": {"summary": "Use Java `Set.of` instead of `Collections.emptySet`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54006", "title": "WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above", "description": "Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with  WithAggregationKinesisBackedBlockRDDSuite: *** RUN ABORTED ***", "status": "Open", "priority": "Minor", "reporter": "Vlad Rozov", "assignee": null, "created": "2025-10-23T18:45:57.000+0000", "updated": "2025-10-24T15:45:26.000+0000", "labels": ["pull-request-available"], "components": ["Tests"], "comments": [], "derived_tasks": {"summary": "WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above - Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/ki...", "classifications": ["test", "bug"], "qa_pairs": []}}
{"id": "SPARK-54005", "title": "Remove no-op `spark.shuffle.blockTransferService` configuration", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-23T18:14:08.000+0000", "updated": "2025-10-24T05:20:43.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52709 [https://github.com/apache/spark/pull/52709]", "created": "2025-10-24T04:58:54.233+0000"}], "derived_tasks": {"summary": "Remove no-op `spark.shuffle.blockTransferService` configuration", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54004", "title": "Fix uncaching table by name without cascading", "description": "{{cacheManager.uncacheTableOrView(spark, Seq(\"testcat\", \"tbl\"), cascade = false)}} does not find the table reference because it is wrapped into a subquery alias.", "status": "Resolved", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": "Anton Okolnychyi", "created": "2025-10-23T14:45:28.000+0000", "updated": "2025-10-24T14:41:42.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52712 [https://github.com/apache/spark/pull/52712]", "created": "2025-10-24T05:06:15.200+0000"}], "derived_tasks": {"summary": "Fix uncaching table by name without cascading - {{cacheManager", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-54003", "title": "Use the staging directory as the output path then move to final path", "description": "SparkSQL uses the partition location or table location as the commit path (except in *_dynamic partition overwrite_* mode and *_custom partition path_*\u00a0mode). This has at least the following issues: * As described in SPARK-37210, conflicts can occur when multiple partitions job of the same table are run concurrently. Using a staging directory can avoid this issue. * As described in SPARK-53937, using a staging directory allows for near-atomic operations. _*Dynamic partition overwrite*_ mode and *_custom partition path_* mode already use the staging directory. And *_dynamic partition overwrite_* mode and _*custom partition path*_ are implemented differently, which can be further simplified into a unified process. And in https://github.com/apache/spark/pull/29000, reset the staging directory as the output directory of FileOutputCommitter. This way is more safer. It should be modified to this way.", "status": "Open", "priority": "Major", "reporter": "Chenyu Zheng", "assignee": null, "created": "2025-10-23T13:12:48.000+0000", "updated": "2025-10-24T07:18:14.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Use the staging directory as the output path then move to final path - SparkSQL uses the partition location or table location as the commit path (e...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-54002", "title": "Support integrating BeeLine with Connect JDBC driver", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-10-23T12:04:46.000+0000", "updated": "2025-10-24T14:56:51.000+0000", "labels": ["pull-request-available"], "components": ["Deploy"], "comments": [], "derived_tasks": {"summary": "Support integrating BeeLine with Connect JDBC driver", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54001", "title": "Reduce memory footprint from cached local relations upon cloning", "description": "Cloning sessions is a common operation in Spark applications (e.g., for creating isolated execution contexts). The current approach of duplicating cached data can significantly increase memory footprint, especially when: * Sessions are cloned frequently * Cached relations contain large datasets * Multiple clones exist simultaneously An improvement can be made by implementing reference counting as opposed to data replication for the block manager entries that reference cached local relations.", "status": "Resolved", "priority": "Major", "reporter": "Venkata Sai Akhil Gudesa", "assignee": "Pranav Dev", "created": "2025-10-23T09:38:20.000+0000", "updated": "2025-10-24T22:22:34.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "Spark Core"], "comments": [{"author": "Herman van H\u00f6vell", "body": "Issue resolved by pull request 52651 [https://github.com/apache/spark/pull/52651]", "created": "2025-10-23T19:31:52.720+0000"}], "derived_tasks": {"summary": "Reduce memory footprint from cached local relations upon cloning - Cloning sessions is a common operation in Spark applications (e", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-54000", "title": "Complex sql with expand operator and code gen enabled, very slow", "description": "Complex sql with expand operator and code gen enabled, very slow sql format like select keya,keyb,count(distinct case when),...,count(distinct case when),sum(a),sum(b) from x group by keya,keyb when disable whole stage code gen, run will speed up 20x times when add executor jvm parameter -XX:-TieredCompilation, run will speed up 20x times reduce select column count, such as 28 -> 27, can speed up 10x times", "status": "Open", "priority": "Major", "reporter": "lifulong", "assignee": null, "created": "2025-10-23T08:46:01.000+0000", "updated": "2025-10-23T08:51:49.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "lifulong", "body": "!https://wiki.in.zhihu.com/download/attachments/640447372/image2025-10-2_13-32-26.png?version=1&modificationDate=1759383146774&api=v2! from flame graph we can see, most time cost is setNullAt call when enable whole stage code gen and not add -XX:-TieredCompilation jvm parameter", "created": "2025-10-23T08:47:50.193+0000"}], "derived_tasks": {"summary": "Complex sql with expand operator and code gen enabled, very slow sql format like select keya,keyb,count(distinct case when),", "classifications": ["improvement", "performance"], "qa_pairs": []}}
{"id": "SPARK-53999", "title": "Native KQueue Transport support on BSD/MacOS", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "created": "2025-10-23T07:27:17.000+0000", "updated": "2025-10-24T05:21:30.000+0000", "labels": ["pull-request-available"], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52703 [https://github.com/apache/spark/pull/52703]", "created": "2025-10-23T17:37:00.535+0000"}], "derived_tasks": {"summary": "Native KQueue Transport support on BSD/MacOS", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53998", "title": "Add addition E2E tests for RTM", "description": "", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "created": "2025-10-23T04:51:16.000+0000", "updated": "2025-10-24T05:09:52.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [], "derived_tasks": {"summary": "Add addition E2E tests for RTM", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53997", "title": "Add PipelineAnalysisContext message to support pipeline analysis during Spark Connect query execution", "description": "When a user invokes an API like `spark.sql` inside a query function that triggers analysis, we want to include information that allows us to do SDP-specific special handling. Adding a PipelineAnalysisContext will allow us to include information along with related RPCs so that the server knows to do this special handling.", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Jessie Luo", "created": "2025-10-23T03:14:09.000+0000", "updated": "2025-10-23T03:15:02.000+0000", "labels": [], "components": ["Declarative Pipelines"], "comments": [{"author": "Sanford Ryza", "body": "PR: https://github.com/apache/spark/pull/52685", "created": "2025-10-23T03:15:02.845+0000"}], "derived_tasks": {"summary": "Add PipelineAnalysisContext message to support pipeline analysis during Spark Connect query execution - When a user invokes an API like `spark", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53996", "title": "InferFiltersFromConstraints rule does not infer filter conditions for complex join expressions", "description": "The Spark optimizer's {{InferFiltersFromConstraints}} rule is currently unable to infer filter conditions when join constraints involve complex expressions. While it works for simple attribute equalities ({{{}a = b{}}}), it can't infer constraint from anything more complex than that. *Example (works as expected):*  SELECT * FROM t1 JOIN t2 ON t1.a = t2.b WHERE t2.b = 1  In this case, the optimizer correctly infers the additional constraint {{{}t1.a = 1{}}}. *Example (does not work):*  SELECT * FROM t1 JOIN right ON t1.a = t2.b + 2 WHERE t2.b = 1  In this case, it is clear that {{t1.a = 3}} (since {{b = 1}} and {{{}a = b + 2{}}}), but the optimizer does not infer this constraint. *How to Reproduce:*  spark.sql(\"CREATE TABLE t1(a INT)\") spark.sql(\"CREATE TABLE t2(b INT)\") spark.sql(\"\"\" SELECT * FROM t1 INNER JOIN t2 ON t2.b = t1.a + 2 WHERE t1.a = 1 \"\"\").explain   == Physical Plan == AdaptiveSparkPlan +- BroadcastHashJoin [(a#2 + 2)], [b#3], Inner, BuildRight, false :- Filter (isnotnull(a#2) AND (a#2 = 1)) : +- FileScan spark_catalog.default.t1[a#2] +- Filter isnotnull(b#3) +- FileScan spark_catalog.default.t2[b#3]  *Expected Behavior:* The optimizer should be able to statically evaluate and infer that {{t2.b = 3}} given the join condition and the filter on {{{}t1.a{}}}. *Impact:* This limits the optimizer's ability to push down filters and optimize query execution plans for queries with complex join conditions.", "status": "Open", "priority": "Minor", "reporter": "Jaromir Vanek", "assignee": null, "created": "2025-10-23T00:29:21.000+0000", "updated": "2025-10-23T01:14:51.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "InferFiltersFromConstraints rule does not infer filter conditions for complex join expressions - The Spark optimizer's {{InferFiltersFromConstraint...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53995", "title": "Add support to track `spark_conf` at the dataset level", "description": "We need to add {{sql_confs}}\u00a0to\u00a0{{DefineOutput}}\u00a0in SDP to support following use case SQL file #1  SET some_conf = some_value; CREATE STREAMING TABLE st;  SQL file #2 CREATE FLOW INSERT INTO st FROM .... Since we don't store\u00a0{{sql_confs}} at the Dataset level, the conf would be ignored. We should store it at dataset level and will merge the {{sql_conf}} stored at the dataset level to its corresponding flow when we construct the DataflowGraph.", "status": "Open", "priority": "Major", "reporter": "Yuheng Chang", "assignee": null, "created": "2025-10-22T23:30:17.000+0000", "updated": "2025-10-24T05:10:07.000+0000", "labels": [], "components": ["Declarative Pipelines"], "comments": [], "derived_tasks": {"summary": "Add support to track `spark_conf` at the dataset level - We need to add {{sql_confs}}\u00a0to\u00a0{{DefineOutput}}\u00a0in SDP to support following use case SQL ...", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53994", "title": "Exclude `KerberosConfDriverFeatureStep` during benchmarking", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T22:22:00.000+0000", "updated": "2025-10-22T23:16:50.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 404 [https://github.com/apache/spark-kubernetes-operator/pull/404]", "created": "2025-10-22T23:16:50.142+0000"}], "derived_tasks": {"summary": "Exclude `KerberosConfDriverFeatureStep` during benchmarking", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53993", "title": "Support `spark.logConf` configuration", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T22:00:18.000+0000", "updated": "2025-10-22T22:12:10.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 403 [https://github.com/apache/spark-kubernetes-operator/pull/403]", "created": "2025-10-22T22:12:10.559+0000"}], "derived_tasks": {"summary": "Support `spark.logConf` configuration", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53992", "title": "Add `SparkOperatorConfManager.getAll` method", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T21:15:34.000+0000", "updated": "2025-10-22T21:35:31.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 402 [https://github.com/apache/spark-kubernetes-operator/pull/402]", "created": "2025-10-22T21:35:18.981+0000"}], "derived_tasks": {"summary": "Add `SparkOperatorConfManager.getAll` method", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53991", "title": "Add support for KLL quantiles functions based on DataSketches", "description": "Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html]. DataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] Reference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]", "status": "In Progress", "priority": "Major", "reporter": "Daniel Tenedorio", "assignee": "Daniel Tenedorio", "created": "2025-10-22T20:42:35.000+0000", "updated": "2025-10-24T05:10:22.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "Daniel Tenedorio", "body": "We can use the following function names. For each category, we can support long integer, single-precision floating-point, and double-precision floating-point variants (it is necessary to specify which one we are using since the representation of the sketch buffers is different for different input value data types). Aggregate functions to consume input values and return a sketch buffer: {{kll_sketch_agg_bigint(col)}} {{kll_sketch_agg_float(col)}} {{kll_sketch_agg_double(col)}} Scalar functions to merge two sketch buffers together into another sketch buffer: {{kll_sketch_merge_bigint(sketch1, sketch2)}} {{kll_sketch_merge_float(sketch1, sketch2)}} {{kll_sketch_merge_double(sketch1, sketch2)}} Scalar functions to extract a single value from the quantiles sketch representing the desired quantile given the input rank (for example, \"float median = sketch.getQuantile(0.5)\"). We can also implement the functions in this category to also support accepting an array of input ranks and return an array of result quantiles. {{kll_sketch_get_quantile_bigint(sketch, rank)}} {{kll_sketch_get_quantile_float(sketch, rank)}} {{kll_sketch_get_quantile_double(sketch, rank)}} Scalar functions to extract a single value from the quantiles sketch representing the desired rank given the input quantile (for example, \"double rankOf1000 = sketch.getRank(1000)\"). We can also implement the functions in this category to also support accepting an array of input quantiles and return an array of result ranks. {{kll_sketch_get_rank_bigint(sketch, quantile)}} {{kll_sketch_get_rank_float(sketch, quantile)}} {{kll_sketch_get_rank_double(sketch, quantile)}} Optional, scalar functions to return a string representation of a sketch buffer: {{kll_sketch_to_string_bigint(sketch)}} {{kll_sketch_to_string_float(sketch)}} {{kll_sketch_to_string_double(sketch)}}", "created": "2025-10-22T21:55:04.517+0000"}], "derived_tasks": {"summary": "Add support for KLL quantiles functions based on DataSketches - Documentation reference: [https://datasketches", "classifications": ["feature", "new feature"], "qa_pairs": []}}
{"id": "SPARK-53990", "title": "Use Java `(Map|Set).of` instead of `Collections.(empty|singleton)(Set|Map)`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T20:36:22.000+0000", "updated": "2025-10-22T21:08:57.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 401 [https://github.com/apache/spark-kubernetes-operator/pull/401]", "created": "2025-10-22T21:08:57.648+0000"}], "derived_tasks": {"summary": "Use Java `(Map|Set).of` instead of `Collections.(empty|singleton)(Set|Map)`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53989", "title": "Optimize `sparkapps.sh` to use `kubectl delete --all`", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T19:31:05.000+0000", "updated": "2025-10-22T20:00:43.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 400 [https://github.com/apache/spark-kubernetes-operator/pull/400]", "created": "2025-10-22T20:00:43.644+0000"}], "derived_tasks": {"summary": "Optimize `sparkapps.sh` to use `kubectl delete --all`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53988", "title": "Upgrade `Spotless` to 8.0.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T18:52:41.000+0000", "updated": "2025-10-22T19:59:52.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 399 [https://github.com/apache/spark-kubernetes-operator/pull/399]", "created": "2025-10-22T19:59:52.492+0000"}], "derived_tasks": {"summary": "Upgrade `Spotless` to 8.0.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53987", "title": "Upgrade `JaCoCo` to 0.8.14 for official Java 25 support", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T18:37:18.000+0000", "updated": "2025-10-22T18:51:17.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 398 [https://github.com/apache/spark-kubernetes-operator/pull/398]", "created": "2025-10-22T18:50:59.258+0000"}], "derived_tasks": {"summary": "Upgrade `JaCoCo` to 0.8.14 for official Java 25 support", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53986", "title": "Reorganize Python streaming TWS test", "description": "Reorganize the Python TWS tests: * moving tws related tests to a new `/streaming` directory * further split the Python TWS tests to smaller ones to speed up the CI", "status": "Resolved", "priority": "Major", "reporter": "Huanli Wang", "assignee": "Huanli Wang", "created": "2025-10-22T18:36:07.000+0000", "updated": "2025-10-24T22:19:51.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark", "Structured Streaming"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52691 [https://github.com/apache/spark/pull/52691]", "created": "2025-10-24T09:21:43.406+0000"}], "derived_tasks": {"summary": "Reorganize Python streaming TWS test - Reorganize the Python TWS tests: * moving tws related tests to a new `/streaming` directory * further split ...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53985", "title": "Suppress `StatusRecorder` warning messages", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T17:53:28.000+0000", "updated": "2025-10-22T18:06:47.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 397 [https://github.com/apache/spark-kubernetes-operator/pull/397]", "created": "2025-10-22T18:06:33.430+0000"}], "derived_tasks": {"summary": "Suppress `StatusRecorder` warning messages", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53984", "title": "Use CRD `v1` instead of `v1beta1` in `benchmark` script", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T17:44:13.000+0000", "updated": "2025-10-22T18:00:26.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 396 [https://github.com/apache/spark-kubernetes-operator/pull/396]", "created": "2025-10-22T18:00:15.672+0000"}], "derived_tasks": {"summary": "Use CRD `v1` instead of `v1beta1` in `benchmark` script", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53983", "title": "Increase `s.k.o.reconciler.foregroundRequestTimeoutSeconds` to `60s`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-22T17:39:42.000+0000", "updated": "2025-10-22T17:59:45.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 395 [https://github.com/apache/spark-kubernetes-operator/pull/395]", "created": "2025-10-22T17:59:32.542+0000"}], "derived_tasks": {"summary": "Increase `s.k.o.reconciler.foregroundRequestTimeoutSeconds` to `60s`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53982", "title": "Spark aggregation is incorrect (floating point error)", "description": "List<Row> data = Arrays.asList( RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95), RowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95) ); StructType schema = DataTypes.createStructType(new StructField[] { DataTypes.createStructField(\"timestamp\", DataTypes.StringType, false), DataTypes.createStructField(\"id\", DataTypes.StringType, false), DataTypes.createStructField(\"value\", DataTypes.DoubleType, false) }); Dataset<Row> df = spark.createDataFrame(data, schema); // Show the input data System.out.println(\"Input data:\"); df.show(); // Perform the aggregation Dataset<Row> result = df.groupBy(\"id\") .agg( avg(\"value\").as(METADATA_COL_METRICVALUE), sum(\"value\").as(METADATA_COL_SUM_VALUE) ); // Show the results System.out.println(\"Aggregation results:\"); result.show(); // Collect the results List<Row> results = result.collectAsList(); // Print the results System.out.println(\"Number of results: \" + results.size()); for (Row row : results) { System.out.println(\"Metric value: \" + row.getDouble(row.fieldIndex(METADATA_COL_METRICVALUE))); System.out.println(\"Sum value: \" + row.getDouble(row.fieldIndex(METADATA_COL_SUM_VALUE))); } // Verify the results assertEquals(1, results.size(), \"Expected 1 aggregated result\"); Row resultRow = results.get(0); doublesumValue = resultRow.getDouble(resultRow.fieldIndex(METADATA_COL_SUM_VALUE)); doubleexpectedSum = 799.6; // 8 * 99.95 System.out.println(\"Expected sum: \" + expectedSum); System.out.println(\"Actual sum: \" + sumValue); System.out.println(\"Difference: \" + Math.abs(expectedSum - sumValue)); // Check if the sum is close to the expected value assertTrue(Math.abs(expectedSum - sumValue) < 0.001, \"Sum value should be close to \" + expectedSum + \" but was \" + sumValue);   Input data: +--------------------+------+-----+ | timestamp| id|value| +--------------------+------+-----+ |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| |2021-01-01T00:00:...|pod123|99.95| +--------------------+------+-----+ Aggregation results: +------+-----------------+-----------------+ | id| metric_value| sum_value| +------+-----------------+-----------------+ |pod123|99.95000000000002|799.6000000000001| +------+-----------------+-----------------+ Number of results: 1 Metric value: 99.95000000000002 Sum value: 799.6000000000001 Expected sum: 799.6 Actual sum: 799.6000000000001 Difference: 1.1368683772161603E-13", "status": "Open", "priority": "Major", "reporter": "Ian Manning", "assignee": null, "created": "2025-10-22T15:33:34.000+0000", "updated": "2025-10-22T15:35:49.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "Ian Manning", "body": "This does work with DecimalType - but I am not sure why it should not work with DoubleType given the level of precision of 2 digits.", "created": "2025-10-22T15:35:49.689+0000"}], "derived_tasks": {"summary": "Spark aggregation is incorrect (floating point error) - List<Row> data = Arrays", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53914", "title": "Add Connect JDBC module", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-15T04:52:40.000+0000", "updated": "2025-10-24T02:33:05.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 52619 [https://github.com/apache/spark/pull/52619]", "created": "2025-10-21T06:21:30.496+0000"}], "derived_tasks": {"summary": "Add Connect JDBC module", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53913", "title": "Document newly added K8s configurations", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-15T03:33:44.000+0000", "updated": "2025-10-24T15:18:17.000+0000", "labels": ["pull-request-available"], "components": ["Documentation"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52618 [https://github.com/apache/spark/pull/52618]", "created": "2025-10-15T03:59:37.302+0000"}], "derived_tasks": {"summary": "Document newly added K8s configurations", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53907", "title": "Support `spark.kubernetes.allocation.maximum`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-14T20:52:12.000+0000", "updated": "2025-10-24T15:18:05.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52615 [https://github.com/apache/spark/pull/52615]", "created": "2025-10-15T00:19:29.875+0000"}], "derived_tasks": {"summary": "Support `spark.kubernetes.allocation.maximum`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53906", "title": "Protect `ExecutorPodsAllocator.numOutstandingPods` as `protected val`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-14T19:04:03.000+0000", "updated": "2025-10-24T15:17:54.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52614 [https://github.com/apache/spark/pull/52614]", "created": "2025-10-14T20:05:49.503+0000"}], "derived_tasks": {"summary": "Protect `ExecutorPodsAllocator.numOutstandingPods` as `protected val`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53901", "title": "Address memory leak in SparkConnectAddArtifactsHandler", "description": "[stagedArtifacts|https://github.com/apache/spark/blob/master/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L48-L49] buffer is never cleared when artifacts are flushed/on error", "status": "Open", "priority": "Major", "reporter": "Venkata Sai Akhil Gudesa", "assignee": null, "created": "2025-10-14T13:31:12.000+0000", "updated": "2025-10-24T05:13:24.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [], "derived_tasks": {"summary": "Address memory leak in SparkConnectAddArtifactsHandler - [stagedArtifacts|https://github", "classifications": ["feature", "bug"], "qa_pairs": []}}
{"id": "SPARK-53900", "title": "Thread.wait(0) unintentionally called under rare conditions in ExecuteGrpcResponseSender", "description": "A bug in ExecuteGrpcResponseSender causes RPC streams to hang indefinitely when the configured deadline passes. The bug was introduced in [[PR|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]\u00a0during migration from System.currentTimeMillis() to System.nanoTime(), where an integer division error converts sub-millisecond timeout values to 0, triggering Java's wait(0) behavior (infinite wait). h2. Root Cause executionObserver.responseLock.wait(timeoutNs / NANOS_PER_MILLIS) \u00a0// \u2190 BUG {*}The Problem{*}: When deadlineTimeNs < System.nanoTime() (deadline has passed): # Math.max(1, negative_value) clamps to 1 nanosecond # Math.min(progressInterval_ns, 1) remains 1 nanosecond # Integer division: 1 / 1,000,000 = 0 milliseconds # wait(0) in Java means *wait indefinitely until notified* # No notification arrives (execution already completed), thread hangs forever While one the loop conditions\u00a0guards against deadlineTimeNs < System.nanoTime(), it isn\u2019t sufficient as the deadline can elapse while inside the loop (the time is freshly fetched in the latter timeout calculation). The probability of occurence can exacerbated by GC pauses h2. Conditions Required for Bug to Trigger The bug manifests when *all* of the following conditions are met: # *Reattachable execution enabled* (CONNECT_EXECUTE_REATTACHABLE_ENABLED = true) # *Execution completes prior* to the deadline within the inner loop # (all responses sent before deadline) # *Deadline passes* within the inner loop h2. Proposed fix Have timeoutNs always contain a positive value. executionObserver.responseLock.wait(Math.max(1, timeoutNs / NANOS_PER_MILLIS))", "status": "Resolved", "priority": "Major", "reporter": "Venkata Sai Akhil Gudesa", "assignee": "Venkata Sai Akhil Gudesa", "created": "2025-10-14T12:42:32.000+0000", "updated": "2025-10-24T05:13:24.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [{"author": "Herman van H\u00f6vell", "body": "Issue resolved by pull request 52609 [https://github.com/apache/spark/pull/52609]", "created": "2025-10-14T16:05:14.475+0000"}], "derived_tasks": {"summary": "Thread.wait(0) unintentionally called under rare conditions in ExecuteGrpcResponseSender - A bug in ExecuteGrpcResponseSender causes RPC streams to...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53898", "title": "MapOutputTrackerMaster.shufflestatuses is mistakenly cleaned by Shuffle cleanup", "description": "MapOutputTrackerMaster.shufflestatuses can be mistakenly cleaned by Shuffle Cleanup feature, leading to SparkException (crashing the SparkContext) by the subsequent access to that already removed shuffle metadata. A real case (limited to local cluster currently) is the ongoing subquery could access the shuffle metadata which has been already cleanedup after the main query completes. See the detailed discussion at: [https://github.com/apache/spark/pull/52213#discussion_r2415632474].", "status": "Open", "priority": "Major", "reporter": "wuyi", "assignee": null, "created": "2025-10-14T03:48:40.000+0000", "updated": "2025-10-22T13:45:19.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [], "derived_tasks": {"summary": "MapOutputTrackerMaster.shufflestatuses is mistakenly cleaned by Shuffle cleanup - MapOutputTrackerMaster", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53894", "title": "Upgrade `docker-java` to 3.6.0", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-14T02:22:37.000+0000", "updated": "2025-10-24T15:42:48.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52601 [https://github.com/apache/spark/pull/52601]", "created": "2025-10-14T04:52:05.824+0000"}], "derived_tasks": {"summary": "Upgrade `docker-java` to 3.6.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53891", "title": "Model DSV2 Commit Operation Metrics API", "description": "SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of string, long.\u00a0 It would be cleaner to model it as a proper object so that it is more clear what metrics Spark sends, and to handle future cases where metrics may not be long values. Suggestion from [~aokolnychyi]", "status": "Resolved", "priority": "Minor", "reporter": "Szehon Ho", "assignee": "Szehon Ho", "created": "2025-10-13T18:57:39.000+0000", "updated": "2025-10-23T23:44:06.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52595 [https://github.com/apache/spark/pull/52595]", "created": "2025-10-23T23:43:51.362+0000"}], "derived_tasks": {"summary": "Model DSV2 Commit Operation Metrics API - SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of str...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53889", "title": "Introduce APPROX_PERCENTILE_COMBINE", "description": "*APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])*\u00a0: Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-13T17:43:41.000+0000", "updated": "2025-10-24T05:13:00.000+0000", "labels": [], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Introduce APPROX_PERCENTILE_COMBINE - *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])*\u00a0: Merges two intermediate sketch states to enable distri...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53888", "title": "Introduce APPROX_PERCENTILE_ESTIMATE", "description": "*APPROX_PERCENTILE_ESTIMATE(state\u00a0[, k]\u00a0])*\u00a0: Returns the approximate percentile value(s) from a previously accumulated sketch state.", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-13T17:43:22.000+0000", "updated": "2025-10-24T05:13:00.000+0000", "labels": [], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Introduce APPROX_PERCENTILE_ESTIMATE - *APPROX_PERCENTILE_ESTIMATE(state\u00a0[, k]\u00a0])*\u00a0: Returns the approximate percentile value(s) from a previously ...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53887", "title": "Introduce APPROX_PERCENTILE_ACCUMULATE", "description": "*APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):*\u00a0Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-13T17:43:04.000+0000", "updated": "2025-10-24T05:13:00.000+0000", "labels": [], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Introduce APPROX_PERCENTILE_ACCUMULATE - *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):*\u00a0Creates an intermediate state object that increme...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53886", "title": "Percentile estimation functions", "description": "Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples. * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm. * *APPROX_PERCENTILE_ESTIMATE(state [, k]\u00a0])* : Returns the approximate percentile value(s) from a previously accumulated sketch state. * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-13T17:42:15.000+0000", "updated": "2025-10-24T05:13:00.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "cc [~cboumalh] [~yhuang95] \u2014 would you be interested in taking on these new functions?", "created": "2025-10-13T17:45:27.811+0000"}, {"author": "Christopher Boumalhab", "body": "[~Gengliang.Wang] I can take these on.", "created": "2025-10-14T20:09:04.660+0000"}, {"author": "Christopher Boumalhab", "body": "[~Gengliang.Wang] Are we interested in creating something like this [https://github.com/apache/datasketches-hive], but for spark? Basically introducing sketch dependencies through connector repo instead of adding them directly to the spark codebase.", "created": "2025-10-14T20:16:17.422+0000"}, {"author": "Gengliang Wang", "body": "[~cboumalh] Thank for taking this! > Are we interested in creating something like this\u00a0[https://github.com/apache/datasketches-hive], but for spark? Not for now. I think we have already introduced related dependency. This project is to add 3 new functions.", "created": "2025-10-14T22:49:14.969+0000"}], "derived_tasks": {"summary": "Percentile estimation functions - Similar to https://issues", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "SPARK-53885", "title": "Frequency estimation functions", "description": "Introduce the following frequency estimation functions: * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])*\u00a0\u2013 Returns an approximate list of the top _k_ most frequent values in the input expression using a probabilistic algorithm. * *APPROX_TOP_K_ACCUMULATE(expr[, maxItemsTracked])*\u00a0\u2013 Creates a state object that accumulates frequency statistics for use in later estimation or combination. * *APPROX_TOP_K_ESTIMATE(state [, k] ])* \u2013 Extracts and returns the approximate top _k_ values and their estimated frequencies from an accumulated state. * *APPROX_TOP_K_COMBINE(expr[, maxItemsTracked])* \u2013 Merges intermediate APPROX_TOP_K state objects, allowing distributed or parallel computation.", "status": "Resolved", "priority": "Major", "reporter": "Gengliang Wang", "assignee": "Yuchuan Huang", "created": "2025-10-13T17:28:44.000+0000", "updated": "2025-10-24T22:18:14.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "To improve the visibility of this feature, I linked this to SPARK-51166 and marked as `Resolved` according to the subtasks. I believe we can add more subtasks (if needed) until we release Apache Spark 4.1.0 release. Thank you, [~yhuang95] and [~Gengliang.Wang].", "created": "2025-10-24T22:18:14.195+0000"}], "derived_tasks": {"summary": "Frequency estimation functions - Introduce the following frequency estimation functions: * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])*\u00a0\u2013 Returns a...", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "SPARK-53881", "title": "Upgrade `Selenium` to 4.32.0", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-12T18:45:03.000+0000", "updated": "2025-10-24T15:42:39.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52579 [https://github.com/apache/spark/pull/52579]", "created": "2025-10-12T23:55:58.003+0000"}], "derived_tasks": {"summary": "Upgrade `Selenium` to 4.32.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53880", "title": "Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants", "description": "This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.", "status": "Resolved", "priority": "Major", "reporter": "L. C. Hsieh", "assignee": "L. C. Hsieh", "created": "2025-10-12T18:22:52.000+0000", "updated": "2025-10-25T04:54:32.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52578 [https://github.com/apache/spark/pull/52578]", "created": "2025-10-23T23:46:30.448+0000"}, {"author": "L. C. Hsieh", "body": "[~dongjoon] Thank you for updating this ticket.", "created": "2025-10-24T16:49:50.349+0000"}, {"author": "Dongjoon Hyun", "body": "Thank YOU for the fix. :)", "created": "2025-10-25T04:54:32.422+0000"}], "derived_tasks": {"summary": "Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants - This goes to add DSv2 support to the optimization rule PushVariantIntoScan", "classifications": ["feature", "bug", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53873", "title": "ExplodeBase.eval Iterate directly on input", "description": "It was noticed that `ExplodeBase.eval` returns an IterableOnce[InternalRow].\u00a0 The current implementation creates a pre-allocated array, populates the array appropriately, and returns the Array.\u00a0 This works as the is an implicit conversion from Array to IterableOnce. However Allocating and populating an array does not seem to provide benefits over exposing an iterator over the input data type. A proposed PR removes the creation and population of the array and instead returns a IterableOnce object that iterates over the underlying input.", "status": "Open", "priority": "Trivial", "reporter": "Andy Sautins", "assignee": null, "created": "2025-10-10T21:52:23.000+0000", "updated": "2025-10-24T05:13:00.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "ExplodeBase.eval Iterate directly on input - It was noticed that `ExplodeBase", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53860", "title": "Upgrade `sbt-jupiter-interface` to 0.17.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-09T23:01:57.000+0000", "updated": "2025-10-24T15:42:29.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52562 [https://github.com/apache/spark/pull/52562]", "created": "2025-10-10T03:20:23.749+0000"}], "derived_tasks": {"summary": "Upgrade `sbt-jupiter-interface` to 0.17.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53859", "title": "Upgrade `JUnit` to 6.0.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-09T22:39:09.000+0000", "updated": "2025-10-24T15:36:25.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52561 [https://github.com/apache/spark/pull/52561]", "created": "2025-10-10T05:10:41.222+0000"}], "derived_tasks": {"summary": "Upgrade `JUnit` to 6.0.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53849", "title": "Upgrade Netty to 4.2.6.Final", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-08T22:21:01.000+0000", "updated": "2025-10-24T05:16:55.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52552 [https://github.com/apache/spark/pull/52552]", "created": "2025-10-09T06:57:59.729+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.2.6.Final", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53847", "title": "Add ContinuousMemorySink for RTM testing", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-10-08T20:43:28.000+0000", "updated": "2025-10-24T05:10:48.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "L. C. Hsieh", "body": "Issue resolved by pull request 52550 [https://github.com/apache/spark/pull/52550]", "created": "2025-10-12T21:00:03.402+0000"}], "derived_tasks": {"summary": "Add ContinuousMemorySink for RTM testing", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53843", "title": "Upgrade `netty-tcnative` to 2.0.74.Final", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-08T17:19:17.000+0000", "updated": "2025-10-24T05:19:37.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52547 [https://github.com/apache/spark/pull/52547]", "created": "2025-10-08T20:02:27.442+0000"}], "derived_tasks": {"summary": "Upgrade `netty-tcnative` to 2.0.74.Final", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53832", "title": "Make `KubernetesClientUtils` Java-friendly", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-08T04:04:11.000+0000", "updated": "2025-10-24T15:17:43.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52542 [https://github.com/apache/spark/pull/52542]", "created": "2025-10-08T07:09:07.333+0000"}], "derived_tasks": {"summary": "Make `KubernetesClientUtils` Java-friendly", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53823", "title": "Allow list for operators", "description": "", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "created": "2025-10-07T19:19:43.000+0000", "updated": "2025-10-24T05:11:13.000+0000", "labels": [], "components": ["Structured Streaming"], "comments": [], "derived_tasks": {"summary": "Allow list for operators", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53805", "title": "Push Variant into DSv2 scan", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Huaxin Gao", "assignee": "Huaxin Gao", "created": "2025-10-06T03:35:53.000+0000", "updated": "2025-10-24T16:19:21.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Push Variant into DSv2 scan", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53803", "title": "Add ArimaRegression for time series forecasting in MLlib", "description": "The new components will implement the ARIMA (AutoRegressive Integrated Moving Average) algorithm for univariate time series forecasting within the Spark ML pipeline API. This work will include: - Implementation of ARIMA estimator with parameters (p, d, q) - A fitted model `ArimaRegressionModel` for prediction - Parameter support for (p, d, q) accessible from Scala and Python APIs - PySpark bindings under `pyspark.ml.regression` - Unit tests in Scala and Python for fit/transform, persistence, and predict - An example usage added to `examples/ml/ArimaRegressionExample.scala`", "status": "Open", "priority": "Major", "reporter": "Anand", "assignee": null, "created": "2025-10-05T08:45:08.000+0000", "updated": "2025-10-21T02:31:25.000+0000", "labels": ["pull-request-available"], "components": ["ML", "MLlib", "PySpark"], "comments": [], "derived_tasks": {"summary": "Add ArimaRegression for time series forecasting in MLlib - The new components will implement the ARIMA (AutoRegressive Integrated Moving Average) a...", "classifications": ["feature", "new feature"], "qa_pairs": []}}
{"id": "SPARK-53785", "title": "Memory Source for RTM", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-10-02T05:59:55.000+0000", "updated": "2025-10-24T05:11:23.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "L. C. Hsieh", "body": "Issue resolved by pull request 52502 [https://github.com/apache/spark/pull/52502]", "created": "2025-10-17T15:46:29.950+0000"}], "derived_tasks": {"summary": "Memory Source for RTM", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53784", "title": "Additional Source APIs needed to support RTM execution", "description": "Currently in Structured Streaming, start and end offsets are determined at the driver prior to running the micro-batch. \u00a0In real-time mode, end offsets are not known apriori. They are communicated to the driver later by the executors at the end of microbatch that runs for a fixed amount of time.\u00a0 Thus, we need to add additional APIs in the source to support this kind of behavior. The lifecycle of the new API is the following. Driver side: # prepareForRealTimeMode ** Called during logical planning to inform the source if it's in real time mode # planInputPartitions ** The driver plans partitions via planPartitions but only a starting offset is provided (Compared to existing execution modes that require planPartitions to provide both a starting and end offset) # mergeOffsets ** Merge partitioned offsets coming from partitions/tasks to a single global offset. Task side: # nextWithTimeout ** Alternative function to be called than next(), that proceed to the next record. The different from next() is that, if there is no more records, the call needs to keep waiting until the timeout # getOffset ** Get the offset of the next record, or the start offset if no records have been read. The execution engine will call this method along with get() to keep track of the current offset. When a task ends, the offset in each partition will be passed back to the driver. They will be used as the start offsets of the next batch.", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-10-02T04:57:51.000+0000", "updated": "2025-10-24T05:11:31.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "L. C. Hsieh", "body": "Issue resolved by pull request 52501 [https://github.com/apache/spark/pull/52501]", "created": "2025-10-07T16:43:00.996+0000"}], "derived_tasks": {"summary": "Additional Source APIs needed to support RTM execution - Currently in Structured Streaming, start and end offsets are determined at the driver prio...", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53757", "title": "Upgrade Jetty to 12.0.12", "description": "Jetty-http 11.0.25 contains CVE-2024-6763", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "created": "2025-09-30T09:16:32.000+0000", "updated": "2025-10-21T13:14:59.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [], "derived_tasks": {"summary": "Upgrade Jetty to 12.0.12 - Jetty-http 11", "classifications": ["dependency upgrade"], "qa_pairs": []}}
{"id": "SPARK-53755", "title": "Logging support by blockmanager", "description": "Add logging support by blockmanager for collecting/storing python worker logs.", "status": "Resolved", "priority": "Major", "reporter": "Tengfei Huang", "assignee": "Tengfei Huang", "created": "2025-09-30T03:06:11.000+0000", "updated": "2025-10-21T00:54:40.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Takuya Ueshin", "body": "Issue resolved by pull request 52643 [https://github.com/apache/spark/pull/52643]", "created": "2025-10-21T00:54:40.382+0000"}], "derived_tasks": {"summary": "Logging support by blockmanager - Add logging support by blockmanager for collecting/storing python worker logs", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53752", "title": "Publish Apache Spark 4.1.0-preview2 to docker registry", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-29T20:36:28.000+0000", "updated": "2025-10-24T15:17:31.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [], "derived_tasks": {"summary": "Publish Apache Spark 4.1.0-preview2 to docker registry", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53738", "title": "PlannedWrite should preserve custom sort order when query output contains literal", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-09-27T01:43:54.000+0000", "updated": "2025-10-22T17:36:53.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Peter Toth", "body": "Issue resolved by pull request 52474 [https://github.com/apache/spark/pull/52474]", "created": "2025-10-08T12:16:43.986+0000"}, {"author": "Peter Toth", "body": "[https://github.com/apache/spark/pull/52474] was reverted from `master` in [https://github.com/apache/spark/commit/ce3437af0179db986d57b464e69b845337f469ac] due to an alternative fix proposed. Please find details here: [https://github.com/apache/spark/pull/52474#issuecomment-3384559926].", "created": "2025-10-09T08:10:36.264+0000"}, {"author": "Peter Toth", "body": "Issue reresolved by pull request [https://github.com/apache/spark/pull/52584]", "created": "2025-10-21T13:26:05.006+0000"}], "derived_tasks": {"summary": "PlannedWrite should preserve custom sort order when query output contains literal", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53737", "title": "Add Real-time Mode trigger", "description": "Introduce a new trigger type for Real-time Mode (RTM) in Structured Streaming.\u00a0 This new trigger will be how users enable their Structured Streaming query to run in Real-time Mode.", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-09-27T01:03:12.000+0000", "updated": "2025-10-24T05:11:52.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "L. C. Hsieh", "body": "Issue resolved by pull request 52473 [https://github.com/apache/spark/pull/52473]", "created": "2025-10-02T00:20:32.974+0000"}], "derived_tasks": {"summary": "Add Real-time Mode trigger - Introduce a new trigger type for Real-time Mode (RTM) in Structured Streaming", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53732", "title": "Add TimeTravelSpec to DataSourceV2Relation", "description": "", "status": "Open", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": null, "created": "2025-09-26T08:08:11.000+0000", "updated": "2025-10-24T17:31:34.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Add TimeTravelSpec to DataSourceV2Relation", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "SPARK-53729", "title": "Fix serialization of `pyspark.sql.connect.window.WindowSpec`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-09-26T03:26:55.000+0000", "updated": "2025-10-24T05:13:24.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52464 [https://github.com/apache/spark/pull/52464]", "created": "2025-09-26T09:25:20.009+0000"}], "derived_tasks": {"summary": "Fix serialization of `pyspark.sql.connect.window.WindowSpec`", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53726", "title": "Release Spark Kubernetes Operator 0.6.0", "description": "", "status": "Open", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": null, "created": "2025-09-25T23:53:46.000+0000", "updated": "2025-10-24T15:22:14.000+0000", "labels": [], "components": ["Kubernetes"], "comments": [], "derived_tasks": {"summary": "Release Spark Kubernetes Operator 0.6.0", "classifications": ["umbrella"], "qa_pairs": []}}
{"id": "SPARK-53696", "title": "Make the data type for BINARY in PySpark UDF bytes by default", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dmitry", "assignee": "Dmitry", "created": "2025-09-24T14:23:34.000+0000", "updated": "2025-10-20T22:42:31.000+0000", "labels": ["pull-request-available"], "components": ["PySpark", "SQL"], "comments": [{"author": "Aparna Garg", "body": "User 'xianzhe-databricks' has created a pull request for this issue: https://github.com/apache/spark/pull/52438", "created": "2025-09-25T17:15:34.514+0000"}, {"author": "Hyukjin Kwon", "body": "Issue resolved by pull request 52467 [https://github.com/apache/spark/pull/52467]", "created": "2025-10-20T22:42:31.645+0000"}], "derived_tasks": {"summary": "Make the data type for BINARY in PySpark UDF bytes by default", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53694", "title": "Improve V1WriteHiveCommandSuite test coverage", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-09-24T10:15:17.000+0000", "updated": "2025-10-22T08:01:19.000+0000", "labels": ["pull-request-available"], "components": ["SQL", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52436 [https://github.com/apache/spark/pull/52436]", "created": "2025-09-25T05:50:02.822+0000"}], "derived_tasks": {"summary": "Improve V1WriteHiveCommandSuite test coverage", "classifications": ["improvement", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53687", "title": "Introduce WATERMARK clause in SQL statement", "description": "With Spark Declarative Pipeline, Apache Spark supports to define the streaming query with SQL statement (Flow or Streaming Table). The SQL statement is expected to be composed with streaming semantic (STREAM relation), and this converts the query part to be streaming. With this, the query can be stateful, e.g. aggregation will be a streaming aggregation, deduplication and stream-stream join, etc. And in majority of cases, we require watermark to be defined per source for the stateful query to work properly. Databricks introduced WATERMARK clause to cover this functionality in Lakeflow Declarative Pipeline, but when open sourcing this, we missed to introduce this clause. This ticket is to open source WATERMARK clause.", "status": "Resolved", "priority": "Major", "reporter": "Jungtaek Lim", "assignee": "Jungtaek Lim", "created": "2025-09-24T01:32:53.000+0000", "updated": "2025-10-22T21:57:57.000+0000", "labels": ["pull-request-available"], "components": ["Declarative Pipelines", "SQL", "Structured Streaming"], "comments": [{"author": "Wenchen Fan", "body": "Issue resolved by pull request 52428 [https://github.com/apache/spark/pull/52428]", "created": "2025-10-22T17:08:41.184+0000"}], "derived_tasks": {"summary": "Introduce WATERMARK clause in SQL statement - With Spark Declarative Pipeline, Apache Spark supports to define the streaming query with SQL stateme...", "classifications": ["task"], "qa_pairs": []}}
{"id": "SPARK-53681", "title": "Upgrade `snowflake-jdbc` to 3.26.1", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-23T20:23:59.000+0000", "updated": "2025-10-24T15:42:18.000+0000", "labels": ["pull-request-available"], "components": ["Build", "SQL", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52425 [https://github.com/apache/spark/pull/52425]", "created": "2025-09-24T01:13:33.374+0000"}], "derived_tasks": {"summary": "Upgrade `snowflake-jdbc` to 3.26.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53666", "title": "Add script to generate llms.txt file for Spark main website", "description": "", "status": "Open", "priority": "Major", "reporter": "Allison Wang", "assignee": null, "created": "2025-09-22T20:33:45.000+0000", "updated": "2025-10-20T21:04:59.000+0000", "labels": ["pull-request-available"], "components": ["Documentation"], "comments": [], "derived_tasks": {"summary": "Add script to generate llms.txt file for Spark main website", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53661", "title": "Upgrade `bouncycastle` to 1.82", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-22T16:09:56.000+0000", "updated": "2025-10-24T15:52:13.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52407 [https://github.com/apache/spark/pull/52407]", "created": "2025-09-22T23:24:52.418+0000"}], "derived_tasks": {"summary": "Upgrade `bouncycastle` to 1.82", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53659", "title": "Infer Variant shredding schema in parquet writer", "description": "", "status": "Open", "priority": "Major", "reporter": "David Cashman", "assignee": null, "created": "2025-09-22T13:39:59.000+0000", "updated": "2025-10-24T16:20:25.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Infer Variant shredding schema in parquet writer", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53656", "title": "Refactor MemoryStream to use SparkSession instead of SQLContext", "description": "Update the {{MemoryStream}} object to replace {{SQLContext}} with {{{}SparkSession{}}}, as {{SQLContext}} is deprecated in newer versions of Spark.", "status": "Resolved", "priority": "Major", "reporter": "Ganesha S", "assignee": "Ganesha S", "created": "2025-09-22T03:15:41.000+0000", "updated": "2025-10-20T06:44:26.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Anand", "body": "[~ganeshas] * Why was {{MemoryStream}} originally built on {{SQLContext}} instead of {{{}SparkSession{}}}? * What are the key benefits of switching to {{SparkSession}} now?", "created": "2025-09-22T04:12:16.344+0000"}, {"author": "Ganesha S", "body": "[~anandexplore] Thanks for reviewing this. Please see the answers below. ??Why was {{MemoryStream}}\u00a0originally built on\u00a0{{SQLContext}}\u00a0instead of\u00a0{{{}SparkSession{}}}??? Because SQLContext was the main API for Spark SQL and Structured Streaming when MemoryStream was introduced, before SparkSession existed. ??What are the key benefits of switching to {{SparkSession}}\u00a0now??? This change aligns {{MemoryStream}} with Spark\u2019s current best practices, improves code clarity, and ensures better compatibility with future Spark features by moving away from the older, more fragmented {{SQLContext}} APIs.", "created": "2025-09-22T17:20:44.478+0000"}, {"author": "Jungtaek Lim", "body": "Issue resolved by pull request 52402 [https://github.com/apache/spark/pull/52402]", "created": "2025-10-20T06:44:26.055+0000"}], "derived_tasks": {"summary": "Refactor MemoryStream to use SparkSession instead of SQLContext - Update the {{MemoryStream}} object to replace {{SQLContext}} with {{{}SparkSessio...", "classifications": ["improvement", "task"], "qa_pairs": []}}
{"id": "SPARK-53636", "title": "SortShuffleManager.unregisterShuffle is not thread safe", "description": "Details can be found at https://github.com/apache/spark/pull/52337#issuecomment-3301137691", "status": "Resolved", "priority": "Major", "reporter": "wuyi", "assignee": "wuyi", "created": "2025-09-18T13:12:07.000+0000", "updated": "2025-10-21T15:48:17.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52386 [https://github.com/apache/spark/pull/52386]", "created": "2025-10-21T15:48:17.108+0000"}], "derived_tasks": {"summary": "SortShuffleManager.unregisterShuffle is not thread safe - Details can be found at https://github", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53622", "title": "Improve UninterruptibleThread test", "description": "Implement several code improvement in {{UninterruptibleThreadSuite}}: - run several iterations of stress test - log {{InterruptedException}} - improve assert error message (\"false did not equal true\" => \"hasInterruptedException was false\") - use {{await}} instead of {{sleep}} - fail test fast", "status": "Open", "priority": "Minor", "reporter": "Vlad Rozov", "assignee": null, "created": "2025-09-17T19:43:57.000+0000", "updated": "2025-10-21T04:11:50.000+0000", "labels": ["pull-request-available"], "components": ["Tests"], "comments": [], "derived_tasks": {"summary": "Improve UninterruptibleThread test - Implement several code improvement in {{UninterruptibleThreadSuite}}: - run several iterations of stress test ...", "classifications": ["improvement", "test"], "qa_pairs": []}}
{"id": "SPARK-53614", "title": "Introduce iterator API for ApplyInPandas", "description": "introduce iterator api  def func( batches: Iterator[pandas.DataFrame] ) -> Iterator[pandas.DataFrame]: ... df.groupby(\"id\").applyInPandas(func, schema='v long')  The implementation should be similar to https://issues.apache.org/jira/browse/SPARK-49547", "status": "Open", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Yicong Huang", "created": "2025-09-17T06:34:41.000+0000", "updated": "2025-10-23T22:05:28.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [{"author": "Ruifeng Zheng", "body": "[~yiconghuang] would you like have a try?", "created": "2025-10-10T01:28:52.083+0000"}, {"author": "Yicong Huang", "body": "I will work on this!", "created": "2025-10-19T23:54:46.258+0000"}], "derived_tasks": {"summary": "Introduce iterator API for ApplyInPandas - introduce iterator api  def func( batches: Iterator[pandas", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53610", "title": "Limit Arrow batch sizes in CoGrouped applyInPandas and applyInArrow", "description": "Apply batching in * SQL_GROUPED_MAP_PANDAS_UDF * SQL_GROUPED_AGG_PANDAS_UDF", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-09-17T06:29:29.000+0000", "updated": "2025-10-24T03:07:17.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52700 [https://github.com/apache/spark/pull/52700]", "created": "2025-10-24T03:07:02.713+0000"}], "derived_tasks": {"summary": "Limit Arrow batch sizes in CoGrouped applyInPandas and applyInArrow - Apply batching in * SQL_GROUPED_MAP_PANDAS_UDF * SQL_GROUPED_AGG_PANDAS_UDF", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53608", "title": "Improve Python Aggregation UDFs", "description": "1, optimize the data exchange in Grouped UDFs, e.g. apply arrow batching; 2, introduce new iterator API to process the data within a group;", "status": "Open", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": null, "created": "2025-09-17T06:25:30.000+0000", "updated": "2025-10-24T22:20:54.000+0000", "labels": [], "components": ["Connect", "PySpark", "SQL"], "comments": [], "derived_tasks": {"summary": "Improve Python Aggregation UDFs - 1, optimize the data exchange in Grouped UDFs, e", "classifications": ["umbrella", "improvement"], "qa_pairs": []}}
{"id": "SPARK-53603", "title": "Upgrade Checkstyle to 11.0.1", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-16T22:47:15.000+0000", "updated": "2025-10-24T15:41:57.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52361 [https://github.com/apache/spark/pull/52361]", "created": "2025-09-17T03:42:16.276+0000"}], "derived_tasks": {"summary": "Upgrade Checkstyle to 11.0.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53599", "title": "Upgrade Netty to 4.1.127.Final", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-09-16T15:31:35.000+0000", "updated": "2025-10-24T05:28:53.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52356 [https://github.com/apache/spark/pull/52356]", "created": "2025-09-16T22:11:13.643+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.1.127.Final", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53586", "title": "Upgrade `kubernetes-client` to 7.4.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-15T19:00:46.000+0000", "updated": "2025-10-24T15:17:20.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52345 [https://github.com/apache/spark/pull/52345]", "created": "2025-09-15T22:47:06.676+0000"}], "derived_tasks": {"summary": "Upgrade `kubernetes-client` to 7.4.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53585", "title": "Upgrade Scala to 2.13.17", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Vlad Rozov", "created": "2025-09-15T18:50:31.000+0000", "updated": "2025-10-24T15:51:09.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52509 [https://github.com/apache/spark/pull/52509]", "created": "2025-10-13T04:18:11.505+0000"}], "derived_tasks": {"summary": "Upgrade Scala to 2.13.17", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53573", "title": "Expand usage of parameter markers by means of a pre-processing parser", "description": "The current implementation of parameter markers (:Parm and ?) is limited to two scenarios which must both be true: 1. An expression, such as a WHERE clause or a SELECT list 2. A query, a DML statement, or SET VARIABLE. There is a lot of value in expanding their use to: 1. DDL and utility statements (such as CREATE VIEW, or SHOW) 2. Any place a literal can go (such as TBLPROPERTIES, or DECIMAL(?, ?). Unfortunately the current implementation is very hard to expand in that direction. For DDL we must persist the effective text, or remember the parameter marker values And literals in general do NOT go through the analyzer. They are often handled straight in te visit methods. This proposed solution is to add a pre-parser which's sole job is replace any parameter markers it finds with literals without the regular parser having to be aware.", "status": "Resolved", "priority": "Major", "reporter": "Serge Rielau", "assignee": "Serge Rielau", "created": "2025-09-13T21:50:00.000+0000", "updated": "2025-10-26T03:19:24.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Aparna Garg", "body": "User 'srielau' has created a pull request for this issue: https://github.com/apache/spark/pull/52334", "created": "2025-09-16T08:21:03.602+0000"}, {"author": "Daniel Tenedorio", "body": "Issue resolved by pull request 52334 [https://github.com/apache/spark/pull/52334]", "created": "2025-10-16T21:20:51.715+0000"}], "derived_tasks": {"summary": "Expand usage of parameter markers by means of a pre-processing parser - The current implementation of parameter markers (:Parm and ", "classifications": ["improvement"], "qa_pairs": [{"question": "The current implementation of parameter markers (:Parm and ?", "answer": "User 'srielau' has created a pull request for this issue: https://github.com/apache/spark/pull/52334"}, {"question": "Any place a literal can go (such as TBLPROPERTIES, or DECIMAL(?", "answer": "User 'srielau' has created a pull request for this issue: https://github.com/apache/spark/pull/52334"}, {"question": ", ?", "answer": "User 'srielau' has created a pull request for this issue: https://github.com/apache/spark/pull/52334"}]}}
{"id": "SPARK-53567", "title": "Upgrade Netty to 4.1.125.Final", "description": "Upgrade Netty to 4.1.125.Final in order to fix CVE-2025-58056 and CVE-2025-58057. https://github.com/netty/netty/security/advisories/GHSA-fghv-69vj-qj49 A flaw in netty's parsing of chunk extensions in HTTP/1.1 messages with chunked encoding can lead to request smuggling issues with some reverse proxies. https://github.com/netty/netty/security/advisories/GHSA-3p8m-j85q-pgmj With specially crafted input, BrotliDecoder and some other decompressing decoders will allocate a large number of reachable byte buffers, which can lead to denial of service.", "status": "Resolved", "priority": "Major", "reporter": "Jota Martos", "assignee": "Bj\u00f8rn J\u00f8rgensen", "created": "2025-09-12T12:48:48.000+0000", "updated": "2025-10-24T05:19:24.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Jota Martos", "body": "sorry for the noise, didn't see you already worked on this. https://github.com/apache/spark/pull/52239 You can close the ticket now.", "created": "2025-09-12T12:53:00.057+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.1.125.Final - Upgrade Netty to 4", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53541", "title": "Update K8s IT CI to use K8s 1.34", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-09T22:28:30.000+0000", "updated": "2025-10-24T15:17:07.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Project Infra"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 52293 [https://github.com/apache/spark/pull/52293]", "created": "2025-09-10T04:09:58.997+0000"}], "derived_tasks": {"summary": "Update K8s IT CI to use K8s 1.34", "classifications": ["improvement", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53539", "title": "Add `libwebp-dev` to recover `spark-rm/Dockerfile` building", "description": "", "status": "Resolved", "priority": "Blocker", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-09T20:27:27.000+0000", "updated": "2025-10-24T15:16:56.000+0000", "labels": ["pull-request-available"], "components": ["Project Infra"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52290 [https://github.com/apache/spark/pull/52290]", "created": "2025-09-09T21:36:17.532+0000"}], "derived_tasks": {"summary": "Add `libwebp-dev` to recover `spark-rm/Dockerfile` building", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53535", "title": "Missing columns inside a struct in Parquet files are not handled correctly", "description": "The mechanism that is used to fill missing columns with NULLs does not work correctly when confronted with a missing column inside of a STRUCT. h3. Repro {*}Step 1{*}: We\u2019re going to consider three different schemas: # One with STRUCT<INT a>, which we\u2019re going to write to disk. # One with STRUCT<INT b>, which we\u2019ll use to demonstrate the missing columns being handled incorrectly. # One with STRUCT<INT a, INT b>, which we\u2019ll use to demonstrate the missing columns being handled well.  df_a = sql('SELECT 1 as id, named_struct(\"a\", 1) AS s') df_b = sql('SELECT 2 as id, named_struct(\"b\", 3) AS s') df_ab = sql('SELECT 2 as id, named_struct(\"a\", 2, \"b\", 3) AS s')  {*}Step 2{*}: We write the sample data to disk.  path = \"/tmp/missing_col_test\" df_a.write.format(\"parquet\").save(path)  {*}Step 3{*}: We read this data with the three different schemas explained above.  # This is the schema that matches the data. We read correct data. spark.read.format(\"parquet\").schema(df_a.schema).load(path).show() +---+---+ | id| s| +---+---+ | 1|{1}| # <- GOOD +---+---+ # This is the schema that has struct s, but doesn't have columns # inside of s in common with what was written to disk. spark.read.format(\"parquet\").schema(df_b.schema).load(path).show() +---+----+ | id| s| +---+----+ | 1|NULL| # <- WRONG! Should be {NULL} instead. +---+----+ # This is the shcema that has more columns in struct s than what # was written to disk. spark.read.format(\"parquet\").schema(df_ab.schema).load(path).show() +---+---------+ | id| s| +---+---------+ | 1|{1, NULL}| # <- GOOD +---+---------+  {*}Step 4{*}: To demonstrate that this is not a display glitch, but genuinely leads to incorrect query results, we can show that we evaluate a function differently depending on the number of columns insidea struct in the read schema.  from pyspark.sql.functions import col spark.read.format(\"parquet\").schema(df_a.schema).load(path).withColumn(\"isnull\", col(\"s\").isNull()).show() +---+---+------+ | id| s|isnull| +---+---+------+ | 1|{1}| false| <- GOOD +---+---+------+ spark.read.format(\"parquet\").schema(df_b.schema).load(path).withColumn(\"isnull\", col(\"s\").isNull()).show() +---+----+------+ | id| s|isnull| +---+----+------+ | 1|NULL| true| <- WRONG!!! Should be false. +---+----+------+ spark.read.format(\"parquet\").schema(df_ab.schema).load(path).withColumn(\"isnull\", col(\"s\").isNull()).show() +---+---------+------+ | id| s|isnull| +---+---------+------+ | 1|{1, NULL}| false| <- GOOD +---+---------+------+  h3. Solution This is happening when all the fields of the struct we are trying to read is missing in a Parquet file, and we assume null structs because we are not reading any field with definition & repetition levels. We should look at the file schema to see if the struct has another non-requested field, whose definition levels can be used for struct's nullability.", "status": "Resolved", "priority": "Major", "reporter": "Ziya Mukhtarov", "assignee": "Ziya Mukhtarov", "created": "2025-09-09T11:57:06.000+0000", "updated": "2025-10-20T23:47:23.000+0000", "labels": ["parquet", "parquetReader", "pull-request-available"], "components": ["SQL"], "comments": [{"author": "Ziya Mukhtarov", "body": "I'm already working on fixing this issue.", "created": "2025-09-09T12:06:09.968+0000"}, {"author": "Wenchen Fan", "body": "Issue resolved by pull request 52557 [https://github.com/apache/spark/pull/52557]", "created": "2025-10-20T23:47:23.815+0000"}], "derived_tasks": {"summary": "Missing columns inside a struct in Parquet files are not handled correctly - The mechanism that is used to fill missing columns with NULLs does not...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53525", "title": "Spark Connect ArrowBatch Result Chunking", "description": "Currently, we enforce gRPC message limits on both the client and the server. These limits are largely meant to protect both sides from potential OOMs by rejecting abnormally large messages. However, there are cases in which the server incorrectly sends oversized messages that exceed these limits and cause execution failures. Specifically, the large message issue from the server to the client we\u2019re solving here, comes from the Arrow batch data in ExecutePlanResponse being too large. It\u2019s caused by a single arrow row exceeding the 128MB message limit, and Arrow cannot partition further and return the single large row in one gRPC message. To improve Spark Connect stability, this PR implements chunking large Arrow batches when returning query results from the server to the client, ensuring each ExecutePlanResponse chunk remains within the size limit, and the chunks from a batch will be reassembled on the client when parsing as an arrow batch.", "status": "Resolved", "priority": "Major", "reporter": "Xi Lyu", "assignee": "Xi Lyu", "created": "2025-09-08T13:22:33.000+0000", "updated": "2025-10-24T12:40:22.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [{"author": "Herman van H\u00f6vell", "body": "Issue resolved by pull request 52271 [https://github.com/apache/spark/pull/52271]", "created": "2025-09-11T15:35:33.206+0000"}], "derived_tasks": {"summary": "Spark Connect ArrowBatch Result Chunking - Currently, we enforce gRPC message limits on both the client and the server", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53494", "title": "Upgrade Netty to 4.1.126.Final", "description": "https://netty.io/news/2025/09/03/4-1-126-Final.html", "status": "Resolved", "priority": "Major", "reporter": "Bj\u00f8rn J\u00f8rgensen", "assignee": "Bj\u00f8rn J\u00f8rgensen", "created": "2025-09-04T19:38:43.000+0000", "updated": "2025-10-24T05:18:41.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52239 [https://github.com/apache/spark/pull/52239]", "created": "2025-09-04T20:09:51.110+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.1.126.Final - https://netty", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53475", "title": "Use `setup-minikube` in `spark-docker` repository", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-03T17:27:39.000+0000", "updated": "2025-10-24T15:16:44.000+0000", "labels": ["pull-request-available"], "components": ["Project Infra"], "comments": [], "derived_tasks": {"summary": "Use `setup-minikube` in `spark-docker` repository", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53473", "title": "Publish Apache Spark 4.1.0-preview1 to docker registry", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-03T16:52:00.000+0000", "updated": "2025-10-24T15:33:35.000+0000", "labels": ["pull-request-available"], "components": ["Project Infra"], "comments": [], "derived_tasks": {"summary": "Publish Apache Spark 4.1.0-preview1 to docker registry", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53436", "title": "Upgrade Netty to 4.1.124.Final", "description": "Upgrade Netty to 4.1.124.Final in order to fix CVE-2025-55163. Netty is an asynchronous, event-driven network application framework. Prior to versions 4.1.124.Final and 4.2.4.Final, Netty is vulnerable to MadeYouReset DDoS. This is a logical vulnerability in the HTTP/2 protocol, that uses malformed HTTP/2 control frames in order to break the max concurrent streams limit - which results in resource exhaustion and distributed denial of service. This issue has been patched in versions 4.1.124.Final and 4.2.4.Final.", "status": "Resolved", "priority": "Major", "reporter": "Jota Martos", "assignee": "Bj\u00f8rn J\u00f8rgensen", "created": "2025-08-29T10:04:11.000+0000", "updated": "2025-10-24T05:18:27.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Bj\u00f8rn J\u00f8rgensen", "body": "I'm working on a PR for this one https://github.com/apache/spark/pull/52181", "created": "2025-08-29T21:28:33.119+0000"}, {"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52181 [https://github.com/apache/spark/pull/52181]", "created": "2025-08-30T20:08:04.730+0000"}, {"author": "Jota Martos", "body": "Thanks for working on this. Have a good week!", "created": "2025-09-01T06:50:06.923+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.1.124.Final - Upgrade Netty to 4", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53412", "title": "Upgrade Volcano to 1.12.2", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-08-28T05:34:28.000+0000", "updated": "2025-10-24T15:16:32.000+0000", "labels": ["pull-request-available"], "components": ["Documentation", "Kubernetes", "Project Infra"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52156 [https://github.com/apache/spark/pull/52156]", "created": "2025-08-28T15:54:49.712+0000"}], "derived_tasks": {"summary": "Upgrade Volcano to 1.12.2", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53406", "title": "Support Shuffle Spec in Direct Partition ID Pass Through", "description": "", "status": "Open", "priority": "Major", "reporter": "Shujing Yang", "assignee": null, "created": "2025-08-28T00:02:15.000+0000", "updated": "2025-10-23T23:37:31.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Support Shuffle Spec in Direct Partition ID Pass Through", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53339", "title": "Fix an issue which occurs when an operation in pending state is interrupted", "description": "When an operation in pending state is concurrently interrupted, the interruption doesn't work correctly. You can easily reproduce this issue by modifying SparkConnectExecutionManager#createExecuteHolderAndAttach like as follows.  val executeHolder = createExecuteHolder(executeKey, request, sessionHolder) try { + Thread.sleep(1000) executeHolder.eventsManager.postStarted() executeHolder.start() } catch {  And then run a test \"interrupt all - background queries, foreground interrupt\" in SparkSessionE2ESuite.  $ build/sbt 'connect-client-jvm/testOnly org.apache.spark.sql.connect.SparkSessionE2ESuite -- -z \"interrupt all - background queries, foreground interrupt\"'  You will see the following error.  [info] - interrupt all - background queries, foreground interrupt *** FAILED *** (20 seconds, 344 milliseconds) [info] The code passed to eventually never returned normally. Attempted 28 times over 20.285258458 seconds. Last failure message: Some(\"unexpected failure in q2: org.apache.spark.SparkException: java.lang.IllegalStateException: Operation was orphaned because of an internal error.\") was not empty Error not empty: Some(unexpected failure in q2: org.apache.spark.SparkException: java.lang.IllegalStateException: Operation was orphaned because of an internal error.). (SparkSessionE2ESuite.scala:72) [info] org.scalatest.exceptions.TestFailedDueToTimeoutException: [info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219) [info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226) [info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313) [info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312) [info] at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457) [info] at org.apache.spark.sql.connect.SparkSessionE2ESuite.$anonfun$new$1(SparkSessionE2ESuite.scala:72)", "status": "Open", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": null, "created": "2025-08-20T15:26:41.000+0000", "updated": "2025-10-22T14:03:24.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [], "derived_tasks": {"summary": "Fix an issue which occurs when an operation in pending state is interrupted - When an operation in pending state is concurrently interrupted, the i...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53335", "title": "Support `spark.kubernetes.driver.annotateExitException`", "description": "For jobs which run on kubernetes there is no native concept of diagnostics (like there is in YARN), which means that for debugging and triaging errors users\u00a0_must_ go to logs. For many jobs which run on YARN this is often not necessary, since the diagnostics contains the root cause reason for failure. Additionally, for platforms which provide automation of failure insights, or make decisions based on failures, there must be a custom solution or deciding why the application failed (e.g. log and stack trace parsing). We should provide a way for yarn-equivalent diagnostics for spark jobs on kubernetes.", "status": "Resolved", "priority": "Major", "reporter": "Victor Sunderland", "assignee": "Victor Sunderland", "created": "2025-08-20T03:49:56.000+0000", "updated": "2025-10-25T04:52:12.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Spark Core"], "comments": [{"author": "Victor Sunderland", "body": "https://github.com/apache/spark/pull/52068", "created": "2025-08-20T03:50:39.512+0000"}, {"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52068 [https://github.com/apache/spark/pull/52068]", "created": "2025-10-25T04:50:30.310+0000"}, {"author": "Dongjoon Hyun", "body": "To improve the visibility of this feature, I collected this as a subtask of SPARK-54016.", "created": "2025-10-25T04:52:12.959+0000"}], "derived_tasks": {"summary": "Support `spark.kubernetes.driver.annotateExitException` - For jobs which run on kubernetes there is no native concept of diagnostics (like there is...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53324", "title": "Support max pending pods per rp id", "description": "https://issues.apache.org/jira/browse/SPARK-36052 introduces a max pending pod limit.\u00a0 We can also have a max pending pods per rp id limit, so we have this at a more granular level.", "status": "Resolved", "priority": "Major", "reporter": "Victor Sunderland", "assignee": "Victor Sunderland", "created": "2025-08-18T22:07:32.000+0000", "updated": "2025-10-24T15:28:07.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Victor Sunderland", "body": "https://github.com/apache/spark/pull/51913", "created": "2025-08-18T22:08:16.091+0000"}, {"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51913 [https://github.com/apache/spark/pull/51913]", "created": "2025-10-23T20:22:00.990+0000"}], "derived_tasks": {"summary": "Support max pending pods per rp id - https://issues", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53319", "title": "Support the time type by try_make_timestamp_ltz()", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-08-18T13:42:29.000+0000", "updated": "2025-10-22T17:01:46.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Uro\u0161 Bojani\u0107", "body": "Work in progress: https://github.com/apache/spark/pull/52063.", "created": "2025-08-18T13:43:39.760+0000"}, {"author": "Wenchen Fan", "body": "Issue resolved by pull request 52063 [https://github.com/apache/spark/pull/52063]", "created": "2025-10-22T17:01:46.546+0000"}], "derived_tasks": {"summary": "Support the time type by try_make_timestamp_ltz()", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53289", "title": "To prevent execution of canceled tasks, cancelJobGroup should be invoked prior to dispatching the ExecutionEnd event.", "description": "We optimized Gluten's broadcast hash join by ensuring the hash table is built only once per executor. When the ExecutionEnd event is received, the corresponding hash table is released. However, while running Gluten unit test, we encountered an exception: After the ExecutionEnd event was triggered and the hash table was released, the hash table was unexpectedly recreated, leading to a core dump. After investigating with [~yumwang] , we found that the unit test result was None, which triggered the AQEPropagateEmptyRelation rule to skip the join operation. Despite this, the task was not actually canceled, which caused the hash table to be rebuilt after it had already been released.", "status": "Open", "priority": "Major", "reporter": "Ke Jia", "assignee": null, "created": "2025-08-15T07:02:22.000+0000", "updated": "2025-10-25T14:08:27.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "To prevent execution of canceled tasks, cancelJobGroup should be invoked prior to dispatching the ExecutionEnd event. - We optimized Gluten's broad...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53218", "title": "Upgrade `bouncycastle` to 1.81", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-08-09T03:30:02.000+0000", "updated": "2025-10-24T15:52:01.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51943 [https://github.com/apache/spark/pull/51943]", "created": "2025-08-09T13:42:51.899+0000"}], "derived_tasks": {"summary": "Upgrade `bouncycastle` to 1.81", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53096", "title": "Support `spark.kubernetes.executor.terminationGracePeriodSeconds`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-08-04T06:34:51.000+0000", "updated": "2025-10-24T15:16:22.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51811 [https://github.com/apache/spark/pull/51811]", "created": "2025-08-04T15:11:38.921+0000"}], "derived_tasks": {"summary": "Support `spark.kubernetes.executor.terminationGracePeriodSeconds`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53093", "title": "Drop K8s v1.31 Support", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-08-04T05:00:36.000+0000", "updated": "2025-10-24T15:13:22.000+0000", "labels": ["pull-request-available"], "components": ["Documentation", "Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51808 [https://github.com/apache/spark/pull/51808]", "created": "2025-08-04T05:51:23.730+0000"}], "derived_tasks": {"summary": "Drop K8s v1.31 Support", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53079", "title": "Update `YuniKorn` docs with `1.7.0`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-08-02T20:59:51.000+0000", "updated": "2025-10-24T15:16:11.000+0000", "labels": ["pull-request-available"], "components": ["Documentation", "Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51791 [https://github.com/apache/spark/pull/51791]", "created": "2025-08-03T14:55:55.447+0000"}], "derived_tasks": {"summary": "Update `YuniKorn` docs with `1.7.0`", "classifications": ["improvement", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53078", "title": "Remove `commons-(io|lang3)` test dependencies from `kvstore`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-08-02T15:33:40.000+0000", "updated": "2025-10-24T15:43:41.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core", "Tests"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 51789 [https://github.com/apache/spark/pull/51789]", "created": "2025-08-03T06:38:29.662+0000"}], "derived_tasks": {"summary": "Remove `commons-(io|lang3)` test dependencies from `kvstore`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53023", "title": "Remove `commons-io` dependency from `sql/api` module", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-07-30T18:45:35.000+0000", "updated": "2025-10-24T15:49:52.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51729 [https://github.com/apache/spark/pull/51729]", "created": "2025-07-30T22:57:35.944+0000"}], "derived_tasks": {"summary": "Remove `commons-io` dependency from `sql/api` module", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53021", "title": "Switch `common/utils` module to have `commons-io` test dependency", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-07-30T15:00:32.000+0000", "updated": "2025-10-24T15:43:31.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51726 [https://github.com/apache/spark/pull/51726]", "created": "2025-07-30T18:41:39.470+0000"}], "derived_tasks": {"summary": "Switch `common/utils` module to have `commons-io` test dependency", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53002", "title": "RocksDB Bounded Memory Fix", "description": "Currently, RocksDB metrics show 0 bytes used if bounded memory is enabled. This PR will provide an approximation of memory used by fetching the memory used by RocksDB per executor, then dividing it by the open RocksDB instances per executor.", "status": "Closed", "priority": "Major", "reporter": "Eric Marnadi", "assignee": "Eric Marnadi", "created": "2025-07-29T18:36:40.000+0000", "updated": "2025-10-26T01:02:43.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "Anish Shrigondekar", "body": "PR merged here - https://github.com/apache/spark/pull/51709", "created": "2025-08-04T20:08:11.580+0000"}], "derived_tasks": {"summary": "RocksDB Bounded Memory Fix - Currently, RocksDB metrics show 0 bytes used if bounded memory is enabled", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-52999", "title": "Clean up the deprecated APIs usage in the kubernetes-integration-tests module", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-07-29T16:17:43.000+0000", "updated": "2025-10-24T15:26:27.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51702 [https://github.com/apache/spark/pull/51702]", "created": "2025-07-29T19:21:17.649+0000"}, {"author": "Dongjoon Hyun", "body": "I collect this as a subtask of SPARK-54016 to improve visibility.", "created": "2025-10-24T15:26:27.864+0000"}], "derived_tasks": {"summary": "Clean up the deprecated APIs usage in the kubernetes-integration-tests module", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52937", "title": "Add support for pipeline sinks", "description": "As proposed in the [SPIP|https://docs.google.com/document/d/1PsSTngFuRVEOvUGzp_25CQL1yfzFHFr02XdMfQ7jOM4/edit?tab=t.0], a sink is a generic target for a flow to send data that is external to the pipeline. Add support for defining them and executing flows that target them.", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Jacky Wang", "created": "2025-07-24T03:50:00.000+0000", "updated": "2025-10-23T03:15:19.000+0000", "labels": ["pull-request-available"], "components": ["Declarative Pipelines"], "comments": [{"author": "Aparna Garg", "body": "User 'sryza' has created a pull request for this issue: https://github.com/apache/spark/pull/51644", "created": "2025-09-19T16:49:49.177+0000"}], "derived_tasks": {"summary": "Add support for pipeline sinks - As proposed in the [SPIP|https://docs", "classifications": ["feature", "improvement"], "qa_pairs": [{"question": "com/document/d/1PsSTngFuRVEOvUGzp_25CQL1yfzFHFr02XdMfQ7jOM4/edit?", "answer": "User 'sryza' has created a pull request for this issue: https://github.com/apache/spark/pull/51644"}]}}
{"id": "SPARK-52933", "title": "Verify if the executor pod cpu request exceeds limit", "description": "h2. *Context* By mistake, the configuration *spark.kubernetes.executor.request.cores* or *spark.executor.cores*\u00a0can be set to exceed the number of {*}spark.kubernetes.executor.limit.cores{*}. This can cause unwanted behavior. N.B. The scope of this ticket is not about how to fix the mistake, given there are lots of hints on how to troubleshoot and solve such issues (e.g. logs) but rather about the failure mode when error happens. h3. Current behaviour: Assuming the driver pod is scheduled correctly: When a driver pod is sending POST request to kubeapi server for executor pod creation, when *spark.executor.cores*\u00a0or *spark.kubernetes.executor.request.cores* is larger than the {*}spark.kubernetes.executor.limit.cores{*}, this POST request fails with 422 status code without executor pods being created, because *request.cores* exceeds the {*}limit.cores{*}. Later driver will retry every second ({*}spark.kubernetes.allocation.batch.delay{*} default 1 second) to do the POST request for executor creation and fail continuously. There is no limit on the number of retries for such request. Two problems arise: * From the spark job status perspective (seen from the spark UI): this job is in running status however it will be stuck for \"indefinite\" time (in theory) if there is no interruption. * When a great deal of spark jobs are having such issue, loads to kubeapi server increases wastefully given drivers are sending pod creation requests that are for sure going to fail. h3. Expected behaviour: Failing the spark job as early as possible when *spark.kubernetes.executor.request.cores* or *spark.executor.cores* exceeds the number of {*}spark.kubernetes.executor.limit.cores{*}, given the spark job for sure won't work without executors being scheduled. In this case: * No wasteful loads to kubeapi server given the whole job already fails. * Spark job in failed status: direct feedback to users on things not right. h2. *Proposal:* At the `KubernetesClusterManager.scala`, add a failing fast step (e.g. require) for ensuring `spark.kubernetes.executor.limit.cores` is always higher than `spark.kubernetes.executor.request.cores` and `spark.executor.cores`", "status": "Resolved", "priority": "Minor", "reporter": "Zemin Piao", "assignee": "Dongjoon Hyun", "created": "2025-07-23T20:18:55.000+0000", "updated": "2025-10-24T15:15:18.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Thank you for reporting JIRA issue, [~zeminpiao].", "created": "2025-07-27T21:15:05.975+0000"}, {"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51678 [https://github.com/apache/spark/pull/51678]", "created": "2025-07-28T03:16:46.299+0000"}], "derived_tasks": {"summary": "Verify if the executor pod cpu request exceeds limit", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52902", "title": "Support {{SPARK_VERSION}} placeholder in K8s image config values", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-07-21T07:42:08.000+0000", "updated": "2025-10-24T15:21:18.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51592 [https://github.com/apache/spark/pull/51592]", "created": "2025-07-21T15:54:08.907+0000"}], "derived_tasks": {"summary": "Support {{SPARK_VERSION}} placeholder in K8s image config values", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52855", "title": "Prevent creating and dropping temp views on the session within Pipelines Python definition files", "description": "Creating/replacing/dropping the temp views on the Spark session is an imperative construct that can cause friction and unexpected behavior from within a pipeline declaration. E.g. it makes pipeline behavior sensitive to the order that Python files are imported in, which can be unpredictable. Temporary views can already be defined using the `@temporary_view` decorator. Raising an error when someone tries to invoke to create/replace/drop a temprary view in this situation would avoid this unpredictable behavior. The ways to do this in Python are: * DataFrame.createOrReplaceTempView * DataFrame.createTempView * spark.catalog.dropTempView * DataFrame.createOrReplaceGlobalTempView * DataFrame.createGlobalTempView * spark.catalog.dropGlobalTempView", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Jacky Wang", "created": "2025-07-17T20:39:30.000+0000", "updated": "2025-10-24T16:15:13.000+0000", "labels": [], "components": ["Declarative Pipelines"], "comments": [{"author": "Sanford Ryza", "body": "PR: https://github.com/apache/spark/pull/51590", "created": "2025-10-24T16:15:13.370+0000"}], "derived_tasks": {"summary": "Prevent creating and dropping temp views on the session within Pipelines Python definition files - Creating/replacing/dropping the temp views on th...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52854", "title": "Prevent setCurrentDatabase and setCurrentCatalog within Pipelines Python definition files", "description": "Setting the spark session default catalog and database is an imperative construct that can cause friction and unexpected behavior from within a pipeline declaration. E.g. it makes pipeline behavior sensitive to the order that Python files are imported in, which can be unpredictable. There are already existing mechanisms for setting Spark catalog and database for pipelines: * The catalog and database settings in the pipeline spec * The name argument on the dataset decorators accepts a fully-qualified name Raising an error when someone tries to invoke to set a catalog or database in this situation would avoid this unpredictable behavior. The ways to set the catalog and database from Python are: * spark.catalog.setCurrentCatalog * spark.sql(\"USE CATALOG\") * spark.catalog.setCurrentDatabase * spark.sql(\"USE DATABASE\") The spark.sql usages will be covered by a separate JIRA.", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Jacky Wang", "created": "2025-07-17T20:33:33.000+0000", "updated": "2025-10-24T16:16:29.000+0000", "labels": [], "components": ["Declarative Pipelines"], "comments": [{"author": "Sanford Ryza", "body": "PR: https://github.com/apache/spark/pull/51590", "created": "2025-10-24T16:16:10.459+0000"}], "derived_tasks": {"summary": "Prevent setCurrentDatabase and setCurrentCatalog within Pipelines Python definition files - Setting the spark session default catalog and database ...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52830", "title": "Support `spark.kubernetes.(driver|executor).pod.excludedFeatureSteps`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-07-16T22:17:40.000+0000", "updated": "2025-10-24T15:15:06.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51522 [https://github.com/apache/spark/pull/51522]", "created": "2025-07-17T03:57:00.086+0000"}], "derived_tasks": {"summary": "Support `spark.kubernetes.(driver|executor).pod.excludedFeatureSteps`", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-52800", "title": "Remove several deprecated commons-lang3 methods", "description": "Several deprecated commons-lang3 methods are still used in the code, for example: * StringUtils.contains(CharSequence, CharSequence) * StringUtils.endsWith(CharSequence, CharSequence) * StringUtils.equals(CharSequence, CharSequence) * StringUtils.replace(String, String, String)", "status": "Resolved", "priority": "Minor", "reporter": "morvenhuang", "assignee": "morvenhuang", "created": "2025-07-15T10:00:06.000+0000", "updated": "2025-10-24T05:31:53.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes", "Spark Core", "SQL"], "comments": [{"author": "morvenhuang", "body": "I'm working on it.", "created": "2025-07-15T10:00:24.142+0000"}, {"author": "Yang Jie", "body": "Issue resolved by pull request 51495 [https://github.com/apache/spark/pull/51495]", "created": "2025-07-20T15:30:49.529+0000"}], "derived_tasks": {"summary": "Remove several deprecated commons-lang3 methods - Several deprecated commons-lang3 methods are still used in the code, for example: * StringUtils", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52798", "title": "Add approx_top_k functions: combine", "description": "Following https://issues.apache.org/jira/browse/SPARK-52588 This PR\u00a0 introduces the following function: * approx_top_k_combine: aggregation function, merges multiple intermediate sketch status into one.", "status": "Resolved", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": "Yuchuan Huang", "created": "2025-07-15T00:43:24.000+0000", "updated": "2025-10-24T22:14:50.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "Issue resolved by pull request 51505 [https://github.com/apache/spark/pull/51505]", "created": "2025-10-18T02:51:57.991+0000"}], "derived_tasks": {"summary": "Add approx_top_k functions: combine - Following https://issues", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-52792", "title": "Remove `commons-lang3` dependency from `network-common`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-07-14T16:27:03.000+0000", "updated": "2025-10-24T15:47:32.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51476 [https://github.com/apache/spark/pull/51476]", "created": "2025-07-14T22:37:31.266+0000"}], "derived_tasks": {"summary": "Remove `commons-lang3` dependency from `network-common`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52790", "title": "Introduce new grid testing method which provides better naming", "description": "Currently, gridTest accepts test name prefix and sequence of parameters. Final test name is made like this `testNamePrefix + s\" ($param)\"`. Which is not good since developers often don't know how final test name would look like and pass here sequence of map, or sequence of booleans, which results in unintuitive test case names. E.g.  gridTest(\"Select with limit\")(Seq(true, false)) { pushdownEnabled => ... }  Will result in registering of next test cases: * Select with limit (true) * Select with limit (false) Instead of that, developers should provide descriptive name suffix:  gridTest(\"Select with limit\")(Seq( GridTestCase(params = true, suffix = \"pushdown enabled\"), GridTestCase(params = false, suffix = \"pushdown disabled\"), )) { pushdownEnabled => \u00a0... }  Instead of relying on developers to look for base implementation and make some case class for parameters with overriden `toString` implementation, we should enforce engineers to provide suffix. (Even with proper `toString` implementation, intent of test case may be unknown).", "status": "Open", "priority": "Major", "reporter": "Uros Stankovic", "assignee": null, "created": "2025-07-14T12:20:51.000+0000", "updated": "2025-10-26T00:29:56.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [], "derived_tasks": {"summary": "Introduce new grid testing method which provides better naming - Currently, gridTest accepts test name prefix and sequence of parameters", "classifications": ["feature", "improvement"], "qa_pairs": []}}
{"id": "SPARK-52780", "title": "Expose Local Iterator in Spark Connect Go Client (For Streaming Rows)", "description": "The current Spark Connect Go client implementation fetches all DataFrame rows at once using the `Collect()` method, which limits its practicality for handling very large datasets (GBs/TBs). In the Scala Spark Connect client, a `toLocalIterator()` method allows incremental streaming of rows, which is advantageous for efficiently processing large result sets by external clients or services. In my case, I have an API gateway which polls Spark Connect allowing me to distribute the results of my OLAP queries as a stream with fairly lightweight instance sizes (when multiple GB of rows are returned). Super useful! Originally I wanted to develop this gateway using the Spark Connect Go client but found it doesn't expose the `ToLocalIterator()` method within the Spark Connect Scala DataFrame interface, returning an iterator that streams rows incrementally as they land in Arrow. Quite explicitly in `client/sql/dataframe.go`:  func (df *dataFrameImpl) Collect() ([]Row, error) { responseClient, err := df.sparkSession.executePlan(df.createPlan()) if err != nil { return nil, fmt.Errorf(\"failed to execute plan: %w\", err) } var schema *StructType var allRows []Row for { response, err := responseClient.Recv() if err != nil { if errors.Is(err, io.EOF) { return allRows, nil } else { return nil, fmt.Errorf(\"failed to receive plan execution response: %w\", err) } } dataType := response.GetSchema() if dataType != nil { schema = convertProtoDataTypeToStructType(dataType) continue } arrowBatch := response.GetArrowBatch() if arrowBatch == nil { continue } rowBatch, err := readArrowBatchData(arrowBatch.Data, schema) if err != nil { return nil, err } if allRows == nil { allRows = make([]Row, 0, len(rowBatch)) } allRows = append(allRows, rowBatch...) } return allRows, nil }  The parts are already there for us to define a clear `ToLocalIterator()` implementation., I'd probably avoid using channels so call-site can determine themselves whether they want the behaviour to be asynchronous and implement something like this that tries to gracefully handle the Arrow resources OOTB:  package sql import ( \"errors\" \"fmt\" \"io\" ) // RowIterator streams rows batch\u2011by\u2011batch, keeping only the // current batch in memory (i guess what Spark\u2019s Scala // `toLocalIterator()` does under the hood). // //\u00a0 \u00a0\u2022 At any time we hold one `[]Row` converted via the existing //\u00a0 \u00a0 \u00a0`readArrowBatchData` helper. //\u00a0 \u00a0\u2022 When the slice is exhausted we fetch the next Arrow batch //\u00a0 \u00a0 \u00a0and replace it. Whatever row we do return is denoted by the cursor of the current []Row. type RowIterator interface { \u00a0 \u00a0 HasNext() bool \u00a0 \u00a0 Next() (Row, error) \u00a0 \u00a0 Close() error } type rowIteratorImpl struct { \u00a0 \u00a0 responseClient proto.SparkConnectService_ExecutePlanClient \u00a0 \u00a0 schema\u00a0 \u00a0 \u00a0 \u00a0 \u00a0*StructType \u00a0 \u00a0 \u00a0 batch\u00a0 \u00a0[]Row // current batch \u00a0 \u00a0 rowIdx\u00a0 int\u00a0 \u00a0// index inside current batch \u00a0 \u00a0 exhausted bool } func (it *rowIteratorImpl) HasNext() bool { if it.exhausted { \u00a0 \u00a0 \u00a0 \u00a0 return false \u00a0 \u00a0 } if it.rowIdx < len(it.batch) { \u00a0 \u00a0 \u00a0 \u00a0 return true \u00a0 \u00a0 } // current batch consumed -> try to pull next batch return it.fetchNextBatch() == nil && len(it.batch) > 0 } func (it *rowIteratorImpl) Next() (Row, error) { if !it.HasNext() { \u00a0 \u00a0 \u00a0 \u00a0 return nil, io.EOF \u00a0 \u00a0 } r := it.batch[it.rowIdx] it.rowIdx++ return r, nil } func (it *rowIteratorImpl) Close() error { \u00a0 \u00a0 return it.responseClient.CloseSend() } // fetchNextBatch reads Arrow messages until we convert a non\u2011empty batch. func (it *rowIteratorImpl) fetchNextBatch() error { it.batch = nil it.rowIdx = 0 for { resp, err := it.responseClient.Recv() if err != nil { if errors.Is(err, io.EOF) { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 it.exhausted = true \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return io.EOF \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 } return fmt.Errorf(\"receive execute\u2011plan response: %w\", err) } // Schema message comes first (once) if sch := resp.GetSchema(); sch != nil && it.schema == nil { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 it.schema = convertProtoDataTypeToStructType(sch) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue \u00a0 \u00a0 \u00a0 \u00a0 } // Arrow batch if batch := resp.GetArrowBatch(); batch != nil { rows, err := readArrowBatchData(batch.Data, it.schema) if err != nil { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 } // Skip empty batches just in case if len(rows) == 0 { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 } it.batch = rows return nil } } } // ToLocalIterator returns a streaming RowIterator for the DataFrame. func (df *dataFrameImpl) ToLocalIterator() (RowIterator, error) { client, err := df.sparkSession.executePlan(df.createPlan()) if err != nil { \u00a0 \u00a0 \u00a0 \u00a0 return nil, fmt.Errorf(\"execute plan: %w\", err) \u00a0 \u00a0 } return &rowIteratorImpl\\{responseClient: client}, nil }  I'm not sold on the `ToLocalIterator` naming convention, because it obfuscates the idea that we strictly avoid pulling all the data into memory before enumerating it, which Arrow helps with a great deal, I'd prefer `ToStreamingIterator`.", "status": "Open", "priority": "Major", "reporter": "Callum Dempsey Leach", "assignee": null, "created": "2025-07-12T16:05:50.000+0000", "updated": "2025-10-22T22:25:07.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [], "derived_tasks": {"summary": "Expose Local Iterator in Spark Connect Go Client (For Streaming Rows) - The current Spark Connect Go client implementation fetches all DataFrame ro...", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "SPARK-52765", "title": "Use StandardCharsets.UTF_8 instead of Charsets.UTF_8", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-07-11T03:37:40.000+0000", "updated": "2025-10-24T15:27:11.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 51449 [https://github.com/apache/spark/pull/51449]", "created": "2025-07-11T08:59:46.143+0000"}], "derived_tasks": {"summary": "Use StandardCharsets.UTF_8 instead of Charsets.UTF_8", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52639", "title": "Upgrade Volcano to 1.12.1", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-07-01T19:48:53.000+0000", "updated": "2025-10-24T15:14:51.000+0000", "labels": ["pull-request-available"], "components": ["Documentation", "Kubernetes", "Project Infra"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51343 [https://github.com/apache/spark/pull/51343]", "created": "2025-07-01T23:38:30.713+0000"}], "derived_tasks": {"summary": "Upgrade Volcano to 1.12.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52636", "title": "Support `java_image_name` ARG in K8s `Dockerfile`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-07-01T17:08:00.000+0000", "updated": "2025-10-24T15:27:49.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51338 [https://github.com/apache/spark/pull/51338]", "created": "2025-07-01T19:44:40.581+0000"}], "derived_tasks": {"summary": "Support `java_image_name` ARG in K8s `Dockerfile`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52604", "title": "Release Spark Kubernetes Operator 0.5.0 (v1)", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-06-28T18:25:19.000+0000", "updated": "2025-10-24T15:22:32.000+0000", "labels": [], "components": ["Kubernetes"], "comments": [], "derived_tasks": {"summary": "Release Spark Kubernetes Operator 0.5.0 (v1)", "classifications": ["umbrella"], "qa_pairs": []}}
{"id": "SPARK-52598", "title": "Reorganize docs for Spark Connect", "description": "", "status": "Open", "priority": "Minor", "reporter": "Nicholas Chammas", "assignee": null, "created": "2025-06-27T15:55:07.000+0000", "updated": "2025-10-25T00:26:56.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "Documentation"], "comments": [], "derived_tasks": {"summary": "Reorganize docs for Spark Connect", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-52578", "title": "Add metrics for rows to track case and action in MergeRowsExec", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Szehon Ho", "assignee": "Szehon Ho", "created": "2025-06-25T22:23:16.000+0000", "updated": "2025-10-23T23:49:55.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51285 [https://github.com/apache/spark/pull/51285]", "created": "2025-06-30T14:29:21.819+0000"}, {"author": "ci-cassandra.apache.org", "body": "User 'szehon-ho' has created a pull request for this issue: https://github.com/apache/spark/pull/51285", "created": "2025-06-30T14:29:24.774+0000"}], "derived_tasks": {"summary": "Add metrics for rows to track case and action in MergeRowsExec", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-52544", "title": "Allow configuring Json datasource string length limit through SQLConf", "description": "In https://issues.apache.org/jira/browse/SPARK-43263, Spark upgraded Jackson to 2.15.0. This brought in a breaking change that set string max length to 20,000 which Json data source picked up. Before the upgrade, there was no limit. The limit could only be modified through Json option \"maxStringLen\". This ticket attempts to restore the old no limit behavior, by introducing a new JSON_MAX_STRING_LENGTH feature flag.", "status": "Open", "priority": "Major", "reporter": "Tianhan Hu", "assignee": null, "created": "2025-06-20T17:28:18.000+0000", "updated": "2025-10-20T03:22:32.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Anastasia Filippova", "body": "User 'tianhanhu' has created a pull request for this issue: https://github.com/apache/spark/pull/51235", "created": "2025-07-07T17:06:17.675+0000"}], "derived_tasks": {"summary": "Allow configuring Json datasource string length limit through SQLConf - In https://issues", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-52527", "title": "Upgrade `junit` to 5.13.1", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-06-18T16:28:59.000+0000", "updated": "2025-10-24T15:36:41.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51217 [https://github.com/apache/spark/pull/51217]", "created": "2025-06-18T18:46:14.578+0000"}], "derived_tasks": {"summary": "Upgrade `junit` to 5.13.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52509", "title": "Fallback storage accumulates removed shuffle data", "description": "The fallback storage removes migrated shuffle data on Spark context shutdown. Ideally, it should remove individual shuffles once they get removed from the Spark context. Otherwise, all shuffle data ever migrated to the fallback storage accumulate on the remote storage until the Spark context shuts down. For long running jobs with a lot of decommissioning, this can be orders of magnitude more data than what is actively been used / usable / referenced.", "status": "Resolved", "priority": "Major", "reporter": "Enrico Minack", "assignee": "Enrico Minack", "created": "2025-06-17T08:55:17.000+0000", "updated": "2025-10-25T05:08:43.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Subham Singhal", "body": "[~enricomi] Do we have to create a custom Spark event(UnregisterShuffle) and add a listener which will remove shuffleId from fallback storage? I am willing to contribte but need guidance.", "created": "2025-07-05T14:18:33.905+0000"}, {"author": "Enrico Minack", "body": "This is fixed in [https://github.com/apache/spark/pull/51199.] Looks like the PR did not get linked to the issue. Spark already exists the infrastructure to notify the fallback storage to remove shuffles, the message {{RemoveShuffle}} was just not handled.", "created": "2025-07-07T20:52:40.827+0000"}, {"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51199 [https://github.com/apache/spark/pull/51199]", "created": "2025-10-25T05:08:01.120+0000"}], "derived_tasks": {"summary": "Fallback storage accumulates removed shuffle data - The fallback storage removes migrated shuffle data on Spark context shutdown", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52495", "title": "Include partition columns in the single variant column", "description": "", "status": "Open", "priority": "Major", "reporter": "Xiaonan Yang", "assignee": "Xiaonan Yang", "created": "2025-06-16T19:33:28.000+0000", "updated": "2025-10-20T00:29:48.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Anastasia Filippova", "body": "User 'xiaonanyang-db' has created a pull request for this issue: https://github.com/apache/spark/pull/51206", "created": "2025-07-08T03:38:08.526+0000"}], "derived_tasks": {"summary": "Include partition columns in the single variant column", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52457", "title": "ParseToDate/ParseToTimestamp can return incorrect value for TimestampNTZ", "description": "set spark.sql.session.timeZone = America/Los_Angeles; select to_timestamp(timestamp_ntz'2000-09-01 01:00:00', '')  This returns `2000-08-31`, when it should return `2000-09-01`. This is because `ParseToDate` and `ParseToTimestamp` do not handle `TimestampNTZ` when creating `GetTimestamp`, which runs into issues when `GetTimestamp` is evaluated and then casted to date.", "status": "Open", "priority": "Major", "reporter": "Kelvin Jiang", "assignee": null, "created": "2025-06-11T23:54:47.000+0000", "updated": "2025-10-23T00:27:20.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "SeongJaeGong", "body": "i'm working on it", "created": "2025-07-13T04:43:02.731+0000"}], "derived_tasks": {"summary": "ParseToDate/ParseToTimestamp can return incorrect value for TimestampNTZ - set spark", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-52389", "title": "Drop K8s v1.30 Support", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-06-03T17:13:52.000+0000", "updated": "2025-10-24T15:13:10.000+0000", "labels": ["pull-request-available"], "components": ["Documentation", "Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51074 [https://github.com/apache/spark/pull/51074]", "created": "2025-06-03T20:14:42.539+0000"}], "derived_tasks": {"summary": "Drop K8s v1.30 Support", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52314", "title": "Upgrade kubernetes-client to 7.3.1", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "William Hyun", "assignee": "William Hyun", "created": "2025-05-26T21:06:21.000+0000", "updated": "2025-10-24T15:18:58.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 51024 [https://github.com/apache/spark/pull/51024]", "created": "2025-05-26T22:26:13.003+0000"}], "derived_tasks": {"summary": "Upgrade kubernetes-client to 7.3.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52213", "title": "Upgrade kubernetes-client to 7.3.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-05-19T05:09:58.000+0000", "updated": "2025-10-24T15:14:00.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 50730 [https://github.com/apache/spark/pull/50730]", "created": "2025-05-19T15:59:18.629+0000"}, {"author": "Hyukjin Kwon", "body": "Reverted in https://github.com/apache/spark/commit/14fca6551e393cc60a7bb23a1c4c8237f8e0fb08", "created": "2025-05-20T01:24:44.930+0000"}, {"author": "Yang Jie", "body": "Issue resolved by pull request 50730 [https://github.com/apache/spark/pull/50730]", "created": "2025-05-21T07:05:09.586+0000"}], "derived_tasks": {"summary": "Upgrade kubernetes-client to 7.3.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52193", "title": "Add `Spark Connect` port to Spark Driver pod and service", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-05-16T19:21:14.000+0000", "updated": "2025-10-24T15:13:36.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 50925 [https://github.com/apache/spark/pull/50925]", "created": "2025-05-16T21:37:47.339+0000"}], "derived_tasks": {"summary": "Add `Spark Connect` port to Spark Driver pod and service", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-52181", "title": "Increase variant size limit to 128MiB", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Chenhao Li", "assignee": "Chenhao Li", "created": "2025-05-16T04:18:42.000+0000", "updated": "2025-10-24T16:22:34.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Wenchen Fan", "body": "Issue resolved by pull request 50927 [https://github.com/apache/spark/pull/50927]", "created": "2025-05-17T06:25:41.490+0000"}], "derived_tasks": {"summary": "Increase variant size limit to 128MiB", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-52163", "title": "Spark3.4+ LogicalRDD stats estimation may throw bigInt.reportOverflow", "description": "Due to updates in LogicalRDD stats calculation [SPARK-39748[SQL][SS] Include the origin logical plan for LogicalRDD if it comes from DataFrame by HeartSaVioR \u00b7 Pull Request #37161 \u00b7 apache/spark|https://github.com/apache/spark/pull/37161/files#diff-aa0d83f9a15ee1a818569d9cfee343dcd09de7e5e5a539719afcb039aee6aafa] LogicalRDD sizeInBytes can explode and cause bigInteger overflow  Stack Trace: java.base/java.math.BigInteger.reportOverflow(BigInteger.java:1153), java.base/java.math.BigInteger.multiply(BigInteger.java:1658), java.base/java.math.BigInteger.multiply(BigInteger.java:1566), scala.math.BigInt.$times(BigInt.scala:196), scala.math.Numeric$BigIntIsIntegral.times(Numeric.scala:37), scala.math.Numeric$BigIntIsIntegral.times$(Numeric.scala:37), scala.math.Numeric$BigIntIsIntegral$.times(Numeric.scala:47), scala.math.Numeric$BigIntIsIntegral$.times(Numeric.scala:47), scala.collection.TraversableOnce.$anonfun$product$1(TraversableOnce.scala:264), scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196), scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194), scala.collection.Iterator.foreach(Iterator.scala:943), scala.collection.Iterator.foreach$(Iterator.scala:943), scala.collection.AbstractIterator.foreach(Iterator.scala:1431), scala.collection.IterableLike.foreach(IterableLike.scala:74), scala.collection.IterableLike.foreach$(IterableLike.scala:73), scala.collection.AbstractIterable.foreach(Iterable.scala:56), scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199), scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192), scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108), scala.collection.TraversableOnce.product(TraversableOnce.scala:264), scala.collection.TraversableOnce.product$(TraversableOnce.scala:264), scala.collection.AbstractTraversable.product(Traversable.scala:108), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:58), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitJoin(SizeInBytesOnlyStatsPlanVisitor.scala:124), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitJoin(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit(LogicalPlanVisitor.scala:35), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit$(LogicalPlanVisitor.scala:25), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.$anonfun$stats$1(LogicalPlanStats.scala:37), scala.Option.getOrElse(Option.scala:189), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats$(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:31), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitUnaryNode(SizeInBytesOnlyStatsPlanVisitor.scala:40), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitProject(SizeInBytesOnlyStatsPlanVisitor.scala:149), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitProject(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit(LogicalPlanVisitor.scala:38), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit$(LogicalPlanVisitor.scala:25), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.$anonfun$stats$1(LogicalPlanStats.scala:37), scala.Option.getOrElse(Option.scala:189), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats$(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:31), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitUnaryNode(SizeInBytesOnlyStatsPlanVisitor.scala:40), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitAggregate(SizeInBytesOnlyStatsPlanVisitor.scala:67), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visitAggregate(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit(LogicalPlanVisitor.scala:26), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit$(LogicalPlanVisitor.scala:25), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.$anonfun$stats$1(LogicalPlanStats.scala:37), scala.Option.getOrElse(Option.scala:189), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats$(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:31), org.apache.spark.sql.execution.adaptive.LogicalQueryStage.$anonfun$computeStats$3(LogicalQueryStage.scala:70), scala.Option.getOrElse(Option.scala:189), org.apache.spark.sql.execution.adaptive.LogicalQueryStage.computeStats(LogicalQueryStage.scala:70), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:56), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit(LogicalPlanVisitor.scala:49), org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit$(LogicalPlanVisitor.scala:25), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit(SizeInBytesOnlyStatsPlanVisitor.scala:28), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.$anonfun$stats$1(LogicalPlanStats.scala:37), scala.Option.getOrElse(Option.scala:189), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats$(LogicalPlanStats.scala:33), org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:31), org.apache.spark.sql.execution.adaptive.LogicalQueryStage.maxRows(LogicalQueryStage.scala:73), org.apache.spark.sql.catalyst.plans.logical.Union.$anonfun$maxRows$3(basicLogicalOperators.scala:420), org.apache.spark.sql.catalyst.plans.logical.Union.$anonfun$maxRows$3$adapted(basicLogicalOperators.scala:419), scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62), scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55), scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49), org.apache.spark.sql.catalyst.plans.logical.Union.maxRows(basicLogicalOperators.scala:419), org.apache.spark.sql.catalyst.optimizer.OptimizeOneRowPlan$$anonfun$apply$2.applyOrElse(OptimizeOneRowPlan.scala:42), org.apache.spark.sql.catalyst.optimizer.OptimizeOneRowPlan$$anonfun$apply$2.applyOrElse(OptimizeOneRowPlan.scala:37), org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:566), org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104), org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:566), org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:31), org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:279), org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:275), org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:31), org.apache.spark.sql.catalyst.optimizer.OptimizeOneRowPlan$.apply(OptimizeOneRowPlan.scala:37), org.apache.spark.sql.catalyst.optimizer.OptimizeOneRowPlan$.apply(OptimizeOneRowPlan.scala:35), org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222), scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60), scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68), scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38), org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219), org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211), scala.collection.immutable.List.foreach(List.scala:431), org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211), org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.reOptimize(AdaptiveSparkPlanExec.scala:664), org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:318), org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827), org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:242), org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:387), org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:372), org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195), org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246), org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151), org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243), org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191), org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:697), org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206), org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526), org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204), org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118), org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195), org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103), org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827), org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65), org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204), org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:696), org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:659)eholder  Minimal example:  import org.apache.spark.sql.functions.col import org.apache.spark.sql.{Row, SparkSession} import org.apache.spark.sql.types._// Define your schema val schema = StructType(Seq( StructField(\"id\", IntegerType, nullable = true), StructField(\"name\", StringType, nullable = true) ))// Create empty RDD[Row] val emptyRDD = spark.sparkContext.emptyRDD[Row]// Create empty DataFrame var emptyDF = spark.createDataFrame(emptyRDD, schema) sc.setCheckpointDir(\"dbfs:/tmp/rdd-checkpoints\")try { for (i <- 1 to 10) { emptyDF = emptyDF.select(col(\"id\")) emptyDF = emptyDF.join(emptyDF, Seq(\"id\")) emptyDF = emptyDF.select(col(\"id\")) if (i % 2 == 0) { println(\"Running explain cost\") emptyDF = emptyDF.checkpoint() // to truncate lineage emptyDF.explain(\"cost\") } } } catch { case e: Exception => println(s\"Exception caught during execution: ${e.getMessage}\") e.printStackTrace() }  You can quickly see stats exploding: == Optimized Logical Plan == LogicalRDD [id#104207|#104207], false, Statistics(sizeInBytes=7.12E+17879 B, ColumnStat: N/A) In ~30 iterations number will be so big that digits won't fit into Integer.MAX_VALUE sized array and job will fail with exception provided in the beginning In Spark 3.3.2 that would yield on each iteration: == Optimized Logical Plan == LogicalRDD [id#2|#2], false, Statistics(sizeInBytes=8.0 EiB)", "status": "Open", "priority": "Major", "reporter": "Igor Railean", "assignee": null, "created": "2025-05-15T17:23:02.000+0000", "updated": "2025-10-21T12:41:07.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "derived_tasks": {"summary": "Spark3.4+ LogicalRDD stats estimation may throw bigInt.reportOverflow - Due to updates in LogicalRDD stats calculation [SPARK-39748[SQL][SS] Includ...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-52012", "title": "Restore IDE Index with type annotations", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "created": "2025-05-06T03:43:16.000+0000", "updated": "2025-10-24T22:36:44.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "Spark Core", "SQL"], "comments": [], "derived_tasks": {"summary": "Restore IDE Index with type annotations", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-52011", "title": "Reduce HDFS NameNode RPC on vectorized Parquet reader", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-05-06T02:20:11.000+0000", "updated": "2025-10-24T22:35:02.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Dongjoon Hyun", "body": "Hi, [~chengpan]. Do you have more subtasks for this umbrella JIRA issue?", "created": "2025-10-24T22:35:02.993+0000"}], "derived_tasks": {"summary": "Reduce HDFS NameNode RPC on vectorized Parquet reader", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-51973", "title": "Upgrade `kubernetes-client` to 7.2.0 for K8s 1.33", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-05-01T12:36:43.000+0000", "updated": "2025-10-24T15:13:49.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Kubernetes"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 50775 [https://github.com/apache/spark/pull/50775]", "created": "2025-05-01T15:25:21.976+0000"}], "derived_tasks": {"summary": "Upgrade `kubernetes-client` to 7.2.0 for K8s 1.33", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51883", "title": "Python Data Source filter pushdown docs", "description": "", "status": "Open", "priority": "Major", "reporter": "Haoyu Weng", "assignee": null, "created": "2025-04-23T17:15:04.000+0000", "updated": "2025-10-19T00:30:59.000+0000", "labels": ["pull-request-available"], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "Python Data Source filter pushdown docs", "classifications": ["documentation"], "qa_pairs": []}}
{"id": "SPARK-51760", "title": "Upgrade ASM to 9.8", "description": "h3. 29 March 2025: ASM 9.8 (tag ASM_9_8) * new Opcodes.V25 constant for Java 25 * bug fixes ** Fix one more copy operation on DUP2 ** 318015: Valid bytecode for jvm, but failed to pass the CheckClassAdapter. ** `ASMifier` should print calls to `valueOf` instead of deprecated constructors of primitive wrappers", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-04-10T06:53:34.000+0000", "updated": "2025-10-20T21:28:08.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 50543 [https://github.com/apache/spark/pull/50543]", "created": "2025-04-10T08:32:34.655+0000"}], "derived_tasks": {"summary": "Upgrade ASM to 9.8", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51742", "title": "Upgrade snowflake-jdbc to 3.23.2", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-04-08T03:42:14.000+0000", "updated": "2025-10-24T15:42:08.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 50535 [https://github.com/apache/spark/pull/50535]", "created": "2025-04-08T09:03:52.489+0000"}], "derived_tasks": {"summary": "Upgrade snowflake-jdbc to 3.23.2", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51658", "title": "SPIP: Add geospatial types in Spark", "description": "*Q1. What are you trying to do? Articulate your objectives using absolutely no jargon.* Add two new data types to Spark SQL and PySpark, GEOMETRY and GEOGRAPHY, for handling location and shape data, specifically: # GEOMETRY: For working with shapes on a Cartesian space (a flat surface, like a paper map) # GEOGRAPHY: For working with shapes on an ellipsoidal surface (like the Earth's surface) These will let Spark users work with standard shape types like: * POINTS, MULTIPOINTS * LINESTRINGS, MULTILINESTRINGS * POLYGONS, MULTIPOLYGONS * GEOMETRY COLLECTIONS These are [standard shape types|https://portal.ogc.org/files/?artifact_id=25355] as defined by the [Open Geospatial Consortium (OGC)|https://www.ogc.org/]. OGC is the international governing body for standardizing Geographic Information Systems (GIS). New SQL types will be parametrized with an integer value (Spatial Reference IDentifier - SRID) that defines the underlying coordinate reference system that geospatial values live in (like GPS coordinates vs. local measurements). Popular data storage formats ([Parquet|https://github.com/apache/parquet-format/blob/master/Geospatial.md] and [Iceberg|https://github.com/apache/iceberg/pull/10981]) are adding support for GEOMETRY and GEOGRAPHY, and the proposal is to extend Spark such that it can read / write and operate with new geo types. *Q2. What problem is this proposal NOT designed to solve?* Build a comprehensive system for advanced geospatial analysis in Spark. While we're adding basic support for storing and reading location/shape data, we are not: * Creating complex geospatial processing functions * Building spatial analysis tools * Developing geometric or geographic calculations and transformations This proposal is laying the foundation - building the infrastructure to handle geospatial data, but not creating a full-featured geospatial processing system. Such extension can be done in existing frameworks like Apache Sedona (a well-known Apache project that extends Spark to add support for geospatial processing), or can be done later as a separate improvement. *Q3. How is it done today, and what are the limits of current practice?* Current State: # Users store geospatial data by converting it into basic text (string) or binary formats because Spark doesn't understand geospatial data directly. # To work with this data, users need to use external tools to make sense of and process the data. Limitations: * Have to use external libs * Have to do conversions * Cannot write / read geospatial values in a native way * External tools cannot natively understand the data as geospatial * No file / data skipping *Q4. What is new in your approach and why do you think it will be successful?* The high-level approach is not new, and we have a clear picture of how to split the work by sub-tasks based on our experience of adding new types such as ANSI intervals and TIMESTAMP_NTZ. Most existing data processing systems support working with Geospatial data. *Q5. Who cares? If you are successful, what difference will it make?* New types create the foundation for working with geospatial data on Spark. Other data analytics systems such as PostgreSQL, Redshift, Snowflake, Big Query, all have geospatial support. Spark stays relevant and compatible with the latest Parquet and Iceberg community developments.\u00a0Projects like Apache Sedona can capitalize on the work proposed in this document to benefit from built-in types and storage support. *Q6. What are the risks?* The addition of the new data types will allow for strictly new workloads, so the risks in this direction are minimal. The only overlap with existing functionality is at the type system level (e.g., casts between the new geospatial types and the existing Spark SQL types). The risk is low however, and can be handled through testing. *Q7. How long will it take?* In total it might take around 9 months. The estimation is based on similar tasks: ANSI intervals (SPARK-27790) and TIMESTAMP_NTZ (SPARK-35662). We can split the work by function blocks: # Base functionality - 2 weeks Add new type geospatial types, literals, type constructor, and external types. # Persistence - 2.5 months Ability to create parquet tables of the type GEOSPATIAL, read/write from/to Parquet and other built-in data types, stats, predicate push down. # Basic data skipping operator - 1 months # Implement rudimentary operators for data skipping - (e.g., ST_BoxIntersects - checks if the bounding boxes of two geospatial values intersect each other) # Clients support - 1 month JDBC, Hive, Thrift server, connect # PySpark integration - 1 month DataFrame support, pandas API, python UDFs, Arrow column vectors # Docs + testing/benchmarking - 1 month *Q8. What are the mid-term and final \u201cexams\u201d to check for success?* Mid-term criteria: New type definitions and read/write geospatial data from Parquet. Final criteria: Equivalent functionality with any other scalar data type. *Appendix A. Proposed API Changes.* h4. _Geospatial data types_ We propose to support two new parametrized data types: GEOMETRY and GEOGRAPHY. Both take as input a non-negative integer value called SRID (referring to a spatial reference identifier). See Appendix B for more details on the type system. Examples: ||Syntax||Comments|| |GEOMETRY(0)|Materializable, every row has SRID 0| |GEOMETRY(ANY)|Not materializable, every row can have different SRID| |GEOGRAPHY(4326)\u00a0|Materializable, every row has SRID 4326| |GEOGRAPHY(ANY)|Not materializable, every row can have different SRID| GEOMETRY(ANY) and GEOGRAPHY(ANY) act as least common types among the parametrized GEOMETRY and GEOGRAPHY types. h4. _Table creation_ Users will be able to create GEOMETRY(<srid>) or GEOGRAPHY(<srid>) columns:  CREATE TABLE tbl (geom GEOMETRY(0), geog GEOGRAPHY(4326));  Users will not be able to create GEOMETRY(ANY) or GEOGRAPHY(ANY) columns. See details in Appendix B. Users will be able to insert geospatial values to such columns:  INSERT INTO tbl VALUES(X\u20180101000000000000000000f03f0000000000000040\u2019::GEOMETRY(0), X\u20190101000000000000000000f03f0000000000000040\u2019::GEOGRAPHY(4326));  h4. _Casts_ The following explicit casts as allowed (all other casts are initially disallowed): * GEOMETRY/GEOGRAPHY column to BINARY (output is WKB format) * BINARY to GEOMETRY/GEOGRAPHY column (input is expected to be in WKB format) * GEOMETRY(ANY) to GEOMETRY(<srid>) * GEOGRAPHY(ANY) to GEOGRAPHY(<srid>) The following implicit casts are allowed (also supported as explicit): * GEOMETRY(<srid>) to GEOMETRY(ANY) * GEOGRAPHY(<srid>) to GEOGRAPHY(ANY) h4. _Geospatial expressions_ The following geospatial expressions are to be exposed to users: * Scalar expressions ** Boolean ST_BoxesIntersect(BoxStruct1, BoxStruct2) ** BoxStruct ST_Box2D(GeoVal) ** BinaryVal ST_AsBinary(GeoVal) ** GeoVal ST_GeomFromWKB(BinaryVal, IntVal) ** GeoVal ST_GeogFromWKB(BinaryVal, IntVal) * Aggregate expressions ** BoxStruct ST_Extent(GeoCol) BoxStruct above refers to a struct of 4 double values. h4. _Ecosystem_ Support for SQL, Dataframe, and PySpark APIs including Spark Connect APIs. *Appendix B.* h4. _Type System_ We propose to introduce two parametrized types: GEOMETRY and GEOGRAPHY. The parameter would either be a non-negative integer value or the special specifier ANY. * The integer value represents a Spatial Reference IDentifier (SRID) which uniquely defines the coordinate reference system. These integers will be mapped to coordinate reference system (CRS) definitions defined by authorities like OGC, EPSG, or ESRI. * The ANY specifier refers to GEOMETRY or GEOGRAPHY columns where the SRID value can be different across the column rows. GEOMETRY(ANY) and GEOGRAPHY(ANY) will not be materialized. This constraint is imposed by the storage specifications (the Parquet and Iceberg specs require a unique SRID value per GEOMETRY or GEOGRAPHY column). * GEOMETRY(ANY) and GEOGRAPHY(ANY) act as the least common type across all GEOMETRY and GEOGRAPHY parametrized types. They are also needed for the proposed ST_GeomFromWKB and ST_GeogFromWKB expressions when the second argument is not foldable; for non-foldable arguments the return type for these expressions is GEOMETRY(ANY) and GEOGRAPHY(ANY), respectively. * All known SRID values are positive integers. For the GEOMETRY data type we will also allow the SRID value of 0. This is the typical way of specifying geometric values with unknown or unspecified underlying CRS. Mathematically these values are understood as embedded in a Cartesian space. h4. _In-memory representation_ We propose to internally represent geospatial values as a byte array containing the concatenation of the BINARY value and an INTEGER value. The BINARY value is the WKB representation of the GEOMETRY or GEOGRAPHY value. The INTEGER value is the SRID value. WKB stands for Well-Known Binary and it is one of the standard formats for representing geospatial values (see [https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry#Well-known_binary]). WKB is also the format used by the latest Parquet and Iceberg specs for representing geospatial values. h4. _Serialization_ We propose to only serialize GEOMETRY and GEOGRAPHY columns that have a numeric SRID value. For such columns the SRID value can be deciphered from the column metadata. The serialization format for values will be WKB. h4. _Casting_ Implicit casting from GEOMETRY(<srid>) to GEOMETRY(ANY) and GEOGRAPHY(<srid>) to GEOGRAPHY(ANY) is primarily introduced in order to smoothly support expressions like CASE/WHEN, COALESCE, NVL2, etc., that depend on least common type logic. h4. _Dependencies_ We propose to add a build dependency on the Proj library (\u200b\u200b[https://proj.org/en/stable]) for generating a list of supported CRS definitions. We will need to expand this list with SRID 0 which is necessary when dealing with geometry inputs with unspecified SRID.", "status": "Open", "priority": "Major", "reporter": "Menelaos Karavelas", "assignee": null, "created": "2025-03-28T17:27:36.000+0000", "updated": "2025-10-20T13:11:16.000+0000", "labels": ["SPIP", "pull-request-available"], "components": ["SQL"], "comments": [{"author": "Menelaos Karavelas", "body": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0", "created": "2025-03-29T15:02:50.079+0000"}], "derived_tasks": {"summary": "SPIP: Add geospatial types in Spark", "classifications": ["feature", "new feature"], "qa_pairs": [{"question": "What are you trying to do?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "org/files/?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "What problem is this proposal NOT designed to solve?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "How is it done today, and what are the limits of current practice?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "What is new in your approach and why do you think it will be successful?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "Who cares?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "If you are successful, what difference will it make?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "What are the risks?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "How long will it take?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}, {"question": "What are the mid-term and final \u201cexams\u201d to check for success?", "answer": "Google doc for discussion and comments: https://docs.google.com/document/d/1cYSNPGh95OjnpS0k_KDHGM9Ae3j-_0Wnc_eGBZL4D3w/edit?tab=t.0"}]}}
{"id": "SPARK-51511", "title": "Upgrade apache-rat in the dev/check-license to 0.16.1", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-03-14T07:19:40.000+0000", "updated": "2025-10-24T15:49:18.000+0000", "labels": ["pull-request-available"], "components": ["Project Infra"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 50276 [https://github.com/apache/spark/pull/50276]", "created": "2025-03-14T17:41:36.778+0000"}], "derived_tasks": {"summary": "Upgrade apache-rat in the dev/check-license to 0.16.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51489", "title": "[M1] Represent SQL Scripts in Spark UI", "description": "We need to figure out a way to represent SQL Scripts in the Spark UI. To start, we need to be able to correlate single queries with scripts that executed them. Further work includes representing a table of scripts and basic script-level metrics.", "status": "Open", "priority": "Major", "reporter": "Dusan Tisma", "assignee": null, "created": "2025-03-12T17:40:25.000+0000", "updated": "2025-10-20T01:11:58.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [], "derived_tasks": {"summary": "[M1] Represent SQL Scripts in Spark UI - We need to figure out a way to represent SQL Scripts in the Spark UI", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51387", "title": "Upgrade Netty to 4.1.119.Final", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-03-04T15:16:21.000+0000", "updated": "2025-10-24T05:18:07.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 50150 [https://github.com/apache/spark/pull/50150]", "created": "2025-03-04T18:00:07.786+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.1.119.Final", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51224", "title": "Upgrade Maven to 4.0.0", "description": "", "status": "Open", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Cheng Pan", "created": "2025-02-15T23:52:56.000+0000", "updated": "2025-10-19T00:30:56.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Since you are working on this actively, I assigned this JIRA issue to you in advance as a recognition your work, [~chengpan]. I'm sure that we can deliver this in Apache Spark 4.1.0 timeframe successfully. Thank you for working on this.", "created": "2025-07-07T19:59:37.177+0000"}, {"author": "Cheng Pan", "body": "[~dongjoon] Ack, will keep eyes on the Maven 4.0 release process and do testing.", "created": "2025-07-08T02:34:55.839+0000"}], "derived_tasks": {"summary": "Upgrade Maven to 4.0.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51195", "title": "Upgrade `kubernetes-client` to 7.1.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Wei Guo", "assignee": "Wei Guo", "created": "2025-02-13T05:15:35.000+0000", "updated": "2025-10-24T15:15:59.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 49925 [https://github.com/apache/spark/pull/49925]", "created": "2025-02-13T07:39:07.911+0000"}, {"author": "Dongjoon Hyun", "body": "This is backported to branch-4.0 via https://github.com/apache/spark/commit/dcb3b448faf89376ccb1e0797f1dbfe22cfb25ef", "created": "2025-02-14T17:23:29.834+0000"}], "derived_tasks": {"summary": "Upgrade `kubernetes-client` to 7.1.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51194", "title": "Upgrade `scalafmt` to 3.8.6", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Wei Guo", "assignee": "Wei Guo", "created": "2025-02-13T05:02:41.000+0000", "updated": "2025-10-24T15:39:25.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 49924 [https://github.com/apache/spark/pull/49924]", "created": "2025-02-13T07:53:51.626+0000"}], "derived_tasks": {"summary": "Upgrade `scalafmt` to 3.8.6", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51171", "title": "Upgrade `checkstyle` to 10.21.2", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Wei Guo", "assignee": "Wei Guo", "created": "2025-02-12T01:58:49.000+0000", "updated": "2025-10-24T15:39:14.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 49899 [https://github.com/apache/spark/pull/49899]", "created": "2025-02-12T06:06:31.312+0000"}], "derived_tasks": {"summary": "Upgrade `checkstyle` to 10.21.2", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-51166", "title": "Prepare Apache Spark 4.1.0", "description": "This umbrella issue aims to include the subtask and links which are retargeted to Apache Spark 4.1.0 from Apache Spark 4.0.0. Apache Spark 4.1 is scheduled to occur after 6 months from Apache Spark 4.0.0 release. * [https://spark.apache.org/versioning-policy.html] ||Date||Event|| |2025-11-01|Code freeze. Release branch cut.| |2025-11-15|QA period. Focus on bug fixes, tests, stability and docs. Generally, no new features merged.| |2025-11-23|Release candidates (RC), voting, etc. until final release passes|", "status": "Open", "priority": "Critical", "reporter": "Dongjoon Hyun", "assignee": null, "created": "2025-02-11T21:53:17.000+0000", "updated": "2025-10-24T22:20:54.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [], "derived_tasks": {"summary": "Prepare Apache Spark 4.1.0 - This umbrella issue aims to include the subtask and links which are retargeted to Apache Spark 4", "classifications": ["umbrella"], "qa_pairs": []}}
{"id": "SPARK-51162", "title": "SPIP: Add the TIME data type", "description": "*Q1. What are you trying to do? Articulate your objectives using absolutely no jargon.* Add new data type *TIME* to Spark SQL which represents a time value with fields hour, minute, second, up to microseconds. All operations over the type are performed without taking any time zone into account. New data type should conform to the type *TIME\\(n\\) WITHOUT TIME ZONE* defined by the SQL standard where 0 <= n <= 6. *Q2. What problem is this proposal NOT designed to solve?* Don't support the TIME type with time zone defined by the SQL standard: {*}TIME\\(n\\) WITH TIME ZONE{*}. Also don't support TIME with local timezone. *Q3. How is it done today, and what are the limits of current practice?* The TIME type can be emulated via the TIMESTAMP_NTZ data type by setting the date part to the some constant value like 1970-01-01, 0001-01-01 or 0000-00-00 (though this is out of supported range of dates). Although the type can be emulation via TIMESTAMP_NTZ, Spark SQL cannot recognize it in data sources, and for instance cannot load the TIME values from parquet files. *Q4. What is new in your approach and why do you think it will be successful?* The approach is not new, and we have clear picture how to split the work by sub-tasks based on our experience of adding new types ANSI intervals and TIMESTAMP_NTZ. *Q5. Who cares? If you are successful, what difference will it make?* The new type simplifies migrations to Spark SQL from other DBMS like PostgreSQL, Snowflake, Google SQL, Amazon Redshift, Teradata, DB2. Such users don't have to rewrite their SQL code to emulate the TIME type. Also new functionality impacts on existing Spark SQL users who need to load data w/ the TIME values that were stored by other systems. *Q6. What are the risks?* Additional handling new type in operators, expression and data sources can cause performance regressions. Such risk can be compensated by developing time benchmarks in parallel with supporting new type in different places in Spark SQL. *Q7. How long will it take?* In total it might take around {*}9 months{*}. The estimation is based on similar tasks: ANSI intervals (SPARK-27790) and TIMESTAMP_NTZ (SPARK-35662). We can split the work by function blocks: # Base functionality - *3 weeks* Add new type TimeType, forming/parsing time literals, type constructor, and external types. # Persistence - *3.5 months* Ability to create tables of the type TIME, read/write from/to Parquet and other built-in data types, partitioning, stats, predicate push down. # Time operators - *2 months* Arithmetic ops, field extract, sorting, and aggregations. # Clients support - *1 month* JDBC, Hive, Thrift server, connect # PySpark integration - *1 month* DataFrame support, pandas API, python UDFs, Arrow column vectors # Docs + testing/benchmarking - *1 month* *Q8. What are the mid-term and final \u201cexams\u201d to check for success?* The mid-term is in 4 month: basic functionality, read/write new type to built-in datasources, basic time operations such as arithmetic ops, casting. The final \"exams\" is to support the same functionality as other time types: TIMESTAMP_NTZ, DATE, TIMESTAMP. *Appendix A. Proposed API Changes.* Add new case class *TimeType* to {_}org.apache.spark.sql.types{_}:  /** * The time type represents a time value with fields hour, minute, second, up to microseconds. * The range of times supported is 00:00:00.000000 to 23:59:59.999999. * * Please use the singleton `DataTypes.TimeType` to refer the type. */ class TimeType(precisionField: Byte) extends DatetimeType { /** * The default size of a value of the TimeType is 8 bytes. */ override def defaultSize: Int = 8 private[spark] override def asNullable: DateType = this }  *Appendix B:* As the external types for the new TIME type, we propose: - Java/Scala: [java.time.LocalTime|https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/time/LocalTime.html] - PySpark: [time|https://docs.python.org/3/library/datetime.html#time-objects]", "status": "Open", "priority": "Major", "reporter": "Max Gekk", "assignee": "Max Gekk", "created": "2025-02-11T13:46:09.000+0000", "updated": "2025-10-24T14:53:53.000+0000", "labels": ["SPIP", "pull-request-available", "releasenotes"], "components": ["SQL"], "comments": [{"author": "Serge Rielau", "body": "Also out of scope: WITH LOCAL TIMEZONE", "created": "2025-02-11T17:15:42.716+0000"}, {"author": "Dongjoon Hyun", "body": "I added the label, `releasenotes`, not to forget this important feature in the release note.", "created": "2025-07-25T16:32:56.978+0000"}], "derived_tasks": {"summary": "SPIP: Add the TIME data type", "classifications": ["feature", "new feature"], "qa_pairs": [{"question": "What are you trying to do?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "What problem is this proposal NOT designed to solve?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "How is it done today, and what are the limits of current practice?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "What is new in your approach and why do you think it will be successful?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "Who cares?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "If you are successful, what difference will it make?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "What are the risks?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "How long will it take?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}, {"question": "What are the mid-term and final \u201cexams\u201d to check for success?", "answer": "Also out of scope: WITH LOCAL TIMEZONE"}]}}
{"id": "SPARK-51145", "title": "Upgrade `mysql-connector-j`, `MySQL` docker image to 9.2.0 and `MariaDB` docker image to 11.4.5", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Wei Guo", "assignee": "Wei Guo", "created": "2025-02-10T12:14:03.000+0000", "updated": "2025-10-24T15:39:30.000+0000", "labels": ["pull-request-available"], "components": ["Build", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 49864 [https://github.com/apache/spark/pull/49864]", "created": "2025-02-10T20:25:24.877+0000"}], "derived_tasks": {"summary": "Upgrade `mysql-connector-j`, `MySQL` docker image to 9.2.0 and `MariaDB` docker image to 11.4.5", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-50205", "title": "Re-enable `SparkSessionJobTaggingAndCancellationSuite.Cancellation APIs in SparkSession are isolated`", "description": "https://github.com/apache/spark/actions/runs/10915451051/job/30295259985 This test case needs a refactor to use only 2 threads instead of 3, because having 3 threads is not guaranteed in CI.", "status": "Resolved", "priority": "Critical", "reporter": "Pengfei Xu", "assignee": "Kousuke Saruta", "created": "2024-11-01T14:17:17.000+0000", "updated": "2025-10-23T18:33:54.000+0000", "labels": ["pull-request-available"], "components": ["SQL", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "I moved this to 4.1.0.", "created": "2025-02-12T15:52:40.924+0000"}, {"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52704 [https://github.com/apache/spark/pull/52704]", "created": "2025-10-23T18:33:43.366+0000"}], "derived_tasks": {"summary": "Re-enable `SparkSessionJobTaggingAndCancellationSuite.Cancellation APIs in SparkSession are isolated` - https://github", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-49524", "title": "Improve K8s support", "description": "", "status": "Closed", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2024-09-05T15:47:12.000+0000", "updated": "2025-10-24T15:29:18.000+0000", "labels": [], "components": ["Build", "Kubernetes"], "comments": [], "derived_tasks": {"summary": "Improve K8s support", "classifications": ["umbrella", "improvement"], "qa_pairs": []}}
{"id": "SPARK-47836", "title": "Performance problem with QuantileSummaries", "description": "SPARK-29336 caused a severe performance regression. In practice a partial_aggregate with several approx_percentile calls ran less than hour and the final aggrergation after exchange would have taken over a week. Simple percentile ran about the same time in the first part and the final aggregate ran very quickly. I made a benchmark, and it reveals that the merge operation is very-very slow: https://github.com/tanelk/spark/commit/3b16f429a77b10003572295f42361fbfb2f3c63e From my experiments it looks like it is n^2 with the number of partitions (number of partial aggregations to merge). When I reverted the changes made in this PR, then the \"Only insert\" and \"Insert & merge\" were very similar. The cause seems to be, that compressImmut does not reduce the number samples allmost at all after merges and just keeps iterating over an evergrowing list. I was not able to figure out how to fix the issue without just reverting the PR. I also added a benchmark with KllDoublesSketch from the apache datasketches project and it worked even better than this class before this PR. Only downside was that it is not-deterministic.", "status": "Open", "priority": "Major", "reporter": "Tanel Kiis", "assignee": null, "created": "2024-04-12T13:20:08.000+0000", "updated": "2025-10-23T05:44:34.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Tanel Kiis", "body": "QuantileSummaries: Best Time(ms) Avg Time(ms) Stdev(ms) Rate(M/s) Per Row(ns) Relative ------------------------------------------------------------------------------------------------------------------------ Only insert 168 171 8 6.0 167.6 1.0X Insert & merge 6690 6792 143 0.1 6690.3 0.0X KllFloatsSketch insert 44 47 6 22.5 44.4 3.8X KllFloatsSketch Insert & merge 55 57 6 18.3 54.7 3.1X   object QuantileSummariesBenchmark extends BenchmarkBase { def test(name: String, numValues: Int): Unit = { runBenchmark(name) { val values = (1 to numValues).map(_ => Random.nextDouble()) val benchmark = new Benchmark(name, numValues, output = output) benchmark.addCase(\"Only insert\") { _: Int => var summaries = new QuantileSummaries( compressThreshold = QuantileSummaries.defaultCompressThreshold, relativeError = QuantileSummaries.defaultRelativeError) for (value <- values) { summaries = summaries.insert(value) } summaries = summaries.compress() println(\"Median: \" + summaries.query(0.5)) } benchmark.addCase(\"Insert & merge\") { _: Int => // Insert values in batches of 1000 and merge the summaries. val summaries = values.grouped(1000).map(vs => { var partialSummaries = new QuantileSummaries( compressThreshold = QuantileSummaries.defaultCompressThreshold, relativeError = QuantileSummaries.defaultRelativeError) for (value <- vs) { partialSummaries = partialSummaries.insert(value) } partialSummaries.compress() }).reduce(_.merge(_)) println(\"Median: \" + summaries.query(0.5)) } benchmark.addCase(\"KllFloatsSketch insert\") { _: Int => // Insert values in batches of 1000 and merge the summaries. val summaries = KllDoublesSketch.newHeapInstance( KllSketch.getKFromEpsilon(QuantileSummaries.defaultRelativeError, true) ) for (value <- values) { summaries.update(value) } println(\"Median: \" + summaries.getQuantile(0.5)) } benchmark.addCase(\"KllFloatsSketch Insert & merge\") { _: Int => // Insert values in batches of 1000 and merge the summaries. val summaries = values.grouped(1000).map(vs => { val partialSummaries = KllDoublesSketch.newHeapInstance( KllSketch.getKFromEpsilon(QuantileSummaries.defaultRelativeError, true) ) for (value <- vs) { partialSummaries.update(value) } partialSummaries }).reduce((a, b) => { a.merge(b) a }) println(\"Median: \" + summaries.getQuantile(0.5)) } benchmark.run() } } override def runBenchmarkSuite(mainArgs: Array[String]): Unit = { test(\"QuantileSummaries\", 1_000_000) } }", "created": "2024-04-12T13:23:21.437+0000"}, {"author": "Tanel Kiis", "body": "I would be willing to make a PR, but I do not know the right solution. Migrating from a custom solution to the KllDoublesSketch would be the cleanest, but its results are nondeterministic", "created": "2024-04-12T13:24:59.856+0000"}, {"author": "lifulong", "body": "I've provided an implementation to solve this problem - please help review it.\u200b\u200b which use datasketches to replace gk summary -the sql function PERCENTILE_APPROX has been replaced in this pr, df.stat.approxQuantile is still use the gk now.- (new spark version df.stat.approxQuantile will use approx_percentile func)", "created": "2025-10-23T03:47:33.721+0000"}], "derived_tasks": {"summary": "Performance problem with QuantileSummaries - SPARK-29336 caused a severe performance regression", "classifications": ["improvement", "performance"], "qa_pairs": []}}
{"id": "SPARK-46485", "title": "V1Write should not add Sort when not needed", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Wenchen Fan", "assignee": "Wenchen Fan", "created": "2023-12-22T06:45:09.000+0000", "updated": "2025-10-22T13:06:25.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Wenchen Fan", "body": "Issue resolved by pull request 44458 [https://github.com/apache/spark/pull/44458]", "created": "2023-12-22T16:08:50.022+0000"}], "derived_tasks": {"summary": "V1Write should not add Sort when not needed", "classifications": ["feature", "bug"], "qa_pairs": []}}
{"id": "SPARK-45846", "title": "spark.sql.optimizeNullAwareAntiJoin should respect spark.sql.autoBroadcastJoinThreshold", "description": "Normally broadcast join can be disabled when users set {{spark.sql.autoBroadcastJoinThreshold}} to -1. However this doesn't apply to {{spark.sql.optimizeNullAwareAntiJoin}}:  case j @ ExtractSingleColumnNullAwareAntiJoin(leftKeys, rightKeys) => Seq(joins.BroadcastHashJoinExec(leftKeys, rightKeys, LeftAnti, BuildRight, None, planLater(j.left), planLater(j.right), isNullAwareAntiJoin = true))", "status": "Open", "priority": "Major", "reporter": "Chao Sun", "assignee": null, "created": "2023-11-09T00:02:03.000+0000", "updated": "2025-10-23T12:05:12.000+0000", "labels": [], "components": ["SQL"], "comments": [{"author": "subramaniam", "body": "Seems like a major issue when we are using not in condition. Will the spark obey broadcasting threshold here?", "created": "2025-10-23T08:41:53.902+0000"}], "derived_tasks": {"summary": "spark.sql.optimizeNullAwareAntiJoin should respect spark.sql.autoBroadcastJoinThreshold - Normally broadcast join can be disabled when users set {{...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-45617", "title": "Upgrade Apache Commons Crypto 1.2.0", "description": "Currently used 1.1.0 is more than 3 years ago (2020-08-28 released). We should upgrade the library to latest 1.2.0.", "status": "Closed", "priority": "Minor", "reporter": "L. C. Hsieh", "assignee": null, "created": "2023-10-20T20:39:31.000+0000", "updated": "2025-10-23T18:36:50.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [], "derived_tasks": {"summary": "Upgrade Apache Commons Crypto 1.2.0 - Currently used 1", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-42857", "title": "Unable to CreateDataFrame from Decimal128", "description": "To reproduce:  >>> from decimal import Decimal >>> spark.createDataFrame([Decimal(1.123)]) Traceback (most recent call last): ... pyarrow.lib.ArrowInvalid: Rescaling Decimal128 value would cause data loss >>>  The version of PyArrow is 10.0.1; the issue doesn't exist in vanilla PySpark with the same PyArrow version.", "status": "Resolved", "priority": "Major", "reporter": "Xinrong Meng", "assignee": "Ruifeng Zheng", "created": "2023-03-20T02:28:38.000+0000", "updated": "2025-10-20T11:02:55.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52662 [https://github.com/apache/spark/pull/52662]", "created": "2025-10-20T11:02:24.410+0000"}], "derived_tasks": {"summary": "Unable to CreateDataFrame from Decimal128 - To reproduce:  >>> from decimal import Decimal >>> spark", "classifications": ["story"], "qa_pairs": []}}
{"id": "SPARK-42360", "title": "Transform LeftOuter join with IsNull filter on right side to Anti join", "description": "Optimize left outer join to anti join if exists IsNull filter on the right side. For example, query  SELECT t1.* FROM t1 LEFT JOIN t2 ON t1.id = t2.id WHERE t2.id is null and t1.id = 1 and Rand() < 0.5  can be optimized to  SELECT t1.* FROM t1 LEFT ANTI JOIN t2 ON t1.id = t2.id WHERE t1.id = 1 and Rand() < 0.5", "status": "In Progress", "priority": "Minor", "reporter": "Wan Kun", "assignee": null, "created": "2023-02-06T14:39:33.000+0000", "updated": "2025-10-23T19:52:33.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Apache Spark", "body": "User 'wankunde' has created a pull request for this issue: https://github.com/apache/spark/pull/39908", "created": "2023-02-06T15:17:04.755+0000"}, {"author": "Eugene Koifman", "body": "https://github.com/apache/spark/pull/51762", "created": "2025-08-01T16:44:53.250+0000"}], "derived_tasks": {"summary": "Transform LeftOuter join with IsNull filter on right side to Anti join - Optimize left outer join to anti join if exists IsNull filter on the right...", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-33737", "title": "Use an Informer+Lister API in the ExecutorPodWatcher", "description": "Kubernetes backend uses Fabric8 client and a [watch|https://github.com/apache/spark/blob/master/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsWatchSnapshotSource.scala#L42] to monitor the K8s Api server for pod changes. Every watcher keeps a websocket connection open and has no caching mechanism at that part. Caching at the Spark K8s resource manager exists in other areas where we are hitting the Api Server for Pod CRUD ops like [here|https://github.com/apache/spark/blob/b8ccd755244d3cd8a81a9f4a1eafa2a4e48759d2/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsLifecycleManager.scala#L49]. In an env where a lot of connections are kept due to large scale jobs this could be problematic and impose a lot of load against the API server. A lot of long running jobs should not create pod changes eg. Streaming jobs to justify a continuous watching mechanism. Latest Frabric8 client versions have implemented a SharedInformer API+Lister, an example can be found [here|https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-examples/src/main/java/io/fabric8/kubernetes/examples/InformerExample.java#L37]. This new API follows the implementation of the official java K8s client and the go counterpart and it is backed up by a caching mechanism which is re-synced after a configurable period to avoid hitting the API server all the time. There is also a lister that keeps track of current status of resources. Using such a mechanism is common place when implementing a K8s controller. The suggestion is to update to v4.13.0 the client (has all updates in wrt that API) and use the informer+lister API where applicable. I think the lister could also replace part of the snapshotting/notification mechanism. /cc [~dongjoon] [~eje] [~holden] WDYTH?", "status": "Open", "priority": "Major", "reporter": "Stavros Kontopoulos", "assignee": null, "created": "2020-12-10T11:38:18.000+0000", "updated": "2025-10-21T12:24:00.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Although it sounds like a big change, yes, we may consider it for Apache Spark 3.2.", "created": "2020-12-10T19:01:28.561+0000"}, {"author": "Stavros Kontopoulos", "body": "In addition current implementation has been out for long and it is stable. Need to be sure that any updates will not cause any issues. I can work on a PR and see how things integrate.", "created": "2020-12-11T08:55:19.512+0000"}], "derived_tasks": {"summary": "Use an Informer+Lister API in the ExecutorPodWatcher - Kubernetes backend uses Fabric8 client and a [watch|https://github", "classifications": ["improvement"], "qa_pairs": [{"question": "/cc [~dongjoon] [~eje] [~holden] WDYTH?", "answer": "Although it sounds like a big change, yes, we may consider it for Apache Spark 3.2."}]}}
{"id": "SPARK-28098", "title": "Native ORC reader doesn't support subdirectories with Hive tables", "description": "The Hive ORC reader supports recursive directory reads from S3.\u00a0 Spark's native ORC reader supports recursive directory reads, but not when used with Hive.  val testData = List(1,2,3,4,5) val dataFrame = testData.toDF() dataFrame .coalesce(1) .write .mode(SaveMode.Overwrite) .format(\"orc\") .option(\"compression\", \"zlib\") .save(\"s3://ddrinka.sparkbug/dirTest/dir1/dir2/\") spark.sql(\"DROP TABLE IF EXISTS ddrinka_sparkbug.dirTest\") spark.sql(\"CREATE EXTERNAL TABLE ddrinka_sparkbug.dirTest (val INT) STORED AS ORC LOCATION 's3://ddrinka.sparkbug/dirTest/'\") spark.conf.set(\"hive.mapred.supports.subdirectories\",\"true\") spark.conf.set(\"mapred.input.dir.recursive\",\"true\") spark.conf.set(\"mapreduce.input.fileinputformat.input.dir.recursive\",\"true\") spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"true\") println(spark.sql(\"SELECT * FROM ddrinka_sparkbug.dirTest\").count) //0 spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\") println(spark.sql(\"SELECT * FROM ddrinka_sparkbug.dirTest\").count) //5", "status": "In Progress", "priority": "Major", "reporter": "Douglas Drinka", "assignee": null, "created": "2019-06-18T20:13:13.000+0000", "updated": "2025-10-25T09:27:53.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Hyukjin Kwon", "body": "[~ddrinka], do you mind if I ask to check similar stuff as said in https://issues.apache.org/jira/browse/SPARK-28099?focusedCommentId=16868249&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16868249?", "created": "2019-06-20T04:18:07.801+0000"}, {"author": "lithiumlee-_-", "body": "Also not support subdirectories which generated by tez union all.", "created": "2020-05-19T16:06:57.194+0000"}, {"author": "Apache Spark", "body": "User 'FatalLin' has created a pull request for this issue: https://github.com/apache/spark/pull/32202", "created": "2021-04-16T05:56:35.081+0000"}, {"author": "Apache Spark", "body": "User 'FatalLin' has created a pull request for this issue: https://github.com/apache/spark/pull/32202", "created": "2021-04-16T05:57:25.672+0000"}, {"author": "Yu-Tang Lin", "body": "Hi [~ddrinka], I already made a PR to resolve the issue, maybe we could take a look on it?", "created": "2021-04-22T13:31:44.555+0000"}, {"author": "Apache Spark", "body": "User 'chong0929' has created a pull request for this issue: https://github.com/apache/spark/pull/32679", "created": "2021-05-26T14:49:41.879+0000"}, {"author": "Zhen Wang", "body": "> set spark.sql.hive.convertMetastoreParquet=false With this parameter, it works fine, but I don't think it's a good way.", "created": "2022-03-24T02:56:52.915+0000"}], "derived_tasks": {"summary": "Native ORC reader doesn't support subdirectories with Hive tables - The Hive ORC reader supports recursive directory reads from S3", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53981", "title": "Upgrade Netty to 4.2.7.Final", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Kent Yao", "assignee": "Kent Yao", "created": "2025-10-22T09:52:49.000+0000", "updated": "2025-10-24T05:19:54.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Kent Yao", "body": "Issue resolved by pull request 52695 [https://github.com/apache/spark/pull/52695]", "created": "2025-10-23T05:51:21.356+0000"}], "derived_tasks": {"summary": "Upgrade Netty to 4.2.7.Final", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53980", "title": "Add `SparkConf.getAllWithPrefix(String, String => K)` API", "description": "We need to set some config related to S3 for our inner Spark. The implementation of the function show below.  private def setS3Configs(conf: SparkConf): Unit = { val S3A_PREFIX = \"spark.fs.s3a\" val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\" val s3aConf = conf.getAllWithPrefix(S3A_PREFIX) s3aConf .foreach( confPair => { val keyWithoutPrefix = confPair._1 val oldKey = S3A_PREFIX + keyWithoutPrefix val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix val value = confPair._2 (newKey, value) }) }  These code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.", "status": "Resolved", "priority": "Major", "reporter": "Jiaan Geng", "assignee": "Jiaan Geng", "created": "2025-10-22T07:36:01.000+0000", "updated": "2025-10-24T21:22:08.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52693 [https://github.com/apache/spark/pull/52693]", "created": "2025-10-24T15:00:08.385+0000"}], "derived_tasks": {"summary": "Add `SparkConf.getAllWithPrefix(String, String => K)` API - We need to set some config related to S3 for our inner Spark", "classifications": ["feature", "bug", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53979", "title": "Drop temporary functions in Pandas UDF tests", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-10-22T03:38:12.000+0000", "updated": "2025-10-22T07:00:46.000+0000", "labels": ["pull-request-available"], "components": ["PySpark", "Tests"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52690 [https://github.com/apache/spark/pull/52690]", "created": "2025-10-22T07:00:08.867+0000"}], "derived_tasks": {"summary": "Drop temporary functions in Pandas UDF tests", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53978", "title": "Support logging in driver-side workers", "description": "", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "created": "2025-10-21T23:41:58.000+0000", "updated": "2025-10-21T23:41:58.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "Support logging in driver-side workers", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53977", "title": "Support logging in Pandas/Arrow UDTFs", "description": "", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "created": "2025-10-21T23:41:32.000+0000", "updated": "2025-10-21T23:41:32.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "Support logging in Pandas/Arrow UDTFs", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53976", "title": "Support logging in Pandas/Arrow UDFs", "description": "", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "created": "2025-10-21T23:40:03.000+0000", "updated": "2025-10-21T23:40:03.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "Support logging in Pandas/Arrow UDFs", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53975", "title": "Add basic logging support", "description": "", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "created": "2025-10-21T23:35:01.000+0000", "updated": "2025-10-24T18:47:47.000+0000", "labels": ["pull-request-available"], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "Add basic logging support", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53974", "title": "Bump Jackson 2.20.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-21T18:03:00.000+0000", "updated": "2025-10-22T09:46:42.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Kousuke Saruta", "body": "Issue resolved in https://github.com/apache/spark/pull/52687", "created": "2025-10-22T09:46:42.641+0000"}], "derived_tasks": {"summary": "Bump Jackson 2.20.0", "classifications": ["dependency upgrade"], "qa_pairs": []}}
{"id": "SPARK-53973", "title": "Classify errors for AvroOptions boolean casting failure", "description": "IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to be not boolean. Should classify the error.", "status": "Resolved", "priority": "Major", "reporter": "Siying Dong", "assignee": "Siying Dong", "created": "2025-10-21T17:00:06.000+0000", "updated": "2025-10-22T01:08:10.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Anish Shrigondekar", "body": "Issue resolved by pull request 52686 [https://github.com/apache/spark/pull/52686]", "created": "2025-10-22T01:08:10.359+0000"}], "derived_tasks": {"summary": "Classify errors for AvroOptions boolean casting failure - IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to ...", "classifications": ["task", "bug"], "qa_pairs": []}}
{"id": "SPARK-53972", "title": "Streaming Query RecentProgress performance regression in Classic Pyspark", "description": "We have identified a significant performance regression in Apache Spark's streaming recentProgress method in python notebook starting from version 4.0.0. The time required to fetch recentProgress increases substantially as the number of progress records grows, creating a linear or worse scaling issue. We only observe this behavior in classic pyspark. With the following code, it output charts for time it takes to get recentProgress before and after changes in [this commit|https://github.com/apache/spark/commit/22eb6c4b0a82b9fcf84fc9952b1f6c41dde9bd8d#diff-4d4ed29d139877b160de444add7ee63cfa7a7577d849ab2686f1aa2d5b4aae64] ``` %python from datetime import datetime import time df = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load() q = df.writeStream.format(\"noop\").start() print(\"begin waiting for progress\") progress_list = [] time_diff_list = [] numProgress = len(q.recentProgress) while numProgress < 70 and q.exception() is None: time.sleep(1) beforeTime = datetime.now() print(beforeTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": before we got those progress: \"+str(numProgress)) rep = q.recentProgress numProgress = len(rep) afterTime = datetime.now() print(afterTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": after we got those progress: \"+str(numProgress)) time_diff = (afterTime - beforeTime).total_seconds() print(\"Total Time: \"+str(time_diff) +\" seconds\") progress_list.append(numProgress) time_diff_list.append(time_diff) q.stop() q.awaitTermination() assert(q.exception() is None) import pandas as pd plot_df = pd.DataFrame(\\{'numProgress': progress_list, 'time_diff': time_diff_list}) display(spark.createDataFrame(plot_df).orderBy(\"numProgress\").toPandas().plot.line(x=\"numProgress\", y=\"time_diff\")) ``` See attachment for the generated graph. Attachment 1 is regression shown in current version. Attachment 2 is regression shown in previous version", "status": "Resolved", "priority": "Major", "reporter": "Zifei Feng", "assignee": "Zifei Feng", "created": "2025-10-21T16:27:41.000+0000", "updated": "2025-10-23T20:12:09.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "Anish Shrigondekar", "body": "Issue resolved by pull request 52688 [https://github.com/apache/spark/pull/52688]", "created": "2025-10-22T19:01:07.861+0000"}], "derived_tasks": {"summary": "Streaming Query RecentProgress performance regression in Classic Pyspark - We have identified a significant performance regression in Apache Spark'...", "classifications": ["bug", "performance"], "qa_pairs": []}}
{"id": "SPARK-53971", "title": "Bump zstd-jni 1.5.7-6", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-21T14:42:06.000+0000", "updated": "2025-10-21T17:36:49.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52684 [https://github.com/apache/spark/pull/52684]", "created": "2025-10-21T17:36:07.823+0000"}], "derived_tasks": {"summary": "Bump zstd-jni 1.5.7-6", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53970", "title": "fixing a doc of from_protobuf function pyspark.", "description": "fixing a doc of from_protobuf function pyspark.", "status": "Open", "priority": "Minor", "reporter": "mansoor", "assignee": null, "created": "2025-10-21T13:00:13.000+0000", "updated": "2025-10-21T13:00:13.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "fixing a doc of from_protobuf function pyspark. - fixing a doc of from_protobuf function pyspark", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53969", "title": "Drop temporary functions in Arrow UDF tests", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-10-21T12:11:33.000+0000", "updated": "2025-10-21T17:38:54.000+0000", "labels": ["pull-request-available"], "components": ["PySpark", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52682 [https://github.com/apache/spark/pull/52682]", "created": "2025-10-21T17:38:38.512+0000"}], "derived_tasks": {"summary": "Drop temporary functions in Arrow UDF tests", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53968", "title": "Store decimal precision loss conf in arithmetic expressions", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Stefan Kandic", "assignee": "Stefan Kandic", "created": "2025-10-21T12:08:25.000+0000", "updated": "2025-10-22T13:07:54.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Wenchen Fan", "body": "Issue resolved by pull request 52681 [https://github.com/apache/spark/pull/52681]", "created": "2025-10-22T13:07:54.077+0000"}], "derived_tasks": {"summary": "Store decimal precision loss conf in arithmetic expressions", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53967", "title": "Avoid intermediate pandas dataframe creation in df.toPandas", "description": "", "status": "Open", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": null, "created": "2025-10-21T10:38:00.000+0000", "updated": "2025-10-23T02:15:37.000+0000", "labels": ["pull-request-available"], "components": ["PySpark"], "comments": [], "derived_tasks": {"summary": "Avoid intermediate pandas dataframe creation in df.toPandas", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53966", "title": "Add utility functions to detect JVM GCs", "description": "Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ZPage for java long array, so the really bytes allocated is pageSize+16. So we need consider the object header when allocating spark pages.", "status": "Resolved", "priority": "Major", "reporter": "Wan Kun", "assignee": "Wan Kun", "created": "2025-10-21T08:04:15.000+0000", "updated": "2025-10-24T02:19:05.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52678 [https://github.com/apache/spark/pull/52678]", "created": "2025-10-23T16:13:43.370+0000"}], "derived_tasks": {"summary": "Add utility functions to detect JVM GCs - Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ...", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53965", "title": "Upgrade buf plugins to v29.5", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-10-21T06:44:25.000+0000", "updated": "2025-10-22T02:52:55.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark", "Structured Streaming"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52677 [https://github.com/apache/spark/pull/52677]", "created": "2025-10-21T15:26:34.692+0000"}], "derived_tasks": {"summary": "Upgrade buf plugins to v29.5", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53964", "title": "Simplify Java Home finding for SBT unidoc", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-21T04:22:48.000+0000", "updated": "2025-10-21T15:31:49.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52676 [https://github.com/apache/spark/pull/52676]", "created": "2025-10-21T15:30:39.319+0000"}], "derived_tasks": {"summary": "Simplify Java Home finding for SBT unidoc", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53963", "title": "Drop temporary functions in regular UDF tests", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-10-21T03:15:45.000+0000", "updated": "2025-10-21T15:33:28.000+0000", "labels": ["pull-request-available"], "components": ["PySpark", "Tests"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52674 [https://github.com/apache/spark/pull/52674]", "created": "2025-10-21T08:18:34.754+0000"}], "derived_tasks": {"summary": "Drop temporary functions in regular UDF tests", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53962", "title": "Upgrade ASM to 9.9", "description": "", "status": "In Progress", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-20T21:29:00.000+0000", "updated": "2025-10-20T21:39:37.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [], "derived_tasks": {"summary": "Upgrade ASM to 9.9", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53961", "title": "Fix `FileStreamSinkSuite` flakiness by using `walkFileTree` instead of `walk`", "description": "", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-20T21:04:29.000+0000", "updated": "2025-10-21T03:36:02.000+0000", "labels": ["pull-request-available"], "components": ["SQL", "Tests"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52671 [https://github.com/apache/spark/pull/52671]", "created": "2025-10-21T03:36:02.803+0000"}], "derived_tasks": {"summary": "Fix `FileStreamSinkSuite` flakiness by using `walkFileTree` instead of `walk`", "classifications": ["bug", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53960", "title": "Let approx_top_k_accumulate/combine/estimate handle NULLs", "description": "As a follow-up of https://issues.apache.org/jira/browse/SPARK-53947, let approx_top_k_accumulate/combine/estimate handle NULLs.", "status": "Resolved", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": "Yuchuan Huang", "created": "2025-10-20T20:57:49.000+0000", "updated": "2025-10-21T15:49:44.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "Issue resolved by pull request 52673 [https://github.com/apache/spark/pull/52673]", "created": "2025-10-21T15:49:44.281+0000"}], "derived_tasks": {"summary": "Let approx_top_k_accumulate/combine/estimate handle NULLs - As a follow-up of https://issues", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53959", "title": "Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe", "description": "Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data. Generally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via {quote}spark.createDataFrame([]).show() {quote} or {quote}df = pd.DataFrame() spark.createDataFrame(df).show(){quote} or {quote}df = pd.DataFrame(\\{\"a\": []}) spark.createDataFrame(df).show(){quote} This does not happen when pandas dataframe has an index but no data, e.g. {quote}df = pd.DataFrame(index=range(5)) spark.createDataFrame(df).show(){quote} What happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state and the error is not actionable enough for the user. This should be fixed.", "status": "Open", "priority": "Major", "reporter": "Alex Khakhlyuk", "assignee": null, "created": "2025-10-20T19:52:31.000+0000", "updated": "2025-10-21T08:05:25.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [], "derived_tasks": {"summary": "Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe - Spark Connect Python client do...", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53958", "title": "Simplify Jackson deps management by using BOM", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-20T14:19:05.000+0000", "updated": "2025-10-21T15:32:36.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52668 [https://github.com/apache/spark/pull/52668]", "created": "2025-10-21T15:28:43.170+0000"}], "derived_tasks": {"summary": "Simplify Jackson deps management by using BOM", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53957", "title": "Support Geography and Geometry in SRS mappings", "description": "", "status": "Open", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": null, "created": "2025-10-20T12:48:29.000+0000", "updated": "2025-10-24T23:31:00.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Miko\u0142aj", "body": "And what about this, [~uros-db]? I changed my mind - if you have something that I could take, you can ping me ;)", "created": "2025-10-20T13:08:16.315+0000"}, {"author": "Uro\u0161 Bojani\u0107", "body": "Sorry, also in progress: https://github.com/apache/spark/pull/52667.", "created": "2025-10-20T13:10:40.634+0000"}, {"author": "Uro\u0161 Bojani\u0107", "body": "But don't worry - there will be others soon, I can ping you then [~dekrate]!", "created": "2025-10-20T13:12:00.064+0000"}], "derived_tasks": {"summary": "Support Geography and Geometry in SRS mappings", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53956", "title": "Support TIME in the try_make_timestamp function in Python", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-10-20T11:47:17.000+0000", "updated": "2025-10-23T00:19:46.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Miko\u0142aj", "body": "Hi, can I take this? But as far as I can see, there is a few try_make_timestamp functions. Which one should I modify?", "created": "2025-10-20T12:08:33.154+0000"}, {"author": "Uro\u0161 Bojani\u0107", "body": "Hi [~dekrate], thank you for your help! I already have this one in progress: [https://github.com/apache/spark/pull/52666.] But I can ping you when another task comes up - does that sound good to you?", "created": "2025-10-20T12:12:09.657+0000"}, {"author": "Miko\u0142aj", "body": "Ahh, I did not notice that :D I will try to find another tasks, thanks :)", "created": "2025-10-20T12:13:41.085+0000"}, {"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52666 [https://github.com/apache/spark/pull/52666]", "created": "2025-10-23T00:19:32.694+0000"}], "derived_tasks": {"summary": "Support TIME in the try_make_timestamp function in Python", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53955", "title": "Prefer to detect Java Home from env JAVA_HOME on finding jmap", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-10-20T10:01:06.000+0000", "updated": "2025-10-24T02:15:21.000+0000", "labels": ["pull-request-available"], "components": ["Spark Core"], "comments": [], "derived_tasks": {"summary": "Prefer to detect Java Home from env JAVA_HOME on finding jmap", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53954", "title": "Bump Avro 1.12.1", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-20T09:15:45.000+0000", "updated": "2025-10-21T20:24:03.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "This is resolved via [https://github.com/apache/spark/pull/52664]", "created": "2025-10-21T20:24:03.109+0000"}], "derived_tasks": {"summary": "Bump Avro 1.12.1", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53953", "title": "Bump Avro 1.11.5", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-20T09:11:06.000+0000", "updated": "2025-10-20T17:37:31.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52663 [https://github.com/apache/spark/pull/52663]", "created": "2025-10-20T17:37:17.888+0000"}], "derived_tasks": {"summary": "Bump Avro 1.11.5", "classifications": ["dependency upgrade"], "qa_pairs": []}}
{"id": "SPARK-53952", "title": "Balance GHA CI jobs", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-10-20T07:22:42.000+0000", "updated": "2025-10-21T20:24:51.000+0000", "labels": ["pull-request-available"], "components": ["Project Infra"], "comments": [], "derived_tasks": {"summary": "Balance GHA CI jobs", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53951", "title": "Upgrade `protobuf-java` to 4.33.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-10-20T06:11:00.000+0000", "updated": "2025-10-21T06:37:40.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52660 [https://github.com/apache/spark/pull/52660]", "created": "2025-10-20T23:20:04.547+0000"}], "derived_tasks": {"summary": "Upgrade `protobuf-java` to 4.33.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53950", "title": "Upgrade scala-xml to 2.4.0", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-10-20T06:06:31.000+0000", "updated": "2025-10-21T06:38:18.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Kousuke Saruta", "body": "Issue resolved in https://github.com/apache/spark/pull/52659", "created": "2025-10-20T12:04:59.951+0000"}], "derived_tasks": {"summary": "Upgrade scala-xml to 2.4.0", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53949", "title": "Use `Utils. getRootCause` instead of `Throwables.getRootCause`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Yang Jie", "assignee": "Yang Jie", "created": "2025-10-20T06:00:08.000+0000", "updated": "2025-10-21T06:38:04.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [{"author": "Yang Jie", "body": "Issue resolved by pull request 52658 [https://github.com/apache/spark/pull/52658]", "created": "2025-10-20T07:37:21.466+0000"}], "derived_tasks": {"summary": "Use `Utils. getRootCause` instead of `Throwables.getRootCause`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53948", "title": "Fix deadlock in Observation", "description": "Observation class has been evolved a few times during Spark 3.5 to Spark 4.0.0. Previously it uses locking mechanism (synchronized) between get and onFinish methods to coordinate metrics update and retrieval. But it has a potential deadlocking bug. If get is called before ObservationListener is triggered to call onFinish, get will forever be waiting for metrics because it locks the observation object by synchronized so later onFinish call is locked out from updating the metrics. This locking mechanism was replaced by a promise by SPARK-49423 which is a large refactoring on the observation feature. But in the PR, I don\u2019t see the deadlock bug was mentioned, and there is no bug fix PR proposed to earlier versions. So I think that the bug was not known and the fix is unintentional in Spark 4.0.0. The bug is still in Spark 3.5 branch.", "status": "Resolved", "priority": "Major", "reporter": "L. C. Hsieh", "assignee": "L. C. Hsieh", "created": "2025-10-19T23:22:23.000+0000", "updated": "2025-10-20T16:57:25.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "Issue resolved by pull request 52657 [https://github.com/apache/spark/pull/52657]", "created": "2025-10-20T16:45:17.721+0000"}], "derived_tasks": {"summary": "Fix deadlock in Observation - Observation class has been evolved a few times during Spark 3", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53947", "title": "Let approx_top_k handle NULLs", "description": "Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL values by itself ([https://github.com/apache/datasketches-java/blob/main/src/main/java/org/apache/datasketches/frequencies/FrequentItemsSketch.java#L587).] However, NULL value could be meaningful in some use cases and users might want to include NULL in the approx_top_k output. Therefore, this ticket aims to add a nullCounter associated with the FrequentItemsSketch to count for NULL in the approx_top_k aggregation.", "status": "Resolved", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": "Yuchuan Huang", "created": "2025-10-18T21:55:43.000+0000", "updated": "2025-10-20T22:42:49.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Gengliang Wang", "body": "Issue resolved by pull request 52655 [https://github.com/apache/spark/pull/52655]", "created": "2025-10-20T22:42:49.521+0000"}], "derived_tasks": {"summary": "Let approx_top_k handle NULLs - Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL va...", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53946", "title": "Upgrade SBT to 1.11.7", "description": "We last upgraded SBT two years ago. Let's upgrade SBT to the latest version.", "status": "Resolved", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "created": "2025-10-17T23:44:12.000+0000", "updated": "2025-10-21T23:41:46.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52653 [https://github.com/apache/spark/pull/52653]", "created": "2025-10-20T21:53:32.625+0000"}], "derived_tasks": {"summary": "Upgrade SBT to 1.11.7 - We last upgraded SBT two years ago", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53945", "title": "Upgrade semanticdb-shared to 4.13.10", "description": "Ammonite was upgraded to 3.0.3 so semanticdb-shared should be upgraded correspondingly. https://mvnrepository.com/artifact/com.lihaoyi/ammonite-interp-3.3.6_3.3.6/3.0.3", "status": "Resolved", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "created": "2025-10-17T23:20:33.000+0000", "updated": "2025-10-20T06:49:07.000+0000", "labels": ["pull-request-available"], "components": ["Build"], "comments": [{"author": "Kousuke Saruta", "body": "Issue resolved in https://github.com/apache/spark/pull/52652", "created": "2025-10-20T06:49:07.315+0000"}], "derived_tasks": {"summary": "Upgrade semanticdb-shared to 4.13.10 - Ammonite was upgraded to 3", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53944", "title": "Support `spark.kubernetes.executor.useDriverPodIP`", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-17T18:50:11.000+0000", "updated": "2025-10-24T15:18:36.000+0000", "labels": ["pull-request-available"], "components": ["Kubernetes"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52650 [https://github.com/apache/spark/pull/52650]", "created": "2025-10-18T15:10:53.793+0000"}], "derived_tasks": {"summary": "Support `spark.kubernetes.executor.useDriverPodIP`", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53942", "title": "Support changing stateless shuffle partitions upon restart of streaming query", "description": "We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this. The main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.", "status": "Resolved", "priority": "Major", "reporter": "Jungtaek Lim", "assignee": "Jungtaek Lim", "created": "2025-10-17T07:00:53.000+0000", "updated": "2025-10-25T08:57:53.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "Jungtaek Lim", "body": "Issue resolved by pull request 52645 [https://github.com/apache/spark/pull/52645]", "created": "2025-10-25T08:57:53.988+0000"}], "derived_tasks": {"summary": "Support changing stateless shuffle partitions upon restart of streaming query - We have been having a huge restriction on the number of shuffle par...", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "SPARK-53941", "title": "Support AQE in stateless streaming workloads", "description": "We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively. We have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads.", "status": "Resolved", "priority": "Major", "reporter": "Jungtaek Lim", "assignee": "Jungtaek Lim", "created": "2025-10-17T06:50:11.000+0000", "updated": "2025-10-25T04:12:29.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [{"author": "Jungtaek Lim", "body": "PR is up for review. Will rename the PR.", "created": "2025-10-17T06:50:43.304+0000"}, {"author": "Jungtaek Lim", "body": "Issue resolved by pull request 52642 [https://github.com/apache/spark/pull/52642]", "created": "2025-10-25T04:12:29.114+0000"}], "derived_tasks": {"summary": "Support AQE in stateless streaming workloads - We have been disabling AQE for streaming workloads since stateful operators do not work with AQE", "classifications": ["new feature"], "qa_pairs": []}}
{"id": "SPARK-53940", "title": "Function version() should return full spark version instead of short version", "description": "", "status": "Open", "priority": "Major", "reporter": "Cheng Pan", "assignee": null, "created": "2025-10-17T03:37:37.000+0000", "updated": "2025-10-21T15:58:47.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [], "derived_tasks": {"summary": "Function version() should return full spark version instead of short version", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53938", "title": "Fix decimal rescaling in LocalDataToArrowConversion", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-10-16T13:14:27.000+0000", "updated": "2025-10-20T06:37:13.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [{"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52637 [https://github.com/apache/spark/pull/52637]", "created": "2025-10-20T06:36:53.535+0000"}], "derived_tasks": {"summary": "Fix decimal rescaling in LocalDataToArrowConversion", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53934", "title": "Initial implement Connect JDBC driver", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-16T05:53:06.000+0000", "updated": "2025-10-24T02:31:11.000+0000", "labels": ["pull-request-available"], "components": ["Connect"], "comments": [{"author": "Dongjoon Hyun", "body": "Issue resolved by pull request 52705 [https://github.com/apache/spark/pull/52705]", "created": "2025-10-23T19:41:31.600+0000"}], "derived_tasks": {"summary": "Initial implement Connect JDBC driver", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
{"id": "SPARK-53930", "title": "Support TIME in the make_timestamp function in Python", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-10-16T00:09:18.000+0000", "updated": "2025-10-23T00:23:35.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Uro\u0161 Bojani\u0107", "body": "Work in progress: https://github.com/apache/spark/pull/52648.", "created": "2025-10-17T12:36:35.505+0000"}, {"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52648 [https://github.com/apache/spark/pull/52648]", "created": "2025-10-23T00:23:20.138+0000"}], "derived_tasks": {"summary": "Support TIME in the make_timestamp function in Python", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53927", "title": "Kinesis tests are broken", "description": "Running Kinesis test with\u00a0{{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:  ENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl ... Using endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests. [info] WithoutAggregationKinesisBackedBlockRDDSuite: [info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds) [info] java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy [info] at com.amazonaws.services.kinesis.AmazonKinesisClient.<clinit>(AmazonKinesisClient.java:86) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85) [info] at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45) [info] at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41) [info] at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39) [info] at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26) [info] at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) [info] Cause: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.PropertyNamingStrategy$PascalCaseStrategy [info] at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) [info] at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) [info] at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) [info] at com.amazonaws.services.kinesis.AmazonKinesisClient.<clinit>(AmazonKinesisClient.java:86) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182) [info] at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85) [info] at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45) [info] at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41) [info] at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39) [info] at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26) [info] at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43) [info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212) [info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) [info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) [info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68) [info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321) [info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517) [info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414) [info] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) [info] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [info] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [info] at java.base/java.lang.Thread.run(Thread.java:840) [error] Uncaught exception when running org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite: java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy", "status": "Resolved", "priority": "Major", "reporter": "Vlad Rozov", "assignee": "Vlad Rozov", "created": "2025-10-15T21:04:05.000+0000", "updated": "2025-10-24T15:50:02.000+0000", "labels": ["pull-request-available"], "components": ["Tests"], "comments": [{"author": "Vlad Rozov", "body": "Using maven leads to the same error.", "created": "2025-10-15T21:05:08.100+0000"}, {"author": "Kousuke Saruta", "body": "Issue resolved in https://github.com/apache/spark/pull/52630", "created": "2025-10-17T05:50:26.405+0000"}], "derived_tasks": {"summary": "Kinesis tests are broken - Running Kinesis test with\u00a0{{ENABLE_KINESIS_TEST=1}} fails with {{java", "classifications": ["bug"], "qa_pairs": []}}
{"id": "SPARK-53922", "title": "Introduce Geography and Geometry physical types", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-10-15T17:17:28.000+0000", "updated": "2025-10-23T08:00:08.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Uro\u0161 Bojani\u0107", "body": "Work in progress: https://github.com/apache/spark/pull/52629.", "created": "2025-10-15T19:51:30.535+0000"}, {"author": "Wenchen Fan", "body": "Issue resolved by pull request 52629 [https://github.com/apache/spark/pull/52629]", "created": "2025-10-23T08:00:08.301+0000"}], "derived_tasks": {"summary": "Introduce Geography and Geometry physical types", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53921", "title": "Introduce Geography and Geometry data types to PySpark API", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-10-15T17:16:54.000+0000", "updated": "2025-10-22T00:15:03.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Uro\u0161 Bojani\u0107", "body": "Work in progress: https://github.com/apache/spark/pull/52627.", "created": "2025-10-15T19:51:36.707+0000"}, {"author": "Ruifeng Zheng", "body": "Issue resolved by pull request 52627 [https://github.com/apache/spark/pull/52627]", "created": "2025-10-22T00:14:49.731+0000"}], "derived_tasks": {"summary": "Introduce Geography and Geometry data types to PySpark API", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53920", "title": "Introduce Geography and Geometry data types to Java API", "description": "", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-10-15T17:16:46.000+0000", "updated": "2025-10-23T18:06:49.000+0000", "labels": ["pull-request-available"], "components": ["SQL"], "comments": [{"author": "Uro\u0161 Bojani\u0107", "body": "Work in progress: https://github.com/apache/spark/pull/52623.", "created": "2025-10-15T19:51:50.323+0000"}, {"author": "Wenchen Fan", "body": "Issue resolved by pull request 52623 [https://github.com/apache/spark/pull/52623]", "created": "2025-10-23T18:06:49.774+0000"}], "derived_tasks": {"summary": "Introduce Geography and Geometry data types to Java API", "classifications": ["sub-task"], "qa_pairs": []}}
{"id": "SPARK-53917", "title": "[CONNECT] Supporting large LocalRelations", "description": "h1. Problem description LocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc. In Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages. CachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can\u2019t execute queries with local client data, pandas dataframes, csv files of over 2GB. h1. Design In Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message. Small local relations (under 64MB) are sent directly as part of the ExecutePlanRequest. !image-2025-10-15-13-50-04-179.png! Larger local relations are first sent to the server via addArtifact and stored in memory or on disk via BlockManager. Then an ExecutePlanRequest is sent containing CachedLocalRelation\\{hash}, where hash is the artifact hash. The server retrieves the cached LocalRelation from the BlockManager via the hash, deserializes it, adds it to the LogicalPlan and then executes it. !image-2025-10-15-13-50-44-333.png! The server reads the data from the BlockManager as a stream and tries to create proto.LocalRelation via {quote}proto.Relation .newBuilder() .getLocalRelation .getParserForType .parseFrom(blockData.toInputStream()) {quote} This fails, because java protobuf library has a 2GB limit on deserializing protobuf messages from a string. {quote}org.sparkproject.connect.com.google.protobuf.InvalidProtocolBufferException) CodedInputStream encountered an embedded string or message which claimed to have negative size. {quote} !image-2025-10-15-13-53-40-306.png! To fix this, I propose avoiding the protobuf layer during the serialization on the client and deserialization on the server. Instead of caching the full protobuf LocalRelation message, we cache the data and schema as separate artifacts, send two hashes \\{data_hash, schema_hash} to the server, load them both from BlockManager directly and create a LocalRelation on the server based on the unpacked data and schema. !image-2025-10-15-13-56-46-840.png! After creating a prototype with the new proto message, I discovered that there are additional limits for CachedLocalRelations. Both the Scala Client and the Server store the data in a single Java\u00a0{{{}Array[Byte]{}}}, which has a 2GB size limit in Java. To avoid this limit, I propose transferring data in chunks. The Python and Scala clients will split data into multiple Arrow batches and upload them separately to the server. Each batch will be uploaded and stored a separate artifact. The Server will then load and process each batch separately. We will keep batch sizes around 16MB (TBD), well below the 2GB limit. This way we will avoid 2GB limits on both clients and on the server. !image-2025-10-15-13-59-08-081.png! The final proto message looks like this: {quote}message ChunkedCachedLocalRelation { // (Required) A list of sha-256 hashes for representing LocalRelation.data. repeated string dataHashes = 1; // (Optional) A sha-256 hash of the serialized LocalRelation.schema. optional string schemaHash = 2; } {quote} Implementation details are discussed in the PR [https://github.com/apache/spark/pull/52613].", "status": "Resolved", "priority": "Major", "reporter": "Alex Khakhlyuk", "assignee": "Alex Khakhlyuk", "created": "2025-10-15T11:48:05.000+0000", "updated": "2025-10-22T13:49:49.000+0000", "labels": ["pull-request-available"], "components": ["Connect", "PySpark"], "comments": [{"author": "Herman van H\u00f6vell", "body": "Issue resolved by pull request 52613 [https://github.com/apache/spark/pull/52613]", "created": "2025-10-22T13:49:49.624+0000"}], "derived_tasks": {"summary": "[CONNECT] Supporting large LocalRelations", "classifications": ["improvement"], "qa_pairs": []}}
{"id": "SPARK-53915", "title": "Add RealTimeScanExec and ability to execute long running batches", "description": "", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "created": "2025-10-15T05:59:45.000+0000", "updated": "2025-10-24T23:06:32.000+0000", "labels": ["pull-request-available"], "components": ["Structured Streaming"], "comments": [], "derived_tasks": {"summary": "Add RealTimeScanExec and ability to execute long running batches", "classifications": ["feature", "sub-task"], "qa_pairs": []}}
